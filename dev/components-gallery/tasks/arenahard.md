---
hide:
  - navigation
---
# ArenaHard

This `Task` is based on the "From Live Data to High-Quality Benchmarks: The



Arena-Hard Pipeline" paper that presents Arena Hard, which is a benchmark for
    instruction-tuned LLMs that contains 500 challenging user queries. GPT-4 is used
    as the judge to compare the model responses against a baseline model, which defaults
    to `gpt-4-0314`.



### Note
Arena-Hard-Auto has the highest correlation and separability to Chatbot Arena
among popular open-ended LLM benchmarks.






### Input & Output Columns

``` mermaid
graph TD
	subgraph Dataset
		subgraph Columns
			ICOL0[instruction]
			ICOL1[generations]
		end
		subgraph New columns
			OCOL0[evaluation]
			OCOL1[score]
			OCOL2[model_name]
		end
	end

	subgraph ArenaHard
		StepInput[Input Columns: instruction, generations]
		StepOutput[Output Columns: evaluation, score, model_name]
	end

	ICOL0 --> StepInput
	ICOL1 --> StepInput
	StepOutput --> OCOL0
	StepOutput --> OCOL1
	StepOutput --> OCOL2
	StepInput --> StepOutput

```


#### Inputs


- **instruction** (`str`): The instruction to evaluate the responses.

- **generations** (`List[str]`): The responses generated by two, and only two, LLMs.




#### Outputs


- **evaluation** (`str`): The evaluation of the responses generated by the LLMs.

- **score** (`str`): The score extracted from the evaluation.

- **model_name** (`str`): The model name used to generate the evaluation.





### Examples


#### Evaluate two assistant responses for a given instruction using Arean Hard prompts
```python
from distilabel.pipeline import Pipeline
from distilabel.steps import CombineColumns, LoadDataFromDicts
from distilabel.steps.tasks import ArenaHard, TextGeneration

with Pipeline() as pipeline:
    load_data = LoadDataFromDicts(
        data=[{"instruction": "What is the capital of France?"}],
    )

    text_generation_a = TextGeneration(
        llm=...,  # LLM instance
        output_mappings={"model_name": "generation_model"},
    )

    text_generation_b = TextGeneration(
        llm=...,  # LLM instance
        output_mappings={"model_name": "generation_model"},
    )

    combine = CombineColumns(
        columns=["generation", "generation_model"],
        output_columns=["generations", "generation_models"],
    )

    arena_hard = ArenaHard(
        llm=...,  # LLM instance
    )

    load_data >> [text_generation_a, text_generation_b] >> combine >> arena_hard
```




### References

- [From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline](https://lmsys.org/blog/2024-04-19-arena-hard/)

- [arena-hard-auto](https://github.com/lm-sys/arena-hard-auto/tree/main)


