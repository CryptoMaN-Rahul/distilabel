{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"distilabel","text":"<p>AI Feedback (AIF) framework to build datasets with and for LLMs:</p> <ul> <li>Integrations with the most popular libraries and APIs for LLMs: HF Transformers, OpenAI, vLLM, etc.</li> <li>Multiple tasks for Self-Instruct, Preference datasets and more.</li> <li>Dataset export to Argilla for easy data exploration and further annotation.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p><pre><code>pip install distilabel\n</code></pre> Requires Python 3.8+</p> <p>In addition, the following extras are available:</p> <ul> <li><code>hf-transformers</code>: for using models available in transformers package via the <code>TransformersLLM</code> integration.</li> <li><code>hf-inference-endpoints</code>: for using the HuggingFace Inference Endpoints via the <code>InferenceEndpointsLLM</code> integration.</li> <li><code>openai</code>: for using OpenAI API models via the <code>OpenAILLM</code> integration.</li> <li><code>vllm</code>: for using vllm serving engine via the <code>vLLM</code> integration.</li> <li><code>llama-cpp</code>: for using llama-cpp-python as Python bindings for <code>llama.cpp</code>.</li> <li><code>ollama</code>: for using Ollama and their available models via their Python client.</li> <li><code>together</code>: for using Together Inference via their Python client.</li> <li><code>vertexai</code>: for using both Google Vertex AI offerings: their proprietary models and endpoints via their Python client <code>google-cloud-aiplatform</code>.</li> <li><code>argilla</code>: for exporting the generated datasets to Argilla.</li> </ul>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>from datasets import load_dataset\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import pipeline\nfrom distilabel.tasks import TextGenerationTask\n\ndataset = (\n    load_dataset(\"HuggingFaceH4/instruction-dataset\", split=\"test[:10]\")\n    .remove_columns([\"completion\", \"meta\"])\n    .rename_column(\"prompt\", \"input\")\n)\n\ntask = TextGenerationTask()  # (1)\n\ngenerator = OpenAILLM(task=task, max_new_tokens=512)  # (2)\n\npipeline = pipeline(\"preference\", \"instruction-following\", generator=generator)  # (3)\n\ndataset = pipeline.generate(dataset)\n</code></pre> <ol> <li>Create a <code>Task</code> for generating text given an instruction.</li> <li>Create a <code>LLM</code> for generating text using the <code>Task</code> created in the first step. As the <code>LLM</code> will generate text, it will be a <code>generator</code>.</li> <li>Create a pre-defined <code>Pipeline</code> using the <code>pipeline</code> function and the <code>generator</code> created in step 2. The <code>pipeline</code> function will create a <code>labeller</code> LLM using <code>OpenAILLM</code> with the <code>UltraFeedback</code> task for instruction following assessment.</li> </ol> <p>Note</p> <p>To run the script successfully, ensure you have assigned your OpenAI API key to the <code>OPENAI_API_KEY</code> environment variable.</p> <p>For a more complete example, check out our awesome tutorials in the docs or the example below:</p> <p> </p>"},{"location":"#navigation","title":"Navigation","text":"<ul> <li> <p><p> Concept Guides</p></p> <p>Understand the components and their interactions.</p> </li> <li> <p><p> API Reference</p></p> <p>Technical description of the classes and functions.</p> </li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>This page aims to get you familiarized with the basic concepts of the framework, describing the most important components or classes and how they work together. The following sections will guide you through the primary components of the framework: <code>Pipeline</code>, <code>LLM</code> (both generator and labeller), and the <code>Task</code>.</p> <p> </p> distilabel flow diagram"},{"location":"concepts/#components","title":"Components","text":""},{"location":"concepts/#task","title":"Task","text":"<p>The <code>Task</code> class in the one in charge of defining the behaviour of the <code>LLM</code>, and therefore it can define if an LLM is a <code>generator</code> or a <code>labeller</code>. To do so, the <code>Task</code> class generates the prompt that will be sent to the <code>LLM</code> from a template. It also defines, which input arguments are required to generate the prompt, and which output arguments will be extracted from the <code>LLM</code> response. It's worth mentioning that the <code>Task</code> class doesn't return a <code>str</code>, but a <code>Prompt</code> class which will generate the <code>str</code> format depending on the <code>LLM</code> that is going to be used (Zephyr, Llama, OpenAI, etc).</p> <pre><code>from distilabel.tasks import UltraJudgeTask\n\ntask = UltraJudgeTask()\n\ninput = (\n    \"Can you provide a corrected version of the following sentence using proper \"\n    'English grammar? \"We going to the beach\" Additionally, could you please '\n    \"provide your correction in an Excel table format with the following columns: \"\n    \"| Incorrect Sentence | Corrected Sentence | |-------------------|--------------------|\"\n)\n\ngenerations = [\n    (\n        \"| Incorrect Sentence | Corrected Sentence |\\n|-------------------|-------------------\"\n        '-----|\\n| \"We going to the beach\" | \"We are going to the beach\" |\\n\\nCorrectio'\n        'n: The verb in the second sentence (\"are\") changes to reflect the subject\\'s (\"w'\n        'e\") agreement with the verb \"be.\" This is called subject-verb agreement. In the '\n        'first sentence, the verb \"going\" infers that the action is ongoing or in a contin'\n        \"uous state, which is not the case. Therefore, the second sentence is grammatically \"\n        \"correct.\"\n    ),\n    (\n        \"| Incorrect Sentence | Corrected Sentence |\\n|-------------------|-------------------\"\n        \"-----|\\n| We going to the beach | We are going to the beach | \\n\\nHere's a breakdo\"\n        'wn of the correction:\\n\\n- \"We going to the beach\" is an example of a subject-ve'\n        'rb agreement error. The verb changing from the third person singular (\"is\") to t'\n        'he third person plural (\"are\") in this instance, as there are multiple people go'\n        'ing to the beach.\\n- The \"g\" in \"going\" changes to an \"e\" due to a hard \"g\"'\n        ' sound being followed by an \"e,\" which is a common spelling rule in English.'\n    ),\n]\n\n\nprompt = task.generate_prompt(input, generations)\nprint(prompt.format_as(\"default\"))  # format as \"openai\", \"zephyr\", \"llama\", ...\n</code></pre>"},{"location":"concepts/#llm","title":"LLM","text":"<p>The <code>LLM</code> class represents a language model and implements the way to interact with it. It also defines the generation parameters that can be passed to the model to tweak the generations. As mentioned above, the <code>LLM</code> will have a <code>Task</code> associated that will use to generate the prompt and extract the output from the generation.</p> <pre><code>from distilabel.llm import OpenAILLM\nfrom distilabel.tasks import UltraJudgeTask\n\nlabeller = OpenAILLM(\n    model=\"gpt-3.5-turbo\",\n    task=UltraJudgeTask(),\n    prompt_format=\"openai\",\n    max_new_tokens=2048,\n    temperature=0.0,\n)\n\noutputs = labeller.generate(\n    inputs=[\n        {\n            \"input\": \"Here's a math problem that you need to resolve: 2 + 2 * 3. What's the result of this problem? Explain it\",\n            \"generations\": [\n                (\n                    \"The output of the math problem 2 + 2 * 3 is calculated by following \"\n                    \"the order of operations (PEMDAS). First, perform the multiplication: \"\n                    \"2 * 3 = 6. Then, perform the addition: 2 + 6 = 8. Therefore, the \"\n                    \"output of the problem is 8.\"\n                ),\n                (\n                    \"The correct solution to the math problem is 8. To get the correct \"\n                    \"answer, we follow the order of operations (PEMDAS) and perform \"\n                    \"multiplication before addition. So, first, we solve 2 * 3 = 6, \"\n                    \"then we add 2 to 6 to get 8.\"\n                ),\n            ],\n        }\n    ]\n)\n\nprint(outputs[0][0][\"parsed_output\"])\n</code></pre> <p>Note</p> <p>To run the script successfully, ensure you have assigned your OpenAI API key to the <code>OPENAI_API_KEY</code> environment variable.</p>"},{"location":"concepts/#pipeline","title":"Pipeline","text":"<p>The <code>Pipeline</code> class orchestrates the whole generation and labelling process, and it's in charge of the batching of the input dataset, as well as reporting the generation progress. It's worth mentioning that is not mandatory to pass both a generator <code>LLM</code> and a labeller <code>LLM</code> to the <code>Pipeline</code> class, as it can also be used only for generation or labelling.</p> <p>Pipelines</p> Generator and labellerOnly generatorOnly labeller <pre><code>from datasets import load_dataset\nfrom distilabel.llm import LlamaCppLLM, OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import TextGenerationTask, UltraJudgeTask\nfrom llama_cpp import Llama\n\ndataset = load_dataset(\"argilla/distilabel-docs\", split=\"train\")\ndataset = dataset.remove_columns(\n    [\n        column\n        for column in dataset.column_names\n        if column not in [\"input\", \"generations\"]\n    ]\n)\n\npipeline = Pipeline(\n    generator=LlamaCppLLM(\n        model=Llama(\n            model_path=\"./llama-2-7b-chat.Q4_0.gguf\",\n            verbose=False,\n            n_ctx=1024,\n        ),\n        task=TextGenerationTask(),\n        max_new_tokens=512,\n        prompt_format=\"llama2\",\n    ),\n    labeller=OpenAILLM(\n        model=\"gpt-3.5-turbo\",\n        task=UltraJudgeTask(),\n        prompt_format=\"openai\",\n        max_new_tokens=1024,\n        num_threads=1,\n        temperature=0.0,\n    ),\n)\n\n\ndataset = pipeline.generate(dataset, num_generations=2, batch_size=5)\n</code></pre> <p>Note</p> <p>To run the script successfully, ensure you have assigned your OpenAI API key to the <code>OPENAI_API_KEY</code> environment variable and that you have download the file llama-2-7b-chat.Q4_O.gguf in the same folder as the script.</p> <pre><code>from datasets import load_dataset\nfrom distilabel.llm import LlamaCppLLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import TextGenerationTask\nfrom llama_cpp import Llama\n\ndataset = load_dataset(\"argilla/distilabel-docs\", split=\"train\")\ndataset = dataset.remove_columns(\n    [column for column in dataset.column_names if column not in [\"input\"]]\n)\n\npipeline = Pipeline(\n    generator=LlamaCppLLM(\n        model=Llama(\n            model_path=\"./llama-2-7b-chat.Q4_0.gguf\",\n            verbose=False,\n            n_ctx=1024,\n        ),\n        task=TextGenerationTask(),\n        max_new_tokens=512,\n        prompt_format=\"llama2\",\n    ),\n)\n\n\ndataset = pipeline.generate(dataset, num_generations=2, batch_size=5)\n</code></pre> <pre><code>from datasets import load_dataset\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import UltraJudgeTask\n\ndataset = load_dataset(\"argilla/distilabel-docs\", split=\"train\")\ndataset = dataset.remove_columns(\n    [\n        column\n        for column in dataset.column_names\n        if column not in [\"input\", \"generations\"]\n    ]\n)\n\npipeline = Pipeline(\n    labeller=OpenAILLM(\n        model=\"gpt-3.5-turbo\",\n        task=UltraJudgeTask(),\n        prompt_format=\"openai\",\n        max_new_tokens=1024,\n        num_threads=1,\n        temperature=0.0,\n    ),\n)\n\n\ndataset = pipeline.generate(dataset, num_generations=2, batch_size=5)\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>distilabel<ul> <li>dataset</li> <li>llm<ul> <li>anyscale</li> <li>base</li> <li>google<ul> <li>vertexai</li> </ul> </li> <li>huggingface<ul> <li>inference_endpoints</li> <li>transformers</li> </ul> </li> <li>llama_cpp</li> <li>mistralai</li> <li>ollama</li> <li>openai</li> <li>together</li> <li>utils</li> <li>vllm</li> </ul> </li> <li>logger</li> <li>pipeline</li> <li>progress_bar</li> <li>tasks<ul> <li>base</li> <li>critique<ul> <li>base</li> <li>prometheus</li> <li>ultracm</li> </ul> </li> <li>mixins</li> <li>preference<ul> <li>base</li> <li>complexity_scorer</li> <li>judgelm</li> <li>quality_scorer</li> <li>ultrafeedback</li> <li>ultrajudge</li> </ul> </li> <li>prompt</li> <li>text_generation<ul> <li>base</li> <li>evol_complexity</li> <li>evol_instruct</li> <li>evol_quality</li> <li>mixins</li> <li>principles</li> <li>self_instruct</li> </ul> </li> </ul> </li> <li>utils<ul> <li>argilla</li> <li>dataset</li> <li>dicts</li> <li>futures</li> <li>imports</li> <li>serialization</li> <li>types</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/distilabel/","title":"distilabel","text":""},{"location":"reference/distilabel/dataset/","title":"dataset","text":""},{"location":"reference/distilabel/dataset/#distilabel.dataset.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>A custom dataset class that extends from <code>datasets.Dataset</code> and is used to generate an Argilla <code>FeedbackDataset</code> instance from the pre-defined configuration within the task provided to <code>Pipeline.generate</code>.</p> Source code in <code>src/distilabel/dataset.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"A custom dataset class that extends from `datasets.Dataset` and is used to generate\n    an Argilla `FeedbackDataset` instance from the pre-defined configuration within the task\n    provided to `Pipeline.generate`.\n    \"\"\"\n\n    task: Union[\"Task\", None] = None\n\n    def to_argilla(\n        self,\n        dataset_columns: List[str] = None,\n        vector_strategy: Union[bool, \"SentenceTransformersExtractor\"] = True,\n        metric_strategy: Union[bool, \"TextDescriptivesExtractor\"] = True,\n    ) -&gt; \"FeedbackDataset\":\n        \"\"\"Converts the dataset to an Argilla `FeedbackDataset` instance, based on the\n        task defined in the dataset as part of `Pipeline.generate`.\n\n        Args:\n            dataset_columns (List[str]): the dataset columns or fields to be used for the Argilla `FeedbackDataset` instance.\n                By default, the first 5 columns or fields will be used.\n            vector_strategy (Union[bool, SentenceTransformersExtractor]): the strategy to be used for\n                adding vectors to the dataset. If `True`, the default `SentenceTransformersExtractor`\n                will be used with the `TaylorAI/bge-micro-2` model. If `False`, no vectors will be added to the dataset.\n            metric_strategy (Union[bool, TextDescriptivesExtractor]): the strategy to be used for\n                adding metrics to the dataset. If `True`, the default `TextDescriptivesExtractor`\n                will be used. If `False`, no metrics will be added to the dataset.\n\n        Raises:\n            ImportError: if the argilla library is not installed.\n            ValueError: if the task is not set.\n\n        Returns:\n            FeedbackDataset: the Argilla `FeedbackDataset` instance.\n        \"\"\"\n        if not _ARGILLA_AVAILABLE:\n            raise ImportError(\n                \"To use `to_argilla` method is required to have `argilla` installed. \"\n                \"Please install it with `pip install argilla`.\"\n            )\n\n        if self.task is None:\n            raise ValueError(\n                \"The task is not set. Please set it with `dataset.task = &lt;task&gt;`.\"\n            )\n\n        try:\n            rg_dataset = self.task.to_argilla_dataset(dataset_row=self[0])  # type: ignore\n        except Exception as e:\n            raise ValueError(\n                f\"Error while converting the dataset to an Argilla `FeedbackDataset` instance: {e}\"\n            ) from e\n\n        for dataset_row in self:\n            if any(\n                dataset_row[input_arg_name] is None  # type: ignore\n                for input_arg_name in self.task.input_args_names\n            ):\n                continue\n            try:\n                rg_dataset.add_records(\n                    self.task._to_argilla_record(dataset_row=dataset_row)  # type: ignore\n                )  # type: ignore\n            except Exception as e:\n                warnings.warn(\n                    f\"Error while converting a row into an Argilla `FeedbackRecord` instance: {e}\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n        selected_fields = infer_field_from_dataset_columns(\n            dataset_columns=dataset_columns, dataset=rg_dataset, task=self.task\n        )\n        rg_dataset = self.add_vectors_to_argilla_dataset(\n            dataset=rg_dataset, vector_strategy=vector_strategy, fields=selected_fields\n        )\n        rg_dataset = self.add_metrics_to_argilla_dataset(\n            dataset=rg_dataset, metric_strategy=metric_strategy, fields=selected_fields\n        )\n        return rg_dataset\n\n    def add_vectors_to_argilla_dataset(\n        self,\n        dataset: Union[\"FeedbackRecord\", List[\"FeedbackRecord\"], \"FeedbackDataset\"],\n        vector_strategy: Union[bool, \"SentenceTransformersExtractor\"],\n        fields: List[str] = None,\n    ) -&gt; Union[\"FeedbackRecord\", List[\"FeedbackRecord\"], \"FeedbackDataset\"]:\n        if _ARGILLA_AVAILABLE and vector_strategy:\n            if isinstance(vector_strategy, SentenceTransformersExtractor):\n                ste: SentenceTransformersExtractor = vector_strategy\n            elif vector_strategy:\n                ste = SentenceTransformersExtractor()\n            dataset = ste.update_dataset(dataset=dataset, fields=fields)\n\n        elif not _ARGILLA_AVAILABLE and vector_strategy:\n            warnings.warn(\n                \"An error occurred while adding vectors to the dataset: \"\n                \"The `argilla`/`sentence-transformers` packages are not installed or the installed version is not compatible with the\"\n                \" required version. If you want to add vectors to your dataset, please run `pip install 'distilabel[argilla]'`.\",\n                stacklevel=2,\n            )\n        return dataset\n\n    def add_metrics_to_argilla_dataset(\n        self,\n        dataset: Union[\"FeedbackRecord\", List[\"FeedbackRecord\"], \"FeedbackDataset\"],\n        metric_strategy: Union[bool, \"TextDescriptivesExtractor\"],\n        fields: List[str] = None,\n    ) -&gt; Union[\"FeedbackRecord\", List[\"FeedbackRecord\"], \"FeedbackDataset\"]:\n        if _ARGILLA_AVAILABLE and metric_strategy:\n            if isinstance(metric_strategy, TextDescriptivesExtractor):\n                tde: TextDescriptivesExtractor = metric_strategy\n            elif metric_strategy:\n                tde = TextDescriptivesExtractor()\n\n            dataset = tde.update_dataset(dataset=dataset, fields=fields)\n        elif not _ARGILLA_AVAILABLE and metric_strategy:\n            warnings.warn(\n                \"An error occurred while adding metrics to the dataset: \"\n                \"The `argilla`/`text-descriptives` packages are not installed or the installed version is not compatible with the\"\n                \" required version. If you want to add metrics to your dataset, please run `pip install 'distilabel[argilla]'`.\",\n                stacklevel=2,\n            )\n        return dataset\n\n    def save_to_disk(self, dataset_path: os.PathLike, **kwargs: Any) -&gt; None:\n        \"\"\"Saves the datataset to disk, also saving the task.\n\n        Args:\n            dataset_path: Path to the dataset.\n            **kwargs: Additional arguments to be passed to `datasets.Dataset.save_to_disk`.\n        \"\"\"\n        super().save_to_disk(dataset_path, **kwargs)\n        if self.task is not None:\n            self.task.save(Path(dataset_path))\n\n    @classmethod\n    def load_from_disk(cls, dataset_path: os.PathLike, **kwargs: Any):\n        \"\"\"Load a CustomDataset from disk, also reading the task.\n\n        Args:\n            dataset_path (os.PathLike): Path to the dataset.\n            kwargs (Any): Keyword arguments passed to Dataset.load_from_disk.\n\n        Returns:\n            The loaded dataset.\n        \"\"\"\n        ds = super().load_from_disk(dataset_path, **kwargs)\n        # Dynamically remaps the `datasets.Dataset` to be a `CustomDataset` instance\n        ds.__class__ = cls\n        task = load_task_from_disk(dataset_path)\n        ds.task = task\n        return ds\n\n    def push_to_hub(\n        self,\n        repo_id: str,\n        *args: Optional[Any],\n        push_task: bool = True,\n        **kwargs: Optional[Any],\n    ) -&gt; None:\n        \"\"\"Same method as `datasets.Dataset.push_to_hub`, but also pushes the task to simplify\n        creating a CustomDataset from HuggingFace hub.\n\n        Args:\n            repo_id (str):\n                The ID of the repository to push to in the following format: `&lt;user&gt;/&lt;dataset_name&gt;` or\n                `&lt;org&gt;/&lt;dataset_name&gt;`. Also accepts `&lt;dataset_name&gt;`, which will default to the namespace\n                of the logged-in user.\n            args (Any): Additional arguments to be passed to `datasets.Dataset.push_to_hub`.\n            push_task (bool, optional): _description_. Defaults to True.\n            kwargs (Any): Additional arguments to be passed to `datasets.Dataset.push_to_hub`.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.dataset import CustomDataset\n            &gt;&gt;&gt; dataset = CustomDataset(...)\n            &gt;&gt;&gt; dataset.push_to_hub(\"path/to/dataset\")\n        \"\"\"\n        super().push_to_hub(repo_id, *args, **kwargs)\n        if self.task is not None and push_task:\n            try:\n                logger.info(\"Pushing task to the hub...\")\n                with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n                    f.write(json.dumps(self.task.dump(), indent=2))\n                    f.flush()\n\n                    HfApi().upload_file(\n                        path_or_fileobj=f.name,\n                        path_in_repo=TASK_FILE_NAME,\n                        repo_id=repo_id,\n                        repo_type=\"dataset\",\n                        token=kwargs.get(\"token\"),\n                    )\n            except Exception as e:\n                logger.info(f\"Error while pushing the task to the hub: {e}.\")\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.CustomDataset.load_from_disk","title":"<code>load_from_disk(dataset_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load a CustomDataset from disk, also reading the task.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>PathLike</code> <p>Path to the dataset.</p> required <code>kwargs</code> <code>Any</code> <p>Keyword arguments passed to Dataset.load_from_disk.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The loaded dataset.</p> Source code in <code>src/distilabel/dataset.py</code> <pre><code>@classmethod\ndef load_from_disk(cls, dataset_path: os.PathLike, **kwargs: Any):\n    \"\"\"Load a CustomDataset from disk, also reading the task.\n\n    Args:\n        dataset_path (os.PathLike): Path to the dataset.\n        kwargs (Any): Keyword arguments passed to Dataset.load_from_disk.\n\n    Returns:\n        The loaded dataset.\n    \"\"\"\n    ds = super().load_from_disk(dataset_path, **kwargs)\n    # Dynamically remaps the `datasets.Dataset` to be a `CustomDataset` instance\n    ds.__class__ = cls\n    task = load_task_from_disk(dataset_path)\n    ds.task = task\n    return ds\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.CustomDataset.push_to_hub","title":"<code>push_to_hub(repo_id, *args, push_task=True, **kwargs)</code>","text":"<p>Same method as <code>datasets.Dataset.push_to_hub</code>, but also pushes the task to simplify creating a CustomDataset from HuggingFace hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The ID of the repository to push to in the following format: <code>&lt;user&gt;/&lt;dataset_name&gt;</code> or <code>&lt;org&gt;/&lt;dataset_name&gt;</code>. Also accepts <code>&lt;dataset_name&gt;</code>, which will default to the namespace of the logged-in user.</p> required <code>args</code> <code>Any</code> <p>Additional arguments to be passed to <code>datasets.Dataset.push_to_hub</code>.</p> <code>()</code> <code>push_task</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to <code>datasets.Dataset.push_to_hub</code>.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.dataset import CustomDataset\n&gt;&gt;&gt; dataset = CustomDataset(...)\n&gt;&gt;&gt; dataset.push_to_hub(\"path/to/dataset\")\n</code></pre> Source code in <code>src/distilabel/dataset.py</code> <pre><code>def push_to_hub(\n    self,\n    repo_id: str,\n    *args: Optional[Any],\n    push_task: bool = True,\n    **kwargs: Optional[Any],\n) -&gt; None:\n    \"\"\"Same method as `datasets.Dataset.push_to_hub`, but also pushes the task to simplify\n    creating a CustomDataset from HuggingFace hub.\n\n    Args:\n        repo_id (str):\n            The ID of the repository to push to in the following format: `&lt;user&gt;/&lt;dataset_name&gt;` or\n            `&lt;org&gt;/&lt;dataset_name&gt;`. Also accepts `&lt;dataset_name&gt;`, which will default to the namespace\n            of the logged-in user.\n        args (Any): Additional arguments to be passed to `datasets.Dataset.push_to_hub`.\n        push_task (bool, optional): _description_. Defaults to True.\n        kwargs (Any): Additional arguments to be passed to `datasets.Dataset.push_to_hub`.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.dataset import CustomDataset\n        &gt;&gt;&gt; dataset = CustomDataset(...)\n        &gt;&gt;&gt; dataset.push_to_hub(\"path/to/dataset\")\n    \"\"\"\n    super().push_to_hub(repo_id, *args, **kwargs)\n    if self.task is not None and push_task:\n        try:\n            logger.info(\"Pushing task to the hub...\")\n            with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n                f.write(json.dumps(self.task.dump(), indent=2))\n                f.flush()\n\n                HfApi().upload_file(\n                    path_or_fileobj=f.name,\n                    path_in_repo=TASK_FILE_NAME,\n                    repo_id=repo_id,\n                    repo_type=\"dataset\",\n                    token=kwargs.get(\"token\"),\n                )\n        except Exception as e:\n            logger.info(f\"Error while pushing the task to the hub: {e}.\")\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.CustomDataset.save_to_disk","title":"<code>save_to_disk(dataset_path, **kwargs)</code>","text":"<p>Saves the datataset to disk, also saving the task.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>PathLike</code> <p>Path to the dataset.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to be passed to <code>datasets.Dataset.save_to_disk</code>.</p> <code>{}</code> Source code in <code>src/distilabel/dataset.py</code> <pre><code>def save_to_disk(self, dataset_path: os.PathLike, **kwargs: Any) -&gt; None:\n    \"\"\"Saves the datataset to disk, also saving the task.\n\n    Args:\n        dataset_path: Path to the dataset.\n        **kwargs: Additional arguments to be passed to `datasets.Dataset.save_to_disk`.\n    \"\"\"\n    super().save_to_disk(dataset_path, **kwargs)\n    if self.task is not None:\n        self.task.save(Path(dataset_path))\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.CustomDataset.to_argilla","title":"<code>to_argilla(dataset_columns=None, vector_strategy=True, metric_strategy=True)</code>","text":"<p>Converts the dataset to an Argilla <code>FeedbackDataset</code> instance, based on the task defined in the dataset as part of <code>Pipeline.generate</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_columns</code> <code>List[str]</code> <p>the dataset columns or fields to be used for the Argilla <code>FeedbackDataset</code> instance. By default, the first 5 columns or fields will be used.</p> <code>None</code> <code>vector_strategy</code> <code>Union[bool, SentenceTransformersExtractor]</code> <p>the strategy to be used for adding vectors to the dataset. If <code>True</code>, the default <code>SentenceTransformersExtractor</code> will be used with the <code>TaylorAI/bge-micro-2</code> model. If <code>False</code>, no vectors will be added to the dataset.</p> <code>True</code> <code>metric_strategy</code> <code>Union[bool, TextDescriptivesExtractor]</code> <p>the strategy to be used for adding metrics to the dataset. If <code>True</code>, the default <code>TextDescriptivesExtractor</code> will be used. If <code>False</code>, no metrics will be added to the dataset.</p> <code>True</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>if the argilla library is not installed.</p> <code>ValueError</code> <p>if the task is not set.</p> <p>Returns:</p> Name Type Description <code>FeedbackDataset</code> <code>FeedbackDataset</code> <p>the Argilla <code>FeedbackDataset</code> instance.</p> Source code in <code>src/distilabel/dataset.py</code> <pre><code>def to_argilla(\n    self,\n    dataset_columns: List[str] = None,\n    vector_strategy: Union[bool, \"SentenceTransformersExtractor\"] = True,\n    metric_strategy: Union[bool, \"TextDescriptivesExtractor\"] = True,\n) -&gt; \"FeedbackDataset\":\n    \"\"\"Converts the dataset to an Argilla `FeedbackDataset` instance, based on the\n    task defined in the dataset as part of `Pipeline.generate`.\n\n    Args:\n        dataset_columns (List[str]): the dataset columns or fields to be used for the Argilla `FeedbackDataset` instance.\n            By default, the first 5 columns or fields will be used.\n        vector_strategy (Union[bool, SentenceTransformersExtractor]): the strategy to be used for\n            adding vectors to the dataset. If `True`, the default `SentenceTransformersExtractor`\n            will be used with the `TaylorAI/bge-micro-2` model. If `False`, no vectors will be added to the dataset.\n        metric_strategy (Union[bool, TextDescriptivesExtractor]): the strategy to be used for\n            adding metrics to the dataset. If `True`, the default `TextDescriptivesExtractor`\n            will be used. If `False`, no metrics will be added to the dataset.\n\n    Raises:\n        ImportError: if the argilla library is not installed.\n        ValueError: if the task is not set.\n\n    Returns:\n        FeedbackDataset: the Argilla `FeedbackDataset` instance.\n    \"\"\"\n    if not _ARGILLA_AVAILABLE:\n        raise ImportError(\n            \"To use `to_argilla` method is required to have `argilla` installed. \"\n            \"Please install it with `pip install argilla`.\"\n        )\n\n    if self.task is None:\n        raise ValueError(\n            \"The task is not set. Please set it with `dataset.task = &lt;task&gt;`.\"\n        )\n\n    try:\n        rg_dataset = self.task.to_argilla_dataset(dataset_row=self[0])  # type: ignore\n    except Exception as e:\n        raise ValueError(\n            f\"Error while converting the dataset to an Argilla `FeedbackDataset` instance: {e}\"\n        ) from e\n\n    for dataset_row in self:\n        if any(\n            dataset_row[input_arg_name] is None  # type: ignore\n            for input_arg_name in self.task.input_args_names\n        ):\n            continue\n        try:\n            rg_dataset.add_records(\n                self.task._to_argilla_record(dataset_row=dataset_row)  # type: ignore\n            )  # type: ignore\n        except Exception as e:\n            warnings.warn(\n                f\"Error while converting a row into an Argilla `FeedbackRecord` instance: {e}\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    selected_fields = infer_field_from_dataset_columns(\n        dataset_columns=dataset_columns, dataset=rg_dataset, task=self.task\n    )\n    rg_dataset = self.add_vectors_to_argilla_dataset(\n        dataset=rg_dataset, vector_strategy=vector_strategy, fields=selected_fields\n    )\n    rg_dataset = self.add_metrics_to_argilla_dataset(\n        dataset=rg_dataset, metric_strategy=metric_strategy, fields=selected_fields\n    )\n    return rg_dataset\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.DatasetCheckpoint","title":"<code>DatasetCheckpoint</code>  <code>dataclass</code>","text":"<p>A checkpoint class that contains the information of a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the checkpoint.</p> <code>cwd() / 'ckpt'</code> <code>save_frequency</code> <code>int</code> <p>The frequency at which the checkpoint should be saved By default is set to -1 (no checkpoint is saved to disk, but the dataset is returned upon failure).</p> <code>-1</code> <code>strategy</code> <code>CheckpointStrategies</code> <p>The strategy to be used to save the checkpoint. Available options are: \"disk\" (to save the dataset to disk) and \"hf-hub\" (to push the dataset to the HuggingFace hub). By default is set to \"disk\".</p> <code>'disk'</code> <code>extra_kwargs</code> <code>dict[str, Any]</code> <p>Additional kwargs to be passed to the <code>save_to_disk</code> method of the Dataset.</p> <code>field(default_factory=dict)</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.dataset import DatasetCheckpoint\n&gt;&gt;&gt; # Save the dataset every 10% of the records generated.\n&gt;&gt;&gt; checkpoint = DatasetCheckpoint(save_frequency=len(dataset) // 10)\n&gt;&gt;&gt; # Afterwards, we can access the checkpoint's checkpoint.path.\n&gt;&gt;&gt; # You can also push the dataset to the hub by setting the strategy to `hf-hub`.\n&gt;&gt;&gt; checkpoint = DatasetCheckpoint(\n...     strategy=\"hf-hub\",\n...     extra_kwargs={\"repo_id\": \"username/dataset-name\"}\n... )\n&gt;&gt;&gt; # Keep in mind, if the environmnet variable \"HF_API_TOKEN\" is not set, you are required\n&gt;&gt;&gt; # to pass the token as an argument to the `extra_kwargs`.\n&gt;&gt;&gt; checkpoint = DatasetCheckpoint(\n...     strategy=\"hf-hub\",\n...     extra_kwargs={\"repo_id\": \"username/dataset-name\", \"token\": \"hf_...\"}\n... )\n</code></pre> Source code in <code>src/distilabel/dataset.py</code> <pre><code>@dataclass\nclass DatasetCheckpoint:\n    \"\"\"A checkpoint class that contains the information of a checkpoint.\n\n    Args:\n        path (Path): The path to the checkpoint.\n        save_frequency (int): The frequency at which the checkpoint should be saved\n            By default is set to -1 (no checkpoint is saved to disk, but the dataset\n            is returned upon failure).\n        strategy (CheckpointStrategies): The strategy to be used to save the checkpoint.\n            Available options are: \"disk\" (to save the dataset to disk) and \"hf-hub\"\n            (to push the dataset to the HuggingFace hub). By default is set to \"disk\".\n        extra_kwargs (dict[str, Any]): Additional kwargs to be passed to the `save_to_disk` method of the Dataset.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.dataset import DatasetCheckpoint\n        &gt;&gt;&gt; # Save the dataset every 10% of the records generated.\n        &gt;&gt;&gt; checkpoint = DatasetCheckpoint(save_frequency=len(dataset) // 10)\n        &gt;&gt;&gt; # Afterwards, we can access the checkpoint's checkpoint.path.\n        &gt;&gt;&gt; # You can also push the dataset to the hub by setting the strategy to `hf-hub`.\n        &gt;&gt;&gt; checkpoint = DatasetCheckpoint(\n        ...     strategy=\"hf-hub\",\n        ...     extra_kwargs={\"repo_id\": \"username/dataset-name\"}\n        ... )\n        &gt;&gt;&gt; # Keep in mind, if the environmnet variable \"HF_API_TOKEN\" is not set, you are required\n        &gt;&gt;&gt; # to pass the token as an argument to the `extra_kwargs`.\n        &gt;&gt;&gt; checkpoint = DatasetCheckpoint(\n        ...     strategy=\"hf-hub\",\n        ...     extra_kwargs={\"repo_id\": \"username/dataset-name\", \"token\": \"hf_...\"}\n        ... )\n    \"\"\"\n\n    path: Path = Path.cwd() / \"ckpt\"\n    save_frequency: int = -1\n    strategy: CheckpointStrategies = \"disk\"\n    extra_kwargs: Dict[str, Any] = field(default_factory=dict)\n\n    # Internal fields to keep track of the number of records generated and when to check.\n    _total_checks: int = field(repr=False, default=0)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Checks validity of arguments.\n\n        Raises:\n            ValueError:\n                - If the strategy is not valid.\n                - If the `repo_id` is not passed when using the `hf-hub` strategy.\n        \"\"\"\n        if self.strategy not in get_args(CheckpointStrategies):\n            raise ValueError(\n                f\"Invalid strategy, valid ones are: {get_args(CheckpointStrategies)}\"\n            )\n\n        if self.strategy == \"hf-hub\":\n            if not self.extra_kwargs.get(\"repo_id\"):\n                raise ValueError(\n                    \"The `repo_id` is required when using the `hf-hub` strategy, please pass it as on 'extra_kwargs' argument.\"\n                )\n            if token := os.getenv(\"HF_API_TOKEN\") or (\n                token := self.extra_kwargs.get(\"token\")\n            ):\n                hf_hub_login(token=token)\n            else:\n                raise ValueError(\n                    \"To use the `hf-hub` strategy, a token is required, you can either set it \"\n                    \"as an environment variable `HF_API_TOKEN` or pass it as an argument `token` \"\n                    \"via the `extra_kwargs`.\"\n                )\n\n    def do_checkpoint(self, step: int) -&gt; bool:\n        \"\"\"Determines if a checkpoint should be done.\n\n        Args:\n            step (int): The number of records generated.\n\n        Returns:\n            bool: Whether a checkpoint should be done.\n        \"\"\"\n        if self.save_frequency == -1:\n            return False\n\n        if (step - self._total_checks * self.save_frequency) // self.save_frequency:\n            self._total_checks += 1\n            return True\n        return False\n\n    def save(self, dataset: CustomDataset) -&gt; None:\n        \"\"\"Save the dataset given the strategy selected.\n\n        Args:\n            dataset (CustomDataset): Dataset to be saved.\n\n        Raises:\n            ValueError: If the strategy is not valid.\n        \"\"\"\n        if self.strategy == \"disk\":\n            self._save_to_disk(dataset, **self.extra_kwargs)\n\n        elif self.strategy == \"hf-hub\":\n            self._push_to_hub(dataset, **self.extra_kwargs)\n\n        else:\n            # We shouldn't reach this point after __post_init__, but just in case.\n            raise ValueError(\n                f\"Invalid strategy, valid ones are: {get_args(CheckpointStrategies)}\"\n            )\n\n    def _save_to_disk(\n        self, dataset: Union[Dataset, CustomDataset], **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Saves the dataset to disk.\n\n        Args:\n            dataset (Union[Dataset, CustomDataset]): The dataset to be saved.\n        \"\"\"\n        dataset.save_to_disk(self.path, **kwargs)\n        logger.info(f\"Checkpoint saved to disk: {self.path}.\")\n\n    def _push_to_hub(self, dataset: CustomDataset, **kwargs: Any) -&gt; None:\n        \"\"\"Pushes the dataset to the hub.\n\n        Args:\n            dataset (Union[Dataset, CustomDataset]): The dataset to be pushed.\n        \"\"\"\n        repo_id = kwargs.pop(\"repo_id\", None)\n        if not repo_id:\n            raise ValueError(\n                \"The `repo_id` is required when pushing to the hub, please pass it as on 'extra_kwargs' argument.\"\n            )\n\n        try:\n            dataset.push_to_hub(repo_id, **kwargs)\n            logger.info(f\"Checkpoint saved to hub: {repo_id}.\")\n        except Exception as e:\n            # TODO(plaguss): Check possible errors to provide a more informative message.\n            # And maybe save to disk as a fallback.\n            logger.info(f\"Error while pushing the dataset to the hub: {e}.\")\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.DatasetCheckpoint.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Checks validity of arguments.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the strategy is not valid.</li> <li>If the <code>repo_id</code> is not passed when using the <code>hf-hub</code> strategy.</li> </ul> Source code in <code>src/distilabel/dataset.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Checks validity of arguments.\n\n    Raises:\n        ValueError:\n            - If the strategy is not valid.\n            - If the `repo_id` is not passed when using the `hf-hub` strategy.\n    \"\"\"\n    if self.strategy not in get_args(CheckpointStrategies):\n        raise ValueError(\n            f\"Invalid strategy, valid ones are: {get_args(CheckpointStrategies)}\"\n        )\n\n    if self.strategy == \"hf-hub\":\n        if not self.extra_kwargs.get(\"repo_id\"):\n            raise ValueError(\n                \"The `repo_id` is required when using the `hf-hub` strategy, please pass it as on 'extra_kwargs' argument.\"\n            )\n        if token := os.getenv(\"HF_API_TOKEN\") or (\n            token := self.extra_kwargs.get(\"token\")\n        ):\n            hf_hub_login(token=token)\n        else:\n            raise ValueError(\n                \"To use the `hf-hub` strategy, a token is required, you can either set it \"\n                \"as an environment variable `HF_API_TOKEN` or pass it as an argument `token` \"\n                \"via the `extra_kwargs`.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.DatasetCheckpoint.do_checkpoint","title":"<code>do_checkpoint(step)</code>","text":"<p>Determines if a checkpoint should be done.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The number of records generated.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether a checkpoint should be done.</p> Source code in <code>src/distilabel/dataset.py</code> <pre><code>def do_checkpoint(self, step: int) -&gt; bool:\n    \"\"\"Determines if a checkpoint should be done.\n\n    Args:\n        step (int): The number of records generated.\n\n    Returns:\n        bool: Whether a checkpoint should be done.\n    \"\"\"\n    if self.save_frequency == -1:\n        return False\n\n    if (step - self._total_checks * self.save_frequency) // self.save_frequency:\n        self._total_checks += 1\n        return True\n    return False\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.DatasetCheckpoint.save","title":"<code>save(dataset)</code>","text":"<p>Save the dataset given the strategy selected.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CustomDataset</code> <p>Dataset to be saved.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the strategy is not valid.</p> Source code in <code>src/distilabel/dataset.py</code> <pre><code>def save(self, dataset: CustomDataset) -&gt; None:\n    \"\"\"Save the dataset given the strategy selected.\n\n    Args:\n        dataset (CustomDataset): Dataset to be saved.\n\n    Raises:\n        ValueError: If the strategy is not valid.\n    \"\"\"\n    if self.strategy == \"disk\":\n        self._save_to_disk(dataset, **self.extra_kwargs)\n\n    elif self.strategy == \"hf-hub\":\n        self._push_to_hub(dataset, **self.extra_kwargs)\n\n    else:\n        # We shouldn't reach this point after __post_init__, but just in case.\n        raise ValueError(\n            f\"Invalid strategy, valid ones are: {get_args(CheckpointStrategies)}\"\n        )\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.load_dataset","title":"<code>load_dataset(path, *args, **kwargs)</code>","text":"<p>Load a dataset from HuggingFace hub.</p> <p>Overloads the <code>datasets.load_dataset</code> method to return a <code>CustomDataset</code> instance, downloading the <code>Task</code> from the hub (if any).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset in the hub.</p> required <code>args,</code> <code>kwargs</code> <p>and any other arguments used by <code>datasets.load_dataset</code></p> required <p>Returns:</p> Name Type Description <code>dataset</code> <code>Union[Dataset, CustomDataset]</code> <p>CustomDataset instance, with the <code>Task</code> stored if available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.dataset import load_dataset\n&gt;&gt;&gt; dataset: CustomDataset = load_dataset(\"argilla/distilabel-sample-evol-instruct\", split=\"train\")\n</code></pre> Source code in <code>src/distilabel/dataset.py</code> <pre><code>def load_dataset(path: str, *args: Any, **kwargs: Any) -&gt; Union[Dataset, CustomDataset]:\n    \"\"\"Load a dataset from HuggingFace hub.\n\n    Overloads the `datasets.load_dataset` method to return a `CustomDataset` instance,\n    downloading the `Task` from the hub (if any).\n\n    Args:\n        path (str): Path to the dataset in the hub.\n        args, kwargs: and any other arguments used by `datasets.load_dataset`\n\n    Returns:\n        dataset: CustomDataset instance, with the `Task` stored if available.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.dataset import load_dataset\n        &gt;&gt;&gt; dataset: CustomDataset = load_dataset(\"argilla/distilabel-sample-evol-instruct\", split=\"train\")\n    \"\"\"\n    from datasets import load_dataset as _load_dataset\n\n    ds = _load_dataset(path, *args, **kwargs)\n    cds = CustomDataset(ds.data.table)\n    # download the task\n    try:\n        task_path = hf_hub_download(\n            repo_id=path, filename=TASK_FILE_NAME, repo_type=\"dataset\"\n        )\n        task = load_from_dict(read_json(task_path))\n        cds.task = task\n    except Exception as e:\n        logger.info(f\"Error while downloading the task from the hub: {e}.\")\n    return cds\n</code></pre>"},{"location":"reference/distilabel/logger/","title":"logger","text":""},{"location":"reference/distilabel/pipeline/","title":"pipeline","text":""},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"Source code in <code>src/distilabel/pipeline.py</code> <pre><code>class Pipeline:\n    def __init__(\n        self,\n        generator: Union[\"LLM\", \"ProcessLLM\", \"LLMPool\", None] = None,\n        labeller: Union[\"LLM\", \"ProcessLLM\", None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the Pipeline class.\n\n        Args:\n            generator (Union[\"LLM\", None], optional): the LLM to be used for generation.\n                Defaults to None.\n            labeller (Union[\"LLM\", None], optional): the LLM to be used for labelling.\n                Defaults to None.\n\n        Raises:\n            ValueError: if no LLM is provided.\n\n        Examples:\n            &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n            &gt;&gt;&gt; from distilabel.llm import OpenAILLM, TransformersLLM\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask, UltraFeedbackTask\n            &gt;&gt;&gt; from distilabel.pipeline import Pipeline\n            &gt;&gt;&gt; generator = TransformersLLM(\n            ...     model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n            ...     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n            ...     task=TextGenerationTask(),\n            ...     prompt_format=\"llama2\",\n            ... )\n            &gt;&gt;&gt; labeller = OpenAILLM(\n            ...     model=\"gpt-3.5-turbo\",\n            ...     task=UltraFeedbackTask.for_overall_quality(),\n            ... )\n            &gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n            &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n        \"\"\"\n        if generator is not None and not isinstance(\n            generator, (LLM, ProcessLLM, LLMPool)\n        ):\n            raise ValueError(\n                \"`generator` must be an instance of `LLM`, `ProcessLLM` or `LLMPool`\"\n            )\n\n        if labeller is not None and not isinstance(labeller, (LLM, ProcessLLM)):\n            raise ValueError(\"`labeller` must be an instance of `LLM` or `ProcessLLM`\")\n\n        self.generator = generator\n        self.labeller = labeller\n\n        if self.generator is None and self.labeller is None:\n            raise ValueError(\"Either `generator` or `labeller` must be provided.\")\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"Pipeline(\\n\\tgenerator={self.generator},\\n\\tlabeller={self.labeller}\\n)\"\n        )\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"generator\", self.generator\n        yield \"labeller\", self.labeller\n\n    def _validate_dataset(self, dataset: Dataset) -&gt; None:\n        \"\"\"Validates that the provided dataset contains the columns needed by the LLMs, and\n        warns the user if the columns to be generated already exist.\n\n        Args:\n            dataset (Dataset): the dataset to be validated.\n\n        Raises:\n            KeyError: if the dataset does not contain the columns needed by the LLMs.\n        \"\"\"\n        # Generation LLM has not been provided, so the columns needed by the Labelling\n        # LLM must be in the provided dataset\n        if self.labeller is not None:\n            if self.generator is None:\n                try:\n                    self.labeller.task.validate_dataset(dataset.column_names)\n                except KeyError as err:\n                    raise KeyError(\n                        \"Labelling LLM expects a dataset with at least the following\"\n                        f\" columns: {self.labeller.task.input_args_names}, but the provided\"\n                        f\" dataset just contains: {dataset.column_names}\"\n                    ) from err\n            else:\n                expected_columns = (\n                    dataset.column_names + self.generator.task.output_args_names\n                )\n                try:\n                    self.labeller.task.validate_dataset(expected_columns)\n                except KeyError as err:\n                    raise KeyError(\n                        \"Labelling LLM expects to receive the following columns after the\"\n                        f\" generation process: {self.labeller.task.input_args_names}, but the\"\n                        f\" provided dataset including the columns to generate just contains: {expected_columns}\"\n                    ) from err\n\n        if self.generator is not None:\n            try:\n                self.generator.task.validate_dataset(dataset.column_names)\n            except KeyError as err:\n                raise KeyError(\n                    \"Generation LLM expects a dataset with the following columns:\"\n                    f\" {self.generator.task.input_args_names}, but the provided dataset\"\n                    f\" just contains: {dataset.column_names}\"\n                ) from err\n\n        # Additionally, we need to check that if the columns to be generated already exist,\n        # then we should look for `None`/`null` values and just fulfill those, while skipping\n        # the rest. This is useful to be able to continue a generation that broke or a process\n        # that was interrupted\n        generated_columns = []\n        if self.generator is not None:\n            generated_columns += self.generator.task.output_args_names\n        if self.labeller is not None:\n            generated_columns += self.labeller.task.output_args_names\n\n        if set(generated_columns) == set(dataset.column_names).intersection(\n            set(generated_columns)\n        ):\n            warnings.warn(\n                \"The provided dataset already contains the columns to be generated:\"\n                f\" {generated_columns}; which means that the generation process will\"\n                \" be skipped for the rows with values for those columns. If you want\"\n                \" to re-generate those columns, please remove them from the dataset.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    def _get_batch_generations(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int,\n        shuffle_before_labelling: bool = True,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Gets the batch generations for the given inputs, capturing the futures if the\n        LLM returns them, and then processes the batch generations.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int): the number of generations to be performed for each\n                input.\n            shuffle_before_labelling (bool, optional): whether to shuffle the generations\n                before labelling or not. This is useful to avoid the labelling LLM to be\n                biased by the order of the generations. Defaults to `True`.\n            progress_callback_func (Union[Callable, None], optional): the callback function\n                to be called when the progress of the generation process changes. Defaults\n                to None.\n\n        Returns:\n            List[Dict[str, Any]]: the processed batch generations.\n        \"\"\"\n        outputs = self.generator.generate(  # type: ignore\n            inputs=inputs,\n            num_generations=num_generations,\n            progress_callback_func=progress_callback_func,\n        )\n        batch_generations = []\n        if isinstance(outputs, Future):\n            batch_generations.extend(outputs.result())\n        else:\n            batch_generations = outputs\n        return self._process_batch_generations(\n            batch_generations=batch_generations,\n            shuffle_before_labelling=shuffle_before_labelling,\n        )\n\n    def _get_batch_labels(\n        self,\n        inputs: List[Dict[str, Any]],\n        progress_callback_func: Union[Callable, None] = None,\n        batch_size: Optional[int] = None,\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        \"\"\"Gets the batch labels for the given inputs.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for labelling. Each dict\n                should contain a key with the text generations.\n            progress_callback_func (Union[Callable, None], optional): the callback function\n                to be called when the progress of the labelling process changes. Defaults\n                to `None`.\n            batch_size (Optional[int], optional): the batch size to be used in case of error\n                in the Future processing.\n\n        Returns:\n            List[List[\"LLMOutput\"]]: the batch labels.\n        \"\"\"\n        # `num_generations` is always 1 because labelling the same input multiple times\n        # using the same LLM may not make sense\n        outputs = self.labeller.generate(  # type: ignore\n            inputs=inputs,\n            num_generations=1,\n            progress_callback_func=progress_callback_func,\n        )\n        batch_outputs = []\n        if isinstance(outputs, Future):\n            try:\n                batch_outputs.extend(outputs.result())\n            except Exception as e:\n                logger.error(\n                    f\"An error occurred when getting the result from the labeller: {e}\"\n                )\n                batch_outputs.append(\n                    [\n                        LLMOutput(\n                            model_name=self.labeller.model_name,\n                            prompt_used=None,\n                            raw_output=None,\n                            parsed_output=None,\n                        )\n                        for _ in range(batch_size)\n                    ]\n                )\n        else:\n            batch_outputs = outputs\n\n        return batch_outputs\n\n    def _process_batch_generations(\n        self,\n        batch_generations: List[List[\"LLMOutput\"]],\n        shuffle_before_labelling: bool = True,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Processes the batch generations, combining the outputs of the LLMs into a single\n        dictionary.\n\n        Args:\n            batch_generations (List[List[\"LLMOutput\"]]): the batch generations to be processed.\n            shuffle_before_labelling (bool, optional): whether to shuffle the generations\n                before labelling or not. This is useful to avoid the labelling LLM to be\n                biased by the order of the generations. Defaults to `True`.\n\n        Returns:\n            List[Dict[str, Any]]: the processed batch generations.\n        \"\"\"\n        processed_generations = []\n        for generations in batch_generations:\n            processed_generation = {\n                \"generation_model\": [],\n                \"generation_prompt\": [],\n                \"raw_generation_responses\": [],\n            }\n            if shuffle_before_labelling:\n                random.shuffle(generations)\n            for generation in generations:\n                processed_generation[\"generation_model\"].append(\n                    generation[\"model_name\"]\n                )\n                processed_generation[\"generation_prompt\"].append(\n                    generation[\"prompt_used\"]\n                )\n                processed_generation[\"raw_generation_responses\"].append(\n                    generation[\"raw_output\"]\n                )\n            # Create `generations` column which is a list with N text generations\n            try:\n                processed_generation.update(\n                    **combine_dicts(\n                        *[\n                            generation[\"parsed_output\"]\n                            if generation[\"parsed_output\"] is not None\n                            else {}\n                            for generation in generations\n                        ]\n                    )\n                )\n            except Exception as e:\n                warnings.warn(\n                    f\"Generation processing step failed when combining dicts: {e}\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n            processed_generations.append(processed_generation)\n        return processed_generations\n\n    def _include_generator_outputs_as_inputs(\n        self, inputs: List[Dict[str, Any]], outputs: List[Dict[str, Any]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Includes the outputs of the generator as inputs for the labeller.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for labelling.\n            outputs (List[Dict[str, Any]]): the outputs of the generator.\n\n        Returns:\n            List[Dict[str, Any]]: the inputs to be used for labelling.\n        \"\"\"\n        for input_, output in zip(inputs, outputs):\n            # Skip the keys not required by the labelling LLM\n            input_.update(\n                {\n                    k: v\n                    for k, v in output.items()\n                    if self.labeller is not None\n                    and k in self.labeller.task.input_args_names\n                }\n            )\n        return inputs\n\n    def _process_batch_labels(\n        self, batch_labels: List[List[\"LLMOutput\"]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Processes the batch labels, combining the outputs of the LLMs into a single\n        dictionary.\n\n        Args:\n            batch_labels (List[List[\"LLMOutput\"]]): the batch labels to be processed.\n\n        Returns:\n            List[Dict[str, Any]]: the processed batch labels.\n        \"\"\"\n        processed_labels = []\n        for label in batch_labels:\n            if label[\"parsed_output\"] is not None and not isinstance(\n                label[\"parsed_output\"], (list, dict)\n            ):\n                raise ValueError(f\"Unsupported type: {type(label['parsed_output'])}\")\n\n            processed_label = {\n                # Since all the generations for the same `model_name` also share the same\n                # `prompt_used`, then we just keep the first element in `generations`\n                \"labelling_model\": label[\"model_name\"],\n                \"labelling_prompt\": label[\"prompt_used\"],\n                \"raw_labelling_response\": label[\"raw_output\"],\n            }\n            try:\n                if isinstance(label[\"parsed_output\"], list):\n                    processed_label.update(**combine_dicts(*label[\"parsed_output\"]))\n                elif isinstance(label[\"parsed_output\"], dict):\n                    processed_label.update(**label[\"parsed_output\"])\n            except Exception as e:\n                warnings.warn(\n                    f\"Label processing step failed when combining dicts: {e}\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n            processed_labels.append(processed_label)\n        return processed_labels\n\n    def _transform_dataset_to_expected_format(\n        self, rows: Dict[str, List[Any]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Transforms the `datasets.Dataset` to the expected format required by the LLMs\n        during the `generate` process.\n\n        Args:\n            rows (Dict[str, List[Any]]): the rows to be transformed.\n\n        Returns:\n            List[Dict[str, Any]]: the transformed rows.\n        \"\"\"\n        length = len(next(iter(rows.values())))\n\n        generator_column_names = []\n        if self.generator is not None:\n            generator_column_names = self.generator.task.input_args_names\n        labeller_column_names = []\n        if self.labeller is not None:\n            labeller_column_names = self.labeller.task.input_args_names\n        column_names = generator_column_names + labeller_column_names\n\n        inputs = []\n        for i in range(length):\n            input = {\n                col: values[i] for col, values in rows.items() if col in column_names\n            }\n            inputs.append(input)\n\n        return inputs\n\n    def _build_dataset(  # noqa: C901\n        self,\n        dataset: Dataset,\n        generations: List[Dict[str, Any]],\n        labels: List[List[\"LLMOutput\"]],\n        batch_size: int,\n    ) -&gt; CustomDataset:\n        \"\"\"Builds the final dataset with either the generations, the labels, or both, depending\n        on the LLMs provided to the `Pipeline`.\n\n        Args:\n            dataset (Dataset): the original dataset.\n            generations (List[Dict[str, Any]]): the processed generations.\n            labels (List[List[LLMOutput]]): the processed labels.\n\n        Returns:\n            CustomDataset: the final dataset.\n\n        Raises:\n            RuntimeError: if the `Pipeline` fails during the generation or labelling steps.\n        \"\"\"\n        if self.generator is None:\n            generations = [{} for _ in range(len(dataset))]\n        else:\n            generator_column_names = [\n                \"generation_model\",\n                \"generation_prompt\",\n                \"raw_generation_responses\",\n            ] + self.generator.task.output_args_names\n\n            # Add missing keys/columns with a `None` value\n            for generation in generations:\n                for key in generator_column_names:\n                    if key not in generation:\n                        generation.update({key: None})\n\n        if self.labeller is None:\n            processed_labels = [{} for _ in range(len(dataset))]  # type: ignore\n        else:\n            batch_labels = []\n            for label in labels:\n                batch_labels.extend(label)\n\n            processed_labels = self._process_batch_labels(\n                batch_labels=batch_labels or cast(List[List[\"LLMOutput\"]], labels)\n            )\n\n            labeller_column_names = [\n                \"labelling_model\",\n                \"labelling_prompt\",\n                \"raw_labelling_response\",\n            ] + self.labeller.task.output_args_names\n\n            # Ensure the lengths of the labels and the dataset match (when pipeline\n            # fails in an intermediate step, the labels may be shorter than the dataset)\n            if len(processed_labels) &lt; len(dataset):\n                processed_labels.extend(\n                    [\n                        {key: None for key in labeller_column_names}\n                        for _ in range(len(dataset) - len(processed_labels))\n                    ]\n                )\n\n            # Add missing keys/columns with a `None` value\n            for label in processed_labels:\n                for key in labeller_column_names:\n                    if key not in label:\n                        label.update({key: None})\n\n        _flattened_dataset = dataset.flatten_indices()\n        _dataset = Dataset.from_dict({}, split=Split.TRAIN)\n        for row, generation, processed_label in zip(\n            _flattened_dataset, generations, processed_labels\n        ):\n            _dataset = _dataset.add_item({**row, **generation, **processed_label})  # type: ignore\n        # Dynamically remaps the `datasets.Dataset` to be a `CustomDataset` instance\n        _dataset.__class__ = CustomDataset\n        if self.generator is not None and self.labeller is None:\n            if self.generator.task.__type__ != \"generation\":  # type: ignore\n                self.generator.task.__type__ = \"generation\"  # type: ignore\n            _dataset.task = self.generator.task  # type: ignore\n        elif self.labeller is not None:\n            if self.labeller.task.__type__ != \"labelling\":  # type: ignore\n                self.labeller.task.__type__ = \"labelling\"  # type: ignore\n            _dataset.task = self.labeller.task  # type: ignore\n        return _dataset  # type: ignore\n\n    def _teardown(self) -&gt; None:\n        if self.generator is not None and isinstance(\n            self.generator, (ProcessLLM, LLMPool)\n        ):\n            self.generator.teardown()\n\n        if self.labeller is not None and isinstance(self.labeller, ProcessLLM):\n            self.labeller.teardown()\n\n    def _generate(  # noqa: C901\n        self,\n        dataset: Dataset,\n        num_generations: int = 1,\n        batch_size: int = 1,\n        shuffle_before_labelling: bool = True,\n        checkpoint_strategy: Optional[DatasetCheckpoint] = DatasetCheckpoint(),\n        display_progress_bar: bool = False,\n    ) -&gt; CustomDataset:\n        \"\"\"Generates the outputs for the given dataset using the LLMs provided to the\n        `Pipeline`.\"\"\"\n        if (\n            self.labeller is not None\n            and self.generator is not None\n            and num_generations &lt; 2\n        ):\n            warnings.warn(\n                f\"Provided `num_generations={num_generations}` which implies that the \"\n                \"`generator` LLM will just run once, while the `labelling` LLM expects \"\n                \"to receive a list of N inputs to label, where N is &gt; 1. If this is not \"\n                \"intended, make sure to set `num_generations` to a value higher or \"\n                \"equal to 2.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n        self._validate_dataset(dataset)\n\n        generations: List[Dict[str, Any]] = []\n        labels: Union[\n            List[List[\"LLMOutput\"]],\n            Future[List[List[\"LLMOutput\"]]],\n        ] = []\n\n        (\n            generation_progress_func,\n            labelling_progress_func,\n        ) = get_progress_bars_for_pipeline(\n            num_rows=len(dataset),\n            num_generations=num_generations,\n            display_progress_bar=display_progress_bar,\n            has_labeller=True if self.labeller else False,\n            has_generator=True if self.generator else False,\n            batch_size=batch_size,\n        )\n\n        num_batches = math.ceil(len(dataset) / batch_size)\n\n        for batch_i, rows in enumerate(dataset.iter(batch_size=batch_size), start=1):\n            logger.info(f\"Processing batch {batch_i} of {num_batches}...\")\n            inputs = self._transform_dataset_to_expected_format(rows)  # type: ignore\n\n            if self.generator is not None:\n                logger.info(f\"Calling generator for batch {batch_i}...\")\n                try:\n                    batch_generations = self._get_batch_generations(\n                        inputs=inputs,\n                        num_generations=num_generations,\n                        shuffle_before_labelling=shuffle_before_labelling,\n                        progress_callback_func=generation_progress_func,\n                    )\n                    generations.extend(batch_generations)\n\n                except Exception as e:\n                    if not checkpoint_strategy:\n                        raise RuntimeError(\n                            \"`Pipeline.generate` failed during generation step. Passing a `DatasetCheckpoint` is recommended!\"\n                        ) from e\n                    logger.error(\n                        f\"`Pipeline.generate` failed during generation step with exception: {e}\"\n                    )\n\n                    return self._build_dataset(\n                        dataset,\n                        generations=generations,\n                        labels=labels,\n                        batch_size=batch_size,\n                    )\n\n                inputs = self._include_generator_outputs_as_inputs(\n                    inputs=inputs, outputs=batch_generations\n                )\n\n            if self.labeller is not None:\n                logger.info(f\"Calling labeller for batch {batch_i}...\")\n                try:\n                    batch_labels = self._get_batch_labels(\n                        inputs=inputs,\n                        progress_callback_func=labelling_progress_func,\n                        batch_size=batch_size,\n                    )\n                    labels.extend(batch_labels)  # type: ignore\n\n                except Exception as e:\n                    if not checkpoint_strategy:\n                        raise RuntimeError(\n                            \"`Pipeline.generate` failed during labelling step. Passing a `DatasetCheckpoint` is recommended!\"\n                        ) from e\n                    logger.error(\n                        f\"`Pipeline.generate` failed during labelling step with exception: {e}\"\n                    )\n\n                    return self._build_dataset(\n                        dataset,\n                        generations=generations,\n                        labels=labels,\n                        batch_size=batch_size,\n                    )\n\n            if checkpoint_strategy and checkpoint_strategy.do_checkpoint(\n                batch_i * batch_size\n            ):\n                logger.info(\n                    f\"Saving dataset up to batch {batch_i} at {checkpoint_strategy.path}...\"\n                )\n                ds = self._build_dataset(\n                    dataset,\n                    generations=generations,\n                    labels=labels,\n                    batch_size=batch_size,\n                )\n                checkpoint_strategy.save(ds)\n\n        _pipeline_progress.stop()\n\n        ds = self._build_dataset(\n            dataset, generations=generations, labels=labels, batch_size=batch_size\n        )\n        if checkpoint_strategy:\n            checkpoint_strategy.save(ds)\n            logger.info(f\"Final dataset saved at {checkpoint_strategy.path}\")\n\n        return ds\n\n    def dry_run(self, dataset: Dataset) -&gt; CustomDataset:\n        \"\"\"Performs a dry run over the provided dataset, which consists on generating the\n        outputs for the first row of the dataset, to ensure that the `Pipeline` will be\n        able to generate the outputs for the whole dataset.\n\n        Args:\n            dataset (Dataset): the dataset to be used for generation. Just the first row\n                will be used for the dry run.\n\n        Returns:\n            CustomDataset: the dataset containing the outputs for the first row.\n        \"\"\"\n        try:\n            # First we generate a `Dataset` only with the first row from the whole dataset\n            subset = Dataset.from_dict(\n                {key: [value] for key, value in dataset[0].items()}\n            )\n            # Then we call the `_generate` method with it\n            return self._generate(\n                dataset=subset,\n                # Default kwargs to make the process as simple as possible\n                num_generations=1,\n                batch_size=1,\n                checkpoint_strategy=None,\n                display_progress_bar=False,\n            )\n        except Exception as e:\n            self._teardown()\n            raise RuntimeError(\n                f\"`Pipeline.generate` failed during the dry run over {dataset[0]} with exception: {e}\"\n            ) from e\n\n    def generate(\n        self,\n        dataset: Dataset,\n        num_generations: int = 1,\n        batch_size: int = 1,\n        shuffle_before_labelling: bool = True,\n        checkpoint_strategy: Optional[DatasetCheckpoint] = DatasetCheckpoint(),\n        display_progress_bar: bool = False,\n        skip_dry_run: bool = False,\n    ) -&gt; CustomDataset:\n        \"\"\"Generates the outputs for the given dataset using the LLMs provided to the `Pipeline`.\n\n        Args:\n            dataset (Dataset): the dataset to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to `1`.\n            batch_size (int, optional): the batch size to be used for generation. Defaults to `1`.\n            shuffle_before_labelling: whether to shuffle the generations before labelling\n                or not. This is useful to avoid the labelling LLM to be biased by the order\n                of the generations. Defaults to `True`.\n            checkpoint_strategy (DatasetCheckpoint, optional): the checkpoint strategy.\n                If `None` is provided, no checkpoints will be saved. Defaults to `DatasetCheckpoint()`,\n                which won't save the dataset but returns the generated dataset upon failure.\n            display_progress_bar (bool, optional): whether to display the progress bar or not. Defaults to `False`.\n            skip_dry_run (bool, optional): whether to skip the dry run or not. Defaults to `False`.\n\n        Returns:\n            CustomDataset: the final dataset.\n\n        Raises:\n            RuntimeError: if the `Pipeline` fails during the generation or labelling steps.\n            UserWarning: if the `Pipeline` fails during the generation or labelling steps and\n                `checkpoint_strategy` is set to `None`.\n\n        Examples:\n            &gt;&gt;&gt; from transformers import AutoModelForCaualLM, AutoTokenizer\n            &gt;&gt;&gt; from distilabel.llm import OpenAILLM, TransformersLLM\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask, UltraFeedbackTask\n            &gt;&gt;&gt; from distilabel.pipeline import Pipeline\n            &gt;&gt;&gt; generator = TransformersLLM(\n            ...     model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n            ...     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n            ...     task=TextGenerationTask(),\n            ...     prompt_format=\"llama2\",\n            ... )\n            &gt;&gt;&gt; labeller = OpenAILLM(\n            ...     model=\"gpt-3.5-turbo\",\n            ...     task=UltraFeedbackTask.for_overall_quality(),\n            ... )\n            &gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n            &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n        \"\"\"\n        if not skip_dry_run:\n            logger.info(\"Executing dry-run...\")\n            self.dry_run(dataset)\n            logger.info(\n                \"Dry-run executed with no issues. Starting the actual generation...\"\n            )\n\n        dataset = use_progress_bar(self._generate)(\n            dataset=dataset,\n            num_generations=num_generations,\n            batch_size=batch_size,\n            checkpoint_strategy=checkpoint_strategy,\n            shuffle_before_labelling=shuffle_before_labelling,\n            display_progress_bar=display_progress_bar,\n        )\n\n        self._teardown()\n\n        return dataset\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline.__init__","title":"<code>__init__(generator=None, labeller=None)</code>","text":"<p>Initializes the Pipeline class.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Union['LLM', None]</code> <p>the LLM to be used for generation. Defaults to None.</p> <code>None</code> <code>labeller</code> <code>Union['LLM', None]</code> <p>the LLM to be used for labelling. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if no LLM is provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n&gt;&gt;&gt; from distilabel.llm import OpenAILLM, TransformersLLM\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask, UltraFeedbackTask\n&gt;&gt;&gt; from distilabel.pipeline import Pipeline\n&gt;&gt;&gt; generator = TransformersLLM(\n...     model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n...     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n...     task=TextGenerationTask(),\n...     prompt_format=\"llama2\",\n... )\n&gt;&gt;&gt; labeller = OpenAILLM(\n...     model=\"gpt-3.5-turbo\",\n...     task=UltraFeedbackTask.for_overall_quality(),\n... )\n&gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n&gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n</code></pre> Source code in <code>src/distilabel/pipeline.py</code> <pre><code>def __init__(\n    self,\n    generator: Union[\"LLM\", \"ProcessLLM\", \"LLMPool\", None] = None,\n    labeller: Union[\"LLM\", \"ProcessLLM\", None] = None,\n) -&gt; None:\n    \"\"\"Initializes the Pipeline class.\n\n    Args:\n        generator (Union[\"LLM\", None], optional): the LLM to be used for generation.\n            Defaults to None.\n        labeller (Union[\"LLM\", None], optional): the LLM to be used for labelling.\n            Defaults to None.\n\n    Raises:\n        ValueError: if no LLM is provided.\n\n    Examples:\n        &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n        &gt;&gt;&gt; from distilabel.llm import OpenAILLM, TransformersLLM\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask, UltraFeedbackTask\n        &gt;&gt;&gt; from distilabel.pipeline import Pipeline\n        &gt;&gt;&gt; generator = TransformersLLM(\n        ...     model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n        ...     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n        ...     task=TextGenerationTask(),\n        ...     prompt_format=\"llama2\",\n        ... )\n        &gt;&gt;&gt; labeller = OpenAILLM(\n        ...     model=\"gpt-3.5-turbo\",\n        ...     task=UltraFeedbackTask.for_overall_quality(),\n        ... )\n        &gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n        &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n    \"\"\"\n    if generator is not None and not isinstance(\n        generator, (LLM, ProcessLLM, LLMPool)\n    ):\n        raise ValueError(\n            \"`generator` must be an instance of `LLM`, `ProcessLLM` or `LLMPool`\"\n        )\n\n    if labeller is not None and not isinstance(labeller, (LLM, ProcessLLM)):\n        raise ValueError(\"`labeller` must be an instance of `LLM` or `ProcessLLM`\")\n\n    self.generator = generator\n    self.labeller = labeller\n\n    if self.generator is None and self.labeller is None:\n        raise ValueError(\"Either `generator` or `labeller` must be provided.\")\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline.dry_run","title":"<code>dry_run(dataset)</code>","text":"<p>Performs a dry run over the provided dataset, which consists on generating the outputs for the first row of the dataset, to ensure that the <code>Pipeline</code> will be able to generate the outputs for the whole dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>the dataset to be used for generation. Just the first row will be used for the dry run.</p> required <p>Returns:</p> Name Type Description <code>CustomDataset</code> <code>CustomDataset</code> <p>the dataset containing the outputs for the first row.</p> Source code in <code>src/distilabel/pipeline.py</code> <pre><code>def dry_run(self, dataset: Dataset) -&gt; CustomDataset:\n    \"\"\"Performs a dry run over the provided dataset, which consists on generating the\n    outputs for the first row of the dataset, to ensure that the `Pipeline` will be\n    able to generate the outputs for the whole dataset.\n\n    Args:\n        dataset (Dataset): the dataset to be used for generation. Just the first row\n            will be used for the dry run.\n\n    Returns:\n        CustomDataset: the dataset containing the outputs for the first row.\n    \"\"\"\n    try:\n        # First we generate a `Dataset` only with the first row from the whole dataset\n        subset = Dataset.from_dict(\n            {key: [value] for key, value in dataset[0].items()}\n        )\n        # Then we call the `_generate` method with it\n        return self._generate(\n            dataset=subset,\n            # Default kwargs to make the process as simple as possible\n            num_generations=1,\n            batch_size=1,\n            checkpoint_strategy=None,\n            display_progress_bar=False,\n        )\n    except Exception as e:\n        self._teardown()\n        raise RuntimeError(\n            f\"`Pipeline.generate` failed during the dry run over {dataset[0]} with exception: {e}\"\n        ) from e\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline.generate","title":"<code>generate(dataset, num_generations=1, batch_size=1, shuffle_before_labelling=True, checkpoint_strategy=DatasetCheckpoint(), display_progress_bar=False, skip_dry_run=False)</code>","text":"<p>Generates the outputs for the given dataset using the LLMs provided to the <code>Pipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>the dataset to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>batch_size</code> <code>int</code> <p>the batch size to be used for generation. Defaults to <code>1</code>.</p> <code>1</code> <code>shuffle_before_labelling</code> <code>bool</code> <p>whether to shuffle the generations before labelling or not. This is useful to avoid the labelling LLM to be biased by the order of the generations. Defaults to <code>True</code>.</p> <code>True</code> <code>checkpoint_strategy</code> <code>DatasetCheckpoint</code> <p>the checkpoint strategy. If <code>None</code> is provided, no checkpoints will be saved. Defaults to <code>DatasetCheckpoint()</code>, which won't save the dataset but returns the generated dataset upon failure.</p> <code>DatasetCheckpoint()</code> <code>display_progress_bar</code> <code>bool</code> <p>whether to display the progress bar or not. Defaults to <code>False</code>.</p> <code>False</code> <code>skip_dry_run</code> <code>bool</code> <p>whether to skip the dry run or not. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>CustomDataset</code> <code>CustomDataset</code> <p>the final dataset.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>Pipeline</code> fails during the generation or labelling steps.</p> <code>UserWarning</code> <p>if the <code>Pipeline</code> fails during the generation or labelling steps and <code>checkpoint_strategy</code> is set to <code>None</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from transformers import AutoModelForCaualLM, AutoTokenizer\n&gt;&gt;&gt; from distilabel.llm import OpenAILLM, TransformersLLM\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask, UltraFeedbackTask\n&gt;&gt;&gt; from distilabel.pipeline import Pipeline\n&gt;&gt;&gt; generator = TransformersLLM(\n...     model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n...     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n...     task=TextGenerationTask(),\n...     prompt_format=\"llama2\",\n... )\n&gt;&gt;&gt; labeller = OpenAILLM(\n...     model=\"gpt-3.5-turbo\",\n...     task=UltraFeedbackTask.for_overall_quality(),\n... )\n&gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n&gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n</code></pre> Source code in <code>src/distilabel/pipeline.py</code> <pre><code>def generate(\n    self,\n    dataset: Dataset,\n    num_generations: int = 1,\n    batch_size: int = 1,\n    shuffle_before_labelling: bool = True,\n    checkpoint_strategy: Optional[DatasetCheckpoint] = DatasetCheckpoint(),\n    display_progress_bar: bool = False,\n    skip_dry_run: bool = False,\n) -&gt; CustomDataset:\n    \"\"\"Generates the outputs for the given dataset using the LLMs provided to the `Pipeline`.\n\n    Args:\n        dataset (Dataset): the dataset to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each\n            input. Defaults to `1`.\n        batch_size (int, optional): the batch size to be used for generation. Defaults to `1`.\n        shuffle_before_labelling: whether to shuffle the generations before labelling\n            or not. This is useful to avoid the labelling LLM to be biased by the order\n            of the generations. Defaults to `True`.\n        checkpoint_strategy (DatasetCheckpoint, optional): the checkpoint strategy.\n            If `None` is provided, no checkpoints will be saved. Defaults to `DatasetCheckpoint()`,\n            which won't save the dataset but returns the generated dataset upon failure.\n        display_progress_bar (bool, optional): whether to display the progress bar or not. Defaults to `False`.\n        skip_dry_run (bool, optional): whether to skip the dry run or not. Defaults to `False`.\n\n    Returns:\n        CustomDataset: the final dataset.\n\n    Raises:\n        RuntimeError: if the `Pipeline` fails during the generation or labelling steps.\n        UserWarning: if the `Pipeline` fails during the generation or labelling steps and\n            `checkpoint_strategy` is set to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from transformers import AutoModelForCaualLM, AutoTokenizer\n        &gt;&gt;&gt; from distilabel.llm import OpenAILLM, TransformersLLM\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask, UltraFeedbackTask\n        &gt;&gt;&gt; from distilabel.pipeline import Pipeline\n        &gt;&gt;&gt; generator = TransformersLLM(\n        ...     model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n        ...     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n        ...     task=TextGenerationTask(),\n        ...     prompt_format=\"llama2\",\n        ... )\n        &gt;&gt;&gt; labeller = OpenAILLM(\n        ...     model=\"gpt-3.5-turbo\",\n        ...     task=UltraFeedbackTask.for_overall_quality(),\n        ... )\n        &gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n        &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n    \"\"\"\n    if not skip_dry_run:\n        logger.info(\"Executing dry-run...\")\n        self.dry_run(dataset)\n        logger.info(\n            \"Dry-run executed with no issues. Starting the actual generation...\"\n        )\n\n    dataset = use_progress_bar(self._generate)(\n        dataset=dataset,\n        num_generations=num_generations,\n        batch_size=batch_size,\n        checkpoint_strategy=checkpoint_strategy,\n        shuffle_before_labelling=shuffle_before_labelling,\n        display_progress_bar=display_progress_bar,\n    )\n\n    self._teardown()\n\n    return dataset\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.pipeline","title":"<code>pipeline(task, subtask=None, *, generator=None, labeller=None, **kwargs)</code>","text":"<p>Creates a <code>Pipeline</code> instance with the provided LLMs for a given task, which is useful whenever you want to use a pre-defined <code>Pipeline</code> for a given task, or if you want to create a custom <code>Pipeline</code> for a given task. Ideally one using this function over the <code>Pipeline</code> class, don't want to worry about the details of the <code>labeller</code>, since it will come with a default configuration based on the <code>task</code>, by default the LLM used for <code>labelling</code> will always be <code>gpt-3.5-turbo</code> from OpenAI, as it's the one that provides the most consistent and fast results.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Literal['preference', 'critique']</code> <p>the task to be performed by the <code>Pipeline</code>.</p> required <code>subtask</code> <code>Optional[str]</code> <p>the subtask to be performed by the <code>Pipeline</code>. Defaults to None.</p> <code>None</code> <code>generator</code> <code>Optional['LLM']</code> <p>the LLM to be used for generation. Defaults to None.</p> <code>None</code> <code>labeller</code> <code>Optional['LLM']</code> <p>the LLM to be used for labelling. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>the keyword arguments to be passed to the <code>task</code> and <code>subtask</code> classes.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if an invalid task is provided.</p> <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>the <code>Pipeline</code> instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n&gt;&gt;&gt; from distilabel.llm import TransformersLLM\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.pipeline import pipeline\n&gt;&gt;&gt; generator = TransformersLLM(\n...     model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n...     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n...     task=TextGenerationTask(),\n...     prompt_format=\"llama2\",\n... )\n&gt;&gt;&gt; pipeline = pipeline(\n...     task=\"preference\",\n...     subtask=\"text-quality\",\n...     generator=generator,\n... )\n&gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n</code></pre> Source code in <code>src/distilabel/pipeline.py</code> <pre><code>def pipeline(\n    task: Literal[\"preference\"],\n    subtask: Optional[str] = None,\n    *,\n    generator: Optional[\"LLM\"] = None,\n    labeller: Optional[\"LLM\"] = None,\n    **kwargs: Any,\n) -&gt; Pipeline:\n    \"\"\"Creates a `Pipeline` instance with the provided LLMs for a given task, which is useful\n    whenever you want to use a pre-defined `Pipeline` for a given task, or if you want to\n    create a custom `Pipeline` for a given task. Ideally one using this function over the `Pipeline`\n    class, don't want to worry about the details of the `labeller`, since it will come with a default\n    configuration based on the `task`, by default the LLM used for `labelling` will always be `gpt-3.5-turbo`\n    from OpenAI, as it's the one that provides the most consistent and fast results.\n\n    Args:\n        task (Literal[\"preference\", \"critique\"]): the task to be performed by the `Pipeline`.\n        subtask (Optional[str], optional): the subtask to be performed by the `Pipeline`.\n            Defaults to None.\n        generator (Optional[\"LLM\"], optional): the LLM to be used for generation. Defaults to None.\n        labeller (Optional[\"LLM\"], optional): the LLM to be used for labelling. Defaults to None.\n        **kwargs: the keyword arguments to be passed to the `task` and `subtask` classes.\n\n    Raises:\n        ValueError: if an invalid task is provided.\n\n    Returns:\n        Pipeline: the `Pipeline` instance.\n\n    Examples:\n        &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n        &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.pipeline import pipeline\n        &gt;&gt;&gt; generator = TransformersLLM(\n        ...     model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n        ...     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\"),\n        ...     task=TextGenerationTask(),\n        ...     prompt_format=\"llama2\",\n        ... )\n        &gt;&gt;&gt; pipeline = pipeline(\n        ...     task=\"preference\",\n        ...     subtask=\"text-quality\",\n        ...     generator=generator,\n        ... )\n        &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n    \"\"\"\n    if task == \"preference\":\n        if labeller is None:\n            from dataclasses import fields\n\n            from distilabel.llm.openai import OpenAILLM\n            from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n\n            task_cls = UltraFeedbackTask\n            task_kwargs = {\n                key: kwargs.get(key.name)\n                for key in fields(task_cls)\n                if key.name in kwargs and not key.name.startswith(\"__\")\n            }\n\n            # Dynamically call the appropriate classmethod using getattr\n            if subtask is not None:\n                if subtask not in task_cls.__subtasks__:\n                    raise ValueError(\n                        f\"Invalid subtask: {subtask}, available subtasks are {task_cls.__subtasks__}\"\n                    )\n                classmethod_name = f\"for_{subtask.lower().replace('-', '_')}\"\n                if hasattr(task_cls, classmethod_name):\n                    task_cls = getattr(task_cls, classmethod_name)\n\n            logger.info(\n                \"Since no `labeller` was provided, `OpenAILLM` will be used as the default labeller with `UltraFeedback`.\"\n            )\n\n            labeller = OpenAILLM(\n                model=kwargs.get(\"openai_model\") or \"gpt-3.5-turbo\",\n                task=task_cls(**task_kwargs),  # type: ignore\n                max_new_tokens=kwargs.get(\"max_new_tokens\") or 256,\n                num_threads=kwargs.get(\"num_threads\") or 4,\n                api_key=kwargs.get(\"api_key\") or os.getenv(\"OPENAI_API_KEY\"),\n                temperature=kwargs.get(\"temperature\") or 0.0,\n            )\n        else:\n            from distilabel.tasks.preference.judgelm import JudgeLMTask\n            from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n            from distilabel.tasks.preference.ultrajudge import UltraJudgeTask\n\n            if not isinstance(\n                labeller.task, (UltraFeedbackTask, JudgeLMTask, UltraJudgeTask)\n            ):\n                warnings.warn(\n                    \"The `labeller` task for `preference` must be an instance of `UltraFeedbackTask`,\"\n                    f\" `JudgeLMTask` or `UltraJudge`, got {labeller.task.__class__.__name__}.\"\n                    \"If you are planning to use a custom `labeller` for a `preference` \"\n                    \"task, use it at your own risk.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n        if generator is not None:\n            assert (\n                generator.task.input_args_names + generator.task.output_args_names\n                == labeller.task.input_args_names\n            ), (\n                f\"`generator` outputs do not match `labeller` inputs: \"\n                f\"{generator.task.input_args_names + generator.task.output_args_names} != {labeller.task.input_args_names}\"\n            )\n    else:\n        raise ValueError(f\"Invalid task: {task}, available tasks are: `preference`.\")\n\n    return Pipeline(generator=generator, labeller=labeller)\n</code></pre>"},{"location":"reference/distilabel/progress_bar/","title":"progress_bar","text":""},{"location":"reference/distilabel/llm/","title":"llm","text":""},{"location":"reference/distilabel/llm/#distilabel.llm.AnyscaleLLM","title":"<code>AnyscaleLLM</code>","text":"<p>             Bases: <code>OpenAILLM</code></p> Source code in <code>src/distilabel/llm/anyscale.py</code> <pre><code>class AnyscaleLLM(OpenAILLM):\n    def __init__(\n        self,\n        model: str,\n        task: \"Task\",\n        client: Union[\"OpenAI\", None] = None,\n        api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the AnyscaleLLM class.\n\n        Args:\n            model (str): the model to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n                If `None`, a new client will be created. Defaults to `None`.\n            api_key (Union[str, None], optional): the Anyscale API key to be used for generation.\n                If `None`, the `ANYSCALE_API_KEY` environment variable will be used. Defaults to `None`.\n                Visit \"https://docs.endpoints.anyscale.com/guides/authenticate/\" for more information.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in your OpenAI account.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import AnyscaleLLM\n            &gt;&gt;&gt; llm = AnyscaleLLM(model=\"HuggingFaceH4/zephyr-7b-beta\", task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        LLM.__init__(\n            self,\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _OPENAI_AVAILABLE:\n            raise ImportError(\n                \"`AnyscaleLLM` cannot be used as `openai` is not installed, please \"\n                \" install it with `pip install openai`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n\n        self.client = client or OpenAI(\n            api_key=api_key or os.getenv(\"ANYSCALE_API_KEY\"),\n            max_retries=6,\n            base_url=\"https://api.endpoints.anyscale.com/v1\",\n        )\n\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in your Anyscale account, available models are {self.available_models}\"\n        self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.AnyscaleLLM.__init__","title":"<code>__init__(model, task, client=None, api_key=None, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the AnyscaleLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>the model to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>client</code> <code>Union[OpenAI, None]</code> <p>an OpenAI client to be used for generation. If <code>None</code>, a new client will be created. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Union[str, None]</code> <p>the Anyscale API key to be used for generation. If <code>None</code>, the <code>ANYSCALE_API_KEY</code> environment variable will be used. Defaults to <code>None</code>. Visit \"https://docs.endpoints.anyscale.com/guides/authenticate/\" for more information.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in your OpenAI account.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import AnyscaleLLM\n&gt;&gt;&gt; llm = AnyscaleLLM(model=\"HuggingFaceH4/zephyr-7b-beta\", task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/anyscale.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    task: \"Task\",\n    client: Union[\"OpenAI\", None] = None,\n    api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the AnyscaleLLM class.\n\n    Args:\n        model (str): the model to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n            If `None`, a new client will be created. Defaults to `None`.\n        api_key (Union[str, None], optional): the Anyscale API key to be used for generation.\n            If `None`, the `ANYSCALE_API_KEY` environment variable will be used. Defaults to `None`.\n            Visit \"https://docs.endpoints.anyscale.com/guides/authenticate/\" for more information.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in your OpenAI account.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import AnyscaleLLM\n        &gt;&gt;&gt; llm = AnyscaleLLM(model=\"HuggingFaceH4/zephyr-7b-beta\", task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    LLM.__init__(\n        self,\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _OPENAI_AVAILABLE:\n        raise ImportError(\n            \"`AnyscaleLLM` cannot be used as `openai` is not installed, please \"\n            \" install it with `pip install openai`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.frequency_penalty = frequency_penalty\n    self.presence_penalty = presence_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n\n    self.client = client or OpenAI(\n        api_key=api_key or os.getenv(\"ANYSCALE_API_KEY\"),\n        max_retries=6,\n        base_url=\"https://api.endpoints.anyscale.com/v1\",\n    )\n\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in your Anyscale account, available models are {self.available_models}\"\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.InferenceEndpointsLLM","title":"<code>InferenceEndpointsLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>class InferenceEndpointsLLM(LLM):\n    def __init__(\n        self,\n        endpoint_name_or_model_id: str,\n        task: \"Task\",\n        endpoint_namespace: Union[str, None] = None,\n        token: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        repetition_penalty: Union[float, None] = None,\n        seed: Union[int, None] = None,\n        do_sample: bool = False,\n        temperature: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n        top_p: Union[float, None] = None,\n        typical_p: Union[float, None] = None,\n        stop_sequences: Union[List[str], None] = None,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the InferenceEndpointsLLM class.\n\n        Args:\n            endpoint_name_or_model_id (str): The name of the endpoint or a Hugging Face Model Id.\n            task (Task): The task to be performed by the LLM.\n            endpoint_namespace (Union[str, None]): The namespace of the endpoint. Defaults to None.\n            token (Union[str, None]): The token for the endpoint. Defaults to None.\n            max_new_tokens (int): The maximum number of tokens to be generated. Defaults to 128.\n            repetition_penalty (Union[float, None]): The repetition penalty to be used for generation. Defaults to None.\n            seed (Union[int, None]): The seed for generation. Defaults to None.\n            do_sample (bool): Whether to do sampling. Defaults to False.\n            temperature (Union[float, None]): The temperature for generation. Defaults to None.\n            top_k (Union[int, None]): The top_k for generation. Defaults to None.\n            top_p (Union[float, None]): The top_p for generation. Defaults to None.\n            typical_p (Union[float, None]): The typical_p for generation. Defaults to None.\n            stop_sequences (Union[List[str], None]): The stop sequences for generation. Defaults to None.\n            num_threads (Union[int, None]): The number of threads. Defaults to None.\n            prompt_format (Union[\"SupportedFormats\", None]): The format of the prompt. Defaults to None.\n            prompt_formatting_fn (Union[Callable[..., str], None]): The function for formatting the prompt. Defaults to None.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n            &gt;&gt;&gt; llm = InferenceEndpointsLLM(\n            ...     endpoint_name_or_model_id=\"&lt;MODEL_ID_OR_INFERENCE_ENDPOINT&gt;\",\n            ...     task=TextGenerationTask(),\n            ... )\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _HUGGINGFACE_HUB_AVAILABLE:\n            raise ImportError(\n                \"`InferenceEndpointsLLM` cannot be used as `huggingface-hub` is not \"\n                \"installed, please install it with `pip install huggingface-hub`.\"\n            )\n\n        self.do_sample = do_sample\n        self.max_new_tokens = max_new_tokens\n        self.repetition_penalty = repetition_penalty\n        self.seed = seed\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.typical_p = typical_p\n        self.stop_sequences = stop_sequences\n\n        if is_serverless_endpoint_available(model_id=endpoint_name_or_model_id):\n            logger.info(\"Using Serverless Inference Endpoint\")\n            self.client = InferenceClient(model=endpoint_name_or_model_id, token=token)\n            self._model_name = endpoint_name_or_model_id\n        else:\n            logger.info(\"Using Dedicated Inference Endpoint\")\n            inference_endpoint = get_inference_endpoint(\n                name=endpoint_name_or_model_id,\n                namespace=endpoint_namespace,\n                token=token,\n            )\n            if inference_endpoint.status in [\"paused\", \"scaledToZero\"]:\n                logger.info(\"Waiting for Inference Endpoint to be ready...\")\n                inference_endpoint.resume().wait(timeout=30)\n\n            self.client = inference_endpoint.client\n            self._model_name = inference_endpoint.repository\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"do_sample\": self.do_sample,\n                \"max_new_tokens\": self.max_new_tokens,\n                \"repetition_penalty\": self.repetition_penalty,\n                \"seed\": self.seed,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"typical_p\": self.typical_p,\n                \"stop_sequences\": self.stop_sequences,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name of the endpoint.\"\"\"\n        return self._model_name\n\n    @retry(\n        retry=retry_if_exception_type(_INFERENCE_ENDPOINTS_API_RETRY_ON_EXCEPTIONS),\n        stop=stop_after_attempt(_INFERENCE_ENDPOINTS_API_STOP_AFTER_ATTEMPT),\n        wait=wait_random_exponential(\n            multiplier=_INFERENCE_ENDPOINTS_API_WAIT_RANDOM_EXPONENTIAL_MULTIPLIER,\n            max=_INFERENCE_ENDPOINTS_API_WAIT_RANDOM_EXPONENTIAL_MAX,\n        ),\n        before_sleep=before_sleep_log(logger, logging.INFO),\n        after=after_log(logger, logging.INFO),\n    )\n    def _text_generation_with_backoff(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Performs text generation with backoff in case of an error.\"\"\"\n        return self.client.text_generation(**kwargs)  # type: ignore\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        outputs = []\n        for prompt in prompts:\n            raw_responses = [\n                self._text_generation_with_backoff(\n                    prompt=prompt,\n                    do_sample=self.do_sample,\n                    max_new_tokens=self.max_new_tokens,\n                    repetition_penalty=self.repetition_penalty,\n                    seed=self.seed,\n                    temperature=self.temperature,\n                    top_k=self.top_k,\n                    top_p=self.top_p,\n                    typical_p=self.typical_p,\n                    stop_sequences=self.stop_sequences,\n                )\n                for _ in range(num_generations)\n            ]\n            output = []\n            for raw_response in raw_responses:\n                try:\n                    parsed_response = self.task.parse_output(raw_response)\n                except Exception as e:\n                    logger.error(f\"Error parsing Inference Endpoints output: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_response,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.InferenceEndpointsLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name of the endpoint.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.InferenceEndpointsLLM.__init__","title":"<code>__init__(endpoint_name_or_model_id, task, endpoint_namespace=None, token=None, max_new_tokens=128, repetition_penalty=None, seed=None, do_sample=False, temperature=None, top_k=None, top_p=None, typical_p=None, stop_sequences=None, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the InferenceEndpointsLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name_or_model_id</code> <code>str</code> <p>The name of the endpoint or a Hugging Face Model Id.</p> required <code>task</code> <code>Task</code> <p>The task to be performed by the LLM.</p> required <code>endpoint_namespace</code> <code>Union[str, None]</code> <p>The namespace of the endpoint. Defaults to None.</p> <code>None</code> <code>token</code> <code>Union[str, None]</code> <p>The token for the endpoint. Defaults to None.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>repetition_penalty</code> <code>Union[float, None]</code> <p>The repetition penalty to be used for generation. Defaults to None.</p> <code>None</code> <code>seed</code> <code>Union[int, None]</code> <p>The seed for generation. Defaults to None.</p> <code>None</code> <code>do_sample</code> <code>bool</code> <p>Whether to do sampling. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Union[float, None]</code> <p>The temperature for generation. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>Union[int, None]</code> <p>The top_k for generation. Defaults to None.</p> <code>None</code> <code>top_p</code> <code>Union[float, None]</code> <p>The top_p for generation. Defaults to None.</p> <code>None</code> <code>typical_p</code> <code>Union[float, None]</code> <p>The typical_p for generation. Defaults to None.</p> <code>None</code> <code>stop_sequences</code> <code>Union[List[str], None]</code> <p>The stop sequences for generation. Defaults to None.</p> <code>None</code> <code>num_threads</code> <code>Union[int, None]</code> <p>The number of threads. Defaults to None.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>The format of the prompt. Defaults to None.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>The function for formatting the prompt. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n&gt;&gt;&gt; llm = InferenceEndpointsLLM(\n...     endpoint_name_or_model_id=\"&lt;MODEL_ID_OR_INFERENCE_ENDPOINT&gt;\",\n...     task=TextGenerationTask(),\n... )\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>def __init__(\n    self,\n    endpoint_name_or_model_id: str,\n    task: \"Task\",\n    endpoint_namespace: Union[str, None] = None,\n    token: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    repetition_penalty: Union[float, None] = None,\n    seed: Union[int, None] = None,\n    do_sample: bool = False,\n    temperature: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n    top_p: Union[float, None] = None,\n    typical_p: Union[float, None] = None,\n    stop_sequences: Union[List[str], None] = None,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the InferenceEndpointsLLM class.\n\n    Args:\n        endpoint_name_or_model_id (str): The name of the endpoint or a Hugging Face Model Id.\n        task (Task): The task to be performed by the LLM.\n        endpoint_namespace (Union[str, None]): The namespace of the endpoint. Defaults to None.\n        token (Union[str, None]): The token for the endpoint. Defaults to None.\n        max_new_tokens (int): The maximum number of tokens to be generated. Defaults to 128.\n        repetition_penalty (Union[float, None]): The repetition penalty to be used for generation. Defaults to None.\n        seed (Union[int, None]): The seed for generation. Defaults to None.\n        do_sample (bool): Whether to do sampling. Defaults to False.\n        temperature (Union[float, None]): The temperature for generation. Defaults to None.\n        top_k (Union[int, None]): The top_k for generation. Defaults to None.\n        top_p (Union[float, None]): The top_p for generation. Defaults to None.\n        typical_p (Union[float, None]): The typical_p for generation. Defaults to None.\n        stop_sequences (Union[List[str], None]): The stop sequences for generation. Defaults to None.\n        num_threads (Union[int, None]): The number of threads. Defaults to None.\n        prompt_format (Union[\"SupportedFormats\", None]): The format of the prompt. Defaults to None.\n        prompt_formatting_fn (Union[Callable[..., str], None]): The function for formatting the prompt. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n        &gt;&gt;&gt; llm = InferenceEndpointsLLM(\n        ...     endpoint_name_or_model_id=\"&lt;MODEL_ID_OR_INFERENCE_ENDPOINT&gt;\",\n        ...     task=TextGenerationTask(),\n        ... )\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _HUGGINGFACE_HUB_AVAILABLE:\n        raise ImportError(\n            \"`InferenceEndpointsLLM` cannot be used as `huggingface-hub` is not \"\n            \"installed, please install it with `pip install huggingface-hub`.\"\n        )\n\n    self.do_sample = do_sample\n    self.max_new_tokens = max_new_tokens\n    self.repetition_penalty = repetition_penalty\n    self.seed = seed\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.typical_p = typical_p\n    self.stop_sequences = stop_sequences\n\n    if is_serverless_endpoint_available(model_id=endpoint_name_or_model_id):\n        logger.info(\"Using Serverless Inference Endpoint\")\n        self.client = InferenceClient(model=endpoint_name_or_model_id, token=token)\n        self._model_name = endpoint_name_or_model_id\n    else:\n        logger.info(\"Using Dedicated Inference Endpoint\")\n        inference_endpoint = get_inference_endpoint(\n            name=endpoint_name_or_model_id,\n            namespace=endpoint_namespace,\n            token=token,\n        )\n        if inference_endpoint.status in [\"paused\", \"scaledToZero\"]:\n            logger.info(\"Waiting for Inference Endpoint to be ready...\")\n            inference_endpoint.resume().wait(timeout=30)\n\n        self.client = inference_endpoint.client\n        self._model_name = inference_endpoint.repository\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.JSONOpenAILLM","title":"<code>JSONOpenAILLM</code>","text":"<p>             Bases: <code>OpenAILLM</code></p> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>class JSONOpenAILLM(OpenAILLM):\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"gpt-3.5-turbo-1106\",\n        client: Union[\"OpenAI\", None] = None,\n        api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the JSONOpenAILLM class for generating JSON.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n            client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n                If `None`, a new client will be created. Defaults to `None`.\n            api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n                If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in your OpenAI account.\n            AssertionError: if the provided `model` does not support JSON input.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import JSONOpenAILLM\n            &gt;&gt;&gt; llm = JSONOpenAILLM(task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"json for 'What's the capital of Spain?'\"}])\n        \"\"\"\n        super().__init__(\n            task,\n            model=model,\n            client=client,\n            api_key=api_key,\n            max_new_tokens=max_new_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in your OpenAI account.\"\"\"\n        all_available_models = [model.id for model in self.client.models.list().data]\n        json_supporting_models = [\n            \"gpt-4-0125-preview\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-3.5-turbo-1106\",\n        ]\n        available_json_supporting_models = list(\n            set(all_available_models) &amp; set(json_supporting_models)\n        )\n        return available_json_supporting_models\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs` in JSON format.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"openai\")\n        outputs = []\n        for prompt in prompts:\n            chat_completions = self.client.chat.completions.create(\n                messages=prompt,\n                model=self.model,\n                n=num_generations,\n                max_tokens=self.max_tokens,\n                frequency_penalty=self.frequency_penalty,\n                presence_penalty=self.presence_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                timeout=50,\n                response_format={\"type\": \"json_object\"},\n            )\n\n            output = []\n            for chat_completion in chat_completions.choices:\n                try:\n                    parsed_response = self.task.parse_output(\n                        chat_completion.message.content.strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing OpenAI response: {e}\")\n                    parsed_response = None\n                try:\n                    json.loads(chat_completion.message.content)\n                except json.JSONDecodeError:\n                    warnings.warn(\n                        \"The response is not a valid JSON.\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=chat_completion.message.content,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.JSONOpenAILLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in your OpenAI account.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.JSONOpenAILLM.__init__","title":"<code>__init__(task, model='gpt-3.5-turbo-1106', client=None, api_key=None, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the JSONOpenAILLM class for generating JSON.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo-1106'</code> <code>client</code> <code>Union[OpenAI, None]</code> <p>an OpenAI client to be used for generation. If <code>None</code>, a new client will be created. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Union[str, None]</code> <p>the OpenAI API key to be used for generation. If <code>None</code>, the <code>OPENAI_API_KEY</code> environment variable will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in your OpenAI account.</p> <code>AssertionError</code> <p>if the provided <code>model</code> does not support JSON input.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import JSONOpenAILLM\n&gt;&gt;&gt; llm = JSONOpenAILLM(task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"json for 'What's the capital of Spain?'\"}])\n</code></pre> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"gpt-3.5-turbo-1106\",\n    client: Union[\"OpenAI\", None] = None,\n    api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the JSONOpenAILLM class for generating JSON.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n        client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n            If `None`, a new client will be created. Defaults to `None`.\n        api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n            If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in your OpenAI account.\n        AssertionError: if the provided `model` does not support JSON input.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import JSONOpenAILLM\n        &gt;&gt;&gt; llm = JSONOpenAILLM(task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"json for 'What's the capital of Spain?'\"}])\n    \"\"\"\n    super().__init__(\n        task,\n        model=model,\n        client=client,\n        api_key=api_key,\n        max_new_tokens=max_new_tokens,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLM","title":"<code>LLM</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>class LLM(ABC):\n    def __init__(\n        self,\n        task: Task,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the LLM base class.\n\n        Note:\n            This class is intended to be used internally, but you anyone can still create\n            a subclass, implement the `abstractmethod`s and use it.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[\"SupportedFormats\", None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n        \"\"\"\n        self.task = task\n\n        self.thread_pool_executor = (\n            ThreadPoolExecutor(max_workers=num_threads)\n            if num_threads is not None\n            else None\n        )\n\n        self.prompt_format = prompt_format\n        self.prompt_formatting_fn = prompt_formatting_fn\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the thread pool executor if it is not `None`.\"\"\"\n        if self.thread_pool_executor is not None:\n            self.thread_pool_executor.shutdown()\n\n    @property\n    def num_threads(self) -&gt; Union[int, None]:\n        if self.thread_pool_executor:\n            return self.thread_pool_executor._max_workers\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(task={self.task.__class__.__name__}, num_threads={self.num_threads}, promp_format='{self.prompt_format}', model='{self.model_name}')\"\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"task\", self.task\n        yield \"num_threads\", self.num_threads\n        yield \"prompt_format\", self.prompt_format\n        if self.prompt_formatting_fn is not None:\n            args = f\"({', '.join(self.prompt_formatting_fn.__code__.co_varnames)})\"\n            representation = self.prompt_formatting_fn.__name__ + args\n            yield \"prompt_formatting_fn\", representation\n        yield \"model\", self.model_name\n\n    @property\n    @abstractmethod\n    def model_name(self) -&gt; str:\n        pass\n\n    def _generate_prompts(\n        self,\n        inputs: List[Dict[str, Any]],\n        default_format: Union[\"SupportedFormats\", None] = None,\n    ) -&gt; List[Any]:\n        \"\"\"Generates the prompts to be used for generation.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            default_format (Union[\"SupportedFormats\", None], optional): the default format to be used\n                for the prompt if no `prompt_format` is specified. Defaults to `None`.\n\n        Returns:\n            List[Any]: the generated prompts.\n\n        Raises:\n            ValueError: if the generated prompt is not of the expected type.\n        \"\"\"\n        prompts = []\n        for input in inputs:\n            prompt = self.task.generate_prompt(**input)\n            if not isinstance(prompt, Prompt) and self.prompt_formatting_fn is not None:\n                warnings.warn(\n                    \"The method `generate_prompt` is not returning a `Prompt` class but a prompt\"\n                    f\" of `type={type(prompt)}`, meaning that a pre-formatting has already been\"\n                    \" applied in the `task.generate_prompt` method, so the usage of a `prompt_formatting_fn`\"\n                    \" is discouraged.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                prompt = self.prompt_formatting_fn(prompt)\n            elif isinstance(prompt, Prompt) and self.prompt_formatting_fn is None:\n                if self.prompt_format is not None or default_format is not None:\n                    prompt = prompt.format_as(\n                        format=self.prompt_format or default_format  # type: ignore\n                    )\n                else:\n                    warnings.warn(\n                        \"No `prompt_format` has been specified and no `default_format` is set, so\"\n                        \" the prompt will be concatenated with a line-break and no specific formatting\"\n                        \" by default.\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n                    prompt = prompt.format_as(format=\"default\")\n            prompts.append(prompt)\n        return prompts\n\n    @abstractmethod\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        pass\n\n    def _get_valid_inputs(\n        self, inputs: List[Dict[str, Any]]\n    ) -&gt; Tuple[List[Dict[str, Any]], List[int]]:\n        \"\"\"Returns the valid inputs and the indices of the invalid inputs.\n\n        A valid input is an input that contains all the arguments required by the task.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n\n        Returns:\n            Tuple[List[Dict[str, Any]], List[int]]: a tuple containing the valid inputs and\n                the indices of the invalid inputs.\n        \"\"\"\n\n        valid_inputs = []\n        not_valid_inputs_indices = []\n        for i, input in enumerate(inputs):\n            if not all(input_arg in input for input_arg in self.task.input_args_names):\n                logger.warn(\n                    f\"Missing {self.task.__class__.__name__} input argument in batch element {i}\"\n                )\n                not_valid_inputs_indices.append(i)\n                continue\n\n            valid_inputs.append(input)\n\n        return valid_inputs, not_valid_inputs_indices\n\n    def _fill_missing_inputs(\n        self,\n        generations: List[List[LLMOutput]],\n        invalid_inputs_indices: List[int],\n        num_generations: int,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Fills the `generations` list with empty `LLMOutput`s for the inputs that were\n        not valid for the associated task of this `LLM`.\n\n        Args:\n            generations (List[List[LLMOutput]]): the generations to be filled.\n            invalid_inputs_indices (List[int]): the indices of the inputs that were not\n                valid for the associated task of this `LLM`.\n            num_generations (int): the number of generations to be performed for each input.\n\n        Returns:\n            List[List[LLMOutput]]: the filled generations.\n        \"\"\"\n\n        filled_generations = generations.copy()\n        for idx in invalid_inputs_indices:\n            filled_generations.insert(\n                idx,\n                [\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=None,\n                        raw_output=None,\n                        parsed_output=None,\n                    )\n                    for _ in range(num_generations)\n                ],\n            )\n        return filled_generations\n\n    def generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; Union[List[List[\"LLMOutput\"]], Future[List[List[\"LLMOutput\"]]]]:\n        \"\"\"Generates the outputs for the given inputs using the LLM.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each input.\n                Defaults to `1`.\n            progress_callback_func (Union[Callable, None], optional): a function to be called at each\n                generation step. Defaults to `None`.\n\n        Returns:\n            Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.\n        \"\"\"\n\n        def _progress():\n            if progress_callback_func is not None:\n                progress_callback_func(advance=num_generations * len(inputs))\n\n        valid_inputs, invalid_inputs_indices = self._get_valid_inputs(inputs)\n\n        if self.thread_pool_executor is not None:\n            futures = []\n            for input in valid_inputs:\n                future = self.thread_pool_executor.submit(\n                    self._generate, [input], num_generations\n                )\n                futures.append(future)\n            future = when_all_complete(\n                futures=futures,\n                callback=lambda generations: self._fill_missing_inputs(\n                    generations, invalid_inputs_indices, num_generations\n                ),\n            )\n            future.add_done_callback(lambda _: _progress())\n            return future\n\n        generations = self._generate(valid_inputs, num_generations)\n\n        generations = self._fill_missing_inputs(\n            generations, invalid_inputs_indices, num_generations\n        )\n\n        _progress()\n        return generations\n\n    @property\n    def return_futures(self) -&gt; bool:\n        \"\"\"Whether the `LLM` returns futures\"\"\"\n        return self.thread_pool_executor is not None\n\n    def validate_prompts(\n        self,\n        inputs: List[Dict[str, Any]],\n        default_format: Union[\"SupportedFormats\", None] = None,\n    ) -&gt; str:\n        \"\"\"Generates the prompts to be used for generation, can be used to check the prompts visually.\n\n        Args:\n            inputs (List[Dict[str, Any]]):\n                The inputs to be used for generation.\n\n        Returns:\n            str: The prompts that would be used for the generation.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; llm.validate_prompts([{\"input\": \"Your input\"}])[0]\n            You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n            I'm valid for text generation task\n        \"\"\"\n        return self._generate_prompts(inputs, default_format=default_format)\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLM.return_futures","title":"<code>return_futures: bool</code>  <code>property</code>","text":"<p>Whether the <code>LLM</code> returns futures</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLM.__del__","title":"<code>__del__()</code>","text":"<p>Shuts down the thread pool executor if it is not <code>None</code>.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shuts down the thread pool executor if it is not `None`.\"\"\"\n    if self.thread_pool_executor is not None:\n        self.thread_pool_executor.shutdown()\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLM.__init__","title":"<code>__init__(task, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the LLM base class.</p> Note <p>This class is intended to be used internally, but you anyone can still create a subclass, implement the <code>abstractmethod</code>s and use it.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union['SupportedFormats', None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __init__(\n    self,\n    task: Task,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the LLM base class.\n\n    Note:\n        This class is intended to be used internally, but you anyone can still create\n        a subclass, implement the `abstractmethod`s and use it.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[\"SupportedFormats\", None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n    \"\"\"\n    self.task = task\n\n    self.thread_pool_executor = (\n        ThreadPoolExecutor(max_workers=num_threads)\n        if num_threads is not None\n        else None\n    )\n\n    self.prompt_format = prompt_format\n    self.prompt_formatting_fn = prompt_formatting_fn\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLM.generate","title":"<code>generate(inputs, num_generations=1, progress_callback_func=None)</code>","text":"<p>Generates the outputs for the given inputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>the inputs to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>progress_callback_func</code> <code>Union[Callable, None]</code> <p>a function to be called at each generation step. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[List['LLMOutput']], Future[List[List['LLMOutput']]]]</code> <p>Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[Dict[str, Any]],\n    num_generations: int = 1,\n    progress_callback_func: Union[Callable, None] = None,\n) -&gt; Union[List[List[\"LLMOutput\"]], Future[List[List[\"LLMOutput\"]]]]:\n    \"\"\"Generates the outputs for the given inputs using the LLM.\n\n    Args:\n        inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each input.\n            Defaults to `1`.\n        progress_callback_func (Union[Callable, None], optional): a function to be called at each\n            generation step. Defaults to `None`.\n\n    Returns:\n        Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.\n    \"\"\"\n\n    def _progress():\n        if progress_callback_func is not None:\n            progress_callback_func(advance=num_generations * len(inputs))\n\n    valid_inputs, invalid_inputs_indices = self._get_valid_inputs(inputs)\n\n    if self.thread_pool_executor is not None:\n        futures = []\n        for input in valid_inputs:\n            future = self.thread_pool_executor.submit(\n                self._generate, [input], num_generations\n            )\n            futures.append(future)\n        future = when_all_complete(\n            futures=futures,\n            callback=lambda generations: self._fill_missing_inputs(\n                generations, invalid_inputs_indices, num_generations\n            ),\n        )\n        future.add_done_callback(lambda _: _progress())\n        return future\n\n    generations = self._generate(valid_inputs, num_generations)\n\n    generations = self._fill_missing_inputs(\n        generations, invalid_inputs_indices, num_generations\n    )\n\n    _progress()\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLM.validate_prompts","title":"<code>validate_prompts(inputs, default_format=None)</code>","text":"<p>Generates the prompts to be used for generation, can be used to check the prompts visually.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>The inputs to be used for generation.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The prompts that would be used for the generation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; llm.validate_prompts([{\"input\": \"Your input\"}])[0]\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\nI'm valid for text generation task\n</code></pre> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def validate_prompts(\n    self,\n    inputs: List[Dict[str, Any]],\n    default_format: Union[\"SupportedFormats\", None] = None,\n) -&gt; str:\n    \"\"\"Generates the prompts to be used for generation, can be used to check the prompts visually.\n\n    Args:\n        inputs (List[Dict[str, Any]]):\n            The inputs to be used for generation.\n\n    Returns:\n        str: The prompts that would be used for the generation.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; llm.validate_prompts([{\"input\": \"Your input\"}])[0]\n        You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n        I'm valid for text generation task\n    \"\"\"\n    return self._generate_prompts(inputs, default_format=default_format)\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLMPool","title":"<code>LLMPool</code>","text":"<p>LLMPool is a class that wraps multiple <code>ProcessLLM</code>s and performs generation in parallel using them. Depending on the number of <code>LLM</code>s and the parameter <code>num_generations</code>, the <code>LLMPool</code> will decide how many generations to perform for each <code>LLM</code>:</p> <ul> <li> <p>If <code>num_generations</code> is less than the number of <code>LLM</code>s, then <code>num_generations</code> LLMs will be chosen randomly and each of them will perform 1 generation.</p> </li> <li> <p>If <code>num_generations</code> is equal to the number of <code>LLM</code>s, then each <code>LLM</code> will perform 1 generation.</p> </li> <li> <p>If <code>num_generations</code> is greater than the number of <code>LLM</code>s, then each <code>LLM</code> will perform <code>num_generations // num_llms</code> generations, and the remaining <code>num_generations % num_llms</code> generations will be performed by <code>num_generations % num_llms</code> randomly chosen <code>LLM</code>s.</p> </li> </ul> <p>Attributes:</p> Name Type Description <code>llms</code> <code>List[ProcessLLM]</code> <p>the <code>ProcessLLM</code>s to be used for generation.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>class LLMPool:\n    \"\"\"LLMPool is a class that wraps multiple `ProcessLLM`s and performs generation in\n    parallel using them. Depending on the number of `LLM`s and the parameter `num_generations`,\n    the `LLMPool` will decide how many generations to perform for each `LLM`:\n\n    - If `num_generations` is less than the number of `LLM`s, then `num_generations` LLMs\n    will be chosen randomly and each of them will perform 1 generation.\n\n\n    - If `num_generations` is equal to the number of `LLM`s, then each `LLM` will perform\n    1 generation.\n\n    - If `num_generations` is greater than the number of `LLM`s, then each `LLM` will\n    perform `num_generations // num_llms` generations, and the remaining `num_generations % num_llms`\n    generations will be performed by `num_generations % num_llms` randomly chosen `LLM`s.\n\n    Attributes:\n        llms (List[ProcessLLM]): the `ProcessLLM`s to be used for generation.\n    \"\"\"\n\n    def __init__(self, llms: List[ProcessLLM]) -&gt; None:\n        \"\"\"Initializes the `LLMPool` class.\n\n        Args:\n            llms: the `ProcessLLM`s to be used for generation. The list must contain at\n                least 2 `ProcessLLM`s.\n\n        Raises:\n            ValueError: if the `llms` argument contains less than 2 `ProcessLLM`s, the\n                `llms` argument contains `ProcessLLM`s that are not `ProcessLLM`s, or\n                if the `llms` argument contains `ProcessLLM`s with different tasks.\n        \"\"\"\n        if len(llms) &lt; 2:\n            raise ValueError(\n                \"The `llms` argument must contain at least 2 `ProcessLLM`s. If you want\"\n                \" to use a single `ProcessLLM`, use the `ProcessLLM` directly instead.\"\n            )\n\n        if not all(isinstance(llm, ProcessLLM) for llm in llms):\n            raise ValueError(\"The `llms` argument must contain only `ProcessLLM`s.\")\n\n        # Note: The following piece of code is used to check that all the `ProcessLLM`s\n        # have the same task or a subclass of it.\n        mros = [(type(llm.task), len(type(llm.task).mro())) for llm in llms]\n        min_common_class = min(mros, key=lambda x: x[1])[0]\n        if not all(isinstance(llm.task, min_common_class) for llm in llms):\n            # This can fail for example with 3 different TextGenerationTasks\n            # Task1(TextGenerationTask), Task2(TextGenerationTask), Task2(TextGenerationTask)\n            # because they share the same parent class but we don't check the common one\n            # TODO(plaguss): We check that they all have the same parent class, this should be simplified\n            # with the previous check\n            parent_classes = [type(llm.task).mro()[1] for llm in llms]\n            if not len(set(parent_classes)) == 1:\n                raise ValueError(\n                    \"All the `ProcessLLM` in `llms` must share the same task (either as the instance or the parent class).\"\n                )\n\n        self.llms = llms\n        self.num_llms = len(llms)\n\n    def _get_num_generations_per_llm(self, num_generations: int) -&gt; Dict[int, int]:\n        \"\"\"Returns the number of generations to be performed by each `LLM`.\n\n        Args:\n            num_generations: the number of generations to be performed.\n\n        Returns:\n            Dict[int, int]: a dictionary where the keys are the ids of the `LLM`s and the\n            values are the number of generations to be performed by each `LLM`.\n        \"\"\"\n        llms_ids = list(range(self.num_llms))\n        generations_per_llm = {i: num_generations // self.num_llms for i in llms_ids}\n\n        for i in random.sample(llms_ids, k=num_generations % self.num_llms):\n            generations_per_llm[i] += 1\n\n        return generations_per_llm\n\n    def generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        \"\"\"Generates the outputs for the given inputs using the pool of `ProcessLLM`s.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each input.\n                Defaults to `1`.\n            progress_callback_func (Union[Callable, None], optional): a function to be called at each\n                generation step. Defaults to `None`.\n\n        Returns:\n            Future[List[List[\"LLMOutput\"]]]: the generated outputs as a `Future`.\n        \"\"\"\n        num_generations_per_llm = self._get_num_generations_per_llm(num_generations)\n\n        futures = [\n            llm.generate(\n                inputs,\n                num_generations=num_generations_per_llm[i],\n                progress_callback_func=progress_callback_func,\n            )\n            for i, llm in enumerate(self.llms)\n            if num_generations_per_llm[i] &gt; 0\n        ]\n        llms_generations = [future.result() for future in futures]\n\n        generations = []\n        for llms_row_generations in zip(*llms_generations):\n            row_generations = []\n            for llm_row_generations in llms_row_generations:\n                for generation in llm_row_generations:\n                    row_generations.append(generation)\n            generations.append(row_generations)\n\n        return generations\n\n    def teardown(self) -&gt; None:\n        \"\"\"Stops the `ProcessLLM`s.\"\"\"\n        for llm in self.llms:\n            llm.teardown()\n\n    @property\n    def task(self) -&gt; \"Task\":\n        \"\"\"Returns the task that will be used by the `ProcessLLM`s of this pool.\n\n        Returns:\n            Task: the task that will be used by the `ProcessLLM`s of this pool.\n        \"\"\"\n        return self.llms[0].task\n\n    @property\n    def return_futures(self) -&gt; bool:\n        \"\"\"Whether the `LLM` returns futures\"\"\"\n        return False\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLMPool.return_futures","title":"<code>return_futures: bool</code>  <code>property</code>","text":"<p>Whether the <code>LLM</code> returns futures</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLMPool.task","title":"<code>task: 'Task'</code>  <code>property</code>","text":"<p>Returns the task that will be used by the <code>ProcessLLM</code>s of this pool.</p> <p>Returns:</p> Name Type Description <code>Task</code> <code>'Task'</code> <p>the task that will be used by the <code>ProcessLLM</code>s of this pool.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLMPool.__init__","title":"<code>__init__(llms)</code>","text":"<p>Initializes the <code>LLMPool</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>llms</code> <code>List[ProcessLLM]</code> <p>the <code>ProcessLLM</code>s to be used for generation. The list must contain at least 2 <code>ProcessLLM</code>s.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>llms</code> argument contains less than 2 <code>ProcessLLM</code>s, the <code>llms</code> argument contains <code>ProcessLLM</code>s that are not <code>ProcessLLM</code>s, or if the <code>llms</code> argument contains <code>ProcessLLM</code>s with different tasks.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __init__(self, llms: List[ProcessLLM]) -&gt; None:\n    \"\"\"Initializes the `LLMPool` class.\n\n    Args:\n        llms: the `ProcessLLM`s to be used for generation. The list must contain at\n            least 2 `ProcessLLM`s.\n\n    Raises:\n        ValueError: if the `llms` argument contains less than 2 `ProcessLLM`s, the\n            `llms` argument contains `ProcessLLM`s that are not `ProcessLLM`s, or\n            if the `llms` argument contains `ProcessLLM`s with different tasks.\n    \"\"\"\n    if len(llms) &lt; 2:\n        raise ValueError(\n            \"The `llms` argument must contain at least 2 `ProcessLLM`s. If you want\"\n            \" to use a single `ProcessLLM`, use the `ProcessLLM` directly instead.\"\n        )\n\n    if not all(isinstance(llm, ProcessLLM) for llm in llms):\n        raise ValueError(\"The `llms` argument must contain only `ProcessLLM`s.\")\n\n    # Note: The following piece of code is used to check that all the `ProcessLLM`s\n    # have the same task or a subclass of it.\n    mros = [(type(llm.task), len(type(llm.task).mro())) for llm in llms]\n    min_common_class = min(mros, key=lambda x: x[1])[0]\n    if not all(isinstance(llm.task, min_common_class) for llm in llms):\n        # This can fail for example with 3 different TextGenerationTasks\n        # Task1(TextGenerationTask), Task2(TextGenerationTask), Task2(TextGenerationTask)\n        # because they share the same parent class but we don't check the common one\n        # TODO(plaguss): We check that they all have the same parent class, this should be simplified\n        # with the previous check\n        parent_classes = [type(llm.task).mro()[1] for llm in llms]\n        if not len(set(parent_classes)) == 1:\n            raise ValueError(\n                \"All the `ProcessLLM` in `llms` must share the same task (either as the instance or the parent class).\"\n            )\n\n    self.llms = llms\n    self.num_llms = len(llms)\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLMPool.generate","title":"<code>generate(inputs, num_generations=1, progress_callback_func=None)</code>","text":"<p>Generates the outputs for the given inputs using the pool of <code>ProcessLLM</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>the inputs to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>progress_callback_func</code> <code>Union[Callable, None]</code> <p>a function to be called at each generation step. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List['LLMOutput']]</code> <p>Future[List[List[\"LLMOutput\"]]]: the generated outputs as a <code>Future</code>.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[Dict[str, Any]],\n    num_generations: int = 1,\n    progress_callback_func: Union[Callable, None] = None,\n) -&gt; List[List[\"LLMOutput\"]]:\n    \"\"\"Generates the outputs for the given inputs using the pool of `ProcessLLM`s.\n\n    Args:\n        inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each input.\n            Defaults to `1`.\n        progress_callback_func (Union[Callable, None], optional): a function to be called at each\n            generation step. Defaults to `None`.\n\n    Returns:\n        Future[List[List[\"LLMOutput\"]]]: the generated outputs as a `Future`.\n    \"\"\"\n    num_generations_per_llm = self._get_num_generations_per_llm(num_generations)\n\n    futures = [\n        llm.generate(\n            inputs,\n            num_generations=num_generations_per_llm[i],\n            progress_callback_func=progress_callback_func,\n        )\n        for i, llm in enumerate(self.llms)\n        if num_generations_per_llm[i] &gt; 0\n    ]\n    llms_generations = [future.result() for future in futures]\n\n    generations = []\n    for llms_row_generations in zip(*llms_generations):\n        row_generations = []\n        for llm_row_generations in llms_row_generations:\n            for generation in llm_row_generations:\n                row_generations.append(generation)\n        generations.append(row_generations)\n\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LLMPool.teardown","title":"<code>teardown()</code>","text":"<p>Stops the <code>ProcessLLM</code>s.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def teardown(self) -&gt; None:\n    \"\"\"Stops the `ProcessLLM`s.\"\"\"\n    for llm in self.llms:\n        llm.teardown()\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LlamaCppLLM","title":"<code>LlamaCppLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/llama_cpp.py</code> <pre><code>class LlamaCppLLM(LLM):\n    def __init__(\n        self,\n        model: \"Llama\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        temperature: float = 0.8,\n        top_p: float = 0.95,\n        top_k: int = 40,\n        repeat_penalty: float = 1.1,\n        seed: int = 1337,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[SupportedFormats, None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the LlamaCppLLM class.\n\n        Args:\n            model (Llama): the llama-cpp model to be used.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 0.8.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 0.95.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 40.\n            repeat_penalty (float, optional): the repeat penalty to be used for generation.\n                Defaults to 1.1.\n            seed (int, optional): the seed to be used for generation, setting it to -1 implies\n                that a different response will be generated on each generation, similarly to\n                HuggingFace's `do_sample` arg. Defaults to 1337.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Examples:\n            &gt;&gt;&gt; from llama_cpp import Llama\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n            &gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n            &gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _LLAMA_CPP_AVAILABLE:\n            raise ImportError(\n                \"`LlamaCppLLM` cannot be used as `llama_cpp` is not installed, please \"\n                \" install it with `pip install llama-cpp-python`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.repeat_penalty = repeat_penalty\n        self.seed = seed\n\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_tokens,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n                \"repeat_penalty\": self.repeat_penalty,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the llama-cpp model, which is the same as the model path.\"\"\"\n        return self.model.model_path\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        outputs = []\n        for prompt in prompts:\n            output = []\n            for _ in range(num_generations):\n                raw_output = self.model.create_completion(\n                    prompt,\n                    max_tokens=self.max_tokens,\n                    temperature=self.temperature,\n                    top_p=self.top_p,\n                    top_k=self.top_k,\n                    repeat_penalty=self.repeat_penalty,\n                )\n                try:\n                    parsed_output = self.task.parse_output(\n                        raw_output[\"choices\"][0][\"text\"].strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing llama-cpp output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LlamaCppLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the llama-cpp model, which is the same as the model path.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.LlamaCppLLM.__init__","title":"<code>__init__(model, task, max_new_tokens=128, temperature=0.8, top_p=0.95, top_k=40, repeat_penalty=1.1, seed=1337, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the LlamaCppLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>the llama-cpp model to be used.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 0.8.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 0.95.</p> <code>0.95</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 40.</p> <code>40</code> <code>repeat_penalty</code> <code>float</code> <p>the repeat penalty to be used for generation. Defaults to 1.1.</p> <code>1.1</code> <code>seed</code> <code>int</code> <p>the seed to be used for generation, setting it to -1 implies that a different response will be generated on each generation, similarly to HuggingFace's <code>do_sample</code> arg. Defaults to 1337.</p> <code>1337</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from llama_cpp import Llama\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n&gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n&gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/llama_cpp.py</code> <pre><code>def __init__(\n    self,\n    model: \"Llama\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    temperature: float = 0.8,\n    top_p: float = 0.95,\n    top_k: int = 40,\n    repeat_penalty: float = 1.1,\n    seed: int = 1337,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[SupportedFormats, None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the LlamaCppLLM class.\n\n    Args:\n        model (Llama): the llama-cpp model to be used.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 0.8.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 0.95.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 40.\n        repeat_penalty (float, optional): the repeat penalty to be used for generation.\n            Defaults to 1.1.\n        seed (int, optional): the seed to be used for generation, setting it to -1 implies\n            that a different response will be generated on each generation, similarly to\n            HuggingFace's `do_sample` arg. Defaults to 1337.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from llama_cpp import Llama\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n        &gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n        &gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _LLAMA_CPP_AVAILABLE:\n        raise ImportError(\n            \"`LlamaCppLLM` cannot be used as `llama_cpp` is not installed, please \"\n            \" install it with `pip install llama-cpp-python`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.repeat_penalty = repeat_penalty\n    self.seed = seed\n\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.MistralAILLM","title":"<code>MistralAILLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/mistralai.py</code> <pre><code>class MistralAILLM(LLM):\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"mistral-medium\",\n        client: Optional[\"MistralClient\"] = None,\n        api_key: Optional[str] = os.environ.get(\"MISTRALAI_API_KEY\"),\n        max_tokens: int = 128,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        seed: Optional[int] = None,\n        safe_prompt: bool = False,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the MistralAILLM class.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"mistral-medium\".\n            client (MistralClient, optional):\n                A MistralClient client to be used for generation. Defaults to None.\n            api_key (Optional[str], optional):\n                The MistralAI API key to be used for generation. Will try to grab it from the environment variable\n                if not informed. Visit \"https://docs.mistral.ai/#api-access\" for more information.\n            max_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation. Defaults to 1.0.\n            seed (Optional[int], optional): the random seed to use for sampling, e.g. 42. Defaults to None.\n            safe_prompt (_type_, optional):\n                whether to use safe prompt, e.g. True. Defaults to False.\n                Visit \"https://docs.mistral.ai/platform/guardrailing/\" for more information.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n        Raises:\n            AssertionError: if the provided `model` is not available in your MistralAI account.\n\n        Examples:\n            &gt;&gt;&gt; import os\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import MistralAILLM\n            &gt;&gt;&gt; llm = MistralAILLM(model=\"mistral-medium\", task=TextGenerationTask(), api_key=os.getenv(\"MISTRALAI_API_KEY\"))\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n        if not _MISTRALAI_AVAILABLE:\n            raise ImportError(\n                \"`MistralAILLM` cannot be used as `mistralai` is not installed, please \"\n                \" install it with `pip install mistralai`.\"\n            )\n\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.seed = seed\n        self.safe_prompt = safe_prompt\n\n        # Explicitly write the default parameters of the model\n        self.client = client or MistralClient(\n            api_key=api_key, max_retries=5, timeout=120\n        )\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in MistralAI, available models are {self.available_models}\"\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"seed\": self.seed,\n                \"safe_prompt\": self.safe_prompt,\n            },\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in MistralAI.\"\"\"\n        return [model.id for model in self.client.list_models().data]\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the MistralAI model.\"\"\"\n        return self.model\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"openai\")\n        # The mistralai format is the same as openai, but needs to be converted to mistralai's ChatMessage (pydantic model)\n        prompts = [[ChatMessage(**p) for p in prompt] for prompt in prompts]\n        outputs = []\n        for prompt in prompts:\n            responses = []\n            for _ in range(num_generations):\n                chat_completion_response = self.client.chat(\n                    self.model,\n                    messages=prompt,\n                    temperature=self.temperature,\n                    max_tokens=self.max_tokens,\n                    top_p=self.top_p,\n                    random_seed=self.seed,\n                    safe_prompt=self.safe_prompt,\n                )\n                responses.append(chat_completion_response)\n\n            output = []\n            for response in responses:\n                chat_completion = response.choices[0]\n                try:\n                    parsed_response = self.task.parse_output(\n                        chat_completion.message.content.strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing MistralAI response: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=[p.model_dump() for p in prompt],\n                        raw_output=chat_completion.message.content,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.MistralAILLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in MistralAI.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.MistralAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the MistralAI model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.MistralAILLM.__init__","title":"<code>__init__(task, model='mistral-medium', client=None, api_key=os.environ.get('MISTRALAI_API_KEY'), max_tokens=128, temperature=1.0, top_p=1.0, seed=None, safe_prompt=False, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the MistralAILLM class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"mistral-medium\".</p> <code>'mistral-medium'</code> <code>client</code> <code>MistralClient</code> <p>A MistralClient client to be used for generation. Defaults to None.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The MistralAI API key to be used for generation. Will try to grab it from the environment variable if not informed. Visit \"https://docs.mistral.ai/#api-access\" for more information.</p> <code>get('MISTRALAI_API_KEY')</code> <code>max_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>seed</code> <code>Optional[int]</code> <p>the random seed to use for sampling, e.g. 42. Defaults to None.</p> <code>None</code> <code>safe_prompt</code> <code>_type_</code> <p>whether to use safe prompt, e.g. True. Defaults to False. Visit \"https://docs.mistral.ai/platform/guardrailing/\" for more information.</p> <code>False</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:     AssertionError: if the provided <code>model</code> is not available in your MistralAI account.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import MistralAILLM\n&gt;&gt;&gt; llm = MistralAILLM(model=\"mistral-medium\", task=TextGenerationTask(), api_key=os.getenv(\"MISTRALAI_API_KEY\"))\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/mistralai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"mistral-medium\",\n    client: Optional[\"MistralClient\"] = None,\n    api_key: Optional[str] = os.environ.get(\"MISTRALAI_API_KEY\"),\n    max_tokens: int = 128,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    seed: Optional[int] = None,\n    safe_prompt: bool = False,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the MistralAILLM class.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"mistral-medium\".\n        client (MistralClient, optional):\n            A MistralClient client to be used for generation. Defaults to None.\n        api_key (Optional[str], optional):\n            The MistralAI API key to be used for generation. Will try to grab it from the environment variable\n            if not informed. Visit \"https://docs.mistral.ai/#api-access\" for more information.\n        max_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation. Defaults to 1.0.\n        seed (Optional[int], optional): the random seed to use for sampling, e.g. 42. Defaults to None.\n        safe_prompt (_type_, optional):\n            whether to use safe prompt, e.g. True. Defaults to False.\n            Visit \"https://docs.mistral.ai/platform/guardrailing/\" for more information.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n    Raises:\n        AssertionError: if the provided `model` is not available in your MistralAI account.\n\n    Examples:\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import MistralAILLM\n        &gt;&gt;&gt; llm = MistralAILLM(model=\"mistral-medium\", task=TextGenerationTask(), api_key=os.getenv(\"MISTRALAI_API_KEY\"))\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n    if not _MISTRALAI_AVAILABLE:\n        raise ImportError(\n            \"`MistralAILLM` cannot be used as `mistralai` is not installed, please \"\n            \" install it with `pip install mistralai`.\"\n        )\n\n    self.max_tokens = max_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.seed = seed\n    self.safe_prompt = safe_prompt\n\n    # Explicitly write the default parameters of the model\n    self.client = client or MistralClient(\n        api_key=api_key, max_retries=5, timeout=120\n    )\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in MistralAI, available models are {self.available_models}\"\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.OllamaLLM","title":"<code>OllamaLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/ollama.py</code> <pre><code>class OllamaLLM(LLM):\n    OLLAMA_HOST = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\n    def __init__(\n        self,\n        model: str,\n        task: \"Task\",\n        max_new_tokens: Union[int, None] = None,\n        temperature: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n        top_p: Union[float, None] = None,\n        mirostat: Union[int, None] = None,\n        mirostat_eta: Union[float, None] = None,\n        mirostat_tau: Union[float, None] = None,\n        num_ctx: Union[int, None] = None,\n        num_gqa: Union[int, None] = None,\n        num_gpu: Union[int, None] = None,\n        num_threads: Union[int, None] = None,\n        repeat_last_n: Union[int, None] = None,\n        repeat_penalty: Union[float, None] = None,\n        seed: Union[int, None] = None,\n        stop: Union[str, None] = None,\n        tfs_z: Union[float, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the OllamaLLM class and aligns with https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n\n        Args:\n            model (str): the model to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to `None`.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to `None`.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to `None`.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to `None`.\n            mirostat (int, optional): the Mirostat value to enable it or set the version.\n                Defaults to `None`.\n            mirostat_eta (float, optional): the eta value to be used for Mirostat.\n                Defaults to `None`.\n            mirostat_tau (float, optional): the tau value to be used for Mirostat.\n                Defaults to `None`.\n            num_ctx (int, optional): the number of contexts to be used for generation.\n                Defaults to `None`.\n            num_gqa (int, optional): the number of GQA to be used for generation.\n                Defaults to `None`.\n            num_gpu (int, optional): the number of GPUs to be used for generation.\n                Defaults to `None`.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            repeat_last_n (Union[int, None], optional): the number of tokens to be used\n                for RepeatLastN. Defaults to `None`.\n            repeat_penalty (Union[float, None], optional): the penalty to be used for RepeatLastN.\n                Defaults to `None`.\n            seed (Union[int, None], optional): the seed to be used for generation.\n                Defaults to `None`.\n            stop (Union[str, None], optional): the stop token to be used for generation. If `None`,\n                no stop token will be used. Defaults to `None`.\n            tfs_z (Union[float, None], optional): the z value to be used for TFS.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`..\n\n        Raises:\n            ValueError: if the model is not available.\n            ValueError: if the Ollama API request failed.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import OllamaLLM\n            &gt;&gt;&gt; llm = OllamaLLM(model=\"notus\", task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        self.model = model\n        self.max_new_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.mirostat = mirostat\n        self.mirostat_eta = mirostat_eta\n        self.mirostat_tau = mirostat_tau\n        self.num_ctx = num_ctx\n        self.num_gqa = num_gqa\n        self.num_gpu = num_gpu\n        self.repeat_last_n = repeat_last_n\n        self.repeat_penalty = repeat_penalty\n        self.seed = seed\n        self.stop = stop\n        self.tfs_z = tfs_z\n\n        self._api_available()\n        self._api_model_available()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the Ollama model.\"\"\"\n        return self.model\n\n    def _api_available(self):\n        \"\"\"Checks if the Ollama API is available.\"\"\"\n        try:\n            ollama.list()\n        except ollama.ResponseError as e:\n            raise ValueError(\n                f\"Could not connect to Ollama at {self.OLLAMA_HOST}. Check https://github.com/ollama/ollama-python/tree/main for deployment guide.\"\n            ) from e\n\n    def _api_model_available(self):\n        \"\"\"Checks if the Ollama model is available\"\"\"\n        try:\n            ollama.show(self.model)\n        except ollama.ResponseError as e:\n            raise ValueError(\n                f\"Model {self.model} is not available. Run `ollama run {self.model}` to serve the model.\"\n            ) from e\n\n    @retry(\n        retry=retry_if_exception_type(_OLLAMA_API_RETRY_ON_EXCEPTIONS),\n        stop=stop_after_attempt(_OLLAMA_API_STOP_AFTER_ATTEMPT),\n        wait=wait_random_exponential(\n            multiplier=_OLLAMA_API_WAIT_RANDOM_EXPONENTIAL_MULTIPLIER,\n            max=_OLLAMA_API_WAIT_RANDOM_EXPONENTIAL_MAX,\n        ),\n        before_sleep=before_sleep_log(logger, logging.INFO),\n        after=after_log(logger, logging.INFO),\n    )\n    def _text_generation_with_backoff(\n        self, prompt: List[Dict[str, str]], **kwargs\n    ) -&gt; str:\n        \"\"\"Generates text using the Ollama API with backoff.\"\"\"\n        try:\n            return ollama.chat(\n                model=self.model,\n                messages=prompt,\n                options={\n                    \"num_predict\": self.max_new_tokens,\n                    \"temperature\": self.temperature,\n                    \"top_p\": self.top_p,\n                    \"top_k\": self.top_k,\n                    \"mirostat\": self.mirostat,\n                    \"mirostat_eta\": self.mirostat_eta,\n                    \"mirostat_tau\": self.mirostat_tau,\n                    \"num_ctx\": self.num_ctx,\n                    \"num_gqa\": self.num_gqa,\n                    \"num_gpu\": self.num_gpu,\n                    \"repeat_last_n\": self.repeat_last_n,\n                    \"repeat_penalty\": self.repeat_penalty,\n                    \"seed\": self.seed,\n                    \"stop\": self.stop,\n                    \"tfs_z\": self.tfs_z,\n                },\n            )\n        except ollama.ResponseError as e:\n            if e.status_code &gt;= 500:\n                raise\n            else:\n                raise ValueError(\n                    f\"Ollama API request failed with status_code {e.status_code}.\"\n                ) from e\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"model\": self.model,\n                \"max_new_tokens\": self.max_new_tokens,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"mirostat\": self.mirostat,\n                \"mirostat_eta\": self.mirostat_eta,\n                \"mirostat_tau\": self.mirostat_tau,\n                \"num_ctx\": self.num_ctx,\n                \"num_gqa\": self.num_gqa,\n                \"num_gpu\": self.num_gpu,\n                \"repeat_last_n\": self.repeat_last_n,\n                \"repeat_penalty\": self.repeat_penalty,\n                \"seed\": self.seed,\n                \"stop\": self.stop,\n                \"tfs_z\": self.tfs_z,\n            },\n        )\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        prompts = self._generate_prompts(inputs, default_format=\"openai\")\n        outputs = []\n        for prompt in prompts:\n            responses = [\n                self._text_generation_with_backoff(prompt=prompt)\n                for _ in range(num_generations)\n            ]\n            output = []\n            for response in responses:\n                raw_output = response.get(\"message\", {}).get(\"content\")  # type: ignore\n                try:\n                    parsed_response = self.task.parse_output(raw_output.strip())\n                except Exception as e:\n                    logger.error(f\"Error parsing OpenAI response: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.OllamaLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the Ollama model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.OllamaLLM.__init__","title":"<code>__init__(model, task, max_new_tokens=None, temperature=None, top_k=None, top_p=None, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gqa=None, num_gpu=None, num_threads=None, repeat_last_n=None, repeat_penalty=None, seed=None, stop=None, tfs_z=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the OllamaLLM class and aligns with https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>the model to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to <code>None</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>mirostat</code> <code>int</code> <p>the Mirostat value to enable it or set the version. Defaults to <code>None</code>.</p> <code>None</code> <code>mirostat_eta</code> <code>float</code> <p>the eta value to be used for Mirostat. Defaults to <code>None</code>.</p> <code>None</code> <code>mirostat_tau</code> <code>float</code> <p>the tau value to be used for Mirostat. Defaults to <code>None</code>.</p> <code>None</code> <code>num_ctx</code> <code>int</code> <p>the number of contexts to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>num_gqa</code> <code>int</code> <p>the number of GQA to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>num_gpu</code> <code>int</code> <p>the number of GPUs to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>repeat_last_n</code> <code>Union[int, None]</code> <p>the number of tokens to be used for RepeatLastN. Defaults to <code>None</code>.</p> <code>None</code> <code>repeat_penalty</code> <code>Union[float, None]</code> <p>the penalty to be used for RepeatLastN. Defaults to <code>None</code>.</p> <code>None</code> <code>seed</code> <code>Union[int, None]</code> <p>the seed to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>stop</code> <code>Union[str, None]</code> <p>the stop token to be used for generation. If <code>None</code>, no stop token will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>tfs_z</code> <code>Union[float, None]</code> <p>the z value to be used for TFS. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>..</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the model is not available.</p> <code>ValueError</code> <p>if the Ollama API request failed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import OllamaLLM\n&gt;&gt;&gt; llm = OllamaLLM(model=\"notus\", task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/ollama.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    task: \"Task\",\n    max_new_tokens: Union[int, None] = None,\n    temperature: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n    top_p: Union[float, None] = None,\n    mirostat: Union[int, None] = None,\n    mirostat_eta: Union[float, None] = None,\n    mirostat_tau: Union[float, None] = None,\n    num_ctx: Union[int, None] = None,\n    num_gqa: Union[int, None] = None,\n    num_gpu: Union[int, None] = None,\n    num_threads: Union[int, None] = None,\n    repeat_last_n: Union[int, None] = None,\n    repeat_penalty: Union[float, None] = None,\n    seed: Union[int, None] = None,\n    stop: Union[str, None] = None,\n    tfs_z: Union[float, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the OllamaLLM class and aligns with https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n\n    Args:\n        model (str): the model to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to `None`.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to `None`.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to `None`.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to `None`.\n        mirostat (int, optional): the Mirostat value to enable it or set the version.\n            Defaults to `None`.\n        mirostat_eta (float, optional): the eta value to be used for Mirostat.\n            Defaults to `None`.\n        mirostat_tau (float, optional): the tau value to be used for Mirostat.\n            Defaults to `None`.\n        num_ctx (int, optional): the number of contexts to be used for generation.\n            Defaults to `None`.\n        num_gqa (int, optional): the number of GQA to be used for generation.\n            Defaults to `None`.\n        num_gpu (int, optional): the number of GPUs to be used for generation.\n            Defaults to `None`.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        repeat_last_n (Union[int, None], optional): the number of tokens to be used\n            for RepeatLastN. Defaults to `None`.\n        repeat_penalty (Union[float, None], optional): the penalty to be used for RepeatLastN.\n            Defaults to `None`.\n        seed (Union[int, None], optional): the seed to be used for generation.\n            Defaults to `None`.\n        stop (Union[str, None], optional): the stop token to be used for generation. If `None`,\n            no stop token will be used. Defaults to `None`.\n        tfs_z (Union[float, None], optional): the z value to be used for TFS.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`..\n\n    Raises:\n        ValueError: if the model is not available.\n        ValueError: if the Ollama API request failed.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import OllamaLLM\n        &gt;&gt;&gt; llm = OllamaLLM(model=\"notus\", task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    self.model = model\n    self.max_new_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.mirostat = mirostat\n    self.mirostat_eta = mirostat_eta\n    self.mirostat_tau = mirostat_tau\n    self.num_ctx = num_ctx\n    self.num_gqa = num_gqa\n    self.num_gpu = num_gpu\n    self.repeat_last_n = repeat_last_n\n    self.repeat_penalty = repeat_penalty\n    self.seed = seed\n    self.stop = stop\n    self.tfs_z = tfs_z\n\n    self._api_available()\n    self._api_model_available()\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.OpenAILLM","title":"<code>OpenAILLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>class OpenAILLM(LLM):\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"gpt-3.5-turbo\",\n        client: Union[\"OpenAI\", None] = None,\n        api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the OpenAILLM class.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n            client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n                If `None`, a new client will be created. Defaults to `None`.\n            api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n                If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in your OpenAI account.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import OpenAILLM\n            &gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _OPENAI_AVAILABLE:\n            raise ImportError(\n                \"`OpenAILLM` cannot be used as `openai` is not installed, please \"\n                \" install it with `pip install openai`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n\n        self.client = client or OpenAI(api_key=api_key, max_retries=6)\n\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in your OpenAI account, available models are {self.available_models}\"\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"frequency_penalty\": self.frequency_penalty,\n                \"presence_penalty\": self.presence_penalty,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n            },\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in your OpenAI account.\"\"\"\n        return [model.id for model in self.client.models.list().data]\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the OpenAI model.\"\"\"\n        return self.model\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"openai\")\n        outputs = []\n        for prompt in prompts:\n            chat_completions = self.client.chat.completions.create(\n                messages=prompt,\n                model=self.model,\n                n=num_generations,\n                max_tokens=self.max_tokens,\n                frequency_penalty=self.frequency_penalty,\n                presence_penalty=self.presence_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                timeout=50,\n            )\n\n            output = []\n            for chat_completion in chat_completions.choices:\n                try:\n                    parsed_response = self.task.parse_output(\n                        chat_completion.message.content.strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing OpenAI response: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=chat_completion.message.content,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.OpenAILLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in your OpenAI account.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.OpenAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the OpenAI model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.OpenAILLM.__init__","title":"<code>__init__(task, model='gpt-3.5-turbo', client=None, api_key=None, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the OpenAILLM class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo'</code> <code>client</code> <code>Union[OpenAI, None]</code> <p>an OpenAI client to be used for generation. If <code>None</code>, a new client will be created. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Union[str, None]</code> <p>the OpenAI API key to be used for generation. If <code>None</code>, the <code>OPENAI_API_KEY</code> environment variable will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in your OpenAI account.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import OpenAILLM\n&gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"gpt-3.5-turbo\",\n    client: Union[\"OpenAI\", None] = None,\n    api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the OpenAILLM class.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n        client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n            If `None`, a new client will be created. Defaults to `None`.\n        api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n            If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in your OpenAI account.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import OpenAILLM\n        &gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _OPENAI_AVAILABLE:\n        raise ImportError(\n            \"`OpenAILLM` cannot be used as `openai` is not installed, please \"\n            \" install it with `pip install openai`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.frequency_penalty = frequency_penalty\n    self.presence_penalty = presence_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n\n    self.client = client or OpenAI(api_key=api_key, max_retries=6)\n\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in your OpenAI account, available models are {self.available_models}\"\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.ProcessLLM","title":"<code>ProcessLLM</code>","text":"<p>A class that wraps an <code>LLM</code> and performs generation in a separate process. The result is a <code>Future</code> that will be set when the generation is completed.</p> <p>This class creates a new child process that will load the <code>LLM</code> and perform the text generation. In order to communicate with this child process, a bridge thread is created in the main process. The bridge thread will send and receive the results from the child process using <code>multiprocessing.Queue</code>s. The communication between the bridge thread and the main process is done using <code>Future</code>s. This architecture was inspired by the <code>ProcessPoolExecutor</code> from the <code>concurrent.futures</code> module and it's a simplified version of it.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>class ProcessLLM:\n    \"\"\"A class that wraps an `LLM` and performs generation in a separate process. The\n    result is a `Future` that will be set when the generation is completed.\n\n    This class creates a new child process that will load the `LLM` and perform the\n    text generation. In order to communicate with this child process, a bridge thread\n    is created in the main process. The bridge thread will send and receive the results\n    from the child process using `multiprocessing.Queue`s. The communication between the\n    bridge thread and the main process is done using `Future`s. This architecture was\n    inspired by the `ProcessPoolExecutor` from the `concurrent.futures` module and it's\n    a simplified version of it.\n    \"\"\"\n\n    def __init__(self, task: Task, load_llm_fn: Callable[[Task], LLM]) -&gt; None:\n        \"\"\"Initializes the `ProcessLLM` class.\n\n        Args:\n            task: the task to be performed by the `LLM`. This task will be used by the\n                child process when calling the `load_llm_fn`.\n            load_llm_fn (Callable[[Task], LLM]): a function that will be executed in the\n                child process to load the `LLM`. It must return an `LLM` instance.\n        \"\"\"\n        self.task = task\n\n        self._load_llm_fn = load_llm_fn\n\n        # The bridge thread will act as a bridge between the main process and the child\n        # process for communication. It will send the generation requests to the child\n        # process and receive the results from the child process.\n        self._bridge_thread = None\n\n        # The child process which will load the `LLM` and perform the generation.\n        self._generation_process = None\n\n        # The `Semaphore` that will be used to synchronize the loading of the `LLM`.\n        # `_BridgeThread` will be blocked until `_GenerationProcess` has called the\n        # `load_llm_fn` and the `LLM` has been loaded.\n        self._load_llm_sem = mp.Semaphore(0)\n\n        # This thread will create text generation requests\n        self.pending_text_generation_request: Dict[int, _TextGenerationRequest] = {}\n        self.text_generation_request_count = 0\n        self.text_generation_request_ids_queue: queue.Queue[int] = queue.Queue()\n\n        # Queues for the communication between the `_BridgeThread` and the `_GenerationProcess`\n        self._call_queue = mp.Queue()\n        self._result_queue = mp.Queue()\n\n        # Shared memory object for transfering the `model_name` to the main process\n        # once the `LLM` is loaded\n        self._model_name = mp.Array(c_char, MAX_MODEL_NAME_LENGTH)\n\n    def _start_bridge_thread(self) -&gt; None:\n        \"\"\"Starts the bridge thread and the generation process.\"\"\"\n        if self._bridge_thread is None:\n            self._generation_process = _GenerationProcess(self)\n            self._generation_process.start()\n            pid = self._generation_process.pid\n            logger.debug(f\"Generation process with PID {pid} started!\")\n\n            self._bridge_thread = _BridgeThread(self)\n            self._bridge_thread.start()\n            logger.debug(\"Bridge thread for process with PID {pid} started!\")\n\n    def _add_text_generation_request(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; Future[List[List[\"LLMOutput\"]]]:\n        \"\"\"Creates and send a new text generation request to the bridge thread. This thread\n        and the bridge thread shares a dictionary used to store the text generation requests.\n        This thread will add the text generation requests to the dictionary and the bridge\n        thread will only read from it. In order for the bridge thread to know that a new\n        text generation request has been added to the dictionary, this thread will put the\n        id of the request in a queue. The bridge thread will read from this queue and get\n        the text generation request from the dictionary.\n        \"\"\"\n\n        def _progress():\n            if progress_callback_func is not None:\n                progress_callback_func(advance=num_generations * len(inputs))\n\n        text_generation_request = _TextGenerationRequest(\n            inputs=inputs, num_generations=num_generations\n        )\n        # Put the request information in the dictionary associated to the request id\n        self.pending_text_generation_request[\n            self.text_generation_request_count\n        ] = text_generation_request\n        # Put the request id in the queue (for the `_BridgeThread` to consume it)\n        self.text_generation_request_ids_queue.put(self.text_generation_request_count)\n        self.text_generation_request_count += 1\n        text_generation_request.future.add_done_callback(lambda _: _progress())\n        return text_generation_request.future\n\n    def generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; Future[List[List[\"LLMOutput\"]]]:\n        \"\"\"Generates the outputs for the given inputs using the `ProcessLLM` and its loaded\n        `LLM`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each input.\n                Defaults to `1`.\n            progress_callback_func (Union[Callable, None], optional): a function to be called at each\n                generation step. Defaults to `None`.\n\n        Returns:\n            Future[List[List[\"LLMOutput\"]]]: the generated outputs as a `Future`.\n        \"\"\"\n        self._start_bridge_thread()\n        return self._add_text_generation_request(\n            inputs, num_generations, progress_callback_func\n        )\n\n    def teardown(self) -&gt; None:\n        \"\"\"Stops the bridge thread and the generation process.\"\"\"\n        if self._generation_process is not None:\n            self._generation_process.stop()\n            self._generation_process.join()\n\n        if self._bridge_thread is not None:\n            self._bridge_thread.stop()\n            self._bridge_thread.join()\n\n    @cached_property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name of the `LLM` once it has been loaded.\"\"\"\n        with self._model_name:\n            return \"\".join([c.decode() for c in self._model_name if c != b\"\\0\"])\n\n    @property\n    def return_futures(self) -&gt; bool:\n        \"\"\"Whether the `LLM` returns futures\"\"\"\n        return True\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.ProcessLLM.model_name","title":"<code>model_name: str</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the model name of the <code>LLM</code> once it has been loaded.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.ProcessLLM.return_futures","title":"<code>return_futures: bool</code>  <code>property</code>","text":"<p>Whether the <code>LLM</code> returns futures</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.ProcessLLM.__init__","title":"<code>__init__(task, load_llm_fn)</code>","text":"<p>Initializes the <code>ProcessLLM</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the <code>LLM</code>. This task will be used by the child process when calling the <code>load_llm_fn</code>.</p> required <code>load_llm_fn</code> <code>Callable[[Task], LLM]</code> <p>a function that will be executed in the child process to load the <code>LLM</code>. It must return an <code>LLM</code> instance.</p> required Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __init__(self, task: Task, load_llm_fn: Callable[[Task], LLM]) -&gt; None:\n    \"\"\"Initializes the `ProcessLLM` class.\n\n    Args:\n        task: the task to be performed by the `LLM`. This task will be used by the\n            child process when calling the `load_llm_fn`.\n        load_llm_fn (Callable[[Task], LLM]): a function that will be executed in the\n            child process to load the `LLM`. It must return an `LLM` instance.\n    \"\"\"\n    self.task = task\n\n    self._load_llm_fn = load_llm_fn\n\n    # The bridge thread will act as a bridge between the main process and the child\n    # process for communication. It will send the generation requests to the child\n    # process and receive the results from the child process.\n    self._bridge_thread = None\n\n    # The child process which will load the `LLM` and perform the generation.\n    self._generation_process = None\n\n    # The `Semaphore` that will be used to synchronize the loading of the `LLM`.\n    # `_BridgeThread` will be blocked until `_GenerationProcess` has called the\n    # `load_llm_fn` and the `LLM` has been loaded.\n    self._load_llm_sem = mp.Semaphore(0)\n\n    # This thread will create text generation requests\n    self.pending_text_generation_request: Dict[int, _TextGenerationRequest] = {}\n    self.text_generation_request_count = 0\n    self.text_generation_request_ids_queue: queue.Queue[int] = queue.Queue()\n\n    # Queues for the communication between the `_BridgeThread` and the `_GenerationProcess`\n    self._call_queue = mp.Queue()\n    self._result_queue = mp.Queue()\n\n    # Shared memory object for transfering the `model_name` to the main process\n    # once the `LLM` is loaded\n    self._model_name = mp.Array(c_char, MAX_MODEL_NAME_LENGTH)\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.ProcessLLM.generate","title":"<code>generate(inputs, num_generations=1, progress_callback_func=None)</code>","text":"<p>Generates the outputs for the given inputs using the <code>ProcessLLM</code> and its loaded <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>the inputs to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>progress_callback_func</code> <code>Union[Callable, None]</code> <p>a function to be called at each generation step. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Future[List[List['LLMOutput']]]</code> <p>Future[List[List[\"LLMOutput\"]]]: the generated outputs as a <code>Future</code>.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[Dict[str, Any]],\n    num_generations: int = 1,\n    progress_callback_func: Union[Callable, None] = None,\n) -&gt; Future[List[List[\"LLMOutput\"]]]:\n    \"\"\"Generates the outputs for the given inputs using the `ProcessLLM` and its loaded\n    `LLM`.\n\n    Args:\n        inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each input.\n            Defaults to `1`.\n        progress_callback_func (Union[Callable, None], optional): a function to be called at each\n            generation step. Defaults to `None`.\n\n    Returns:\n        Future[List[List[\"LLMOutput\"]]]: the generated outputs as a `Future`.\n    \"\"\"\n    self._start_bridge_thread()\n    return self._add_text_generation_request(\n        inputs, num_generations, progress_callback_func\n    )\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.ProcessLLM.teardown","title":"<code>teardown()</code>","text":"<p>Stops the bridge thread and the generation process.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def teardown(self) -&gt; None:\n    \"\"\"Stops the bridge thread and the generation process.\"\"\"\n    if self._generation_process is not None:\n        self._generation_process.stop()\n        self._generation_process.join()\n\n    if self._bridge_thread is not None:\n        self._bridge_thread.stop()\n        self._bridge_thread.join()\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.TogetherInferenceLLM","title":"<code>TogetherInferenceLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/together.py</code> <pre><code>class TogetherInferenceLLM(LLM):\n    def __init__(\n        self,\n        model: str,\n        task: \"Task\",\n        api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        repetition_penalty: float = 1.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = 1,\n        stop: Union[List[str], None] = None,\n        logprobs: int = 0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the TogetherInferenceLLM class.\n\n        Args:\n            model (str): the model to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            temperature (float, optional): the temperature to be used for generation. From the Together\n                Inference docs: \"A decimal number that determines the degree of randomness in the response.\n                A value of 0 will always yield the same output. A temperature much less than 1 favors more\n                correctness and is appropriate for question answering or summarization. A value approaching\n                1 introduces more randomness in the output.\". Defaults to 1.0.\n            repetition_penalty (float, optional): the repetition penalty to be used for generation. From the\n                Together Inference docs: \"Controls the diversity of generated text by reducing the likelihood\n                of repeated sequences. Higher values decrease repetition.\". Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation. From the Together\n                Inference docs: \"used to dynamically adjust the number of choices for each predicted\n                token based on the cumulative probabilities. It specifies a probability threshold,\n                below which all less likely tokens are filtered out. This technique helps to maintain\n                diversity and generate more fluent and natural-sounding text.\". Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation. From the Together Inference\n                docs: \"used to limit the number of choices for the next predicted word or token. It specifies\n                the maximum number of tokens to consider at each step, based on their probability of occurrence.\n                This technique helps to speed up the generation process and can improve the quality of the\n                generated text by focusing on the most likely options.\". Defaults to 1.\n            stop (List[str], optional): strings to delimitate the generation process, so that when the\n                model generates any of the provided characters, the generation process is considered completed.\n                Defaults to None.\n            logprobs (int, optional): the number of logprobs to be returned for each token. From the\n                Together Inference docs: \"An integer that specifies how many top token log probabilities\n                are included in the response for each token generation step.\". Defaults to None.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in Together Inference.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import TogetherInferenceLLM\n            &gt;&gt;&gt; llm = TogetherInferenceLLM(model=\"togethercomputer/llama-2-7b\", task=TextGenerationTask(), prompt_format=\"llama2\")\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        if not _TOGETHER_AVAILABLE:\n            raise ImportError(\n                \"`TogetherInferenceLLM` cannot be used as `together` is not installed, please \"\n                \" install it with `pip install together`.\"\n            )\n\n        together.api_key = api_key or os.getenv(\"TOGETHER_API_KEY\", None)\n        if together.api_key is None:\n            raise ValueError(\n                \"No `api_key` provided, please provide one or set the `TOGETHER_API_KEY` \"\n                \"environment variable.\"\n            )\n\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in Together Inference, available models are {self.available_models}\"\n        self.model = model\n\n        self.max_new_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.repetition_penalty = repetition_penalty\n        self.stop = stop\n        self.logprobs = logprobs\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_new_tokens,\n                \"temperature\": self.temperature,\n                \"repetition_penalty\": self.repetition_penalty,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n                \"stop\": self.stop,\n                \"logprobs\": self.logprobs,\n            },\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in Together Inference.\"\"\"\n        return [\n            model[\"name\"]\n            for model in together.Models.list()\n            if model[\"display_type\"] != \"image\"\n        ]\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the Together Inference model.\"\"\"\n        return self.model\n\n    def _generate_single_output(self, prompt: str) -&gt; LLMOutput:\n        \"\"\"Runs the Together Inference text generation function over a single prompt\n        producing a single `LLMOutput`.\n\n        Args:\n            prompt (str): the formatted prompt to be provided to the Together Inference\n                endpoint.\n\n        Raises:\n            RuntimeError: raised if the Together Inference endpoint fails.\n        \"\"\"\n        try:\n            output = together.Complete.create(\n                prompt=prompt,\n                model=self.model,\n                max_tokens=self.max_new_tokens,\n                stop=self.stop,\n                temperature=self.temperature,\n                top_k=self.top_k,\n                top_p=self.top_p,\n                repetition_penalty=self.repetition_penalty,\n                logprobs=self.logprobs,\n            )\n        except Exception as e:\n            raise RuntimeError(\n                f\"Together Inference generation failed with exception: {e}\"\n            ) from e\n\n        if output[\"output\"][\"choices\"] is None or len(output[\"output\"][\"choices\"]) &lt; 1:  # type: ignore\n            raise RuntimeError(\"Together Inference generation returned no generations.\")\n\n        choice = output[\"output\"][\"choices\"][0]  # type: ignore\n        try:\n            parsed_response = self.task.parse_output(choice[\"text\"].strip())\n        except Exception as e:\n            logger.error(f\"Error parsing Together Inference response: {e}\")\n            parsed_response = None\n\n        return LLMOutput(\n            model_name=self.model_name,\n            prompt_used=prompt,\n            raw_output=choice[\"text\"] or None,\n            parsed_output=parsed_response,\n        )\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        outputs = []\n        for prompt in prompts:\n            outputs.append(\n                [self._generate_single_output(prompt) for _ in range(num_generations)]\n            )\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.TogetherInferenceLLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in Together Inference.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.TogetherInferenceLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the Together Inference model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.TogetherInferenceLLM.__init__","title":"<code>__init__(model, task, api_key=None, max_new_tokens=128, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=1, stop=None, logprobs=0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the TogetherInferenceLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>the model to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. From the Together Inference docs: \"A decimal number that determines the degree of randomness in the response. A value of 0 will always yield the same output. A temperature much less than 1 favors more correctness and is appropriate for question answering or summarization. A value approaching 1 introduces more randomness in the output.\". Defaults to 1.0.</p> <code>1.0</code> <code>repetition_penalty</code> <code>float</code> <p>the repetition penalty to be used for generation. From the Together Inference docs: \"Controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition.\". Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. From the Together Inference docs: \"used to dynamically adjust the number of choices for each predicted token based on the cumulative probabilities. It specifies a probability threshold, below which all less likely tokens are filtered out. This technique helps to maintain diversity and generate more fluent and natural-sounding text.\". Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. From the Together Inference docs: \"used to limit the number of choices for the next predicted word or token. It specifies the maximum number of tokens to consider at each step, based on their probability of occurrence. This technique helps to speed up the generation process and can improve the quality of the generated text by focusing on the most likely options.\". Defaults to 1.</p> <code>1</code> <code>stop</code> <code>List[str]</code> <p>strings to delimitate the generation process, so that when the model generates any of the provided characters, the generation process is considered completed. Defaults to None.</p> <code>None</code> <code>logprobs</code> <code>int</code> <p>the number of logprobs to be returned for each token. From the Together Inference docs: \"An integer that specifies how many top token log probabilities are included in the response for each token generation step.\". Defaults to None.</p> <code>0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in Together Inference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import TogetherInferenceLLM\n&gt;&gt;&gt; llm = TogetherInferenceLLM(model=\"togethercomputer/llama-2-7b\", task=TextGenerationTask(), prompt_format=\"llama2\")\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/together.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    task: \"Task\",\n    api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    repetition_penalty: float = 1.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = 1,\n    stop: Union[List[str], None] = None,\n    logprobs: int = 0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the TogetherInferenceLLM class.\n\n    Args:\n        model (str): the model to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        temperature (float, optional): the temperature to be used for generation. From the Together\n            Inference docs: \"A decimal number that determines the degree of randomness in the response.\n            A value of 0 will always yield the same output. A temperature much less than 1 favors more\n            correctness and is appropriate for question answering or summarization. A value approaching\n            1 introduces more randomness in the output.\". Defaults to 1.0.\n        repetition_penalty (float, optional): the repetition penalty to be used for generation. From the\n            Together Inference docs: \"Controls the diversity of generated text by reducing the likelihood\n            of repeated sequences. Higher values decrease repetition.\". Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation. From the Together\n            Inference docs: \"used to dynamically adjust the number of choices for each predicted\n            token based on the cumulative probabilities. It specifies a probability threshold,\n            below which all less likely tokens are filtered out. This technique helps to maintain\n            diversity and generate more fluent and natural-sounding text.\". Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation. From the Together Inference\n            docs: \"used to limit the number of choices for the next predicted word or token. It specifies\n            the maximum number of tokens to consider at each step, based on their probability of occurrence.\n            This technique helps to speed up the generation process and can improve the quality of the\n            generated text by focusing on the most likely options.\". Defaults to 1.\n        stop (List[str], optional): strings to delimitate the generation process, so that when the\n            model generates any of the provided characters, the generation process is considered completed.\n            Defaults to None.\n        logprobs (int, optional): the number of logprobs to be returned for each token. From the\n            Together Inference docs: \"An integer that specifies how many top token log probabilities\n            are included in the response for each token generation step.\". Defaults to None.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in Together Inference.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import TogetherInferenceLLM\n        &gt;&gt;&gt; llm = TogetherInferenceLLM(model=\"togethercomputer/llama-2-7b\", task=TextGenerationTask(), prompt_format=\"llama2\")\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    if not _TOGETHER_AVAILABLE:\n        raise ImportError(\n            \"`TogetherInferenceLLM` cannot be used as `together` is not installed, please \"\n            \" install it with `pip install together`.\"\n        )\n\n    together.api_key = api_key or os.getenv(\"TOGETHER_API_KEY\", None)\n    if together.api_key is None:\n        raise ValueError(\n            \"No `api_key` provided, please provide one or set the `TOGETHER_API_KEY` \"\n            \"environment variable.\"\n        )\n\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in Together Inference, available models are {self.available_models}\"\n    self.model = model\n\n    self.max_new_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.repetition_penalty = repetition_penalty\n    self.stop = stop\n    self.logprobs = logprobs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.TransformersLLM","title":"<code>TransformersLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/huggingface/transformers.py</code> <pre><code>class TransformersLLM(LLM):\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        typical_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the TransformersLLM class.\n\n        Args:\n            model (PreTrainedModel): the model to be used for generation.\n            tokenizer (PreTrainedTokenizer): the tokenizer to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            do_sample (bool, optional): whether to sample from the model or not.\n                Defaults to False.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 50.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            typical_p (float, optional): the typical-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used for generation.\n                If `None`, the number of threads will be set to the number of available CPUs.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for formatting the prompts. If `None`, the prompts will not be formatted.\n                Defaults to `None`.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): the function to be used\n                for formatting the prompts. If `None`, the prompts will not be formatted.\n\n        Examples:\n            &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n            &gt;&gt;&gt; llm = TransformersLLM(\n            ...     model=model,\n            ...     tokenizer=tokenizer,\n            ...     task=TextGenerationTask(),\n            ... )\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        self.max_new_tokens = max_new_tokens\n        self.do_sample = do_sample\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.typical_p = typical_p\n\n        self.model = model\n        self.tokenizer = tokenizer\n\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        if (\n            hasattr(self.tokenizer, \"use_default_system_prompt\")\n            and self.tokenizer.use_default_system_prompt  # type: ignore\n        ):\n            # The `tokenizer` also has a method named `apply_chat_template` that expects a `Conversation` as OpenAI does with the ChatML format\n            warnings.warn(\n                \"The provided `tokenizer` has `use_default_system_prompt=True` which means that the default system prompt will be used, which may collide with the `task` provided as an arg to this class.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_new_tokens,\n                \"do_sample\": self.do_sample,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"typical_p\": self.typical_p,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the Transformers model.\"\"\"\n        return self.model.config.name_or_path\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        encodings = self.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n        encodings = encodings.to(self.model.device)\n        with torch.inference_mode():\n            generated_ids = self.model.generate(\n                **encodings,  # type: ignore\n                pad_token_id=self.tokenizer.eos_token_id,\n                generation_config=GenerationConfig(\n                    do_sample=self.do_sample,\n                    temperature=self.temperature,\n                    max_new_tokens=self.max_new_tokens,\n                    top_k=self.top_k,\n                    top_p=self.top_p,\n                    typical_p=self.typical_p,\n                    num_return_sequences=num_generations,\n                ),\n            )\n        raw_outputs = self.tokenizer.batch_decode(\n            generated_ids[:, encodings.input_ids.shape[1] :],\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n        outputs = []\n        for prompt, i in zip(prompts, range(0, len(raw_outputs), num_generations)):\n            output = []\n            for raw_output in raw_outputs[i : i + num_generations]:\n                try:\n                    parsed_output = self.task.parse_output(raw_output)\n                except Exception as e:\n                    logger.error(f\"Error parsing Transformers output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.TransformersLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the Transformers model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.TransformersLLM.__init__","title":"<code>__init__(model, tokenizer, task, max_new_tokens=128, do_sample=False, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the TransformersLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>the model to be used for generation.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>the tokenizer to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>do_sample</code> <code>bool</code> <p>whether to sample from the model or not. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>typical_p</code> <code>float</code> <p>the typical-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for generation. If <code>None</code>, the number of threads will be set to the number of available CPUs. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for formatting the prompts. If <code>None</code>, the prompts will not be formatted. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>the function to be used for formatting the prompts. If <code>None</code>, the prompts will not be formatted.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import TransformersLLM\n&gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n&gt;&gt;&gt; llm = TransformersLLM(\n...     model=model,\n...     tokenizer=tokenizer,\n...     task=TextGenerationTask(),\n... )\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/huggingface/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    do_sample: bool = False,\n    temperature: float = 1.0,\n    top_k: int = 50,\n    top_p: float = 1.0,\n    typical_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the TransformersLLM class.\n\n    Args:\n        model (PreTrainedModel): the model to be used for generation.\n        tokenizer (PreTrainedTokenizer): the tokenizer to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        do_sample (bool, optional): whether to sample from the model or not.\n            Defaults to False.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 50.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        typical_p (float, optional): the typical-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used for generation.\n            If `None`, the number of threads will be set to the number of available CPUs.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for formatting the prompts. If `None`, the prompts will not be formatted.\n            Defaults to `None`.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): the function to be used\n            for formatting the prompts. If `None`, the prompts will not be formatted.\n\n    Examples:\n        &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n        &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        &gt;&gt;&gt; llm = TransformersLLM(\n        ...     model=model,\n        ...     tokenizer=tokenizer,\n        ...     task=TextGenerationTask(),\n        ... )\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    self.max_new_tokens = max_new_tokens\n    self.do_sample = do_sample\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.typical_p = typical_p\n\n    self.model = model\n    self.tokenizer = tokenizer\n\n    if self.tokenizer.pad_token is None:\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n    if (\n        hasattr(self.tokenizer, \"use_default_system_prompt\")\n        and self.tokenizer.use_default_system_prompt  # type: ignore\n    ):\n        # The `tokenizer` also has a method named `apply_chat_template` that expects a `Conversation` as OpenAI does with the ChatML format\n        warnings.warn(\n            \"The provided `tokenizer` has `use_default_system_prompt=True` which means that the default system prompt will be used, which may collide with the `task` provided as an arg to this class.\",\n            UserWarning,\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.VertexAIEndpointLLM","title":"<code>VertexAIEndpointLLM</code>","text":"<p>             Bases: <code>LLM</code></p> <p>An <code>LLM</code> which uses a Vertex AI Online prediction endpoint for the generation.</p> <p>More information about Vertex AI Endpoints can be found here: https://cloud.google.com/vertex-ai/docs/general/deployment#deploy_a_model_to_an_endpoint</p> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>class VertexAIEndpointLLM(LLM):\n    \"\"\"An `LLM` which uses a Vertex AI Online prediction endpoint for the generation.\n\n    More information about Vertex AI Endpoints can be found here:\n    https://cloud.google.com/vertex-ai/docs/general/deployment#deploy_a_model_to_an_endpoint\n    \"\"\"\n\n    def __init__(\n        self,\n        endpoint_id: str,\n        task: \"Task\",\n        project: Optional[str] = None,\n        location: str = \"us-central1\",\n        generation_kwargs: Optional[Dict[str, Any]] = None,\n        prompt_argument: str = \"prompt\",\n        num_generations_argument: str = \"n\",\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the `VertexAIEndpointLLM` class.\n\n        Args:\n            endpoint_id (str): the ID of the Vertex AI endpoint to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            project (Optional[str], optional): the project to be used for generation. If `None`,\n                the default project will be used. Defaults to `None`.\n            location (str, optional): the location of the Vertex AI endpoint to be used for\n                generation. Defaults to \"us-central1\".\n            generation_kwargs (Optional[Dict[str, Any]], optional): the generation parameters\n                to be used for generation. The name of the parameters will depend on the\n                Docker image used to deploy the model to the Vertex AI endpoint. Defaults\n                to `None`.\n            prompt_argument (str, optional): the name of the Vertex AI Endpoint key to\n                be used for the prompt. Defaults to \"prompt\".\n            num_generations_argument (str, optional): the name of the Vertex AI Endpoint\n                key to be used to specify the number of generations per prompt. Defaults\n                to \"n\".\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _VERTEXAI_AVAILABLE:\n            raise ImportError(\n                \"`VertexAIEndpointLLM` cannot be used as `google-cloud-aiplatform` is not\"\n                \" installed, please install it with `pip install google-cloud-aiplatform`\"\n            )\n\n        if project is None:\n            try:\n                project = google.auth.default()[1]\n            except DefaultCredentialsError as e:\n                raise ValueError(\n                    \"No `project` was specified and no default credentials were found.\"\n                ) from e\n\n        if generation_kwargs is None:\n            generation_kwargs = {}\n\n        self.endpoint_id = endpoint_id\n        self.project = project\n        self.location = location\n        self.generation_kwargs = generation_kwargs\n        self.prompt_argument = prompt_argument\n        self.num_generations_argument = num_generations_argument\n\n        self.client = PredictionServiceClient(\n            client_options=ClientOptions(\n                api_endpoint=f\"{self.location}-aiplatform.googleapis.com\"\n            )\n        )\n\n    @cached_property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the model used for generation.\"\"\"\n        client = EndpointServiceClient(\n            client_options=ClientOptions(\n                api_endpoint=f\"{self.location}-aiplatform.googleapis.com\"\n            )\n        )\n        endpoint = client.get_endpoint(name=self.endpoint_path)\n        return endpoint.deployed_models[0].display_name\n\n    @property\n    def endpoint_path(self) -&gt; str:\n        \"\"\"Returns the path of the Vertex AI endpoint to be used for generation.\"\"\"\n        return self.client.endpoint_path(\n            project=self.project,  # type: ignore\n            location=self.location,\n            endpoint=self.endpoint_id,\n        )\n\n    @_vertexai_retry_decorator\n    def _call_vertexai_endpoint(self, instances: List[Any]) -&gt; Any:\n        return self.client.predict(endpoint=self.endpoint_path, instances=instances)\n\n    def _prepare_instances(\n        self, prompts: List[str], num_generations: int\n    ) -&gt; List[\"Value\"]:\n        \"\"\"Prepares the instances to be sent to the Vertex AI endpoint.\n\n        Args:\n            prompts (List[str]): the prompts to be used for generation.\n            num_generations (int): the number of generations to be performed for each prompt.\n\n        Returns:\n            The instances to be sent to the Vertex AI endpoint.\n        \"\"\"\n        instances = []\n        for prompt in prompts:\n            instance = json_format.ParseDict(\n                {\n                    self.prompt_argument: prompt,\n                    self.num_generations_argument: num_generations,\n                    **self.generation_kwargs,\n                },\n                Value(),\n            )\n            instances.append(instance)\n        return instances\n\n    def _single_output(self, instance: Any) -&gt; List[LLMOutput]:\n        try:\n            # NOTE: `predict` method accepts a list of instances, but depending on the\n            # deployed Docker image, it can just accept one instance.\n            response = self._call_vertexai_endpoint(instances=[instance])\n        except exceptions.InternalServerError as e:\n            raise ValueError(\n                \"The Vertex AI endpoint returned 500 Internal Server Error. This is\"\n                \" usually caused due to wrong generation parameters. Please check the\"\n                \" `generation_parameters` and try again.\"\n            ) from e\n\n        output = []\n        for prediction in response.predictions:\n            # Vertex endpoint output is `Prompt:\\n{{ model_prompt }}\\nOutput:\\n{{ model_output }}`\n            # so we need to do a pre-parsing to remove the `Prompt:` and `Output:` parts.\n            match = _PARSE_VERTEXAI_ENDPOINT_PREDICTION_REGEX.search(prediction)\n            if not match:\n                raise ValueError(\n                    \"Couldn't parse the response from the Vertex AI endpoint.\"\n                )\n\n            model_output = match.group(1).strip()\n\n            try:\n                parsed_output = self.task.parse_output(model_output)\n            except Exception as e:\n                logger.error(f\"Error parsing Vertex AI endpoint model response: {e}\")\n                parsed_output = None\n            output.append(\n                LLMOutput(\n                    model_name=self.model_name,\n                    prompt_used=instance.struct_value[self.prompt_argument],\n                    raw_output=model_output,\n                    parsed_output=parsed_output,\n                )\n            )\n        return output\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        prompts = self._generate_prompts(inputs)\n        instances = self._prepare_instances(\n            prompts=prompts, num_generations=num_generations\n        )\n        return [self._single_output(instance) for instance in instances]\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.VertexAIEndpointLLM.endpoint_path","title":"<code>endpoint_path: str</code>  <code>property</code>","text":"<p>Returns the path of the Vertex AI endpoint to be used for generation.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.VertexAIEndpointLLM.model_name","title":"<code>model_name: str</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the name of the model used for generation.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.VertexAIEndpointLLM.__init__","title":"<code>__init__(endpoint_id, task, project=None, location='us-central1', generation_kwargs=None, prompt_argument='prompt', num_generations_argument='n', num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the <code>VertexAIEndpointLLM</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_id</code> <code>str</code> <p>the ID of the Vertex AI endpoint to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>project</code> <code>Optional[str]</code> <p>the project to be used for generation. If <code>None</code>, the default project will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>location</code> <code>str</code> <p>the location of the Vertex AI endpoint to be used for generation. Defaults to \"us-central1\".</p> <code>'us-central1'</code> <code>generation_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>the generation parameters to be used for generation. The name of the parameters will depend on the Docker image used to deploy the model to the Vertex AI endpoint. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_argument</code> <code>str</code> <p>the name of the Vertex AI Endpoint key to be used for the prompt. Defaults to \"prompt\".</p> <code>'prompt'</code> <code>num_generations_argument</code> <code>str</code> <p>the name of the Vertex AI Endpoint key to be used to specify the number of generations per prompt. Defaults to \"n\".</p> <code>'n'</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>def __init__(\n    self,\n    endpoint_id: str,\n    task: \"Task\",\n    project: Optional[str] = None,\n    location: str = \"us-central1\",\n    generation_kwargs: Optional[Dict[str, Any]] = None,\n    prompt_argument: str = \"prompt\",\n    num_generations_argument: str = \"n\",\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the `VertexAIEndpointLLM` class.\n\n    Args:\n        endpoint_id (str): the ID of the Vertex AI endpoint to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        project (Optional[str], optional): the project to be used for generation. If `None`,\n            the default project will be used. Defaults to `None`.\n        location (str, optional): the location of the Vertex AI endpoint to be used for\n            generation. Defaults to \"us-central1\".\n        generation_kwargs (Optional[Dict[str, Any]], optional): the generation parameters\n            to be used for generation. The name of the parameters will depend on the\n            Docker image used to deploy the model to the Vertex AI endpoint. Defaults\n            to `None`.\n        prompt_argument (str, optional): the name of the Vertex AI Endpoint key to\n            be used for the prompt. Defaults to \"prompt\".\n        num_generations_argument (str, optional): the name of the Vertex AI Endpoint\n            key to be used to specify the number of generations per prompt. Defaults\n            to \"n\".\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _VERTEXAI_AVAILABLE:\n        raise ImportError(\n            \"`VertexAIEndpointLLM` cannot be used as `google-cloud-aiplatform` is not\"\n            \" installed, please install it with `pip install google-cloud-aiplatform`\"\n        )\n\n    if project is None:\n        try:\n            project = google.auth.default()[1]\n        except DefaultCredentialsError as e:\n            raise ValueError(\n                \"No `project` was specified and no default credentials were found.\"\n            ) from e\n\n    if generation_kwargs is None:\n        generation_kwargs = {}\n\n    self.endpoint_id = endpoint_id\n    self.project = project\n    self.location = location\n    self.generation_kwargs = generation_kwargs\n    self.prompt_argument = prompt_argument\n    self.num_generations_argument = num_generations_argument\n\n    self.client = PredictionServiceClient(\n        client_options=ClientOptions(\n            api_endpoint=f\"{self.location}-aiplatform.googleapis.com\"\n        )\n    )\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.VertexAILLM","title":"<code>VertexAILLM</code>","text":"<p>             Bases: <code>LLM</code></p> <p>An <code>LLM</code> which allows to use Google's proprietary models from the Vertex AI APIs:</p> <ul> <li>Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini</li> <li>Codey API: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview</li> <li>Text API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text</li> </ul> <p>To use the <code>VertexAILLM</code> is necessary to have configured the Google Cloud authentication using one of these methods:</p> <ul> <li>Setting <code>GOOGLE_CLOUD_CREDENTIALS</code> environment variable</li> <li>Using <code>gcloud auth application-default login</code> command</li> <li>Using <code>vertexai.init</code> function from the <code>google-cloud-aiplatform</code> library</li> </ul> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>class VertexAILLM(LLM):\n    \"\"\"An `LLM` which allows to use Google's proprietary models from the Vertex AI APIs:\n\n    - Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n    - Codey API: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview\n    - Text API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text\n\n    To use the `VertexAILLM` is necessary to have configured the Google Cloud authentication\n    using one of these methods:\n\n    - Setting `GOOGLE_CLOUD_CREDENTIALS` environment variable\n    - Using `gcloud auth application-default login` command\n    - Using `vertexai.init` function from the `google-cloud-aiplatform` library\n    \"\"\"\n\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"gemini-pro\",\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        max_new_tokens: int = 128,\n        stop_sequences: Optional[List[str]] = None,\n        num_threads: Union[int, None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the `VertexGenerativeModelLLM` class.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"gemini-pro\".\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 40.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n        \"\"\"\n        super().__init__(task=task, num_threads=num_threads)\n\n        if not _VERTEXAI_AVAILABLE:\n            raise ImportError(\n                \"`VertexAILLM` cannot be used as `google-cloud-aiplatform` is not installed,\"\n                \" please install it with `pip install google-cloud-aiplatform`\"\n            )\n\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.max_output_tokens = max_new_tokens\n        self.stop_sequences = stop_sequences\n\n        if is_gemini_model(model):\n            self.model = GenerativeModel(model)\n        elif is_codey_model(model):\n            self.model = CodeGenerationModel.from_pretrained(model)\n        else:\n            self.model = TextGenerationModel.from_pretrained(model)\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the model used for generation.\"\"\"\n        if isinstance(self.model, GenerativeModel):\n            return self.model._model_name\n\n        return self.model._model_id\n\n    def _generate_contents(self, prompts: List[str]) -&gt; List[List[Dict[str, Any]]]:\n        \"\"\"Generates a list of valid dicts that can be parsed to `vertexai.preview.generative_models.Content`\n        objects for each input.\n\n        Args:\n            prompts (List[str]): the prompts to be used for generation.\n\n        Returns:\n            List[List[Dict[str, Any]]]: the list of valid `vertexai.preview.generative_models.Content`\n                objects.\n        \"\"\"\n        return [[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}] for prompt in prompts]\n\n    @_vertexai_retry_decorator\n    def _call_generative_model_with_backoff(\n        self, contents: List[Dict[str, Any]], **kwargs: Any\n    ) -&gt; \"GenerationResponse\":\n        return self.model.generate_content(  # type: ignore\n            contents=contents,\n            # TODO: update `candidate_count` to have `num_generations` as value once valid range is not [1, 2)\n            generation_config=GenerationConfig(candidate_count=1, **kwargs),\n        )\n\n    def _generative_model_single_output(\n        self, contents: List[Dict[str, Any]]\n    ) -&gt; LLMOutput:\n        raw_output = None\n        try:\n            response = self._call_generative_model_with_backoff(\n                contents=contents,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                top_k=self.top_k,\n                max_output_tokens=self.max_output_tokens,\n                stop_sequences=self.stop_sequences,\n            )\n            raw_output = response.text\n            parsed_output = self.task.parse_output(raw_output)\n        except ValueError as e:\n            logger.error(f\"Vertex AI Gemini API model didn't return content: {e}\")\n            return LLMOutput(\n                model_name=self.model_name,\n                prompt_used=contents,\n                raw_output=None,\n                parsed_output=None,\n            )\n        except Exception as e:\n            logger.error(f\"Error parsing Vertex AI Gemini API model response: {e}\")\n            parsed_output = None\n\n        return LLMOutput(\n            model_name=self.model_name,\n            prompt_used=contents,\n            raw_output=raw_output,\n            parsed_output=parsed_output,\n        )\n\n    def _generate_with_generative_model(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        \"\"\"Generate `num_generations` for each input in `inputs` using a Vertex AI Gemini\n        API model.\"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"default\")\n        inputs_contents = self._generate_contents(prompts)\n        outputs = []\n        for contents in inputs_contents:\n            output = []\n            # TODO: remove this for-loop once `GenerationConfig.candidate_count` valid range is not [1, 2)\n            for _ in range(num_generations):\n                output.append(self._generative_model_single_output(contents=contents))\n            outputs.append(output)\n        return outputs\n\n    @_vertexai_retry_decorator\n    def _call_text_generation_model(\n        self, **kwargs: Any\n    ) -&gt; \"MultiCandidateTextGenerationResponse\":\n        return self.model.predict(**kwargs)  # type: ignore\n\n    def _text_generation_model_single_output(\n        self, prompt: str, num_generations: int\n    ) -&gt; List[LLMOutput]:\n        response = self._call_text_generation_model(\n            prompt=prompt,\n            max_output_tokens=self.max_output_tokens,\n            temperature=self.temperature,\n            top_k=self.top_k,\n            top_p=self.top_p,\n            stop_sequences=self.stop_sequences,\n            # WARNING: The model can return &lt; `candidate_count` generations depending\n            # on the generation parameters and the input.\n            candidate_count=num_generations,\n        )\n\n        output = []\n        for candidate in response.candidates:\n            try:\n                parsed_response = self.task.parse_output(candidate.text)\n            except Exception as e:\n                logger.error(\n                    f\"Error parsing Vertex AI Text/Code API model response: {e}\"\n                )\n                parsed_response = None\n\n            output.append(\n                LLMOutput(\n                    model_name=self.model_name,\n                    prompt_used=prompt,\n                    raw_output=candidate.text,\n                    parsed_output=parsed_response,\n                )\n            )\n        return output\n\n    def _generate_with_text_generation_model(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        \"\"\"Generate `num_generations` for each input in `inputs` using a Vertex AI Text/Code\n        API model.\"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"default\")\n        outputs = []\n        for prompt in prompts:\n            outputs.append(\n                self._text_generation_model_single_output(prompt, num_generations)\n            )\n        return outputs\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        if isinstance(self.model, GenerativeModel):\n            return self._generate_with_generative_model(inputs, num_generations)\n\n        return self._generate_with_text_generation_model(inputs, num_generations)\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.VertexAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the model used for generation.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.VertexAILLM.__init__","title":"<code>__init__(task, model='gemini-pro', temperature=None, top_p=None, top_k=None, max_new_tokens=128, stop_sequences=None, num_threads=None)</code>","text":"<p>Initializes the <code>VertexGenerativeModelLLM</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"gemini-pro\".</p> <code>'gemini-pro'</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 40.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"gemini-pro\",\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    max_new_tokens: int = 128,\n    stop_sequences: Optional[List[str]] = None,\n    num_threads: Union[int, None] = None,\n) -&gt; None:\n    \"\"\"Initializes the `VertexGenerativeModelLLM` class.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"gemini-pro\".\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 40.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n    \"\"\"\n    super().__init__(task=task, num_threads=num_threads)\n\n    if not _VERTEXAI_AVAILABLE:\n        raise ImportError(\n            \"`VertexAILLM` cannot be used as `google-cloud-aiplatform` is not installed,\"\n            \" please install it with `pip install google-cloud-aiplatform`\"\n        )\n\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.max_output_tokens = max_new_tokens\n    self.stop_sequences = stop_sequences\n\n    if is_gemini_model(model):\n        self.model = GenerativeModel(model)\n    elif is_codey_model(model):\n        self.model = CodeGenerationModel.from_pretrained(model)\n    else:\n        self.model = TextGenerationModel.from_pretrained(model)\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.vLLM","title":"<code>vLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/vllm.py</code> <pre><code>class vLLM(LLM):\n    def __init__(\n        self,\n        model: \"_vLLM\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        presence_penalty: float = 0.0,\n        frequency_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the vLLM class.\n\n        Args:\n            model (_vLLM): the vLLM model to be used.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to -1.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n\n        Examples:\n            &gt;&gt;&gt; from vllm import LLM\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import vLLM\n            &gt;&gt;&gt; model = LLM(model=\"gpt2\")\n            &gt;&gt;&gt; llm = vLLM(model=model, task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _VLLM_AVAILABLE:\n            raise ImportError(\n                \"`vLLM` cannot be used as `vllm` is not installed, please \"\n                \" install it with `pip install vllm`.\"\n            )\n\n        self.presence_penalty = presence_penalty\n        self.frequency_penalty = frequency_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.max_tokens = max_new_tokens\n\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"presence_penalty\": self.presence_penalty,\n                \"frequency_penalty\": self.frequency_penalty,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the vLLM model.\"\"\"\n        return self.model.llm_engine.model_config.model  # type: ignore\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        requests = self.model.generate(\n            prompts,\n            SamplingParams(  # type: ignore\n                n=num_generations,\n                presence_penalty=self.presence_penalty,\n                frequency_penalty=self.frequency_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                top_k=self.top_k,\n                max_tokens=self.max_tokens,\n            ),\n            use_tqdm=False,  # type: ignore\n        )\n        outputs = []\n        for request, prompt in zip(requests, prompts):\n            output = []\n            for request_output in request.outputs:\n                try:\n                    parsed_output = self.task.parse_output(request_output.text)\n                except Exception as e:\n                    logger.error(f\"Error parsing vLLM output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=request_output.text,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.vLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the vLLM model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.vLLM.__init__","title":"<code>__init__(model, task, max_new_tokens=128, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the vLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>the vLLM model to be used.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to -1.</p> <code>-1</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from vllm import LLM\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import vLLM\n&gt;&gt;&gt; model = LLM(model=\"gpt2\")\n&gt;&gt;&gt; llm = vLLM(model=model, task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/vllm.py</code> <pre><code>def __init__(\n    self,\n    model: \"_vLLM\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    presence_penalty: float = 0.0,\n    frequency_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = -1,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the vLLM class.\n\n    Args:\n        model (_vLLM): the vLLM model to be used.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to -1.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n\n    Examples:\n        &gt;&gt;&gt; from vllm import LLM\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import vLLM\n        &gt;&gt;&gt; model = LLM(model=\"gpt2\")\n        &gt;&gt;&gt; llm = vLLM(model=model, task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _VLLM_AVAILABLE:\n        raise ImportError(\n            \"`vLLM` cannot be used as `vllm` is not installed, please \"\n            \" install it with `pip install vllm`.\"\n        )\n\n    self.presence_penalty = presence_penalty\n    self.frequency_penalty = frequency_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.max_tokens = max_new_tokens\n\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/anyscale/","title":"anyscale","text":""},{"location":"reference/distilabel/llm/anyscale/#distilabel.llm.anyscale.AnyscaleLLM","title":"<code>AnyscaleLLM</code>","text":"<p>             Bases: <code>OpenAILLM</code></p> Source code in <code>src/distilabel/llm/anyscale.py</code> <pre><code>class AnyscaleLLM(OpenAILLM):\n    def __init__(\n        self,\n        model: str,\n        task: \"Task\",\n        client: Union[\"OpenAI\", None] = None,\n        api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the AnyscaleLLM class.\n\n        Args:\n            model (str): the model to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n                If `None`, a new client will be created. Defaults to `None`.\n            api_key (Union[str, None], optional): the Anyscale API key to be used for generation.\n                If `None`, the `ANYSCALE_API_KEY` environment variable will be used. Defaults to `None`.\n                Visit \"https://docs.endpoints.anyscale.com/guides/authenticate/\" for more information.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in your OpenAI account.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import AnyscaleLLM\n            &gt;&gt;&gt; llm = AnyscaleLLM(model=\"HuggingFaceH4/zephyr-7b-beta\", task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        LLM.__init__(\n            self,\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _OPENAI_AVAILABLE:\n            raise ImportError(\n                \"`AnyscaleLLM` cannot be used as `openai` is not installed, please \"\n                \" install it with `pip install openai`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n\n        self.client = client or OpenAI(\n            api_key=api_key or os.getenv(\"ANYSCALE_API_KEY\"),\n            max_retries=6,\n            base_url=\"https://api.endpoints.anyscale.com/v1\",\n        )\n\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in your Anyscale account, available models are {self.available_models}\"\n        self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/anyscale/#distilabel.llm.anyscale.AnyscaleLLM.__init__","title":"<code>__init__(model, task, client=None, api_key=None, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the AnyscaleLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>the model to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>client</code> <code>Union[OpenAI, None]</code> <p>an OpenAI client to be used for generation. If <code>None</code>, a new client will be created. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Union[str, None]</code> <p>the Anyscale API key to be used for generation. If <code>None</code>, the <code>ANYSCALE_API_KEY</code> environment variable will be used. Defaults to <code>None</code>. Visit \"https://docs.endpoints.anyscale.com/guides/authenticate/\" for more information.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in your OpenAI account.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import AnyscaleLLM\n&gt;&gt;&gt; llm = AnyscaleLLM(model=\"HuggingFaceH4/zephyr-7b-beta\", task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/anyscale.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    task: \"Task\",\n    client: Union[\"OpenAI\", None] = None,\n    api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the AnyscaleLLM class.\n\n    Args:\n        model (str): the model to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n            If `None`, a new client will be created. Defaults to `None`.\n        api_key (Union[str, None], optional): the Anyscale API key to be used for generation.\n            If `None`, the `ANYSCALE_API_KEY` environment variable will be used. Defaults to `None`.\n            Visit \"https://docs.endpoints.anyscale.com/guides/authenticate/\" for more information.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in your OpenAI account.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import AnyscaleLLM\n        &gt;&gt;&gt; llm = AnyscaleLLM(model=\"HuggingFaceH4/zephyr-7b-beta\", task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    LLM.__init__(\n        self,\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _OPENAI_AVAILABLE:\n        raise ImportError(\n            \"`AnyscaleLLM` cannot be used as `openai` is not installed, please \"\n            \" install it with `pip install openai`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.frequency_penalty = frequency_penalty\n    self.presence_penalty = presence_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n\n    self.client = client or OpenAI(\n        api_key=api_key or os.getenv(\"ANYSCALE_API_KEY\"),\n        max_retries=6,\n        base_url=\"https://api.endpoints.anyscale.com/v1\",\n    )\n\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in your Anyscale account, available models are {self.available_models}\"\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/base/","title":"base","text":""},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM","title":"<code>LLM</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>class LLM(ABC):\n    def __init__(\n        self,\n        task: Task,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the LLM base class.\n\n        Note:\n            This class is intended to be used internally, but you anyone can still create\n            a subclass, implement the `abstractmethod`s and use it.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[\"SupportedFormats\", None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n        \"\"\"\n        self.task = task\n\n        self.thread_pool_executor = (\n            ThreadPoolExecutor(max_workers=num_threads)\n            if num_threads is not None\n            else None\n        )\n\n        self.prompt_format = prompt_format\n        self.prompt_formatting_fn = prompt_formatting_fn\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the thread pool executor if it is not `None`.\"\"\"\n        if self.thread_pool_executor is not None:\n            self.thread_pool_executor.shutdown()\n\n    @property\n    def num_threads(self) -&gt; Union[int, None]:\n        if self.thread_pool_executor:\n            return self.thread_pool_executor._max_workers\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(task={self.task.__class__.__name__}, num_threads={self.num_threads}, promp_format='{self.prompt_format}', model='{self.model_name}')\"\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"task\", self.task\n        yield \"num_threads\", self.num_threads\n        yield \"prompt_format\", self.prompt_format\n        if self.prompt_formatting_fn is not None:\n            args = f\"({', '.join(self.prompt_formatting_fn.__code__.co_varnames)})\"\n            representation = self.prompt_formatting_fn.__name__ + args\n            yield \"prompt_formatting_fn\", representation\n        yield \"model\", self.model_name\n\n    @property\n    @abstractmethod\n    def model_name(self) -&gt; str:\n        pass\n\n    def _generate_prompts(\n        self,\n        inputs: List[Dict[str, Any]],\n        default_format: Union[\"SupportedFormats\", None] = None,\n    ) -&gt; List[Any]:\n        \"\"\"Generates the prompts to be used for generation.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            default_format (Union[\"SupportedFormats\", None], optional): the default format to be used\n                for the prompt if no `prompt_format` is specified. Defaults to `None`.\n\n        Returns:\n            List[Any]: the generated prompts.\n\n        Raises:\n            ValueError: if the generated prompt is not of the expected type.\n        \"\"\"\n        prompts = []\n        for input in inputs:\n            prompt = self.task.generate_prompt(**input)\n            if not isinstance(prompt, Prompt) and self.prompt_formatting_fn is not None:\n                warnings.warn(\n                    \"The method `generate_prompt` is not returning a `Prompt` class but a prompt\"\n                    f\" of `type={type(prompt)}`, meaning that a pre-formatting has already been\"\n                    \" applied in the `task.generate_prompt` method, so the usage of a `prompt_formatting_fn`\"\n                    \" is discouraged.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                prompt = self.prompt_formatting_fn(prompt)\n            elif isinstance(prompt, Prompt) and self.prompt_formatting_fn is None:\n                if self.prompt_format is not None or default_format is not None:\n                    prompt = prompt.format_as(\n                        format=self.prompt_format or default_format  # type: ignore\n                    )\n                else:\n                    warnings.warn(\n                        \"No `prompt_format` has been specified and no `default_format` is set, so\"\n                        \" the prompt will be concatenated with a line-break and no specific formatting\"\n                        \" by default.\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n                    prompt = prompt.format_as(format=\"default\")\n            prompts.append(prompt)\n        return prompts\n\n    @abstractmethod\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        pass\n\n    def _get_valid_inputs(\n        self, inputs: List[Dict[str, Any]]\n    ) -&gt; Tuple[List[Dict[str, Any]], List[int]]:\n        \"\"\"Returns the valid inputs and the indices of the invalid inputs.\n\n        A valid input is an input that contains all the arguments required by the task.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n\n        Returns:\n            Tuple[List[Dict[str, Any]], List[int]]: a tuple containing the valid inputs and\n                the indices of the invalid inputs.\n        \"\"\"\n\n        valid_inputs = []\n        not_valid_inputs_indices = []\n        for i, input in enumerate(inputs):\n            if not all(input_arg in input for input_arg in self.task.input_args_names):\n                logger.warn(\n                    f\"Missing {self.task.__class__.__name__} input argument in batch element {i}\"\n                )\n                not_valid_inputs_indices.append(i)\n                continue\n\n            valid_inputs.append(input)\n\n        return valid_inputs, not_valid_inputs_indices\n\n    def _fill_missing_inputs(\n        self,\n        generations: List[List[LLMOutput]],\n        invalid_inputs_indices: List[int],\n        num_generations: int,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Fills the `generations` list with empty `LLMOutput`s for the inputs that were\n        not valid for the associated task of this `LLM`.\n\n        Args:\n            generations (List[List[LLMOutput]]): the generations to be filled.\n            invalid_inputs_indices (List[int]): the indices of the inputs that were not\n                valid for the associated task of this `LLM`.\n            num_generations (int): the number of generations to be performed for each input.\n\n        Returns:\n            List[List[LLMOutput]]: the filled generations.\n        \"\"\"\n\n        filled_generations = generations.copy()\n        for idx in invalid_inputs_indices:\n            filled_generations.insert(\n                idx,\n                [\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=None,\n                        raw_output=None,\n                        parsed_output=None,\n                    )\n                    for _ in range(num_generations)\n                ],\n            )\n        return filled_generations\n\n    def generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; Union[List[List[\"LLMOutput\"]], Future[List[List[\"LLMOutput\"]]]]:\n        \"\"\"Generates the outputs for the given inputs using the LLM.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each input.\n                Defaults to `1`.\n            progress_callback_func (Union[Callable, None], optional): a function to be called at each\n                generation step. Defaults to `None`.\n\n        Returns:\n            Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.\n        \"\"\"\n\n        def _progress():\n            if progress_callback_func is not None:\n                progress_callback_func(advance=num_generations * len(inputs))\n\n        valid_inputs, invalid_inputs_indices = self._get_valid_inputs(inputs)\n\n        if self.thread_pool_executor is not None:\n            futures = []\n            for input in valid_inputs:\n                future = self.thread_pool_executor.submit(\n                    self._generate, [input], num_generations\n                )\n                futures.append(future)\n            future = when_all_complete(\n                futures=futures,\n                callback=lambda generations: self._fill_missing_inputs(\n                    generations, invalid_inputs_indices, num_generations\n                ),\n            )\n            future.add_done_callback(lambda _: _progress())\n            return future\n\n        generations = self._generate(valid_inputs, num_generations)\n\n        generations = self._fill_missing_inputs(\n            generations, invalid_inputs_indices, num_generations\n        )\n\n        _progress()\n        return generations\n\n    @property\n    def return_futures(self) -&gt; bool:\n        \"\"\"Whether the `LLM` returns futures\"\"\"\n        return self.thread_pool_executor is not None\n\n    def validate_prompts(\n        self,\n        inputs: List[Dict[str, Any]],\n        default_format: Union[\"SupportedFormats\", None] = None,\n    ) -&gt; str:\n        \"\"\"Generates the prompts to be used for generation, can be used to check the prompts visually.\n\n        Args:\n            inputs (List[Dict[str, Any]]):\n                The inputs to be used for generation.\n\n        Returns:\n            str: The prompts that would be used for the generation.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; llm.validate_prompts([{\"input\": \"Your input\"}])[0]\n            You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n            I'm valid for text generation task\n        \"\"\"\n        return self._generate_prompts(inputs, default_format=default_format)\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.return_futures","title":"<code>return_futures: bool</code>  <code>property</code>","text":"<p>Whether the <code>LLM</code> returns futures</p>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.__del__","title":"<code>__del__()</code>","text":"<p>Shuts down the thread pool executor if it is not <code>None</code>.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shuts down the thread pool executor if it is not `None`.\"\"\"\n    if self.thread_pool_executor is not None:\n        self.thread_pool_executor.shutdown()\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.__init__","title":"<code>__init__(task, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the LLM base class.</p> Note <p>This class is intended to be used internally, but you anyone can still create a subclass, implement the <code>abstractmethod</code>s and use it.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union['SupportedFormats', None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __init__(\n    self,\n    task: Task,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the LLM base class.\n\n    Note:\n        This class is intended to be used internally, but you anyone can still create\n        a subclass, implement the `abstractmethod`s and use it.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[\"SupportedFormats\", None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n    \"\"\"\n    self.task = task\n\n    self.thread_pool_executor = (\n        ThreadPoolExecutor(max_workers=num_threads)\n        if num_threads is not None\n        else None\n    )\n\n    self.prompt_format = prompt_format\n    self.prompt_formatting_fn = prompt_formatting_fn\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.generate","title":"<code>generate(inputs, num_generations=1, progress_callback_func=None)</code>","text":"<p>Generates the outputs for the given inputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>the inputs to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>progress_callback_func</code> <code>Union[Callable, None]</code> <p>a function to be called at each generation step. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[List['LLMOutput']], Future[List[List['LLMOutput']]]]</code> <p>Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[Dict[str, Any]],\n    num_generations: int = 1,\n    progress_callback_func: Union[Callable, None] = None,\n) -&gt; Union[List[List[\"LLMOutput\"]], Future[List[List[\"LLMOutput\"]]]]:\n    \"\"\"Generates the outputs for the given inputs using the LLM.\n\n    Args:\n        inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each input.\n            Defaults to `1`.\n        progress_callback_func (Union[Callable, None], optional): a function to be called at each\n            generation step. Defaults to `None`.\n\n    Returns:\n        Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.\n    \"\"\"\n\n    def _progress():\n        if progress_callback_func is not None:\n            progress_callback_func(advance=num_generations * len(inputs))\n\n    valid_inputs, invalid_inputs_indices = self._get_valid_inputs(inputs)\n\n    if self.thread_pool_executor is not None:\n        futures = []\n        for input in valid_inputs:\n            future = self.thread_pool_executor.submit(\n                self._generate, [input], num_generations\n            )\n            futures.append(future)\n        future = when_all_complete(\n            futures=futures,\n            callback=lambda generations: self._fill_missing_inputs(\n                generations, invalid_inputs_indices, num_generations\n            ),\n        )\n        future.add_done_callback(lambda _: _progress())\n        return future\n\n    generations = self._generate(valid_inputs, num_generations)\n\n    generations = self._fill_missing_inputs(\n        generations, invalid_inputs_indices, num_generations\n    )\n\n    _progress()\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.validate_prompts","title":"<code>validate_prompts(inputs, default_format=None)</code>","text":"<p>Generates the prompts to be used for generation, can be used to check the prompts visually.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>The inputs to be used for generation.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The prompts that would be used for the generation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; llm.validate_prompts([{\"input\": \"Your input\"}])[0]\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\nI'm valid for text generation task\n</code></pre> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def validate_prompts(\n    self,\n    inputs: List[Dict[str, Any]],\n    default_format: Union[\"SupportedFormats\", None] = None,\n) -&gt; str:\n    \"\"\"Generates the prompts to be used for generation, can be used to check the prompts visually.\n\n    Args:\n        inputs (List[Dict[str, Any]]):\n            The inputs to be used for generation.\n\n    Returns:\n        str: The prompts that would be used for the generation.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; llm.validate_prompts([{\"input\": \"Your input\"}])[0]\n        You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n        I'm valid for text generation task\n    \"\"\"\n    return self._generate_prompts(inputs, default_format=default_format)\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLMPool","title":"<code>LLMPool</code>","text":"<p>LLMPool is a class that wraps multiple <code>ProcessLLM</code>s and performs generation in parallel using them. Depending on the number of <code>LLM</code>s and the parameter <code>num_generations</code>, the <code>LLMPool</code> will decide how many generations to perform for each <code>LLM</code>:</p> <ul> <li> <p>If <code>num_generations</code> is less than the number of <code>LLM</code>s, then <code>num_generations</code> LLMs will be chosen randomly and each of them will perform 1 generation.</p> </li> <li> <p>If <code>num_generations</code> is equal to the number of <code>LLM</code>s, then each <code>LLM</code> will perform 1 generation.</p> </li> <li> <p>If <code>num_generations</code> is greater than the number of <code>LLM</code>s, then each <code>LLM</code> will perform <code>num_generations // num_llms</code> generations, and the remaining <code>num_generations % num_llms</code> generations will be performed by <code>num_generations % num_llms</code> randomly chosen <code>LLM</code>s.</p> </li> </ul> <p>Attributes:</p> Name Type Description <code>llms</code> <code>List[ProcessLLM]</code> <p>the <code>ProcessLLM</code>s to be used for generation.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>class LLMPool:\n    \"\"\"LLMPool is a class that wraps multiple `ProcessLLM`s and performs generation in\n    parallel using them. Depending on the number of `LLM`s and the parameter `num_generations`,\n    the `LLMPool` will decide how many generations to perform for each `LLM`:\n\n    - If `num_generations` is less than the number of `LLM`s, then `num_generations` LLMs\n    will be chosen randomly and each of them will perform 1 generation.\n\n\n    - If `num_generations` is equal to the number of `LLM`s, then each `LLM` will perform\n    1 generation.\n\n    - If `num_generations` is greater than the number of `LLM`s, then each `LLM` will\n    perform `num_generations // num_llms` generations, and the remaining `num_generations % num_llms`\n    generations will be performed by `num_generations % num_llms` randomly chosen `LLM`s.\n\n    Attributes:\n        llms (List[ProcessLLM]): the `ProcessLLM`s to be used for generation.\n    \"\"\"\n\n    def __init__(self, llms: List[ProcessLLM]) -&gt; None:\n        \"\"\"Initializes the `LLMPool` class.\n\n        Args:\n            llms: the `ProcessLLM`s to be used for generation. The list must contain at\n                least 2 `ProcessLLM`s.\n\n        Raises:\n            ValueError: if the `llms` argument contains less than 2 `ProcessLLM`s, the\n                `llms` argument contains `ProcessLLM`s that are not `ProcessLLM`s, or\n                if the `llms` argument contains `ProcessLLM`s with different tasks.\n        \"\"\"\n        if len(llms) &lt; 2:\n            raise ValueError(\n                \"The `llms` argument must contain at least 2 `ProcessLLM`s. If you want\"\n                \" to use a single `ProcessLLM`, use the `ProcessLLM` directly instead.\"\n            )\n\n        if not all(isinstance(llm, ProcessLLM) for llm in llms):\n            raise ValueError(\"The `llms` argument must contain only `ProcessLLM`s.\")\n\n        # Note: The following piece of code is used to check that all the `ProcessLLM`s\n        # have the same task or a subclass of it.\n        mros = [(type(llm.task), len(type(llm.task).mro())) for llm in llms]\n        min_common_class = min(mros, key=lambda x: x[1])[0]\n        if not all(isinstance(llm.task, min_common_class) for llm in llms):\n            # This can fail for example with 3 different TextGenerationTasks\n            # Task1(TextGenerationTask), Task2(TextGenerationTask), Task2(TextGenerationTask)\n            # because they share the same parent class but we don't check the common one\n            # TODO(plaguss): We check that they all have the same parent class, this should be simplified\n            # with the previous check\n            parent_classes = [type(llm.task).mro()[1] for llm in llms]\n            if not len(set(parent_classes)) == 1:\n                raise ValueError(\n                    \"All the `ProcessLLM` in `llms` must share the same task (either as the instance or the parent class).\"\n                )\n\n        self.llms = llms\n        self.num_llms = len(llms)\n\n    def _get_num_generations_per_llm(self, num_generations: int) -&gt; Dict[int, int]:\n        \"\"\"Returns the number of generations to be performed by each `LLM`.\n\n        Args:\n            num_generations: the number of generations to be performed.\n\n        Returns:\n            Dict[int, int]: a dictionary where the keys are the ids of the `LLM`s and the\n            values are the number of generations to be performed by each `LLM`.\n        \"\"\"\n        llms_ids = list(range(self.num_llms))\n        generations_per_llm = {i: num_generations // self.num_llms for i in llms_ids}\n\n        for i in random.sample(llms_ids, k=num_generations % self.num_llms):\n            generations_per_llm[i] += 1\n\n        return generations_per_llm\n\n    def generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        \"\"\"Generates the outputs for the given inputs using the pool of `ProcessLLM`s.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each input.\n                Defaults to `1`.\n            progress_callback_func (Union[Callable, None], optional): a function to be called at each\n                generation step. Defaults to `None`.\n\n        Returns:\n            Future[List[List[\"LLMOutput\"]]]: the generated outputs as a `Future`.\n        \"\"\"\n        num_generations_per_llm = self._get_num_generations_per_llm(num_generations)\n\n        futures = [\n            llm.generate(\n                inputs,\n                num_generations=num_generations_per_llm[i],\n                progress_callback_func=progress_callback_func,\n            )\n            for i, llm in enumerate(self.llms)\n            if num_generations_per_llm[i] &gt; 0\n        ]\n        llms_generations = [future.result() for future in futures]\n\n        generations = []\n        for llms_row_generations in zip(*llms_generations):\n            row_generations = []\n            for llm_row_generations in llms_row_generations:\n                for generation in llm_row_generations:\n                    row_generations.append(generation)\n            generations.append(row_generations)\n\n        return generations\n\n    def teardown(self) -&gt; None:\n        \"\"\"Stops the `ProcessLLM`s.\"\"\"\n        for llm in self.llms:\n            llm.teardown()\n\n    @property\n    def task(self) -&gt; \"Task\":\n        \"\"\"Returns the task that will be used by the `ProcessLLM`s of this pool.\n\n        Returns:\n            Task: the task that will be used by the `ProcessLLM`s of this pool.\n        \"\"\"\n        return self.llms[0].task\n\n    @property\n    def return_futures(self) -&gt; bool:\n        \"\"\"Whether the `LLM` returns futures\"\"\"\n        return False\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLMPool.return_futures","title":"<code>return_futures: bool</code>  <code>property</code>","text":"<p>Whether the <code>LLM</code> returns futures</p>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLMPool.task","title":"<code>task: 'Task'</code>  <code>property</code>","text":"<p>Returns the task that will be used by the <code>ProcessLLM</code>s of this pool.</p> <p>Returns:</p> Name Type Description <code>Task</code> <code>'Task'</code> <p>the task that will be used by the <code>ProcessLLM</code>s of this pool.</p>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLMPool.__init__","title":"<code>__init__(llms)</code>","text":"<p>Initializes the <code>LLMPool</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>llms</code> <code>List[ProcessLLM]</code> <p>the <code>ProcessLLM</code>s to be used for generation. The list must contain at least 2 <code>ProcessLLM</code>s.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>llms</code> argument contains less than 2 <code>ProcessLLM</code>s, the <code>llms</code> argument contains <code>ProcessLLM</code>s that are not <code>ProcessLLM</code>s, or if the <code>llms</code> argument contains <code>ProcessLLM</code>s with different tasks.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __init__(self, llms: List[ProcessLLM]) -&gt; None:\n    \"\"\"Initializes the `LLMPool` class.\n\n    Args:\n        llms: the `ProcessLLM`s to be used for generation. The list must contain at\n            least 2 `ProcessLLM`s.\n\n    Raises:\n        ValueError: if the `llms` argument contains less than 2 `ProcessLLM`s, the\n            `llms` argument contains `ProcessLLM`s that are not `ProcessLLM`s, or\n            if the `llms` argument contains `ProcessLLM`s with different tasks.\n    \"\"\"\n    if len(llms) &lt; 2:\n        raise ValueError(\n            \"The `llms` argument must contain at least 2 `ProcessLLM`s. If you want\"\n            \" to use a single `ProcessLLM`, use the `ProcessLLM` directly instead.\"\n        )\n\n    if not all(isinstance(llm, ProcessLLM) for llm in llms):\n        raise ValueError(\"The `llms` argument must contain only `ProcessLLM`s.\")\n\n    # Note: The following piece of code is used to check that all the `ProcessLLM`s\n    # have the same task or a subclass of it.\n    mros = [(type(llm.task), len(type(llm.task).mro())) for llm in llms]\n    min_common_class = min(mros, key=lambda x: x[1])[0]\n    if not all(isinstance(llm.task, min_common_class) for llm in llms):\n        # This can fail for example with 3 different TextGenerationTasks\n        # Task1(TextGenerationTask), Task2(TextGenerationTask), Task2(TextGenerationTask)\n        # because they share the same parent class but we don't check the common one\n        # TODO(plaguss): We check that they all have the same parent class, this should be simplified\n        # with the previous check\n        parent_classes = [type(llm.task).mro()[1] for llm in llms]\n        if not len(set(parent_classes)) == 1:\n            raise ValueError(\n                \"All the `ProcessLLM` in `llms` must share the same task (either as the instance or the parent class).\"\n            )\n\n    self.llms = llms\n    self.num_llms = len(llms)\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLMPool.generate","title":"<code>generate(inputs, num_generations=1, progress_callback_func=None)</code>","text":"<p>Generates the outputs for the given inputs using the pool of <code>ProcessLLM</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>the inputs to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>progress_callback_func</code> <code>Union[Callable, None]</code> <p>a function to be called at each generation step. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List['LLMOutput']]</code> <p>Future[List[List[\"LLMOutput\"]]]: the generated outputs as a <code>Future</code>.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[Dict[str, Any]],\n    num_generations: int = 1,\n    progress_callback_func: Union[Callable, None] = None,\n) -&gt; List[List[\"LLMOutput\"]]:\n    \"\"\"Generates the outputs for the given inputs using the pool of `ProcessLLM`s.\n\n    Args:\n        inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each input.\n            Defaults to `1`.\n        progress_callback_func (Union[Callable, None], optional): a function to be called at each\n            generation step. Defaults to `None`.\n\n    Returns:\n        Future[List[List[\"LLMOutput\"]]]: the generated outputs as a `Future`.\n    \"\"\"\n    num_generations_per_llm = self._get_num_generations_per_llm(num_generations)\n\n    futures = [\n        llm.generate(\n            inputs,\n            num_generations=num_generations_per_llm[i],\n            progress_callback_func=progress_callback_func,\n        )\n        for i, llm in enumerate(self.llms)\n        if num_generations_per_llm[i] &gt; 0\n    ]\n    llms_generations = [future.result() for future in futures]\n\n    generations = []\n    for llms_row_generations in zip(*llms_generations):\n        row_generations = []\n        for llm_row_generations in llms_row_generations:\n            for generation in llm_row_generations:\n                row_generations.append(generation)\n        generations.append(row_generations)\n\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLMPool.teardown","title":"<code>teardown()</code>","text":"<p>Stops the <code>ProcessLLM</code>s.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def teardown(self) -&gt; None:\n    \"\"\"Stops the `ProcessLLM`s.\"\"\"\n    for llm in self.llms:\n        llm.teardown()\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.ProcessLLM","title":"<code>ProcessLLM</code>","text":"<p>A class that wraps an <code>LLM</code> and performs generation in a separate process. The result is a <code>Future</code> that will be set when the generation is completed.</p> <p>This class creates a new child process that will load the <code>LLM</code> and perform the text generation. In order to communicate with this child process, a bridge thread is created in the main process. The bridge thread will send and receive the results from the child process using <code>multiprocessing.Queue</code>s. The communication between the bridge thread and the main process is done using <code>Future</code>s. This architecture was inspired by the <code>ProcessPoolExecutor</code> from the <code>concurrent.futures</code> module and it's a simplified version of it.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>class ProcessLLM:\n    \"\"\"A class that wraps an `LLM` and performs generation in a separate process. The\n    result is a `Future` that will be set when the generation is completed.\n\n    This class creates a new child process that will load the `LLM` and perform the\n    text generation. In order to communicate with this child process, a bridge thread\n    is created in the main process. The bridge thread will send and receive the results\n    from the child process using `multiprocessing.Queue`s. The communication between the\n    bridge thread and the main process is done using `Future`s. This architecture was\n    inspired by the `ProcessPoolExecutor` from the `concurrent.futures` module and it's\n    a simplified version of it.\n    \"\"\"\n\n    def __init__(self, task: Task, load_llm_fn: Callable[[Task], LLM]) -&gt; None:\n        \"\"\"Initializes the `ProcessLLM` class.\n\n        Args:\n            task: the task to be performed by the `LLM`. This task will be used by the\n                child process when calling the `load_llm_fn`.\n            load_llm_fn (Callable[[Task], LLM]): a function that will be executed in the\n                child process to load the `LLM`. It must return an `LLM` instance.\n        \"\"\"\n        self.task = task\n\n        self._load_llm_fn = load_llm_fn\n\n        # The bridge thread will act as a bridge between the main process and the child\n        # process for communication. It will send the generation requests to the child\n        # process and receive the results from the child process.\n        self._bridge_thread = None\n\n        # The child process which will load the `LLM` and perform the generation.\n        self._generation_process = None\n\n        # The `Semaphore` that will be used to synchronize the loading of the `LLM`.\n        # `_BridgeThread` will be blocked until `_GenerationProcess` has called the\n        # `load_llm_fn` and the `LLM` has been loaded.\n        self._load_llm_sem = mp.Semaphore(0)\n\n        # This thread will create text generation requests\n        self.pending_text_generation_request: Dict[int, _TextGenerationRequest] = {}\n        self.text_generation_request_count = 0\n        self.text_generation_request_ids_queue: queue.Queue[int] = queue.Queue()\n\n        # Queues for the communication between the `_BridgeThread` and the `_GenerationProcess`\n        self._call_queue = mp.Queue()\n        self._result_queue = mp.Queue()\n\n        # Shared memory object for transfering the `model_name` to the main process\n        # once the `LLM` is loaded\n        self._model_name = mp.Array(c_char, MAX_MODEL_NAME_LENGTH)\n\n    def _start_bridge_thread(self) -&gt; None:\n        \"\"\"Starts the bridge thread and the generation process.\"\"\"\n        if self._bridge_thread is None:\n            self._generation_process = _GenerationProcess(self)\n            self._generation_process.start()\n            pid = self._generation_process.pid\n            logger.debug(f\"Generation process with PID {pid} started!\")\n\n            self._bridge_thread = _BridgeThread(self)\n            self._bridge_thread.start()\n            logger.debug(\"Bridge thread for process with PID {pid} started!\")\n\n    def _add_text_generation_request(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; Future[List[List[\"LLMOutput\"]]]:\n        \"\"\"Creates and send a new text generation request to the bridge thread. This thread\n        and the bridge thread shares a dictionary used to store the text generation requests.\n        This thread will add the text generation requests to the dictionary and the bridge\n        thread will only read from it. In order for the bridge thread to know that a new\n        text generation request has been added to the dictionary, this thread will put the\n        id of the request in a queue. The bridge thread will read from this queue and get\n        the text generation request from the dictionary.\n        \"\"\"\n\n        def _progress():\n            if progress_callback_func is not None:\n                progress_callback_func(advance=num_generations * len(inputs))\n\n        text_generation_request = _TextGenerationRequest(\n            inputs=inputs, num_generations=num_generations\n        )\n        # Put the request information in the dictionary associated to the request id\n        self.pending_text_generation_request[\n            self.text_generation_request_count\n        ] = text_generation_request\n        # Put the request id in the queue (for the `_BridgeThread` to consume it)\n        self.text_generation_request_ids_queue.put(self.text_generation_request_count)\n        self.text_generation_request_count += 1\n        text_generation_request.future.add_done_callback(lambda _: _progress())\n        return text_generation_request.future\n\n    def generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; Future[List[List[\"LLMOutput\"]]]:\n        \"\"\"Generates the outputs for the given inputs using the `ProcessLLM` and its loaded\n        `LLM`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each input.\n                Defaults to `1`.\n            progress_callback_func (Union[Callable, None], optional): a function to be called at each\n                generation step. Defaults to `None`.\n\n        Returns:\n            Future[List[List[\"LLMOutput\"]]]: the generated outputs as a `Future`.\n        \"\"\"\n        self._start_bridge_thread()\n        return self._add_text_generation_request(\n            inputs, num_generations, progress_callback_func\n        )\n\n    def teardown(self) -&gt; None:\n        \"\"\"Stops the bridge thread and the generation process.\"\"\"\n        if self._generation_process is not None:\n            self._generation_process.stop()\n            self._generation_process.join()\n\n        if self._bridge_thread is not None:\n            self._bridge_thread.stop()\n            self._bridge_thread.join()\n\n    @cached_property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name of the `LLM` once it has been loaded.\"\"\"\n        with self._model_name:\n            return \"\".join([c.decode() for c in self._model_name if c != b\"\\0\"])\n\n    @property\n    def return_futures(self) -&gt; bool:\n        \"\"\"Whether the `LLM` returns futures\"\"\"\n        return True\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.ProcessLLM.model_name","title":"<code>model_name: str</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the model name of the <code>LLM</code> once it has been loaded.</p>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.ProcessLLM.return_futures","title":"<code>return_futures: bool</code>  <code>property</code>","text":"<p>Whether the <code>LLM</code> returns futures</p>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.ProcessLLM.__init__","title":"<code>__init__(task, load_llm_fn)</code>","text":"<p>Initializes the <code>ProcessLLM</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the <code>LLM</code>. This task will be used by the child process when calling the <code>load_llm_fn</code>.</p> required <code>load_llm_fn</code> <code>Callable[[Task], LLM]</code> <p>a function that will be executed in the child process to load the <code>LLM</code>. It must return an <code>LLM</code> instance.</p> required Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __init__(self, task: Task, load_llm_fn: Callable[[Task], LLM]) -&gt; None:\n    \"\"\"Initializes the `ProcessLLM` class.\n\n    Args:\n        task: the task to be performed by the `LLM`. This task will be used by the\n            child process when calling the `load_llm_fn`.\n        load_llm_fn (Callable[[Task], LLM]): a function that will be executed in the\n            child process to load the `LLM`. It must return an `LLM` instance.\n    \"\"\"\n    self.task = task\n\n    self._load_llm_fn = load_llm_fn\n\n    # The bridge thread will act as a bridge between the main process and the child\n    # process for communication. It will send the generation requests to the child\n    # process and receive the results from the child process.\n    self._bridge_thread = None\n\n    # The child process which will load the `LLM` and perform the generation.\n    self._generation_process = None\n\n    # The `Semaphore` that will be used to synchronize the loading of the `LLM`.\n    # `_BridgeThread` will be blocked until `_GenerationProcess` has called the\n    # `load_llm_fn` and the `LLM` has been loaded.\n    self._load_llm_sem = mp.Semaphore(0)\n\n    # This thread will create text generation requests\n    self.pending_text_generation_request: Dict[int, _TextGenerationRequest] = {}\n    self.text_generation_request_count = 0\n    self.text_generation_request_ids_queue: queue.Queue[int] = queue.Queue()\n\n    # Queues for the communication between the `_BridgeThread` and the `_GenerationProcess`\n    self._call_queue = mp.Queue()\n    self._result_queue = mp.Queue()\n\n    # Shared memory object for transfering the `model_name` to the main process\n    # once the `LLM` is loaded\n    self._model_name = mp.Array(c_char, MAX_MODEL_NAME_LENGTH)\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.ProcessLLM.generate","title":"<code>generate(inputs, num_generations=1, progress_callback_func=None)</code>","text":"<p>Generates the outputs for the given inputs using the <code>ProcessLLM</code> and its loaded <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>the inputs to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>progress_callback_func</code> <code>Union[Callable, None]</code> <p>a function to be called at each generation step. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Future[List[List['LLMOutput']]]</code> <p>Future[List[List[\"LLMOutput\"]]]: the generated outputs as a <code>Future</code>.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[Dict[str, Any]],\n    num_generations: int = 1,\n    progress_callback_func: Union[Callable, None] = None,\n) -&gt; Future[List[List[\"LLMOutput\"]]]:\n    \"\"\"Generates the outputs for the given inputs using the `ProcessLLM` and its loaded\n    `LLM`.\n\n    Args:\n        inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each input.\n            Defaults to `1`.\n        progress_callback_func (Union[Callable, None], optional): a function to be called at each\n            generation step. Defaults to `None`.\n\n    Returns:\n        Future[List[List[\"LLMOutput\"]]]: the generated outputs as a `Future`.\n    \"\"\"\n    self._start_bridge_thread()\n    return self._add_text_generation_request(\n        inputs, num_generations, progress_callback_func\n    )\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.ProcessLLM.teardown","title":"<code>teardown()</code>","text":"<p>Stops the bridge thread and the generation process.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def teardown(self) -&gt; None:\n    \"\"\"Stops the bridge thread and the generation process.\"\"\"\n    if self._generation_process is not None:\n        self._generation_process.stop()\n        self._generation_process.join()\n\n    if self._bridge_thread is not None:\n        self._bridge_thread.stop()\n        self._bridge_thread.join()\n</code></pre>"},{"location":"reference/distilabel/llm/llama_cpp/","title":"llama_cpp","text":""},{"location":"reference/distilabel/llm/llama_cpp/#distilabel.llm.llama_cpp.LlamaCppLLM","title":"<code>LlamaCppLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/llama_cpp.py</code> <pre><code>class LlamaCppLLM(LLM):\n    def __init__(\n        self,\n        model: \"Llama\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        temperature: float = 0.8,\n        top_p: float = 0.95,\n        top_k: int = 40,\n        repeat_penalty: float = 1.1,\n        seed: int = 1337,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[SupportedFormats, None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the LlamaCppLLM class.\n\n        Args:\n            model (Llama): the llama-cpp model to be used.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 0.8.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 0.95.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 40.\n            repeat_penalty (float, optional): the repeat penalty to be used for generation.\n                Defaults to 1.1.\n            seed (int, optional): the seed to be used for generation, setting it to -1 implies\n                that a different response will be generated on each generation, similarly to\n                HuggingFace's `do_sample` arg. Defaults to 1337.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Examples:\n            &gt;&gt;&gt; from llama_cpp import Llama\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n            &gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n            &gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _LLAMA_CPP_AVAILABLE:\n            raise ImportError(\n                \"`LlamaCppLLM` cannot be used as `llama_cpp` is not installed, please \"\n                \" install it with `pip install llama-cpp-python`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.repeat_penalty = repeat_penalty\n        self.seed = seed\n\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_tokens,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n                \"repeat_penalty\": self.repeat_penalty,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the llama-cpp model, which is the same as the model path.\"\"\"\n        return self.model.model_path\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        outputs = []\n        for prompt in prompts:\n            output = []\n            for _ in range(num_generations):\n                raw_output = self.model.create_completion(\n                    prompt,\n                    max_tokens=self.max_tokens,\n                    temperature=self.temperature,\n                    top_p=self.top_p,\n                    top_k=self.top_k,\n                    repeat_penalty=self.repeat_penalty,\n                )\n                try:\n                    parsed_output = self.task.parse_output(\n                        raw_output[\"choices\"][0][\"text\"].strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing llama-cpp output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/llama_cpp/#distilabel.llm.llama_cpp.LlamaCppLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the llama-cpp model, which is the same as the model path.</p>"},{"location":"reference/distilabel/llm/llama_cpp/#distilabel.llm.llama_cpp.LlamaCppLLM.__init__","title":"<code>__init__(model, task, max_new_tokens=128, temperature=0.8, top_p=0.95, top_k=40, repeat_penalty=1.1, seed=1337, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the LlamaCppLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>the llama-cpp model to be used.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 0.8.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 0.95.</p> <code>0.95</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 40.</p> <code>40</code> <code>repeat_penalty</code> <code>float</code> <p>the repeat penalty to be used for generation. Defaults to 1.1.</p> <code>1.1</code> <code>seed</code> <code>int</code> <p>the seed to be used for generation, setting it to -1 implies that a different response will be generated on each generation, similarly to HuggingFace's <code>do_sample</code> arg. Defaults to 1337.</p> <code>1337</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from llama_cpp import Llama\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n&gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n&gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/llama_cpp.py</code> <pre><code>def __init__(\n    self,\n    model: \"Llama\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    temperature: float = 0.8,\n    top_p: float = 0.95,\n    top_k: int = 40,\n    repeat_penalty: float = 1.1,\n    seed: int = 1337,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[SupportedFormats, None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the LlamaCppLLM class.\n\n    Args:\n        model (Llama): the llama-cpp model to be used.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 0.8.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 0.95.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 40.\n        repeat_penalty (float, optional): the repeat penalty to be used for generation.\n            Defaults to 1.1.\n        seed (int, optional): the seed to be used for generation, setting it to -1 implies\n            that a different response will be generated on each generation, similarly to\n            HuggingFace's `do_sample` arg. Defaults to 1337.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from llama_cpp import Llama\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n        &gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n        &gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _LLAMA_CPP_AVAILABLE:\n        raise ImportError(\n            \"`LlamaCppLLM` cannot be used as `llama_cpp` is not installed, please \"\n            \" install it with `pip install llama-cpp-python`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.repeat_penalty = repeat_penalty\n    self.seed = seed\n\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/mistralai/","title":"mistralai","text":""},{"location":"reference/distilabel/llm/mistralai/#distilabel.llm.mistralai.MistralAILLM","title":"<code>MistralAILLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/mistralai.py</code> <pre><code>class MistralAILLM(LLM):\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"mistral-medium\",\n        client: Optional[\"MistralClient\"] = None,\n        api_key: Optional[str] = os.environ.get(\"MISTRALAI_API_KEY\"),\n        max_tokens: int = 128,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        seed: Optional[int] = None,\n        safe_prompt: bool = False,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the MistralAILLM class.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"mistral-medium\".\n            client (MistralClient, optional):\n                A MistralClient client to be used for generation. Defaults to None.\n            api_key (Optional[str], optional):\n                The MistralAI API key to be used for generation. Will try to grab it from the environment variable\n                if not informed. Visit \"https://docs.mistral.ai/#api-access\" for more information.\n            max_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation. Defaults to 1.0.\n            seed (Optional[int], optional): the random seed to use for sampling, e.g. 42. Defaults to None.\n            safe_prompt (_type_, optional):\n                whether to use safe prompt, e.g. True. Defaults to False.\n                Visit \"https://docs.mistral.ai/platform/guardrailing/\" for more information.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n        Raises:\n            AssertionError: if the provided `model` is not available in your MistralAI account.\n\n        Examples:\n            &gt;&gt;&gt; import os\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import MistralAILLM\n            &gt;&gt;&gt; llm = MistralAILLM(model=\"mistral-medium\", task=TextGenerationTask(), api_key=os.getenv(\"MISTRALAI_API_KEY\"))\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n        if not _MISTRALAI_AVAILABLE:\n            raise ImportError(\n                \"`MistralAILLM` cannot be used as `mistralai` is not installed, please \"\n                \" install it with `pip install mistralai`.\"\n            )\n\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.seed = seed\n        self.safe_prompt = safe_prompt\n\n        # Explicitly write the default parameters of the model\n        self.client = client or MistralClient(\n            api_key=api_key, max_retries=5, timeout=120\n        )\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in MistralAI, available models are {self.available_models}\"\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"seed\": self.seed,\n                \"safe_prompt\": self.safe_prompt,\n            },\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in MistralAI.\"\"\"\n        return [model.id for model in self.client.list_models().data]\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the MistralAI model.\"\"\"\n        return self.model\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"openai\")\n        # The mistralai format is the same as openai, but needs to be converted to mistralai's ChatMessage (pydantic model)\n        prompts = [[ChatMessage(**p) for p in prompt] for prompt in prompts]\n        outputs = []\n        for prompt in prompts:\n            responses = []\n            for _ in range(num_generations):\n                chat_completion_response = self.client.chat(\n                    self.model,\n                    messages=prompt,\n                    temperature=self.temperature,\n                    max_tokens=self.max_tokens,\n                    top_p=self.top_p,\n                    random_seed=self.seed,\n                    safe_prompt=self.safe_prompt,\n                )\n                responses.append(chat_completion_response)\n\n            output = []\n            for response in responses:\n                chat_completion = response.choices[0]\n                try:\n                    parsed_response = self.task.parse_output(\n                        chat_completion.message.content.strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing MistralAI response: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=[p.model_dump() for p in prompt],\n                        raw_output=chat_completion.message.content,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/mistralai/#distilabel.llm.mistralai.MistralAILLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in MistralAI.</p>"},{"location":"reference/distilabel/llm/mistralai/#distilabel.llm.mistralai.MistralAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the MistralAI model.</p>"},{"location":"reference/distilabel/llm/mistralai/#distilabel.llm.mistralai.MistralAILLM.__init__","title":"<code>__init__(task, model='mistral-medium', client=None, api_key=os.environ.get('MISTRALAI_API_KEY'), max_tokens=128, temperature=1.0, top_p=1.0, seed=None, safe_prompt=False, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the MistralAILLM class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"mistral-medium\".</p> <code>'mistral-medium'</code> <code>client</code> <code>MistralClient</code> <p>A MistralClient client to be used for generation. Defaults to None.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The MistralAI API key to be used for generation. Will try to grab it from the environment variable if not informed. Visit \"https://docs.mistral.ai/#api-access\" for more information.</p> <code>get('MISTRALAI_API_KEY')</code> <code>max_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>seed</code> <code>Optional[int]</code> <p>the random seed to use for sampling, e.g. 42. Defaults to None.</p> <code>None</code> <code>safe_prompt</code> <code>_type_</code> <p>whether to use safe prompt, e.g. True. Defaults to False. Visit \"https://docs.mistral.ai/platform/guardrailing/\" for more information.</p> <code>False</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:     AssertionError: if the provided <code>model</code> is not available in your MistralAI account.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import MistralAILLM\n&gt;&gt;&gt; llm = MistralAILLM(model=\"mistral-medium\", task=TextGenerationTask(), api_key=os.getenv(\"MISTRALAI_API_KEY\"))\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/mistralai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"mistral-medium\",\n    client: Optional[\"MistralClient\"] = None,\n    api_key: Optional[str] = os.environ.get(\"MISTRALAI_API_KEY\"),\n    max_tokens: int = 128,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    seed: Optional[int] = None,\n    safe_prompt: bool = False,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the MistralAILLM class.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"mistral-medium\".\n        client (MistralClient, optional):\n            A MistralClient client to be used for generation. Defaults to None.\n        api_key (Optional[str], optional):\n            The MistralAI API key to be used for generation. Will try to grab it from the environment variable\n            if not informed. Visit \"https://docs.mistral.ai/#api-access\" for more information.\n        max_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation. Defaults to 1.0.\n        seed (Optional[int], optional): the random seed to use for sampling, e.g. 42. Defaults to None.\n        safe_prompt (_type_, optional):\n            whether to use safe prompt, e.g. True. Defaults to False.\n            Visit \"https://docs.mistral.ai/platform/guardrailing/\" for more information.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n    Raises:\n        AssertionError: if the provided `model` is not available in your MistralAI account.\n\n    Examples:\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import MistralAILLM\n        &gt;&gt;&gt; llm = MistralAILLM(model=\"mistral-medium\", task=TextGenerationTask(), api_key=os.getenv(\"MISTRALAI_API_KEY\"))\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n    if not _MISTRALAI_AVAILABLE:\n        raise ImportError(\n            \"`MistralAILLM` cannot be used as `mistralai` is not installed, please \"\n            \" install it with `pip install mistralai`.\"\n        )\n\n    self.max_tokens = max_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.seed = seed\n    self.safe_prompt = safe_prompt\n\n    # Explicitly write the default parameters of the model\n    self.client = client or MistralClient(\n        api_key=api_key, max_retries=5, timeout=120\n    )\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in MistralAI, available models are {self.available_models}\"\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/ollama/","title":"ollama","text":""},{"location":"reference/distilabel/llm/ollama/#distilabel.llm.ollama.OllamaLLM","title":"<code>OllamaLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/ollama.py</code> <pre><code>class OllamaLLM(LLM):\n    OLLAMA_HOST = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\n    def __init__(\n        self,\n        model: str,\n        task: \"Task\",\n        max_new_tokens: Union[int, None] = None,\n        temperature: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n        top_p: Union[float, None] = None,\n        mirostat: Union[int, None] = None,\n        mirostat_eta: Union[float, None] = None,\n        mirostat_tau: Union[float, None] = None,\n        num_ctx: Union[int, None] = None,\n        num_gqa: Union[int, None] = None,\n        num_gpu: Union[int, None] = None,\n        num_threads: Union[int, None] = None,\n        repeat_last_n: Union[int, None] = None,\n        repeat_penalty: Union[float, None] = None,\n        seed: Union[int, None] = None,\n        stop: Union[str, None] = None,\n        tfs_z: Union[float, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the OllamaLLM class and aligns with https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n\n        Args:\n            model (str): the model to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to `None`.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to `None`.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to `None`.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to `None`.\n            mirostat (int, optional): the Mirostat value to enable it or set the version.\n                Defaults to `None`.\n            mirostat_eta (float, optional): the eta value to be used for Mirostat.\n                Defaults to `None`.\n            mirostat_tau (float, optional): the tau value to be used for Mirostat.\n                Defaults to `None`.\n            num_ctx (int, optional): the number of contexts to be used for generation.\n                Defaults to `None`.\n            num_gqa (int, optional): the number of GQA to be used for generation.\n                Defaults to `None`.\n            num_gpu (int, optional): the number of GPUs to be used for generation.\n                Defaults to `None`.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            repeat_last_n (Union[int, None], optional): the number of tokens to be used\n                for RepeatLastN. Defaults to `None`.\n            repeat_penalty (Union[float, None], optional): the penalty to be used for RepeatLastN.\n                Defaults to `None`.\n            seed (Union[int, None], optional): the seed to be used for generation.\n                Defaults to `None`.\n            stop (Union[str, None], optional): the stop token to be used for generation. If `None`,\n                no stop token will be used. Defaults to `None`.\n            tfs_z (Union[float, None], optional): the z value to be used for TFS.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`..\n\n        Raises:\n            ValueError: if the model is not available.\n            ValueError: if the Ollama API request failed.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import OllamaLLM\n            &gt;&gt;&gt; llm = OllamaLLM(model=\"notus\", task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        self.model = model\n        self.max_new_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.mirostat = mirostat\n        self.mirostat_eta = mirostat_eta\n        self.mirostat_tau = mirostat_tau\n        self.num_ctx = num_ctx\n        self.num_gqa = num_gqa\n        self.num_gpu = num_gpu\n        self.repeat_last_n = repeat_last_n\n        self.repeat_penalty = repeat_penalty\n        self.seed = seed\n        self.stop = stop\n        self.tfs_z = tfs_z\n\n        self._api_available()\n        self._api_model_available()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the Ollama model.\"\"\"\n        return self.model\n\n    def _api_available(self):\n        \"\"\"Checks if the Ollama API is available.\"\"\"\n        try:\n            ollama.list()\n        except ollama.ResponseError as e:\n            raise ValueError(\n                f\"Could not connect to Ollama at {self.OLLAMA_HOST}. Check https://github.com/ollama/ollama-python/tree/main for deployment guide.\"\n            ) from e\n\n    def _api_model_available(self):\n        \"\"\"Checks if the Ollama model is available\"\"\"\n        try:\n            ollama.show(self.model)\n        except ollama.ResponseError as e:\n            raise ValueError(\n                f\"Model {self.model} is not available. Run `ollama run {self.model}` to serve the model.\"\n            ) from e\n\n    @retry(\n        retry=retry_if_exception_type(_OLLAMA_API_RETRY_ON_EXCEPTIONS),\n        stop=stop_after_attempt(_OLLAMA_API_STOP_AFTER_ATTEMPT),\n        wait=wait_random_exponential(\n            multiplier=_OLLAMA_API_WAIT_RANDOM_EXPONENTIAL_MULTIPLIER,\n            max=_OLLAMA_API_WAIT_RANDOM_EXPONENTIAL_MAX,\n        ),\n        before_sleep=before_sleep_log(logger, logging.INFO),\n        after=after_log(logger, logging.INFO),\n    )\n    def _text_generation_with_backoff(\n        self, prompt: List[Dict[str, str]], **kwargs\n    ) -&gt; str:\n        \"\"\"Generates text using the Ollama API with backoff.\"\"\"\n        try:\n            return ollama.chat(\n                model=self.model,\n                messages=prompt,\n                options={\n                    \"num_predict\": self.max_new_tokens,\n                    \"temperature\": self.temperature,\n                    \"top_p\": self.top_p,\n                    \"top_k\": self.top_k,\n                    \"mirostat\": self.mirostat,\n                    \"mirostat_eta\": self.mirostat_eta,\n                    \"mirostat_tau\": self.mirostat_tau,\n                    \"num_ctx\": self.num_ctx,\n                    \"num_gqa\": self.num_gqa,\n                    \"num_gpu\": self.num_gpu,\n                    \"repeat_last_n\": self.repeat_last_n,\n                    \"repeat_penalty\": self.repeat_penalty,\n                    \"seed\": self.seed,\n                    \"stop\": self.stop,\n                    \"tfs_z\": self.tfs_z,\n                },\n            )\n        except ollama.ResponseError as e:\n            if e.status_code &gt;= 500:\n                raise\n            else:\n                raise ValueError(\n                    f\"Ollama API request failed with status_code {e.status_code}.\"\n                ) from e\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"model\": self.model,\n                \"max_new_tokens\": self.max_new_tokens,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"mirostat\": self.mirostat,\n                \"mirostat_eta\": self.mirostat_eta,\n                \"mirostat_tau\": self.mirostat_tau,\n                \"num_ctx\": self.num_ctx,\n                \"num_gqa\": self.num_gqa,\n                \"num_gpu\": self.num_gpu,\n                \"repeat_last_n\": self.repeat_last_n,\n                \"repeat_penalty\": self.repeat_penalty,\n                \"seed\": self.seed,\n                \"stop\": self.stop,\n                \"tfs_z\": self.tfs_z,\n            },\n        )\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        prompts = self._generate_prompts(inputs, default_format=\"openai\")\n        outputs = []\n        for prompt in prompts:\n            responses = [\n                self._text_generation_with_backoff(prompt=prompt)\n                for _ in range(num_generations)\n            ]\n            output = []\n            for response in responses:\n                raw_output = response.get(\"message\", {}).get(\"content\")  # type: ignore\n                try:\n                    parsed_response = self.task.parse_output(raw_output.strip())\n                except Exception as e:\n                    logger.error(f\"Error parsing OpenAI response: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/ollama/#distilabel.llm.ollama.OllamaLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the Ollama model.</p>"},{"location":"reference/distilabel/llm/ollama/#distilabel.llm.ollama.OllamaLLM.__init__","title":"<code>__init__(model, task, max_new_tokens=None, temperature=None, top_k=None, top_p=None, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gqa=None, num_gpu=None, num_threads=None, repeat_last_n=None, repeat_penalty=None, seed=None, stop=None, tfs_z=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the OllamaLLM class and aligns with https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>the model to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to <code>None</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>mirostat</code> <code>int</code> <p>the Mirostat value to enable it or set the version. Defaults to <code>None</code>.</p> <code>None</code> <code>mirostat_eta</code> <code>float</code> <p>the eta value to be used for Mirostat. Defaults to <code>None</code>.</p> <code>None</code> <code>mirostat_tau</code> <code>float</code> <p>the tau value to be used for Mirostat. Defaults to <code>None</code>.</p> <code>None</code> <code>num_ctx</code> <code>int</code> <p>the number of contexts to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>num_gqa</code> <code>int</code> <p>the number of GQA to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>num_gpu</code> <code>int</code> <p>the number of GPUs to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>repeat_last_n</code> <code>Union[int, None]</code> <p>the number of tokens to be used for RepeatLastN. Defaults to <code>None</code>.</p> <code>None</code> <code>repeat_penalty</code> <code>Union[float, None]</code> <p>the penalty to be used for RepeatLastN. Defaults to <code>None</code>.</p> <code>None</code> <code>seed</code> <code>Union[int, None]</code> <p>the seed to be used for generation. Defaults to <code>None</code>.</p> <code>None</code> <code>stop</code> <code>Union[str, None]</code> <p>the stop token to be used for generation. If <code>None</code>, no stop token will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>tfs_z</code> <code>Union[float, None]</code> <p>the z value to be used for TFS. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>..</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the model is not available.</p> <code>ValueError</code> <p>if the Ollama API request failed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import OllamaLLM\n&gt;&gt;&gt; llm = OllamaLLM(model=\"notus\", task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/ollama.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    task: \"Task\",\n    max_new_tokens: Union[int, None] = None,\n    temperature: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n    top_p: Union[float, None] = None,\n    mirostat: Union[int, None] = None,\n    mirostat_eta: Union[float, None] = None,\n    mirostat_tau: Union[float, None] = None,\n    num_ctx: Union[int, None] = None,\n    num_gqa: Union[int, None] = None,\n    num_gpu: Union[int, None] = None,\n    num_threads: Union[int, None] = None,\n    repeat_last_n: Union[int, None] = None,\n    repeat_penalty: Union[float, None] = None,\n    seed: Union[int, None] = None,\n    stop: Union[str, None] = None,\n    tfs_z: Union[float, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the OllamaLLM class and aligns with https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n\n    Args:\n        model (str): the model to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to `None`.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to `None`.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to `None`.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to `None`.\n        mirostat (int, optional): the Mirostat value to enable it or set the version.\n            Defaults to `None`.\n        mirostat_eta (float, optional): the eta value to be used for Mirostat.\n            Defaults to `None`.\n        mirostat_tau (float, optional): the tau value to be used for Mirostat.\n            Defaults to `None`.\n        num_ctx (int, optional): the number of contexts to be used for generation.\n            Defaults to `None`.\n        num_gqa (int, optional): the number of GQA to be used for generation.\n            Defaults to `None`.\n        num_gpu (int, optional): the number of GPUs to be used for generation.\n            Defaults to `None`.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        repeat_last_n (Union[int, None], optional): the number of tokens to be used\n            for RepeatLastN. Defaults to `None`.\n        repeat_penalty (Union[float, None], optional): the penalty to be used for RepeatLastN.\n            Defaults to `None`.\n        seed (Union[int, None], optional): the seed to be used for generation.\n            Defaults to `None`.\n        stop (Union[str, None], optional): the stop token to be used for generation. If `None`,\n            no stop token will be used. Defaults to `None`.\n        tfs_z (Union[float, None], optional): the z value to be used for TFS.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`..\n\n    Raises:\n        ValueError: if the model is not available.\n        ValueError: if the Ollama API request failed.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import OllamaLLM\n        &gt;&gt;&gt; llm = OllamaLLM(model=\"notus\", task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    self.model = model\n    self.max_new_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.mirostat = mirostat\n    self.mirostat_eta = mirostat_eta\n    self.mirostat_tau = mirostat_tau\n    self.num_ctx = num_ctx\n    self.num_gqa = num_gqa\n    self.num_gpu = num_gpu\n    self.repeat_last_n = repeat_last_n\n    self.repeat_penalty = repeat_penalty\n    self.seed = seed\n    self.stop = stop\n    self.tfs_z = tfs_z\n\n    self._api_available()\n    self._api_model_available()\n</code></pre>"},{"location":"reference/distilabel/llm/openai/","title":"openai","text":""},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.JSONOpenAILLM","title":"<code>JSONOpenAILLM</code>","text":"<p>             Bases: <code>OpenAILLM</code></p> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>class JSONOpenAILLM(OpenAILLM):\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"gpt-3.5-turbo-1106\",\n        client: Union[\"OpenAI\", None] = None,\n        api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the JSONOpenAILLM class for generating JSON.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n            client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n                If `None`, a new client will be created. Defaults to `None`.\n            api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n                If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in your OpenAI account.\n            AssertionError: if the provided `model` does not support JSON input.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import JSONOpenAILLM\n            &gt;&gt;&gt; llm = JSONOpenAILLM(task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"json for 'What's the capital of Spain?'\"}])\n        \"\"\"\n        super().__init__(\n            task,\n            model=model,\n            client=client,\n            api_key=api_key,\n            max_new_tokens=max_new_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in your OpenAI account.\"\"\"\n        all_available_models = [model.id for model in self.client.models.list().data]\n        json_supporting_models = [\n            \"gpt-4-0125-preview\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-3.5-turbo-1106\",\n        ]\n        available_json_supporting_models = list(\n            set(all_available_models) &amp; set(json_supporting_models)\n        )\n        return available_json_supporting_models\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs` in JSON format.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"openai\")\n        outputs = []\n        for prompt in prompts:\n            chat_completions = self.client.chat.completions.create(\n                messages=prompt,\n                model=self.model,\n                n=num_generations,\n                max_tokens=self.max_tokens,\n                frequency_penalty=self.frequency_penalty,\n                presence_penalty=self.presence_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                timeout=50,\n                response_format={\"type\": \"json_object\"},\n            )\n\n            output = []\n            for chat_completion in chat_completions.choices:\n                try:\n                    parsed_response = self.task.parse_output(\n                        chat_completion.message.content.strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing OpenAI response: {e}\")\n                    parsed_response = None\n                try:\n                    json.loads(chat_completion.message.content)\n                except json.JSONDecodeError:\n                    warnings.warn(\n                        \"The response is not a valid JSON.\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=chat_completion.message.content,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.JSONOpenAILLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in your OpenAI account.</p>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.JSONOpenAILLM.__init__","title":"<code>__init__(task, model='gpt-3.5-turbo-1106', client=None, api_key=None, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the JSONOpenAILLM class for generating JSON.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo-1106'</code> <code>client</code> <code>Union[OpenAI, None]</code> <p>an OpenAI client to be used for generation. If <code>None</code>, a new client will be created. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Union[str, None]</code> <p>the OpenAI API key to be used for generation. If <code>None</code>, the <code>OPENAI_API_KEY</code> environment variable will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in your OpenAI account.</p> <code>AssertionError</code> <p>if the provided <code>model</code> does not support JSON input.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import JSONOpenAILLM\n&gt;&gt;&gt; llm = JSONOpenAILLM(task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"json for 'What's the capital of Spain?'\"}])\n</code></pre> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"gpt-3.5-turbo-1106\",\n    client: Union[\"OpenAI\", None] = None,\n    api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the JSONOpenAILLM class for generating JSON.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n        client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n            If `None`, a new client will be created. Defaults to `None`.\n        api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n            If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in your OpenAI account.\n        AssertionError: if the provided `model` does not support JSON input.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import JSONOpenAILLM\n        &gt;&gt;&gt; llm = JSONOpenAILLM(task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"json for 'What's the capital of Spain?'\"}])\n    \"\"\"\n    super().__init__(\n        task,\n        model=model,\n        client=client,\n        api_key=api_key,\n        max_new_tokens=max_new_tokens,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n</code></pre>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.OpenAILLM","title":"<code>OpenAILLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>class OpenAILLM(LLM):\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"gpt-3.5-turbo\",\n        client: Union[\"OpenAI\", None] = None,\n        api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the OpenAILLM class.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n            client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n                If `None`, a new client will be created. Defaults to `None`.\n            api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n                If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in your OpenAI account.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import OpenAILLM\n            &gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _OPENAI_AVAILABLE:\n            raise ImportError(\n                \"`OpenAILLM` cannot be used as `openai` is not installed, please \"\n                \" install it with `pip install openai`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n\n        self.client = client or OpenAI(api_key=api_key, max_retries=6)\n\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in your OpenAI account, available models are {self.available_models}\"\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"frequency_penalty\": self.frequency_penalty,\n                \"presence_penalty\": self.presence_penalty,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n            },\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in your OpenAI account.\"\"\"\n        return [model.id for model in self.client.models.list().data]\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the OpenAI model.\"\"\"\n        return self.model\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"openai\")\n        outputs = []\n        for prompt in prompts:\n            chat_completions = self.client.chat.completions.create(\n                messages=prompt,\n                model=self.model,\n                n=num_generations,\n                max_tokens=self.max_tokens,\n                frequency_penalty=self.frequency_penalty,\n                presence_penalty=self.presence_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                timeout=50,\n            )\n\n            output = []\n            for chat_completion in chat_completions.choices:\n                try:\n                    parsed_response = self.task.parse_output(\n                        chat_completion.message.content.strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing OpenAI response: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=chat_completion.message.content,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.OpenAILLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in your OpenAI account.</p>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.OpenAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the OpenAI model.</p>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.OpenAILLM.__init__","title":"<code>__init__(task, model='gpt-3.5-turbo', client=None, api_key=None, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the OpenAILLM class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo'</code> <code>client</code> <code>Union[OpenAI, None]</code> <p>an OpenAI client to be used for generation. If <code>None</code>, a new client will be created. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Union[str, None]</code> <p>the OpenAI API key to be used for generation. If <code>None</code>, the <code>OPENAI_API_KEY</code> environment variable will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in your OpenAI account.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import OpenAILLM\n&gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"gpt-3.5-turbo\",\n    client: Union[\"OpenAI\", None] = None,\n    api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the OpenAILLM class.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n        client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n            If `None`, a new client will be created. Defaults to `None`.\n        api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n            If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in your OpenAI account.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import OpenAILLM\n        &gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _OPENAI_AVAILABLE:\n        raise ImportError(\n            \"`OpenAILLM` cannot be used as `openai` is not installed, please \"\n            \" install it with `pip install openai`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.frequency_penalty = frequency_penalty\n    self.presence_penalty = presence_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n\n    self.client = client or OpenAI(api_key=api_key, max_retries=6)\n\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in your OpenAI account, available models are {self.available_models}\"\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/together/","title":"together","text":""},{"location":"reference/distilabel/llm/together/#distilabel.llm.together.TogetherInferenceLLM","title":"<code>TogetherInferenceLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/together.py</code> <pre><code>class TogetherInferenceLLM(LLM):\n    def __init__(\n        self,\n        model: str,\n        task: \"Task\",\n        api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        repetition_penalty: float = 1.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = 1,\n        stop: Union[List[str], None] = None,\n        logprobs: int = 0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the TogetherInferenceLLM class.\n\n        Args:\n            model (str): the model to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            temperature (float, optional): the temperature to be used for generation. From the Together\n                Inference docs: \"A decimal number that determines the degree of randomness in the response.\n                A value of 0 will always yield the same output. A temperature much less than 1 favors more\n                correctness and is appropriate for question answering or summarization. A value approaching\n                1 introduces more randomness in the output.\". Defaults to 1.0.\n            repetition_penalty (float, optional): the repetition penalty to be used for generation. From the\n                Together Inference docs: \"Controls the diversity of generated text by reducing the likelihood\n                of repeated sequences. Higher values decrease repetition.\". Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation. From the Together\n                Inference docs: \"used to dynamically adjust the number of choices for each predicted\n                token based on the cumulative probabilities. It specifies a probability threshold,\n                below which all less likely tokens are filtered out. This technique helps to maintain\n                diversity and generate more fluent and natural-sounding text.\". Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation. From the Together Inference\n                docs: \"used to limit the number of choices for the next predicted word or token. It specifies\n                the maximum number of tokens to consider at each step, based on their probability of occurrence.\n                This technique helps to speed up the generation process and can improve the quality of the\n                generated text by focusing on the most likely options.\". Defaults to 1.\n            stop (List[str], optional): strings to delimitate the generation process, so that when the\n                model generates any of the provided characters, the generation process is considered completed.\n                Defaults to None.\n            logprobs (int, optional): the number of logprobs to be returned for each token. From the\n                Together Inference docs: \"An integer that specifies how many top token log probabilities\n                are included in the response for each token generation step.\". Defaults to None.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in Together Inference.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import TogetherInferenceLLM\n            &gt;&gt;&gt; llm = TogetherInferenceLLM(model=\"togethercomputer/llama-2-7b\", task=TextGenerationTask(), prompt_format=\"llama2\")\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        if not _TOGETHER_AVAILABLE:\n            raise ImportError(\n                \"`TogetherInferenceLLM` cannot be used as `together` is not installed, please \"\n                \" install it with `pip install together`.\"\n            )\n\n        together.api_key = api_key or os.getenv(\"TOGETHER_API_KEY\", None)\n        if together.api_key is None:\n            raise ValueError(\n                \"No `api_key` provided, please provide one or set the `TOGETHER_API_KEY` \"\n                \"environment variable.\"\n            )\n\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in Together Inference, available models are {self.available_models}\"\n        self.model = model\n\n        self.max_new_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.repetition_penalty = repetition_penalty\n        self.stop = stop\n        self.logprobs = logprobs\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_new_tokens,\n                \"temperature\": self.temperature,\n                \"repetition_penalty\": self.repetition_penalty,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n                \"stop\": self.stop,\n                \"logprobs\": self.logprobs,\n            },\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in Together Inference.\"\"\"\n        return [\n            model[\"name\"]\n            for model in together.Models.list()\n            if model[\"display_type\"] != \"image\"\n        ]\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the Together Inference model.\"\"\"\n        return self.model\n\n    def _generate_single_output(self, prompt: str) -&gt; LLMOutput:\n        \"\"\"Runs the Together Inference text generation function over a single prompt\n        producing a single `LLMOutput`.\n\n        Args:\n            prompt (str): the formatted prompt to be provided to the Together Inference\n                endpoint.\n\n        Raises:\n            RuntimeError: raised if the Together Inference endpoint fails.\n        \"\"\"\n        try:\n            output = together.Complete.create(\n                prompt=prompt,\n                model=self.model,\n                max_tokens=self.max_new_tokens,\n                stop=self.stop,\n                temperature=self.temperature,\n                top_k=self.top_k,\n                top_p=self.top_p,\n                repetition_penalty=self.repetition_penalty,\n                logprobs=self.logprobs,\n            )\n        except Exception as e:\n            raise RuntimeError(\n                f\"Together Inference generation failed with exception: {e}\"\n            ) from e\n\n        if output[\"output\"][\"choices\"] is None or len(output[\"output\"][\"choices\"]) &lt; 1:  # type: ignore\n            raise RuntimeError(\"Together Inference generation returned no generations.\")\n\n        choice = output[\"output\"][\"choices\"][0]  # type: ignore\n        try:\n            parsed_response = self.task.parse_output(choice[\"text\"].strip())\n        except Exception as e:\n            logger.error(f\"Error parsing Together Inference response: {e}\")\n            parsed_response = None\n\n        return LLMOutput(\n            model_name=self.model_name,\n            prompt_used=prompt,\n            raw_output=choice[\"text\"] or None,\n            parsed_output=parsed_response,\n        )\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        outputs = []\n        for prompt in prompts:\n            outputs.append(\n                [self._generate_single_output(prompt) for _ in range(num_generations)]\n            )\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/together/#distilabel.llm.together.TogetherInferenceLLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in Together Inference.</p>"},{"location":"reference/distilabel/llm/together/#distilabel.llm.together.TogetherInferenceLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the Together Inference model.</p>"},{"location":"reference/distilabel/llm/together/#distilabel.llm.together.TogetherInferenceLLM.__init__","title":"<code>__init__(model, task, api_key=None, max_new_tokens=128, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=1, stop=None, logprobs=0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the TogetherInferenceLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>the model to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. From the Together Inference docs: \"A decimal number that determines the degree of randomness in the response. A value of 0 will always yield the same output. A temperature much less than 1 favors more correctness and is appropriate for question answering or summarization. A value approaching 1 introduces more randomness in the output.\". Defaults to 1.0.</p> <code>1.0</code> <code>repetition_penalty</code> <code>float</code> <p>the repetition penalty to be used for generation. From the Together Inference docs: \"Controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition.\". Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. From the Together Inference docs: \"used to dynamically adjust the number of choices for each predicted token based on the cumulative probabilities. It specifies a probability threshold, below which all less likely tokens are filtered out. This technique helps to maintain diversity and generate more fluent and natural-sounding text.\". Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. From the Together Inference docs: \"used to limit the number of choices for the next predicted word or token. It specifies the maximum number of tokens to consider at each step, based on their probability of occurrence. This technique helps to speed up the generation process and can improve the quality of the generated text by focusing on the most likely options.\". Defaults to 1.</p> <code>1</code> <code>stop</code> <code>List[str]</code> <p>strings to delimitate the generation process, so that when the model generates any of the provided characters, the generation process is considered completed. Defaults to None.</p> <code>None</code> <code>logprobs</code> <code>int</code> <p>the number of logprobs to be returned for each token. From the Together Inference docs: \"An integer that specifies how many top token log probabilities are included in the response for each token generation step.\". Defaults to None.</p> <code>0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in Together Inference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import TogetherInferenceLLM\n&gt;&gt;&gt; llm = TogetherInferenceLLM(model=\"togethercomputer/llama-2-7b\", task=TextGenerationTask(), prompt_format=\"llama2\")\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/together.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    task: \"Task\",\n    api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    repetition_penalty: float = 1.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = 1,\n    stop: Union[List[str], None] = None,\n    logprobs: int = 0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the TogetherInferenceLLM class.\n\n    Args:\n        model (str): the model to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        temperature (float, optional): the temperature to be used for generation. From the Together\n            Inference docs: \"A decimal number that determines the degree of randomness in the response.\n            A value of 0 will always yield the same output. A temperature much less than 1 favors more\n            correctness and is appropriate for question answering or summarization. A value approaching\n            1 introduces more randomness in the output.\". Defaults to 1.0.\n        repetition_penalty (float, optional): the repetition penalty to be used for generation. From the\n            Together Inference docs: \"Controls the diversity of generated text by reducing the likelihood\n            of repeated sequences. Higher values decrease repetition.\". Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation. From the Together\n            Inference docs: \"used to dynamically adjust the number of choices for each predicted\n            token based on the cumulative probabilities. It specifies a probability threshold,\n            below which all less likely tokens are filtered out. This technique helps to maintain\n            diversity and generate more fluent and natural-sounding text.\". Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation. From the Together Inference\n            docs: \"used to limit the number of choices for the next predicted word or token. It specifies\n            the maximum number of tokens to consider at each step, based on their probability of occurrence.\n            This technique helps to speed up the generation process and can improve the quality of the\n            generated text by focusing on the most likely options.\". Defaults to 1.\n        stop (List[str], optional): strings to delimitate the generation process, so that when the\n            model generates any of the provided characters, the generation process is considered completed.\n            Defaults to None.\n        logprobs (int, optional): the number of logprobs to be returned for each token. From the\n            Together Inference docs: \"An integer that specifies how many top token log probabilities\n            are included in the response for each token generation step.\". Defaults to None.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in Together Inference.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import TogetherInferenceLLM\n        &gt;&gt;&gt; llm = TogetherInferenceLLM(model=\"togethercomputer/llama-2-7b\", task=TextGenerationTask(), prompt_format=\"llama2\")\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    if not _TOGETHER_AVAILABLE:\n        raise ImportError(\n            \"`TogetherInferenceLLM` cannot be used as `together` is not installed, please \"\n            \" install it with `pip install together`.\"\n        )\n\n    together.api_key = api_key or os.getenv(\"TOGETHER_API_KEY\", None)\n    if together.api_key is None:\n        raise ValueError(\n            \"No `api_key` provided, please provide one or set the `TOGETHER_API_KEY` \"\n            \"environment variable.\"\n        )\n\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in Together Inference, available models are {self.available_models}\"\n    self.model = model\n\n    self.max_new_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.repetition_penalty = repetition_penalty\n    self.stop = stop\n    self.logprobs = logprobs\n</code></pre>"},{"location":"reference/distilabel/llm/utils/","title":"utils","text":""},{"location":"reference/distilabel/llm/utils/#distilabel.llm.utils.LLMOutput","title":"<code>LLMOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A type for the output of an LLM.</p> Source code in <code>src/distilabel/llm/utils.py</code> <pre><code>class LLMOutput(TypedDict):\n    \"\"\"A type for the output of an LLM.\"\"\"\n\n    model_name: str\n    prompt_used: Any\n    raw_output: Any\n    parsed_output: Optional[Any]\n</code></pre>"},{"location":"reference/distilabel/llm/vllm/","title":"vllm","text":""},{"location":"reference/distilabel/llm/vllm/#distilabel.llm.vllm.vLLM","title":"<code>vLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/vllm.py</code> <pre><code>class vLLM(LLM):\n    def __init__(\n        self,\n        model: \"_vLLM\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        presence_penalty: float = 0.0,\n        frequency_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the vLLM class.\n\n        Args:\n            model (_vLLM): the vLLM model to be used.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to -1.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n\n        Examples:\n            &gt;&gt;&gt; from vllm import LLM\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import vLLM\n            &gt;&gt;&gt; model = LLM(model=\"gpt2\")\n            &gt;&gt;&gt; llm = vLLM(model=model, task=TextGenerationTask())\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _VLLM_AVAILABLE:\n            raise ImportError(\n                \"`vLLM` cannot be used as `vllm` is not installed, please \"\n                \" install it with `pip install vllm`.\"\n            )\n\n        self.presence_penalty = presence_penalty\n        self.frequency_penalty = frequency_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.max_tokens = max_new_tokens\n\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"presence_penalty\": self.presence_penalty,\n                \"frequency_penalty\": self.frequency_penalty,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the vLLM model.\"\"\"\n        return self.model.llm_engine.model_config.model  # type: ignore\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        requests = self.model.generate(\n            prompts,\n            SamplingParams(  # type: ignore\n                n=num_generations,\n                presence_penalty=self.presence_penalty,\n                frequency_penalty=self.frequency_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                top_k=self.top_k,\n                max_tokens=self.max_tokens,\n            ),\n            use_tqdm=False,  # type: ignore\n        )\n        outputs = []\n        for request, prompt in zip(requests, prompts):\n            output = []\n            for request_output in request.outputs:\n                try:\n                    parsed_output = self.task.parse_output(request_output.text)\n                except Exception as e:\n                    logger.error(f\"Error parsing vLLM output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=request_output.text,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/vllm/#distilabel.llm.vllm.vLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the vLLM model.</p>"},{"location":"reference/distilabel/llm/vllm/#distilabel.llm.vllm.vLLM.__init__","title":"<code>__init__(model, task, max_new_tokens=128, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the vLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>the vLLM model to be used.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to -1.</p> <code>-1</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from vllm import LLM\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import vLLM\n&gt;&gt;&gt; model = LLM(model=\"gpt2\")\n&gt;&gt;&gt; llm = vLLM(model=model, task=TextGenerationTask())\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/vllm.py</code> <pre><code>def __init__(\n    self,\n    model: \"_vLLM\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    presence_penalty: float = 0.0,\n    frequency_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = -1,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the vLLM class.\n\n    Args:\n        model (_vLLM): the vLLM model to be used.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to -1.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n\n    Examples:\n        &gt;&gt;&gt; from vllm import LLM\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import vLLM\n        &gt;&gt;&gt; model = LLM(model=\"gpt2\")\n        &gt;&gt;&gt; llm = vLLM(model=model, task=TextGenerationTask())\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _VLLM_AVAILABLE:\n        raise ImportError(\n            \"`vLLM` cannot be used as `vllm` is not installed, please \"\n            \" install it with `pip install vllm`.\"\n        )\n\n    self.presence_penalty = presence_penalty\n    self.frequency_penalty = frequency_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.max_tokens = max_new_tokens\n\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/google/","title":"google","text":""},{"location":"reference/distilabel/llm/google/vertexai/","title":"vertexai","text":""},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.VertexAIEndpointLLM","title":"<code>VertexAIEndpointLLM</code>","text":"<p>             Bases: <code>LLM</code></p> <p>An <code>LLM</code> which uses a Vertex AI Online prediction endpoint for the generation.</p> <p>More information about Vertex AI Endpoints can be found here: https://cloud.google.com/vertex-ai/docs/general/deployment#deploy_a_model_to_an_endpoint</p> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>class VertexAIEndpointLLM(LLM):\n    \"\"\"An `LLM` which uses a Vertex AI Online prediction endpoint for the generation.\n\n    More information about Vertex AI Endpoints can be found here:\n    https://cloud.google.com/vertex-ai/docs/general/deployment#deploy_a_model_to_an_endpoint\n    \"\"\"\n\n    def __init__(\n        self,\n        endpoint_id: str,\n        task: \"Task\",\n        project: Optional[str] = None,\n        location: str = \"us-central1\",\n        generation_kwargs: Optional[Dict[str, Any]] = None,\n        prompt_argument: str = \"prompt\",\n        num_generations_argument: str = \"n\",\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the `VertexAIEndpointLLM` class.\n\n        Args:\n            endpoint_id (str): the ID of the Vertex AI endpoint to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            project (Optional[str], optional): the project to be used for generation. If `None`,\n                the default project will be used. Defaults to `None`.\n            location (str, optional): the location of the Vertex AI endpoint to be used for\n                generation. Defaults to \"us-central1\".\n            generation_kwargs (Optional[Dict[str, Any]], optional): the generation parameters\n                to be used for generation. The name of the parameters will depend on the\n                Docker image used to deploy the model to the Vertex AI endpoint. Defaults\n                to `None`.\n            prompt_argument (str, optional): the name of the Vertex AI Endpoint key to\n                be used for the prompt. Defaults to \"prompt\".\n            num_generations_argument (str, optional): the name of the Vertex AI Endpoint\n                key to be used to specify the number of generations per prompt. Defaults\n                to \"n\".\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _VERTEXAI_AVAILABLE:\n            raise ImportError(\n                \"`VertexAIEndpointLLM` cannot be used as `google-cloud-aiplatform` is not\"\n                \" installed, please install it with `pip install google-cloud-aiplatform`\"\n            )\n\n        if project is None:\n            try:\n                project = google.auth.default()[1]\n            except DefaultCredentialsError as e:\n                raise ValueError(\n                    \"No `project` was specified and no default credentials were found.\"\n                ) from e\n\n        if generation_kwargs is None:\n            generation_kwargs = {}\n\n        self.endpoint_id = endpoint_id\n        self.project = project\n        self.location = location\n        self.generation_kwargs = generation_kwargs\n        self.prompt_argument = prompt_argument\n        self.num_generations_argument = num_generations_argument\n\n        self.client = PredictionServiceClient(\n            client_options=ClientOptions(\n                api_endpoint=f\"{self.location}-aiplatform.googleapis.com\"\n            )\n        )\n\n    @cached_property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the model used for generation.\"\"\"\n        client = EndpointServiceClient(\n            client_options=ClientOptions(\n                api_endpoint=f\"{self.location}-aiplatform.googleapis.com\"\n            )\n        )\n        endpoint = client.get_endpoint(name=self.endpoint_path)\n        return endpoint.deployed_models[0].display_name\n\n    @property\n    def endpoint_path(self) -&gt; str:\n        \"\"\"Returns the path of the Vertex AI endpoint to be used for generation.\"\"\"\n        return self.client.endpoint_path(\n            project=self.project,  # type: ignore\n            location=self.location,\n            endpoint=self.endpoint_id,\n        )\n\n    @_vertexai_retry_decorator\n    def _call_vertexai_endpoint(self, instances: List[Any]) -&gt; Any:\n        return self.client.predict(endpoint=self.endpoint_path, instances=instances)\n\n    def _prepare_instances(\n        self, prompts: List[str], num_generations: int\n    ) -&gt; List[\"Value\"]:\n        \"\"\"Prepares the instances to be sent to the Vertex AI endpoint.\n\n        Args:\n            prompts (List[str]): the prompts to be used for generation.\n            num_generations (int): the number of generations to be performed for each prompt.\n\n        Returns:\n            The instances to be sent to the Vertex AI endpoint.\n        \"\"\"\n        instances = []\n        for prompt in prompts:\n            instance = json_format.ParseDict(\n                {\n                    self.prompt_argument: prompt,\n                    self.num_generations_argument: num_generations,\n                    **self.generation_kwargs,\n                },\n                Value(),\n            )\n            instances.append(instance)\n        return instances\n\n    def _single_output(self, instance: Any) -&gt; List[LLMOutput]:\n        try:\n            # NOTE: `predict` method accepts a list of instances, but depending on the\n            # deployed Docker image, it can just accept one instance.\n            response = self._call_vertexai_endpoint(instances=[instance])\n        except exceptions.InternalServerError as e:\n            raise ValueError(\n                \"The Vertex AI endpoint returned 500 Internal Server Error. This is\"\n                \" usually caused due to wrong generation parameters. Please check the\"\n                \" `generation_parameters` and try again.\"\n            ) from e\n\n        output = []\n        for prediction in response.predictions:\n            # Vertex endpoint output is `Prompt:\\n{{ model_prompt }}\\nOutput:\\n{{ model_output }}`\n            # so we need to do a pre-parsing to remove the `Prompt:` and `Output:` parts.\n            match = _PARSE_VERTEXAI_ENDPOINT_PREDICTION_REGEX.search(prediction)\n            if not match:\n                raise ValueError(\n                    \"Couldn't parse the response from the Vertex AI endpoint.\"\n                )\n\n            model_output = match.group(1).strip()\n\n            try:\n                parsed_output = self.task.parse_output(model_output)\n            except Exception as e:\n                logger.error(f\"Error parsing Vertex AI endpoint model response: {e}\")\n                parsed_output = None\n            output.append(\n                LLMOutput(\n                    model_name=self.model_name,\n                    prompt_used=instance.struct_value[self.prompt_argument],\n                    raw_output=model_output,\n                    parsed_output=parsed_output,\n                )\n            )\n        return output\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        prompts = self._generate_prompts(inputs)\n        instances = self._prepare_instances(\n            prompts=prompts, num_generations=num_generations\n        )\n        return [self._single_output(instance) for instance in instances]\n</code></pre>"},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.VertexAIEndpointLLM.endpoint_path","title":"<code>endpoint_path: str</code>  <code>property</code>","text":"<p>Returns the path of the Vertex AI endpoint to be used for generation.</p>"},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.VertexAIEndpointLLM.model_name","title":"<code>model_name: str</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the name of the model used for generation.</p>"},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.VertexAIEndpointLLM.__init__","title":"<code>__init__(endpoint_id, task, project=None, location='us-central1', generation_kwargs=None, prompt_argument='prompt', num_generations_argument='n', num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the <code>VertexAIEndpointLLM</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_id</code> <code>str</code> <p>the ID of the Vertex AI endpoint to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>project</code> <code>Optional[str]</code> <p>the project to be used for generation. If <code>None</code>, the default project will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>location</code> <code>str</code> <p>the location of the Vertex AI endpoint to be used for generation. Defaults to \"us-central1\".</p> <code>'us-central1'</code> <code>generation_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>the generation parameters to be used for generation. The name of the parameters will depend on the Docker image used to deploy the model to the Vertex AI endpoint. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_argument</code> <code>str</code> <p>the name of the Vertex AI Endpoint key to be used for the prompt. Defaults to \"prompt\".</p> <code>'prompt'</code> <code>num_generations_argument</code> <code>str</code> <p>the name of the Vertex AI Endpoint key to be used to specify the number of generations per prompt. Defaults to \"n\".</p> <code>'n'</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>def __init__(\n    self,\n    endpoint_id: str,\n    task: \"Task\",\n    project: Optional[str] = None,\n    location: str = \"us-central1\",\n    generation_kwargs: Optional[Dict[str, Any]] = None,\n    prompt_argument: str = \"prompt\",\n    num_generations_argument: str = \"n\",\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the `VertexAIEndpointLLM` class.\n\n    Args:\n        endpoint_id (str): the ID of the Vertex AI endpoint to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        project (Optional[str], optional): the project to be used for generation. If `None`,\n            the default project will be used. Defaults to `None`.\n        location (str, optional): the location of the Vertex AI endpoint to be used for\n            generation. Defaults to \"us-central1\".\n        generation_kwargs (Optional[Dict[str, Any]], optional): the generation parameters\n            to be used for generation. The name of the parameters will depend on the\n            Docker image used to deploy the model to the Vertex AI endpoint. Defaults\n            to `None`.\n        prompt_argument (str, optional): the name of the Vertex AI Endpoint key to\n            be used for the prompt. Defaults to \"prompt\".\n        num_generations_argument (str, optional): the name of the Vertex AI Endpoint\n            key to be used to specify the number of generations per prompt. Defaults\n            to \"n\".\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _VERTEXAI_AVAILABLE:\n        raise ImportError(\n            \"`VertexAIEndpointLLM` cannot be used as `google-cloud-aiplatform` is not\"\n            \" installed, please install it with `pip install google-cloud-aiplatform`\"\n        )\n\n    if project is None:\n        try:\n            project = google.auth.default()[1]\n        except DefaultCredentialsError as e:\n            raise ValueError(\n                \"No `project` was specified and no default credentials were found.\"\n            ) from e\n\n    if generation_kwargs is None:\n        generation_kwargs = {}\n\n    self.endpoint_id = endpoint_id\n    self.project = project\n    self.location = location\n    self.generation_kwargs = generation_kwargs\n    self.prompt_argument = prompt_argument\n    self.num_generations_argument = num_generations_argument\n\n    self.client = PredictionServiceClient(\n        client_options=ClientOptions(\n            api_endpoint=f\"{self.location}-aiplatform.googleapis.com\"\n        )\n    )\n</code></pre>"},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.VertexAILLM","title":"<code>VertexAILLM</code>","text":"<p>             Bases: <code>LLM</code></p> <p>An <code>LLM</code> which allows to use Google's proprietary models from the Vertex AI APIs:</p> <ul> <li>Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini</li> <li>Codey API: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview</li> <li>Text API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text</li> </ul> <p>To use the <code>VertexAILLM</code> is necessary to have configured the Google Cloud authentication using one of these methods:</p> <ul> <li>Setting <code>GOOGLE_CLOUD_CREDENTIALS</code> environment variable</li> <li>Using <code>gcloud auth application-default login</code> command</li> <li>Using <code>vertexai.init</code> function from the <code>google-cloud-aiplatform</code> library</li> </ul> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>class VertexAILLM(LLM):\n    \"\"\"An `LLM` which allows to use Google's proprietary models from the Vertex AI APIs:\n\n    - Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n    - Codey API: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview\n    - Text API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text\n\n    To use the `VertexAILLM` is necessary to have configured the Google Cloud authentication\n    using one of these methods:\n\n    - Setting `GOOGLE_CLOUD_CREDENTIALS` environment variable\n    - Using `gcloud auth application-default login` command\n    - Using `vertexai.init` function from the `google-cloud-aiplatform` library\n    \"\"\"\n\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"gemini-pro\",\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        max_new_tokens: int = 128,\n        stop_sequences: Optional[List[str]] = None,\n        num_threads: Union[int, None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the `VertexGenerativeModelLLM` class.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"gemini-pro\".\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 40.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n        \"\"\"\n        super().__init__(task=task, num_threads=num_threads)\n\n        if not _VERTEXAI_AVAILABLE:\n            raise ImportError(\n                \"`VertexAILLM` cannot be used as `google-cloud-aiplatform` is not installed,\"\n                \" please install it with `pip install google-cloud-aiplatform`\"\n            )\n\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.max_output_tokens = max_new_tokens\n        self.stop_sequences = stop_sequences\n\n        if is_gemini_model(model):\n            self.model = GenerativeModel(model)\n        elif is_codey_model(model):\n            self.model = CodeGenerationModel.from_pretrained(model)\n        else:\n            self.model = TextGenerationModel.from_pretrained(model)\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the model used for generation.\"\"\"\n        if isinstance(self.model, GenerativeModel):\n            return self.model._model_name\n\n        return self.model._model_id\n\n    def _generate_contents(self, prompts: List[str]) -&gt; List[List[Dict[str, Any]]]:\n        \"\"\"Generates a list of valid dicts that can be parsed to `vertexai.preview.generative_models.Content`\n        objects for each input.\n\n        Args:\n            prompts (List[str]): the prompts to be used for generation.\n\n        Returns:\n            List[List[Dict[str, Any]]]: the list of valid `vertexai.preview.generative_models.Content`\n                objects.\n        \"\"\"\n        return [[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}] for prompt in prompts]\n\n    @_vertexai_retry_decorator\n    def _call_generative_model_with_backoff(\n        self, contents: List[Dict[str, Any]], **kwargs: Any\n    ) -&gt; \"GenerationResponse\":\n        return self.model.generate_content(  # type: ignore\n            contents=contents,\n            # TODO: update `candidate_count` to have `num_generations` as value once valid range is not [1, 2)\n            generation_config=GenerationConfig(candidate_count=1, **kwargs),\n        )\n\n    def _generative_model_single_output(\n        self, contents: List[Dict[str, Any]]\n    ) -&gt; LLMOutput:\n        raw_output = None\n        try:\n            response = self._call_generative_model_with_backoff(\n                contents=contents,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                top_k=self.top_k,\n                max_output_tokens=self.max_output_tokens,\n                stop_sequences=self.stop_sequences,\n            )\n            raw_output = response.text\n            parsed_output = self.task.parse_output(raw_output)\n        except ValueError as e:\n            logger.error(f\"Vertex AI Gemini API model didn't return content: {e}\")\n            return LLMOutput(\n                model_name=self.model_name,\n                prompt_used=contents,\n                raw_output=None,\n                parsed_output=None,\n            )\n        except Exception as e:\n            logger.error(f\"Error parsing Vertex AI Gemini API model response: {e}\")\n            parsed_output = None\n\n        return LLMOutput(\n            model_name=self.model_name,\n            prompt_used=contents,\n            raw_output=raw_output,\n            parsed_output=parsed_output,\n        )\n\n    def _generate_with_generative_model(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        \"\"\"Generate `num_generations` for each input in `inputs` using a Vertex AI Gemini\n        API model.\"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"default\")\n        inputs_contents = self._generate_contents(prompts)\n        outputs = []\n        for contents in inputs_contents:\n            output = []\n            # TODO: remove this for-loop once `GenerationConfig.candidate_count` valid range is not [1, 2)\n            for _ in range(num_generations):\n                output.append(self._generative_model_single_output(contents=contents))\n            outputs.append(output)\n        return outputs\n\n    @_vertexai_retry_decorator\n    def _call_text_generation_model(\n        self, **kwargs: Any\n    ) -&gt; \"MultiCandidateTextGenerationResponse\":\n        return self.model.predict(**kwargs)  # type: ignore\n\n    def _text_generation_model_single_output(\n        self, prompt: str, num_generations: int\n    ) -&gt; List[LLMOutput]:\n        response = self._call_text_generation_model(\n            prompt=prompt,\n            max_output_tokens=self.max_output_tokens,\n            temperature=self.temperature,\n            top_k=self.top_k,\n            top_p=self.top_p,\n            stop_sequences=self.stop_sequences,\n            # WARNING: The model can return &lt; `candidate_count` generations depending\n            # on the generation parameters and the input.\n            candidate_count=num_generations,\n        )\n\n        output = []\n        for candidate in response.candidates:\n            try:\n                parsed_response = self.task.parse_output(candidate.text)\n            except Exception as e:\n                logger.error(\n                    f\"Error parsing Vertex AI Text/Code API model response: {e}\"\n                )\n                parsed_response = None\n\n            output.append(\n                LLMOutput(\n                    model_name=self.model_name,\n                    prompt_used=prompt,\n                    raw_output=candidate.text,\n                    parsed_output=parsed_response,\n                )\n            )\n        return output\n\n    def _generate_with_text_generation_model(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        \"\"\"Generate `num_generations` for each input in `inputs` using a Vertex AI Text/Code\n        API model.\"\"\"\n        prompts = self._generate_prompts(inputs, default_format=\"default\")\n        outputs = []\n        for prompt in prompts:\n            outputs.append(\n                self._text_generation_model_single_output(prompt, num_generations)\n            )\n        return outputs\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        if isinstance(self.model, GenerativeModel):\n            return self._generate_with_generative_model(inputs, num_generations)\n\n        return self._generate_with_text_generation_model(inputs, num_generations)\n</code></pre>"},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.VertexAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the model used for generation.</p>"},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.VertexAILLM.__init__","title":"<code>__init__(task, model='gemini-pro', temperature=None, top_p=None, top_k=None, max_new_tokens=128, stop_sequences=None, num_threads=None)</code>","text":"<p>Initializes the <code>VertexGenerativeModelLLM</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"gemini-pro\".</p> <code>'gemini-pro'</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 40.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"gemini-pro\",\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    max_new_tokens: int = 128,\n    stop_sequences: Optional[List[str]] = None,\n    num_threads: Union[int, None] = None,\n) -&gt; None:\n    \"\"\"Initializes the `VertexGenerativeModelLLM` class.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"gemini-pro\".\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 40.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n    \"\"\"\n    super().__init__(task=task, num_threads=num_threads)\n\n    if not _VERTEXAI_AVAILABLE:\n        raise ImportError(\n            \"`VertexAILLM` cannot be used as `google-cloud-aiplatform` is not installed,\"\n            \" please install it with `pip install google-cloud-aiplatform`\"\n        )\n\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.max_output_tokens = max_new_tokens\n    self.stop_sequences = stop_sequences\n\n    if is_gemini_model(model):\n        self.model = GenerativeModel(model)\n    elif is_codey_model(model):\n        self.model = CodeGenerationModel.from_pretrained(model)\n    else:\n        self.model = TextGenerationModel.from_pretrained(model)\n</code></pre>"},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.is_codey_model","title":"<code>is_codey_model(model)</code>","text":"<p>Returns <code>True</code> if the model is a model from the Vertex AI Codey API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>the model name to be checked.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the model is a model from the Vertex AI Codey API.</p> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>def is_codey_model(model: str) -&gt; bool:\n    \"\"\"Returns `True` if the model is a model from the Vertex AI Codey API.\n\n    Args:\n        model (str): the model name to be checked.\n\n    Returns:\n        bool: `True` if the model is a model from the Vertex AI Codey API.\n    \"\"\"\n    return \"code\" in model\n</code></pre>"},{"location":"reference/distilabel/llm/google/vertexai/#distilabel.llm.google.vertexai.is_gemini_model","title":"<code>is_gemini_model(model)</code>","text":"<p>Returns <code>True</code> if the model is a model from the Vertex AI Gemini API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>the model name to be checked.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the model is a model from the Vertex AI Gemini API.</p> Source code in <code>src/distilabel/llm/google/vertexai.py</code> <pre><code>def is_gemini_model(model: str) -&gt; bool:\n    \"\"\"Returns `True` if the model is a model from the Vertex AI Gemini API.\n\n    Args:\n        model (str): the model name to be checked.\n\n    Returns:\n        bool: `True` if the model is a model from the Vertex AI Gemini API.\n    \"\"\"\n    return \"gemini\" in model\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/","title":"huggingface","text":""},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/","title":"inference_endpoints","text":""},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/#distilabel.llm.huggingface.inference_endpoints.InferenceEndpointsLLM","title":"<code>InferenceEndpointsLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>class InferenceEndpointsLLM(LLM):\n    def __init__(\n        self,\n        endpoint_name_or_model_id: str,\n        task: \"Task\",\n        endpoint_namespace: Union[str, None] = None,\n        token: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        repetition_penalty: Union[float, None] = None,\n        seed: Union[int, None] = None,\n        do_sample: bool = False,\n        temperature: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n        top_p: Union[float, None] = None,\n        typical_p: Union[float, None] = None,\n        stop_sequences: Union[List[str], None] = None,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the InferenceEndpointsLLM class.\n\n        Args:\n            endpoint_name_or_model_id (str): The name of the endpoint or a Hugging Face Model Id.\n            task (Task): The task to be performed by the LLM.\n            endpoint_namespace (Union[str, None]): The namespace of the endpoint. Defaults to None.\n            token (Union[str, None]): The token for the endpoint. Defaults to None.\n            max_new_tokens (int): The maximum number of tokens to be generated. Defaults to 128.\n            repetition_penalty (Union[float, None]): The repetition penalty to be used for generation. Defaults to None.\n            seed (Union[int, None]): The seed for generation. Defaults to None.\n            do_sample (bool): Whether to do sampling. Defaults to False.\n            temperature (Union[float, None]): The temperature for generation. Defaults to None.\n            top_k (Union[int, None]): The top_k for generation. Defaults to None.\n            top_p (Union[float, None]): The top_p for generation. Defaults to None.\n            typical_p (Union[float, None]): The typical_p for generation. Defaults to None.\n            stop_sequences (Union[List[str], None]): The stop sequences for generation. Defaults to None.\n            num_threads (Union[int, None]): The number of threads. Defaults to None.\n            prompt_format (Union[\"SupportedFormats\", None]): The format of the prompt. Defaults to None.\n            prompt_formatting_fn (Union[Callable[..., str], None]): The function for formatting the prompt. Defaults to None.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n            &gt;&gt;&gt; llm = InferenceEndpointsLLM(\n            ...     endpoint_name_or_model_id=\"&lt;MODEL_ID_OR_INFERENCE_ENDPOINT&gt;\",\n            ...     task=TextGenerationTask(),\n            ... )\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _HUGGINGFACE_HUB_AVAILABLE:\n            raise ImportError(\n                \"`InferenceEndpointsLLM` cannot be used as `huggingface-hub` is not \"\n                \"installed, please install it with `pip install huggingface-hub`.\"\n            )\n\n        self.do_sample = do_sample\n        self.max_new_tokens = max_new_tokens\n        self.repetition_penalty = repetition_penalty\n        self.seed = seed\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.typical_p = typical_p\n        self.stop_sequences = stop_sequences\n\n        if is_serverless_endpoint_available(model_id=endpoint_name_or_model_id):\n            logger.info(\"Using Serverless Inference Endpoint\")\n            self.client = InferenceClient(model=endpoint_name_or_model_id, token=token)\n            self._model_name = endpoint_name_or_model_id\n        else:\n            logger.info(\"Using Dedicated Inference Endpoint\")\n            inference_endpoint = get_inference_endpoint(\n                name=endpoint_name_or_model_id,\n                namespace=endpoint_namespace,\n                token=token,\n            )\n            if inference_endpoint.status in [\"paused\", \"scaledToZero\"]:\n                logger.info(\"Waiting for Inference Endpoint to be ready...\")\n                inference_endpoint.resume().wait(timeout=30)\n\n            self.client = inference_endpoint.client\n            self._model_name = inference_endpoint.repository\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"do_sample\": self.do_sample,\n                \"max_new_tokens\": self.max_new_tokens,\n                \"repetition_penalty\": self.repetition_penalty,\n                \"seed\": self.seed,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"typical_p\": self.typical_p,\n                \"stop_sequences\": self.stop_sequences,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name of the endpoint.\"\"\"\n        return self._model_name\n\n    @retry(\n        retry=retry_if_exception_type(_INFERENCE_ENDPOINTS_API_RETRY_ON_EXCEPTIONS),\n        stop=stop_after_attempt(_INFERENCE_ENDPOINTS_API_STOP_AFTER_ATTEMPT),\n        wait=wait_random_exponential(\n            multiplier=_INFERENCE_ENDPOINTS_API_WAIT_RANDOM_EXPONENTIAL_MULTIPLIER,\n            max=_INFERENCE_ENDPOINTS_API_WAIT_RANDOM_EXPONENTIAL_MAX,\n        ),\n        before_sleep=before_sleep_log(logger, logging.INFO),\n        after=after_log(logger, logging.INFO),\n    )\n    def _text_generation_with_backoff(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Performs text generation with backoff in case of an error.\"\"\"\n        return self.client.text_generation(**kwargs)  # type: ignore\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        outputs = []\n        for prompt in prompts:\n            raw_responses = [\n                self._text_generation_with_backoff(\n                    prompt=prompt,\n                    do_sample=self.do_sample,\n                    max_new_tokens=self.max_new_tokens,\n                    repetition_penalty=self.repetition_penalty,\n                    seed=self.seed,\n                    temperature=self.temperature,\n                    top_k=self.top_k,\n                    top_p=self.top_p,\n                    typical_p=self.typical_p,\n                    stop_sequences=self.stop_sequences,\n                )\n                for _ in range(num_generations)\n            ]\n            output = []\n            for raw_response in raw_responses:\n                try:\n                    parsed_response = self.task.parse_output(raw_response)\n                except Exception as e:\n                    logger.error(f\"Error parsing Inference Endpoints output: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_response,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/#distilabel.llm.huggingface.inference_endpoints.InferenceEndpointsLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name of the endpoint.</p>"},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/#distilabel.llm.huggingface.inference_endpoints.InferenceEndpointsLLM.__init__","title":"<code>__init__(endpoint_name_or_model_id, task, endpoint_namespace=None, token=None, max_new_tokens=128, repetition_penalty=None, seed=None, do_sample=False, temperature=None, top_k=None, top_p=None, typical_p=None, stop_sequences=None, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the InferenceEndpointsLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name_or_model_id</code> <code>str</code> <p>The name of the endpoint or a Hugging Face Model Id.</p> required <code>task</code> <code>Task</code> <p>The task to be performed by the LLM.</p> required <code>endpoint_namespace</code> <code>Union[str, None]</code> <p>The namespace of the endpoint. Defaults to None.</p> <code>None</code> <code>token</code> <code>Union[str, None]</code> <p>The token for the endpoint. Defaults to None.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>repetition_penalty</code> <code>Union[float, None]</code> <p>The repetition penalty to be used for generation. Defaults to None.</p> <code>None</code> <code>seed</code> <code>Union[int, None]</code> <p>The seed for generation. Defaults to None.</p> <code>None</code> <code>do_sample</code> <code>bool</code> <p>Whether to do sampling. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Union[float, None]</code> <p>The temperature for generation. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>Union[int, None]</code> <p>The top_k for generation. Defaults to None.</p> <code>None</code> <code>top_p</code> <code>Union[float, None]</code> <p>The top_p for generation. Defaults to None.</p> <code>None</code> <code>typical_p</code> <code>Union[float, None]</code> <p>The typical_p for generation. Defaults to None.</p> <code>None</code> <code>stop_sequences</code> <code>Union[List[str], None]</code> <p>The stop sequences for generation. Defaults to None.</p> <code>None</code> <code>num_threads</code> <code>Union[int, None]</code> <p>The number of threads. Defaults to None.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>The format of the prompt. Defaults to None.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>The function for formatting the prompt. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n&gt;&gt;&gt; llm = InferenceEndpointsLLM(\n...     endpoint_name_or_model_id=\"&lt;MODEL_ID_OR_INFERENCE_ENDPOINT&gt;\",\n...     task=TextGenerationTask(),\n... )\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>def __init__(\n    self,\n    endpoint_name_or_model_id: str,\n    task: \"Task\",\n    endpoint_namespace: Union[str, None] = None,\n    token: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    repetition_penalty: Union[float, None] = None,\n    seed: Union[int, None] = None,\n    do_sample: bool = False,\n    temperature: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n    top_p: Union[float, None] = None,\n    typical_p: Union[float, None] = None,\n    stop_sequences: Union[List[str], None] = None,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the InferenceEndpointsLLM class.\n\n    Args:\n        endpoint_name_or_model_id (str): The name of the endpoint or a Hugging Face Model Id.\n        task (Task): The task to be performed by the LLM.\n        endpoint_namespace (Union[str, None]): The namespace of the endpoint. Defaults to None.\n        token (Union[str, None]): The token for the endpoint. Defaults to None.\n        max_new_tokens (int): The maximum number of tokens to be generated. Defaults to 128.\n        repetition_penalty (Union[float, None]): The repetition penalty to be used for generation. Defaults to None.\n        seed (Union[int, None]): The seed for generation. Defaults to None.\n        do_sample (bool): Whether to do sampling. Defaults to False.\n        temperature (Union[float, None]): The temperature for generation. Defaults to None.\n        top_k (Union[int, None]): The top_k for generation. Defaults to None.\n        top_p (Union[float, None]): The top_p for generation. Defaults to None.\n        typical_p (Union[float, None]): The typical_p for generation. Defaults to None.\n        stop_sequences (Union[List[str], None]): The stop sequences for generation. Defaults to None.\n        num_threads (Union[int, None]): The number of threads. Defaults to None.\n        prompt_format (Union[\"SupportedFormats\", None]): The format of the prompt. Defaults to None.\n        prompt_formatting_fn (Union[Callable[..., str], None]): The function for formatting the prompt. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n        &gt;&gt;&gt; llm = InferenceEndpointsLLM(\n        ...     endpoint_name_or_model_id=\"&lt;MODEL_ID_OR_INFERENCE_ENDPOINT&gt;\",\n        ...     task=TextGenerationTask(),\n        ... )\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _HUGGINGFACE_HUB_AVAILABLE:\n        raise ImportError(\n            \"`InferenceEndpointsLLM` cannot be used as `huggingface-hub` is not \"\n            \"installed, please install it with `pip install huggingface-hub`.\"\n        )\n\n    self.do_sample = do_sample\n    self.max_new_tokens = max_new_tokens\n    self.repetition_penalty = repetition_penalty\n    self.seed = seed\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.typical_p = typical_p\n    self.stop_sequences = stop_sequences\n\n    if is_serverless_endpoint_available(model_id=endpoint_name_or_model_id):\n        logger.info(\"Using Serverless Inference Endpoint\")\n        self.client = InferenceClient(model=endpoint_name_or_model_id, token=token)\n        self._model_name = endpoint_name_or_model_id\n    else:\n        logger.info(\"Using Dedicated Inference Endpoint\")\n        inference_endpoint = get_inference_endpoint(\n            name=endpoint_name_or_model_id,\n            namespace=endpoint_namespace,\n            token=token,\n        )\n        if inference_endpoint.status in [\"paused\", \"scaledToZero\"]:\n            logger.info(\"Waiting for Inference Endpoint to be ready...\")\n            inference_endpoint.resume().wait(timeout=30)\n\n        self.client = inference_endpoint.client\n        self._model_name = inference_endpoint.repository\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/#distilabel.llm.huggingface.inference_endpoints.is_serverless_endpoint_available","title":"<code>is_serverless_endpoint_available(model_id)</code>","text":"<p>Checks input is a valid Hugging Face model and if there is a serverless endpoint available for it.</p> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>def is_serverless_endpoint_available(model_id: str) -&gt; bool:\n    \"\"\"Checks input is a valid Hugging Face model and if there is a serverless endpoint available for it.\"\"\"\n    # 1. First we check if input includes a \"/\" which is indicative of a model name\n    if \"/\" not in model_id:\n        return False\n    if model_id.startswith(\"https:\"):\n        return False\n    # 2. Then we check if the model is currently deployed\n    try:\n        client = InferenceClient()\n        deploy_llms = client.list_deployed_models(\"text-generation-inference\")[\n            \"text-generation\"\n        ]\n        if model_id in deploy_llms:\n            return True\n    except Exception as e:\n        logger.error(e)\n\n    return False\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/transformers/","title":"transformers","text":""},{"location":"reference/distilabel/llm/huggingface/transformers/#distilabel.llm.huggingface.transformers.TransformersLLM","title":"<code>TransformersLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/huggingface/transformers.py</code> <pre><code>class TransformersLLM(LLM):\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        typical_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the TransformersLLM class.\n\n        Args:\n            model (PreTrainedModel): the model to be used for generation.\n            tokenizer (PreTrainedTokenizer): the tokenizer to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            do_sample (bool, optional): whether to sample from the model or not.\n                Defaults to False.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 50.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            typical_p (float, optional): the typical-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used for generation.\n                If `None`, the number of threads will be set to the number of available CPUs.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for formatting the prompts. If `None`, the prompts will not be formatted.\n                Defaults to `None`.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): the function to be used\n                for formatting the prompts. If `None`, the prompts will not be formatted.\n\n        Examples:\n            &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n            &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n            &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n            &gt;&gt;&gt; llm = TransformersLLM(\n            ...     model=model,\n            ...     tokenizer=tokenizer,\n            ...     task=TextGenerationTask(),\n            ... )\n            &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        self.max_new_tokens = max_new_tokens\n        self.do_sample = do_sample\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.typical_p = typical_p\n\n        self.model = model\n        self.tokenizer = tokenizer\n\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        if (\n            hasattr(self.tokenizer, \"use_default_system_prompt\")\n            and self.tokenizer.use_default_system_prompt  # type: ignore\n        ):\n            # The `tokenizer` also has a method named `apply_chat_template` that expects a `Conversation` as OpenAI does with the ChatML format\n            warnings.warn(\n                \"The provided `tokenizer` has `use_default_system_prompt=True` which means that the default system prompt will be used, which may collide with the `task` provided as an arg to this class.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_new_tokens,\n                \"do_sample\": self.do_sample,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"typical_p\": self.typical_p,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the Transformers model.\"\"\"\n        return self.model.config.name_or_path\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(inputs, default_format=None)\n        encodings = self.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n        encodings = encodings.to(self.model.device)\n        with torch.inference_mode():\n            generated_ids = self.model.generate(\n                **encodings,  # type: ignore\n                pad_token_id=self.tokenizer.eos_token_id,\n                generation_config=GenerationConfig(\n                    do_sample=self.do_sample,\n                    temperature=self.temperature,\n                    max_new_tokens=self.max_new_tokens,\n                    top_k=self.top_k,\n                    top_p=self.top_p,\n                    typical_p=self.typical_p,\n                    num_return_sequences=num_generations,\n                ),\n            )\n        raw_outputs = self.tokenizer.batch_decode(\n            generated_ids[:, encodings.input_ids.shape[1] :],\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n        outputs = []\n        for prompt, i in zip(prompts, range(0, len(raw_outputs), num_generations)):\n            output = []\n            for raw_output in raw_outputs[i : i + num_generations]:\n                try:\n                    parsed_output = self.task.parse_output(raw_output)\n                except Exception as e:\n                    logger.error(f\"Error parsing Transformers output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/transformers/#distilabel.llm.huggingface.transformers.TransformersLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the Transformers model.</p>"},{"location":"reference/distilabel/llm/huggingface/transformers/#distilabel.llm.huggingface.transformers.TransformersLLM.__init__","title":"<code>__init__(model, tokenizer, task, max_new_tokens=128, do_sample=False, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the TransformersLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>the model to be used for generation.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>the tokenizer to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>do_sample</code> <code>bool</code> <p>whether to sample from the model or not. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>typical_p</code> <code>float</code> <p>the typical-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for generation. If <code>None</code>, the number of threads will be set to the number of available CPUs. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for formatting the prompts. If <code>None</code>, the prompts will not be formatted. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>the function to be used for formatting the prompts. If <code>None</code>, the prompts will not be formatted.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n&gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n&gt;&gt;&gt; from distilabel.llm import TransformersLLM\n&gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n&gt;&gt;&gt; llm = TransformersLLM(\n...     model=model,\n...     tokenizer=tokenizer,\n...     task=TextGenerationTask(),\n... )\n&gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n</code></pre> Source code in <code>src/distilabel/llm/huggingface/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    do_sample: bool = False,\n    temperature: float = 1.0,\n    top_k: int = 50,\n    top_p: float = 1.0,\n    typical_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the TransformersLLM class.\n\n    Args:\n        model (PreTrainedModel): the model to be used for generation.\n        tokenizer (PreTrainedTokenizer): the tokenizer to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        do_sample (bool, optional): whether to sample from the model or not.\n            Defaults to False.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 50.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        typical_p (float, optional): the typical-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used for generation.\n            If `None`, the number of threads will be set to the number of available CPUs.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for formatting the prompts. If `None`, the prompts will not be formatted.\n            Defaults to `None`.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): the function to be used\n            for formatting the prompts. If `None`, the prompts will not be formatted.\n\n    Examples:\n        &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n        &gt;&gt;&gt; from distilabel.tasks import TextGenerationTask\n        &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n        &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        &gt;&gt;&gt; llm = TransformersLLM(\n        ...     model=model,\n        ...     tokenizer=tokenizer,\n        ...     task=TextGenerationTask(),\n        ... )\n        &gt;&gt;&gt; llm.generate([{\"input\": \"What's the capital of Spain?\"}])\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    self.max_new_tokens = max_new_tokens\n    self.do_sample = do_sample\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.typical_p = typical_p\n\n    self.model = model\n    self.tokenizer = tokenizer\n\n    if self.tokenizer.pad_token is None:\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n    if (\n        hasattr(self.tokenizer, \"use_default_system_prompt\")\n        and self.tokenizer.use_default_system_prompt  # type: ignore\n    ):\n        # The `tokenizer` also has a method named `apply_chat_template` that expects a `Conversation` as OpenAI does with the ChatML format\n        warnings.warn(\n            \"The provided `tokenizer` has `use_default_system_prompt=True` which means that the default system prompt will be used, which may collide with the `task` provided as an arg to this class.\",\n            UserWarning,\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/","title":"tasks","text":""},{"location":"reference/distilabel/tasks/#distilabel.tasks.ComplexityScorerTask","title":"<code>ComplexityScorerTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTaskNoRationale</code></p> <p>A <code>PreferenceTask</code> following the <code>Complexity Scorer</code> specification for rating instructions in terms of complexity.</p> <p>This task is inspired by the Evol Complexity Scorer in the Deita framework: Deita is an open-sourced project designed to facilitate Automatic Data Selection for instruction tuning in Large Language Models (LLMs).</p> <p>The task is defined as follows: Ask an LLM (in the original paper they used ChatGPT) to rate the instructions (the number of instructions is dynamic in the sense that you can compare any number, in Deita the chose 6) to obtain a complexity score c for each instruction.</p> <p>This task will only need to receive the list of <code>generations</code> in a dataset to generate the scores.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/complexity_scorer.py</code> <pre><code>@dataclass\nclass ComplexityScorerTask(PreferenceTaskNoRationale):\n    \"\"\"A `PreferenceTask` following the `Complexity Scorer` specification for rating instructions\n    in terms of complexity.\n\n    This task is inspired by the Evol Complexity Scorer in the Deita framework: *Deita is an open-sourced project\n    designed to facilitate Automatic Data Selection for instruction tuning in Large Language Models (LLMs).*\n\n    The task is defined as follows:\n    Ask an LLM (in the original paper they used ChatGPT) to rate the instructions (the number of instructions\n    is dynamic in the sense that you can compare any number, in *Deita* the chose 6) to obtain a complexity\n    score *c* for each instruction.\n\n    This task will only need to receive the list of `generations` in a dataset to generate the scores.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    system_prompt: str = \"\"\n\n    __jinja2_template__: str = _COMPLEXITY_SCORER_TEMPLATE\n\n    def generate_prompt(self, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the *Evol Complexity* specification in *Deita*.\n\n        Args:\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import ComplexityScorerTask\n            &gt;&gt;&gt; task = ComplexityScorerTask()\n            &gt;&gt;&gt; task.generate_prompt([\"instruction 1\", \"instruction 2\"])\n            Prompt(system_prompt=\"\", formatted_prompt=\"Ranking the following questions...\")\n        \"\"\"\n        render_kwargs = {\"instructions\": generations}\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the input arguments of the task.\"\"\"\n        return [\"generations\"]\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the task, returning a list with the rank/score of each instruction.\n\n        Args:\n            output (str): The output of the LLM raw.\n\n        Returns:\n            Dict[str, List[str]]: A dict with containing the ranks/scores of each instruction.\n        \"\"\"\n        output = output.lower().split(\"\\n\")\n        scores = [float(re.sub(r\"\\[\\d+\\] score:\", \"\", o).strip()) for o in output]\n        return {self.output_args_names[0]: scores}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.ComplexityScorerTask.input_args_names","title":"<code>input_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the input arguments of the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.ComplexityScorerTask.generate_prompt","title":"<code>generate_prompt(generations, **_)</code>","text":"<p>Generates a prompt following the Evol Complexity specification in Deita.</p> <p>Parameters:</p> Name Type Description Default <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import ComplexityScorerTask\n&gt;&gt;&gt; task = ComplexityScorerTask()\n&gt;&gt;&gt; task.generate_prompt([\"instruction 1\", \"instruction 2\"])\nPrompt(system_prompt=\"\", formatted_prompt=\"Ranking the following questions...\")\n</code></pre> Source code in <code>src/distilabel/tasks/preference/complexity_scorer.py</code> <pre><code>def generate_prompt(self, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the *Evol Complexity* specification in *Deita*.\n\n    Args:\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import ComplexityScorerTask\n        &gt;&gt;&gt; task = ComplexityScorerTask()\n        &gt;&gt;&gt; task.generate_prompt([\"instruction 1\", \"instruction 2\"])\n        Prompt(system_prompt=\"\", formatted_prompt=\"Ranking the following questions...\")\n    \"\"\"\n    render_kwargs = {\"instructions\": generations}\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.ComplexityScorerTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the task, returning a list with the rank/score of each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output of the LLM raw.</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict[str, List[str]]: A dict with containing the ranks/scores of each instruction.</p> Source code in <code>src/distilabel/tasks/preference/complexity_scorer.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the task, returning a list with the rank/score of each instruction.\n\n    Args:\n        output (str): The output of the LLM raw.\n\n    Returns:\n        Dict[str, List[str]]: A dict with containing the ranks/scores of each instruction.\n    \"\"\"\n    output = output.lower().split(\"\\n\")\n    scores = [float(re.sub(r\"\\[\\d+\\] score:\", \"\", o).strip()) for o in output]\n    return {self.output_args_names[0]: scores}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.CritiqueTask","title":"<code>CritiqueTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RatingToArgillaMixin</code>, <code>Task</code></p> <p>A <code>Task</code> for critique / judge tasks.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/tasks/critique/base.py</code> <pre><code>@dataclass\nclass CritiqueTask(RatingToArgillaMixin, Task):\n    \"\"\"A `Task` for critique / judge tasks.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n    \"\"\"\n\n    __type__: ClassVar[Literal[\"labelling\"]] = \"labelling\"\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the input arguments of the task.\"\"\"\n        return [\"input\", \"generations\"]\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"critique\", \"score\"]\n\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        score_column: str = \"score\",\n        critique_column: str = \"critique\",\n        score_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackDataset\":\n        return super().to_argilla_dataset(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            ratings_column=score_column,\n            rationale_column=critique_column,\n            ratings_values=score_values or [1, 2, 3, 4, 5],\n        )\n\n    def to_argilla_record(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        score_column: str = \"score\",\n        critique_column: str = \"critique\",\n        score_values: Optional[List[int]] = None,\n    ) -&gt; Union[\"FeedbackRecord\", List[\"FeedbackRecord\"]]:\n        return super().to_argilla_record(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            ratings_column=score_column,\n            rationale_column=critique_column,\n            ratings_values=score_values or [1, 2, 3, 4, 5],\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.CritiqueTask.input_args_names","title":"<code>input_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the input arguments of the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.CritiqueTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.EvolComplexityTask","title":"<code>EvolComplexityTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>EvolInstructTask</code></p> <p>A <code>TextGenerationTask</code> following the <code>EvolComplexity</code> specification for building prompts. This is a special case of the original EvolInstructTask, where the evolution method is fixed to \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\". Additionally, an additional elimation step should be executed to screen out instructions that are not useful.</p> <p>From the reference repository: Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.</p> <p>The task is defined as follows: Starting from an initial (simpler) instruction, select in-depth or in-breadth evolving to upgrade the simple instruction to a more complex one or create a new one (to increase diversity). The In-depth Evolving includes the following operations: \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\". The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.</p> <p>Given the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the answer is a copy from the same used prompt.</p> <p>This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities. Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions in the original dataset. An example can be seen at the following script: examples/pipeline-evol-instruct-alpaca.py</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code></li> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/evol_complexity.py</code> <pre><code>@dataclass\nclass EvolComplexityTask(EvolInstructTask):\n    \"\"\"A `TextGenerationTask` following the `EvolComplexity` specification for building prompts. This is a special case\n    of the original EvolInstructTask, where the evolution method is fixed to \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".\n    Additionally, an additional elimation step should be executed to screen out instructions that are not useful.\n\n    From the reference repository: *Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce\n    open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.*\n\n    The task is defined as follows:\n    Starting from an initial (simpler) instruction, select in-depth or in-breadth evolving to upgrade the simple instruction\n    to a more complex one or create a new one (to increase diversity).\n    The In-depth Evolving includes the following operations: \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".\n    The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.\n\n    Given the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator\n    to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the\n    answer is a copy from the same used prompt.\n\n    This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities.\n    Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions\n    in the original dataset. An example can be seen at the following script:\n    [examples/pipeline-evol-instruct-alpaca.py](https://github.com/argilla-io/distilabel/tree/main/examples/pipeline-evol-instruct-alpaca.py)\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    system_prompt: str = \"\"\n\n    __jinja2_template__: str = _EVOL_COMPLEXITY_TEMPLATE\n\n    def generate_prompt(\n        self, input: str, evolution_method: Optional[EvolutionMethod] = None, **_: Any\n    ) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Evol-Complexity specification of the Deita Paper.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            evolution_method (str, optional): The evolution method to be used. If not provided (the default), a random one is chosen\n                like the original paper. Available ones are \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolComplexityGeneratorTask\n            &gt;&gt;&gt; task = EvolComplexityGeneratorTask()\n            &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\n            Prompt(\n                system_prompt=\"\",\n                formatted_prompt=\"I want you to act as a Prompt ...\",\n            )\n        \"\"\"\n        evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n        return super().generate_prompt(input, evolution_method=evolution_method, **_)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.EvolComplexityTask.generate_prompt","title":"<code>generate_prompt(input, evolution_method=None, **_)</code>","text":"<p>Generates a prompt following the Evol-Complexity specification of the Deita Paper.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>evolution_method</code> <code>str</code> <p>The evolution method to be used. If not provided (the default), a random one is chosen like the original paper. Available ones are \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import EvolComplexityGeneratorTask\n&gt;&gt;&gt; task = EvolComplexityGeneratorTask()\n&gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\nPrompt(\n    system_prompt=\"\",\n    formatted_prompt=\"I want you to act as a Prompt ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/evol_complexity.py</code> <pre><code>def generate_prompt(\n    self, input: str, evolution_method: Optional[EvolutionMethod] = None, **_: Any\n) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Evol-Complexity specification of the Deita Paper.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        evolution_method (str, optional): The evolution method to be used. If not provided (the default), a random one is chosen\n            like the original paper. Available ones are \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolComplexityGeneratorTask\n        &gt;&gt;&gt; task = EvolComplexityGeneratorTask()\n        &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\n        Prompt(\n            system_prompt=\"\",\n            formatted_prompt=\"I want you to act as a Prompt ...\",\n        )\n    \"\"\"\n    evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n    return super().generate_prompt(input, evolution_method=evolution_method, **_)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.EvolInstructTask","title":"<code>EvolInstructTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>InstructTaskMixin</code>, <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> following the <code>EvolInstruct</code> specification for building the prompts.</p> <p>From the reference repository: Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.</p> <p>The task is defined as follows: Starting from an initial (simpler) instruction, select in-depth or in-breadth evolving to upgrade the simple instruction to a more complex one or create a new one (to increase diversity). The In-depth Evolving includes the following operations: add constraints, deepening, concretizing and increase reasoning. The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.</p> <p>Given the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the answer is a copy from the same used prompt.</p> <p>This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities. Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions in the original dataset. An example can be seen at the following script: examples/pipeline-evol-instruct-alpaca.py</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/evol_instruct.py</code> <pre><code>@dataclass\nclass EvolInstructTask(InstructTaskMixin, TextGenerationTask):\n    \"\"\"A `TextGenerationTask` following the `EvolInstruct` specification for building the prompts.\n\n    From the reference repository: *Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce\n    open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.*\n\n    The task is defined as follows:\n    Starting from an initial (simpler) instruction, select in-depth or in-breadth evolving to upgrade the simple instruction\n    to a more complex one or create a new one (to increase diversity).\n    The In-depth Evolving includes the following operations: add constraints, deepening, concretizing and increase reasoning.\n    The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.\n\n    Given the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator\n    to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the\n    answer is a copy from the same used prompt.\n\n    This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities.\n    Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions\n    in the original dataset. An example can be seen at the following script:\n    [examples/pipeline-evol-instruct-alpaca.py](https://github.com/argilla-io/distilabel/tree/main/examples/pipeline-evol-instruct-alpaca.py)\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    system_prompt: str = \"\"\n\n    __jinja2_template__: str = _EVOL_INSTRUCT_TEMPLATE\n\n    def generate_prompt(\n        self, input: str, evolution_method: Optional[EvolutionMethod] = None, **_: Any\n    ) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Evol-Instruct specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            evolution_method (str, optional): The evolution method to be used. If not provided (the default), a random one is chosen\n                like the original paper. Available ones are \"breadth\", \"constraints\", \"deepen\", \"concretizing\" and \"reasoning\".\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolInstructTask\n            &gt;&gt;&gt; task = EvolInstructTask()\n            &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\n            Prompt(\n                system_prompt=\"\",\n                formatted_prompt=\"I want you to act as a Prompt ...\",\n            )\n        \"\"\"\n        evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n        render_kwargs = {\n            \"evol_method\": evolution_method,\n            \"instruction\": input,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        return [\"instructions\"]\n\n    def _elimination_evolving(\n        self, output: str, response_words: Optional[List[str]] = None\n    ) -&gt; Optional[str]:\n        \"\"\"Performs the elimination step of the Evol-Instruct task, steps 2-4 in the paper:\n\n        1. [NOT IMPLEMENTED] The evolved instruction does not provide any information gain compared\n        to the original one. Use ChatGPT to make this determination, this is outlined in Appendix G of the original paper.\n        2. The evolved instruction makes it difficult for the LLM to generate a response. We found that\n        when the generated response contains \u201csorry\u201d and is relatively short in length (i.e., less than\n        80 words), it often indicates that the LLM struggles to respond to the evolved instruction.\n        So we can use this rule to make a judgment.\n        3. The response generated by the LLM only contains punctuation and stop words.\n        4. The evolved instruction obviously copies some words from the evolving prompt, such as\n        \u201cgiven prompt\u201d, \u201crewritten prompt\u201d, \u201c#Rewritten Prompt#\u201d, etc.\n        \"\"\"\n        output = output.strip()\n        if output == \"\":\n            return\n\n        # 2) The evolved instruction makes it difficult for the LLM to generate a response.\n        if \"sorry\" in output.lower() and len(output.split(\" \")) &lt; 80:\n            logger.info(\n                f\"Evolution step removed the output, it's hard for the LLM to generate a response: {output}\"\n            )\n            return\n\n        # 3) The output only contains punctuation and stop words\n        stopwords = _get_stopwords()\n        clean_output = [word for word in output.split(\" \") if word not in stopwords]\n        if set(clean_output).difference(set(string.punctuation)) == 0:\n            logger.info(\n                f\"Evolution step removed the output, it only contains punctuation and stop words: {output}\"\n            )\n            return\n\n        # 4) Remove copied words from the prompt\n        prompt_words = {\n            \"#Given Prompt#\",\n            \"#Created Prompt#\",\n            \"given prompt\",\n            \"created prompt\",\n            \"#The Given Prompt#\",\n            \"#Rewritten Prompt#\",\n            \"rewritten prompt\",\n        }\n        if response_words:\n            prompt_words = prompt_words.union(response_words)\n        if any(word in output for word in prompt_words):\n            logger.info(\n                f\"Evolution step removed the output due to word repetition from the prompt: {output}\"\n            )\n            return\n\n        return output\n\n    def _get_evolution_method(\n        self, chosen_method: EvolutionMethod, available_methods: EvolutionMethod\n    ) -&gt; None:\n        available_methods = get_args(available_methods)\n        if not chosen_method:\n            chosen_method = random.choice(available_methods)\n        if chosen_method not in available_methods:\n            raise ValueError(\n                f\"Evolution method {chosen_method} is not available. Available ones are: {available_methods}\"\n            )\n        return chosen_method\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the model into the desired format, applying the elimination step for bad generations.\n\n        Args:\n            output (str): the output of the model.\n\n        Note:\n            The elimination step is applied to the output, but only steps 2-4 in the paper are implemented.\n            Refer to point 3.2, Elimination Evolving section in [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n            for more information on the elimination evolving step, and take a look at the `_elimination_evolving`\n            method for more information of the implementation.\n        \"\"\"\n        output = self._elimination_evolving(output)\n        return {self.output_args_names[0]: output}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.EvolInstructTask.generate_prompt","title":"<code>generate_prompt(input, evolution_method=None, **_)</code>","text":"<p>Generates a prompt following the Evol-Instruct specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>evolution_method</code> <code>str</code> <p>The evolution method to be used. If not provided (the default), a random one is chosen like the original paper. Available ones are \"breadth\", \"constraints\", \"deepen\", \"concretizing\" and \"reasoning\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import EvolInstructTask\n&gt;&gt;&gt; task = EvolInstructTask()\n&gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\nPrompt(\n    system_prompt=\"\",\n    formatted_prompt=\"I want you to act as a Prompt ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/evol_instruct.py</code> <pre><code>def generate_prompt(\n    self, input: str, evolution_method: Optional[EvolutionMethod] = None, **_: Any\n) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Evol-Instruct specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        evolution_method (str, optional): The evolution method to be used. If not provided (the default), a random one is chosen\n            like the original paper. Available ones are \"breadth\", \"constraints\", \"deepen\", \"concretizing\" and \"reasoning\".\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolInstructTask\n        &gt;&gt;&gt; task = EvolInstructTask()\n        &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\n        Prompt(\n            system_prompt=\"\",\n            formatted_prompt=\"I want you to act as a Prompt ...\",\n        )\n    \"\"\"\n    evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n    render_kwargs = {\n        \"evol_method\": evolution_method,\n        \"instruction\": input,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.EvolInstructTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format, applying the elimination step for bad generations.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>the output of the model.</p> required Note <p>The elimination step is applied to the output, but only steps 2-4 in the paper are implemented. Refer to point 3.2, Elimination Evolving section in <code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code> for more information on the elimination evolving step, and take a look at the <code>_elimination_evolving</code> method for more information of the implementation.</p> Source code in <code>src/distilabel/tasks/text_generation/evol_instruct.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the model into the desired format, applying the elimination step for bad generations.\n\n    Args:\n        output (str): the output of the model.\n\n    Note:\n        The elimination step is applied to the output, but only steps 2-4 in the paper are implemented.\n        Refer to point 3.2, Elimination Evolving section in [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n        for more information on the elimination evolving step, and take a look at the `_elimination_evolving`\n        method for more information of the implementation.\n    \"\"\"\n    output = self._elimination_evolving(output)\n    return {self.output_args_names[0]: output}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.EvolQualityTask","title":"<code>EvolQualityTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>EvolInstructTask</code></p> <p>A <code>TextGenerationTask</code> following the <code>Deita</code> specification for improving the quality of instructions.</p> <p>From the reference repository: DEITA (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach.</p> <p>The task is defined as follows: Starting from an initial (simpler) instruction response, select an evolving-method to upgrade the quality of the instruction. The Evolving methods includes the following operations: add \"helpfulness\", \"relevance\", \"depth\", \"creativity\" and \"details\".</p> <p>Given the evolved responses are generated from LLMs, sometimes the evolving will fail. We adopt an responses eliminator to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the answer is a copy from the same used prompt. Note that we slightly modify the elimination evolving step, from the original paper, to allow for filtering of the responses.</p> <p>This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities. Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions in the original dataset. An example of a similar implementation with <code>EvolInstruct</code> can be seen at the following script: examples/pipeline-evol-instruct-alpaca.py</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> <li><code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/evol_quality.py</code> <pre><code>@dataclass\nclass EvolQualityTask(EvolInstructTask):\n    \"\"\"A `TextGenerationTask` following the `Deita` specification for improving the *quality* of instructions.\n\n    From the reference repository: *DEITA (short for Data-Efficient Instruction Tuning for Alignment),\n    a series of models fine-tuned from LLaMA and Mistral models using data samples automatically\n    selected with our proposed approach*.\n\n    The task is defined as follows:\n    Starting from an initial (simpler) instruction response, select an evolving-method to upgrade the quality\n    of the instruction. The Evolving methods includes the following operations: add \"helpfulness\", \"relevance\",\n    \"depth\", \"creativity\" and \"details\".\n\n    Given the evolved responses are generated from LLMs, sometimes the evolving will fail.\n    We adopt an responses eliminator to filter the failed instructions, called Elimination Evolving,\n    but we don't apply the step of asking again to the LLM it the answer is a copy from the same used\n    prompt. Note that we slightly modify the elimination evolving step, from the original paper, to\n    allow for filtering of the responses.\n\n    This evolutionary process can be repeated for several rounds to obtain instruction data containing various\n    complexities. Currently the task is implemented as a single step, so to generate multiple evolutions you\n    can \"repeat\" the instructions in the original dataset. An example of a similar implementation with\n    `EvolInstruct` can be seen at the following script: [examples/pipeline-evol-instruct-alpaca.py](https://github.com/argilla-io/distilabel/tree/main/examples/pipeline-evol-instruct-alpaca.py)\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n        - [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    system_prompt: str = \"\"\n\n    __jinja2_template__: str = _EVOL_QUALITY_TEMPLATE\n\n    def generate_prompt(\n        self,\n        input: str,\n        generation: str,\n        evolution_method: Optional[EvolutionMethod] = None,\n        **_: Any,\n    ) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Evol-Instruct specification.\n\n        Args:\n            input (str):\n                The input to be used for the prompt. Corresponds to the instruction in the prompt.\n            generation (str):\n                The generation to be used for the prompt, which corresponds to a generated response\n                given the instruction given in the input.\n            evolution_method (str, optional):\n                The evolution method to be used. If not provided (the default), a random one is chosen\n                like the original paper. Available ones are \"helpfulness\", \"relevance\", \"deepen\",\n                \"creativity\" and \"details\".\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolQualityTask\n            &gt;&gt;&gt; task = EvolQualityTask()\n            &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\", \"1. Eat healthy food. 2. Exercise. 3. Sleep well.\")\n            Prompt(\n                system_prompt=\"\",\n                formatted_prompt=\"I want you to act as a Prompt ...\",\n            )\n        \"\"\"\n        evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n        render_kwargs = {\n            \"evol_method\": evolution_method,\n            \"instruction\": input,\n            \"generation\": generation,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        return [\"input\", \"generation\"]\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        return [\"generations\"]\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the model into the desired format, applying the elimination step for bad generations.\n\n        Args:\n            output (str): the output of the model.\n\n        Note:\n            The elimination step is applied to the output, but only steps 2-4 in the paper are implemented.\n            Refer to point 3.2, Elimination Evolving section in [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n            for more information on the elimination evolving step, and take a look at the `_elimination_evolving`\n            method for more information of the implementation.\n        \"\"\"\n        response_words = {\n            \"#Given Response#\",\n            \"#Created Response#\",\n            \"given response\",\n            \"created response\",\n            \"#The Given Response#\",\n            \"#Rewritten Response#\",\n            \"rewritten response\",\n        }\n        output = self._elimination_evolving(output, response_words=response_words)\n        return {self.output_args_names[0]: output}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.EvolQualityTask.generate_prompt","title":"<code>generate_prompt(input, generation, evolution_method=None, **_)</code>","text":"<p>Generates a prompt following the Evol-Instruct specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input to be used for the prompt. Corresponds to the instruction in the prompt.</p> required <code>generation</code> <code>str</code> <p>The generation to be used for the prompt, which corresponds to a generated response given the instruction given in the input.</p> required <code>evolution_method</code> <code>str</code> <p>The evolution method to be used. If not provided (the default), a random one is chosen like the original paper. Available ones are \"helpfulness\", \"relevance\", \"deepen\", \"creativity\" and \"details\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import EvolQualityTask\n&gt;&gt;&gt; task = EvolQualityTask()\n&gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\", \"1. Eat healthy food. 2. Exercise. 3. Sleep well.\")\nPrompt(\n    system_prompt=\"\",\n    formatted_prompt=\"I want you to act as a Prompt ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/evol_quality.py</code> <pre><code>def generate_prompt(\n    self,\n    input: str,\n    generation: str,\n    evolution_method: Optional[EvolutionMethod] = None,\n    **_: Any,\n) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Evol-Instruct specification.\n\n    Args:\n        input (str):\n            The input to be used for the prompt. Corresponds to the instruction in the prompt.\n        generation (str):\n            The generation to be used for the prompt, which corresponds to a generated response\n            given the instruction given in the input.\n        evolution_method (str, optional):\n            The evolution method to be used. If not provided (the default), a random one is chosen\n            like the original paper. Available ones are \"helpfulness\", \"relevance\", \"deepen\",\n            \"creativity\" and \"details\".\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolQualityTask\n        &gt;&gt;&gt; task = EvolQualityTask()\n        &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\", \"1. Eat healthy food. 2. Exercise. 3. Sleep well.\")\n        Prompt(\n            system_prompt=\"\",\n            formatted_prompt=\"I want you to act as a Prompt ...\",\n        )\n    \"\"\"\n    evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n    render_kwargs = {\n        \"evol_method\": evolution_method,\n        \"instruction\": input,\n        \"generation\": generation,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.EvolQualityTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format, applying the elimination step for bad generations.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>the output of the model.</p> required Note <p>The elimination step is applied to the output, but only steps 2-4 in the paper are implemented. Refer to point 3.2, Elimination Evolving section in <code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code> for more information on the elimination evolving step, and take a look at the <code>_elimination_evolving</code> method for more information of the implementation.</p> Source code in <code>src/distilabel/tasks/text_generation/evol_quality.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the model into the desired format, applying the elimination step for bad generations.\n\n    Args:\n        output (str): the output of the model.\n\n    Note:\n        The elimination step is applied to the output, but only steps 2-4 in the paper are implemented.\n        Refer to point 3.2, Elimination Evolving section in [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n        for more information on the elimination evolving step, and take a look at the `_elimination_evolving`\n        method for more information of the implementation.\n    \"\"\"\n    response_words = {\n        \"#Given Response#\",\n        \"#Created Response#\",\n        \"given response\",\n        \"created response\",\n        \"#The Given Response#\",\n        \"#Rewritten Response#\",\n        \"rewritten response\",\n    }\n    output = self._elimination_evolving(output, response_words=response_words)\n    return {self.output_args_names[0]: output}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.JudgeLMTask","title":"<code>JudgeLMTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> following the prompt templated used by JudgeLM.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'You are a helpful and precise assistant for checking the quality of the answer.'</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>'We would like to request your feedback on the performance of {num_responses} AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only {num_responses} values indicating the scores for Assistants 1 to {num_responses}, respectively. The {num_responses} scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.'</code> References <ul> <li><code>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</code></li> <li><code>BAAI/JudgeLM-7B-v1.0</code></li> <li><code>BAAI/JudgeLM-13B-v1.0</code></li> <li><code>BAAI/JudgeLM-33B-v1.0</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>@dataclass\nclass JudgeLMTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` following the prompt templated used by JudgeLM.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n\n    References:\n        - [`Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena`](https://arxiv.org/abs/2306.05685)\n        - [`BAAI/JudgeLM-7B-v1.0`](https://huggingface.co/BAAI/JudgeLM-7B-v1.0)\n        - [`BAAI/JudgeLM-13B-v1.0`](https://huggingface.co/BAAI/JudgeLM-13B-v1.0)\n        - [`BAAI/JudgeLM-33B-v1.0`](https://huggingface.co/BAAI/JudgeLM-33B-v1.0)\n    \"\"\"\n\n    task_description: str = (\n        \"We would like to request your feedback on the performance of {num_responses} AI assistants in response to the\"\n        \" user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details\"\n        \" of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher\"\n        \" score indicates better overall performance.\\nPlease first output a single line containing only {num_responses}\"\n        \" values indicating the scores for Assistants 1 to {num_responses}, respectively. The {num_responses} scores are separated by\"\n        \" a space. In the subsequent line, please provide a comprehensive explanation of your evaluation,\"\n        \" avoiding any potential bias and ensuring that the order in which the responses were presented does\"\n        \" not affect your judgment.\"\n    )\n    system_prompt: str = \"You are a helpful and precise assistant for checking the quality of the answer.\"\n\n    __jinja2_template__: ClassVar[str] = _JUDGELM_TEMPLATE\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the JudgeLM specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n            &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"[Question] What are the first 5 Fibonacci numbers? ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"input\": input,\n            \"responses\": generations,\n            \"task_description\": self.task_description.format(\n                num_responses=len(generations)\n            ),\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; JudgeLMOutput:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        split_output = output.split(\"\\n\")\n        rating = [float(rating) for rating in split_output[0].split(\" \")]\n        rationale = \"\\n\".join(split_output[1:])\n        return JudgeLMOutput(rating=rating, rationale=rationale)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.JudgeLMTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the JudgeLM specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n&gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"You are a helpful assistant.\",\n    formatted_prompt=\"[Question] What are the first 5 Fibonacci numbers? ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the JudgeLM specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n        &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"[Question] What are the first 5 Fibonacci numbers? ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"input\": input,\n        \"responses\": generations,\n        \"task_description\": self.task_description.format(\n            num_responses=len(generations)\n        ),\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.JudgeLMTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>def parse_output(self, output: str) -&gt; JudgeLMOutput:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    split_output = output.split(\"\\n\")\n    rating = [float(rating) for rating in split_output[0].split(\" \")]\n    rationale = \"\\n\".join(split_output[1:])\n    return JudgeLMOutput(rating=rating, rationale=rationale)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.PrometheusTask","title":"<code>PrometheusTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>CritiqueTask</code></p> <p>A <code>CritiqueTask</code> following the prompt templated used by Prometheus.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'You are a fair evaluator language model.'</code> <code>scoring_criteria</code> <code>str</code> <p>the scoring criteria to be used for the task, that defines the scores below, provided via <code>score_descriptions</code>.</p> required <code>score_descriptions</code> <code>Dict[int, str]</code> <p>the descriptions of the scores, where the key is the rating value (ideally those should be consecutive), and the value is the description of each rating.</p> required Disclaimer <p>Since the Prometheus model has been trained with OpenAI API generated data, the prompting strategy may just be consistent / compliant with either GPT-3.5 or GPT-4 from OpenAI API, or with their own model. Any other model may fail on the generation of a structured output, as well as providing an incorrect / inaccurate critique.</p> References <ul> <li><code>Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</code></li> <li><code>kaist-ai/prometheus-13b-v1.0</code></li> <li><code>kaist-ai/prometheus-13b-v1.0</code></li> </ul> Source code in <code>src/distilabel/tasks/critique/prometheus.py</code> <pre><code>@dataclass\nclass PrometheusTask(CritiqueTask):\n    \"\"\"A `CritiqueTask` following the prompt templated used by Prometheus.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        scoring_criteria (str): the scoring criteria to be used for the task, that defines\n            the scores below, provided via `score_descriptions`.\n        score_descriptions (Dict[int, str]): the descriptions of the scores, where\n            the key is the rating value (ideally those should be consecutive), and the\n            value is the description of each rating.\n\n    Disclaimer:\n        Since the Prometheus model has been trained with OpenAI API generated data, the prompting\n        strategy may just be consistent / compliant with either GPT-3.5 or GPT-4 from OpenAI API, or\n        with their own model. Any other model may fail on the generation of a structured output, as\n        well as providing an incorrect / inaccurate critique.\n\n    References:\n        - [`Prometheus: Inducing Fine-grained Evaluation Capability in Language Models`](https://arxiv.org/abs/2310.08491)\n        - [`kaist-ai/prometheus-13b-v1.0`](https://huggingface.co/kaist-ai/prometheus-7b-v1.0)\n        - [`kaist-ai/prometheus-13b-v1.0`](https://huggingface.co/kaist-ai/prometheus-13b-v1.0)\n    \"\"\"\n\n    scoring_criteria: str\n    score_descriptions: Dict[int, str]\n\n    system_prompt: str = \"You are a fair evaluator language model.\"\n\n    __jinja2_template__: ClassVar[str] = _PROMETHEUS_TEMPLATE\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        return super().input_args_names + [\"ref_completion\"]\n\n    def generate_prompt(\n        self, input: str, generations: List[str], ref_completion: str, **_: Any\n    ) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Prometheus specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt, in\n                this case, the ones to be critiqued.\n            ref_completion (str): the reference completion to be used for the prompt,\n                which is the reference one, assuming the one with the highest score.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.critique import PrometheusTask\n            &gt;&gt;&gt; task = PrometheusTask(\n            ...     scoring_criteria=\"Overall quality of the responses provided.\",\n            ...     score_descriptions={0: \"false\", 1: \"partially false\", 2: \"average\", 3: \"partially true\", 4: \"true\"},\n            ... )\n            &gt;&gt;&gt; task.generate_prompt(\n            ...     input=\"What are the first 5 Fibonacci numbers?\",\n            ...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n            ...     ref_completion=\"0 1 1 2 3\",\n            ... )\n            Prompt(\n                system_prompt=\"You are a fair evaluator language model.\",\n                formatted_prompt=\"\"###Task Description:...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"instruction\": input,\n            \"completion\": generations,\n            \"ref_completion\": ref_completion,\n            \"scoring_criteria\": self.scoring_criteria,\n            \"score_descriptions\": self.score_descriptions,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; CritiqueTaskOutput:  # type: ignore\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        # We use a regex instead of splitting by the delimiter because the\n        # critique may contain the delimiter, and using the regex is safer.\n        pattern = r\"(.+?)\\. \\[RESULT\\] (\\d+)\"\n        match = re.search(pattern, output)\n        if match:\n            return CritiqueTaskOutput(\n                score=float(match.group(2)),\n                critique=match.group(1).strip(),\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.PrometheusTask.generate_prompt","title":"<code>generate_prompt(input, generations, ref_completion, **_)</code>","text":"<p>Generates a prompt following the Prometheus specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt, in this case, the ones to be critiqued.</p> required <code>ref_completion</code> <code>str</code> <p>the reference completion to be used for the prompt, which is the reference one, assuming the one with the highest score.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.critique import PrometheusTask\n&gt;&gt;&gt; task = PrometheusTask(\n...     scoring_criteria=\"Overall quality of the responses provided.\",\n...     score_descriptions={0: \"false\", 1: \"partially false\", 2: \"average\", 3: \"partially true\", 4: \"true\"},\n... )\n&gt;&gt;&gt; task.generate_prompt(\n...     input=\"What are the first 5 Fibonacci numbers?\",\n...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n...     ref_completion=\"0 1 1 2 3\",\n... )\nPrompt(\n    system_prompt=\"You are a fair evaluator language model.\",\n    formatted_prompt=\"\"###Task Description:...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/critique/prometheus.py</code> <pre><code>def generate_prompt(\n    self, input: str, generations: List[str], ref_completion: str, **_: Any\n) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Prometheus specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt, in\n            this case, the ones to be critiqued.\n        ref_completion (str): the reference completion to be used for the prompt,\n            which is the reference one, assuming the one with the highest score.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.critique import PrometheusTask\n        &gt;&gt;&gt; task = PrometheusTask(\n        ...     scoring_criteria=\"Overall quality of the responses provided.\",\n        ...     score_descriptions={0: \"false\", 1: \"partially false\", 2: \"average\", 3: \"partially true\", 4: \"true\"},\n        ... )\n        &gt;&gt;&gt; task.generate_prompt(\n        ...     input=\"What are the first 5 Fibonacci numbers?\",\n        ...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n        ...     ref_completion=\"0 1 1 2 3\",\n        ... )\n        Prompt(\n            system_prompt=\"You are a fair evaluator language model.\",\n            formatted_prompt=\"\"###Task Description:...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"instruction\": input,\n        \"completion\": generations,\n        \"ref_completion\": ref_completion,\n        \"scoring_criteria\": self.scoring_criteria,\n        \"score_descriptions\": self.score_descriptions,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.PrometheusTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/critique/prometheus.py</code> <pre><code>def parse_output(self, output: str) -&gt; CritiqueTaskOutput:  # type: ignore\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    # We use a regex instead of splitting by the delimiter because the\n    # critique may contain the delimiter, and using the regex is safer.\n    pattern = r\"(.+?)\\. \\[RESULT\\] (\\d+)\"\n    match = re.search(pattern, output)\n    if match:\n        return CritiqueTaskOutput(\n            score=float(match.group(2)),\n            critique=match.group(1).strip(),\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Prompt","title":"<code>Prompt</code>  <code>dataclass</code>","text":"<p>A <code>dataclass</code> representing a <code>Prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt.</p> required <code>formatted_prompt</code> <code>str</code> <p>the formatted prompt.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n&gt;&gt;&gt; prompt = Prompt(\n...     system_prompt=\"You are a helpful assistant.\",\n...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n... )\n</code></pre> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>@dataclass\nclass Prompt:\n    \"\"\"A `dataclass` representing a `Prompt`.\n\n    Args:\n        system_prompt (str): the system prompt.\n        formatted_prompt (str): the formatted prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n    \"\"\"\n\n    system_prompt: str\n    formatted_prompt: str\n\n    def format_as(self, format: SupportedFormats) -&gt; Union[str, List[ChatCompletion]]:\n        \"\"\"Formats the prompt as the specified format.\n\n        Args:\n            format (SupportedFormats): the format to be used for the prompt. Available formats are\n                `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n        Returns:\n            Union[str, List[ChatCompletion]]: the formatted prompt.\n\n        Raises:\n            ValueError: if the specified format is not supported.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n            &gt;&gt;&gt; prompt = Prompt(\n            ...     system_prompt=\"You are a helpful assistant.\",\n            ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n            ... )\n            &gt;&gt;&gt; prompt.format_as(\"default\")\n            'You are a helpful assistant. What are the first 5 Fibonacci numbers?'\n        \"\"\"\n        if format == \"default\":\n            return f\"{self.system_prompt}\\n{self.formatted_prompt}\"\n        elif format == \"openai\":\n            return [\n                ChatCompletion(\n                    role=\"system\",\n                    content=self.system_prompt,\n                ),\n                ChatCompletion(role=\"user\", content=self.formatted_prompt),\n            ]\n        elif format == \"llama2\":\n            return f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{self.system_prompt}&lt;&lt;/SYS&gt;&gt;\\n\\n{self.formatted_prompt} [/INST]\"\n        elif format == \"chatml\":\n            return f\"&lt;|im_start|&gt;system\\n{self.system_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{self.formatted_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n        elif format in [\"zephyr\", \"notus\"]:\n            return f\"&lt;|system|&gt;\\n{self.system_prompt}&lt;/s&gt;\\n&lt;|user|&gt;\\n{self.formatted_prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"\n        else:\n            raise ValueError(\n                f\"Format {format} not supported, please provide a custom `prompt_formatting_fn`\"\n                \" or use any of the available formats: openai, llama2, chatml, zephyr\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Prompt.format_as","title":"<code>format_as(format)</code>","text":"<p>Formats the prompt as the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>SupportedFormats</code> <p>the format to be used for the prompt. Available formats are <code>default</code>, <code>openai</code>, <code>llama2</code>, <code>chatml</code>, and <code>zephyr</code>.</p> required <p>Returns:</p> Type Description <code>Union[str, List[ChatCompletion]]</code> <p>Union[str, List[ChatCompletion]]: the formatted prompt.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the specified format is not supported.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n&gt;&gt;&gt; prompt = Prompt(\n...     system_prompt=\"You are a helpful assistant.\",\n...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n... )\n&gt;&gt;&gt; prompt.format_as(\"default\")\n'You are a helpful assistant. What are the first 5 Fibonacci numbers?'\n</code></pre> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>def format_as(self, format: SupportedFormats) -&gt; Union[str, List[ChatCompletion]]:\n    \"\"\"Formats the prompt as the specified format.\n\n    Args:\n        format (SupportedFormats): the format to be used for the prompt. Available formats are\n            `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n    Returns:\n        Union[str, List[ChatCompletion]]: the formatted prompt.\n\n    Raises:\n        ValueError: if the specified format is not supported.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n        &gt;&gt;&gt; prompt.format_as(\"default\")\n        'You are a helpful assistant. What are the first 5 Fibonacci numbers?'\n    \"\"\"\n    if format == \"default\":\n        return f\"{self.system_prompt}\\n{self.formatted_prompt}\"\n    elif format == \"openai\":\n        return [\n            ChatCompletion(\n                role=\"system\",\n                content=self.system_prompt,\n            ),\n            ChatCompletion(role=\"user\", content=self.formatted_prompt),\n        ]\n    elif format == \"llama2\":\n        return f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{self.system_prompt}&lt;&lt;/SYS&gt;&gt;\\n\\n{self.formatted_prompt} [/INST]\"\n    elif format == \"chatml\":\n        return f\"&lt;|im_start|&gt;system\\n{self.system_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{self.formatted_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n    elif format in [\"zephyr\", \"notus\"]:\n        return f\"&lt;|system|&gt;\\n{self.system_prompt}&lt;/s&gt;\\n&lt;|user|&gt;\\n{self.formatted_prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"\n    else:\n        raise ValueError(\n            f\"Format {format} not supported, please provide a custom `prompt_formatting_fn`\"\n            \" or use any of the available formats: openai, llama2, chatml, zephyr\"\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.QualityScorerTask","title":"<code>QualityScorerTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTaskNoRationale</code></p> <p>A <code>PreferenceTask</code> following the <code>Quality Scorer</code> specification for rating instructions in terms of quality.</p> <p>This task is inspired by the Evol Quality Scorer in the Deita framework: Deita is an open-sourced project designed to facilitate Automatic Data Selection for instruction tuning in Large Language Models (LLMs).</p> <p>The task follows the same scheme as the Evol Complexity Scorer, but the instructions are scored in terms of quality, obtaining a quality score q for each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/quality_scorer.py</code> <pre><code>@dataclass\nclass QualityScorerTask(PreferenceTaskNoRationale):\n    \"\"\"A `PreferenceTask` following the `Quality Scorer` specification for rating instructions\n    in terms of quality.\n\n    This task is inspired by the Evol Quality Scorer in the Deita framework: *Deita is an open-sourced project\n    designed to facilitate Automatic Data Selection for instruction tuning in Large Language Models (LLMs).*\n\n    The task follows the same scheme as the Evol Complexity Scorer, but the instructions are scored in terms of\n    quality, obtaining a quality score *q* for each instruction.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    system_prompt: str = \"\"\n    task_description: str = \"\"\"Your evaluation should consider factors such as helpfulness, relevance, accuracy, depth,\ncreativity, and level of detail of the response.\"\"\"\n    __jinja2_template__: str = _QUALITY_SCORER_TEMPLATE\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the *Evol Quality* specification in *Deita*.\n\n        Args:\n            input (str): the instruction for which the model will score the responses.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import QualityScorerTask\n            &gt;&gt;&gt; task = QualityScorerTask()\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"\",\n                formatted_prompt=\"Rank the following responses provided ...\"\n            )\n        \"\"\"\n        render_kwargs = {\n            \"instruction\": input,\n            \"responses\": generations,\n            \"task_description\": self.task_description,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the task, returning a list with the rating of each instruction.\n\n        Args:\n            output (str): The output of the LLM raw.\n\n        Returns:\n            Dict[str, List[str]]: A dict with containing the ratings of each instruction.\n        \"\"\"\n        output = output.lower().split(\"\\n\")\n        scores = [\n            float(re.sub(r\"\\[response \\d+\\] score:\", \"\", o).strip()) for o in output\n        ]\n        return {self.output_args_names[0]: scores}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.QualityScorerTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the Evol Quality specification in Deita.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the instruction for which the model will score the responses.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import QualityScorerTask\n&gt;&gt;&gt; task = QualityScorerTask()\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"\",\n    formatted_prompt=\"Rank the following responses provided ...\"\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/quality_scorer.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the *Evol Quality* specification in *Deita*.\n\n    Args:\n        input (str): the instruction for which the model will score the responses.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import QualityScorerTask\n        &gt;&gt;&gt; task = QualityScorerTask()\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"\",\n            formatted_prompt=\"Rank the following responses provided ...\"\n        )\n    \"\"\"\n    render_kwargs = {\n        \"instruction\": input,\n        \"responses\": generations,\n        \"task_description\": self.task_description,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.QualityScorerTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the task, returning a list with the rating of each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output of the LLM raw.</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict[str, List[str]]: A dict with containing the ratings of each instruction.</p> Source code in <code>src/distilabel/tasks/preference/quality_scorer.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the task, returning a list with the rating of each instruction.\n\n    Args:\n        output (str): The output of the LLM raw.\n\n    Returns:\n        Dict[str, List[str]]: A dict with containing the ratings of each instruction.\n    \"\"\"\n    output = output.lower().split(\"\\n\")\n    scores = [\n        float(re.sub(r\"\\[response \\d+\\] score:\", \"\", o).strip()) for o in output\n    ]\n    return {self.output_args_names[0]: scores}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.SelfInstructTask","title":"<code>SelfInstructTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>InstructTaskMixin</code>, <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> following the Self-Instruct specification for building the prompts.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> <code>'You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks. You are given a task description and a set of instructions for how to write the prompts for an specific AI application.'</code> <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>field(default_factory=lambda : {'harmlessness': harmlessness, 'helpfulness': helpfulness, 'truthfulness': truthfulness, 'honesty': honesty, 'verbalized_calibration': verbalized_calibration}, repr=False)</code> <code>principles_distribution</code> <code>Union[Dict[str, float], Literal[balanced], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>None</code> <code>application_description</code> <code>str</code> <p>the description of the AI application. Defaults to \"AI assistant\".</p> <code>'AI assistant'</code> <code>num_instructions</code> <code>int</code> <p>the number of instructions to be used for the prompt. Defaults to 5.</p> <code>5</code> <code>criteria_for_query_generation</code> <code>str</code> <p>the criteria for query generation that we want our model to have. Default value covers default behaviour for SelfInstructTask. This value is passed to the .jinja template, where extra instructions are added to ensure correct output format.</p> <code>'Incorporate a diverse range of verbs, avoiding repetition.\\nEnsure queries are compatible with AI model\\'s text generation functions and are limited to 1-2 sentences.\\nDesign queries to be self-contained and standalone.\\nBlend interrogative (e.g., \"What is the significance of x?\") and imperative (e.g., \"Detail the process of x.\") styles.'</code> References <ul> <li><code>Self-Instruct: Aligning Language Models with Self-Generated Instructions</code></li> <li><code>Self-Instruct - GitHub Repository</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>@dataclass\nclass SelfInstructTask(InstructTaskMixin, TextGenerationTask):\n    \"\"\"A `TextGenerationTask` following the Self-Instruct specification for building\n    the prompts.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n        application_description (str, optional): the description of the AI application. Defaults to\n            \"AI assistant\".\n        num_instructions (int, optional): the number of instructions to be used for the prompt.\n            Defaults to 5.\n        criteria_for_query_generation (str, optional): the criteria for query generation that we want\n            our model to have. Default value covers default behaviour for SelfInstructTask. This value is\n            passed to the .jinja template, where extra instructions are added to ensure correct output format.\n\n    References:\n        - [`Self-Instruct: Aligning Language Models with Self-Generated Instructions`](https://arxiv.org/abs/2212.10560)\n        - [`Self-Instruct - GitHub Repository`](https://github.com/yizhongw/self-instruct)\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks.\"\n        \" You are given a task description and a set of instructions for how to write the prompts for an\"\n        \" specific AI application.\"\n    )\n\n    application_description: str = \"AI assistant\"\n    num_instructions: int = 5\n\n    criteria_for_query_generation: str = (\n        \"Incorporate a diverse range of verbs, avoiding repetition.\\n\"\n        \"Ensure queries are compatible with AI model's text generation functions and are limited to 1-2 sentences.\\n\"\n        \"Design queries to be self-contained and standalone.\\n\"\n        'Blend interrogative (e.g., \"What is the significance of x?\") and imperative (e.g., \"Detail the process of x.\") styles.'\n    )\n\n    __jinja2_template__: str = _SELF_INSTRUCT_TEMPLATE\n\n    def generate_prompt(self, input: str, **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Self-Instruct specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n            &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"# Task Description ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"application_description\": self.application_description,\n            \"num_instructions\": self.num_instructions,\n            \"criteria_for_query_generation\": self.criteria_for_query_generation,\n            \"input\": input,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        return [\"instructions\"]\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        pattern = re.compile(r\"\\d+\\.\\s*(.*?)\\n\")\n        return {\"instructions\": pattern.findall(output)}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.SelfInstructTask.generate_prompt","title":"<code>generate_prompt(input, **_)</code>","text":"<p>Generates a prompt following the Self-Instruct specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n&gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\nPrompt(\n    system_prompt=\"You are a helpful assistant.\",\n    formatted_prompt=\"# Task Description ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>def generate_prompt(self, input: str, **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Self-Instruct specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n        &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"# Task Description ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"application_description\": self.application_description,\n        \"num_instructions\": self.num_instructions,\n        \"criteria_for_query_generation\": self.criteria_for_query_generation,\n        \"input\": input,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.SelfInstructTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    pattern = re.compile(r\"\\d+\\.\\s*(.*?)\\n\")\n    return {\"instructions\": pattern.findall(output)}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Task","title":"<code>Task</code>","text":"<p>             Bases: <code>ABC</code>, <code>_Serializable</code></p> <p>Abstract class used to define the methods required to create a <code>Task</code>, to be used within an <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>__jinja2_template__</code> attribute is not provided.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>class Task(ABC, _Serializable):\n    \"\"\"Abstract class used to define the methods required to create a `Task`, to be used\n    within an `LLM`.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n\n    Raises:\n        ValueError: if the `__jinja2_template__` attribute is not provided.\n    \"\"\"\n\n    system_prompt: str\n    task_description: Union[str, None] = None\n\n    __jinja2_template__: Union[str, None] = None\n    __type__: Union[Literal[\"generation\", \"labelling\"], None] = None\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"system_prompt\", self.system_prompt\n        yield \"task_description\", self.task_description\n        yield \"input_args_names\", self.input_args_names\n        yield \"output_args_names\", self.output_args_names\n\n    @property\n    def template(self) -&gt; \"Template\":\n        if self.__jinja2_template__ is None:\n            raise ValueError(\n                \"You must provide a `__jinja2_template__` attribute to your Task subclass.\"\n            )\n\n        return Template(open(self.__jinja2_template__).read())\n\n    @abstractmethod\n    def generate_prompt(self, **kwargs: Any) -&gt; Prompt:\n        pass\n\n    @abstractmethod\n    def parse_output(self, output: str) -&gt; Any:\n        pass\n\n    @property\n    @abstractmethod\n    def input_args_names(self) -&gt; List[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def output_args_names(self) -&gt; List[str]:\n        pass\n\n    def validate_dataset(self, columns_in_dataset: List[str]) -&gt; None:\n        \"\"\"Validates that the dataset contains the required columns for the task.\n\n        Args:\n            columns_in_dataset (List[str]): the columns in the dataset.\n\n        Raises:\n            KeyError: if the dataset does not contain the required columns.\n        \"\"\"\n        for input_arg_name in self.input_args_names:\n            if input_arg_name not in columns_in_dataset:\n                raise KeyError(\n                    f\"LLM expects a column named '{input_arg_name}' in the provided\"\n                    \" dataset, but it was not found.\"\n                )\n\n    def to_argilla_dataset(\n        self, dataset_row: Dict[str, Any], *args: Any, **kwargs: Any\n    ) -&gt; \"FeedbackDataset\":\n        raise NotImplementedError(\n            \"`to_argilla_dataset` is not implemented, if you want to export your dataset as an Argilla\"\n            \" `FeedbackDataset` you will need to implement this method first.\"\n        )\n\n    def to_argilla_record(\n        self, dataset_row: Dict[str, Any], *args: Any, **kwargs: Any\n    ) -&gt; Union[\"FeedbackRecord\", List[\"FeedbackRecord\"]]:\n        raise NotImplementedError(\n            \"`to_argilla_record` is not implemented, if you want to export your dataset as an Argilla\"\n            \" `FeedbackDataset` you will need to implement this method first.\"\n        )\n\n    # Renamed to _to_argilla_record instead of renaming `to_argilla_record` to protected, as that would\n    # imply more breaking changes.\n    def _to_argilla_record(  # noqa: C901\n        self, dataset_row: Dict[str, Any], *args: Any, **kwargs: Any\n    ) -&gt; Union[\"FeedbackRecord\", List[\"FeedbackRecord\"]]:\n        column_names = list(dataset_row.keys())\n        if self.__type__ is None or self.__type__ == \"generation\":\n            required_column_names = self.input_args_names + self.output_args_names\n        elif self.__type__ == \"labelling\":\n            required_column_names = self.output_args_names\n        else:\n            raise ValueError(\"The task type is not supported.\")\n\n        dataset_rows = [dataset_row]\n        if \"generation_model\" in dataset_row and isinstance(\n            dataset_row[\"generation_model\"], list\n        ):\n            generation_columns = column_names[\n                column_names.index(\"generation_model\") : column_names.index(\n                    \"labelling_model\"\n                )\n                if \"labelling_model\" in column_names\n                else None\n            ]\n            if any(\n                isinstance(nested, list)\n                for column_name in list(\n                    set(generation_columns)\n                    - {\n                        \"generation_model\",\n                        \"generation_prompt\",\n                        \"raw_generation_response\",\n                    }\n                )\n                for nested in dataset_row[column_name]\n            ):\n                if any(\n                    generation_column in required_column_names\n                    for generation_column in generation_columns\n                ):\n                    unwrapped_dataset_rows = []\n                    for row in dataset_rows:\n                        for idx in range(len(dataset_row[\"generation_model\"])):\n                            unwrapped_dataset_row = {}\n                            for key, value in row.items():\n                                if key in generation_columns:\n                                    unwrapped_dataset_row[key] = value[idx]\n                                else:\n                                    unwrapped_dataset_row[key] = value\n                            unwrapped_dataset_rows.append(unwrapped_dataset_row)\n                    dataset_rows = unwrapped_dataset_rows\n\n        if \"labelling_model\" in dataset_row and isinstance(\n            dataset_row[\"labelling_model\"], list\n        ):\n            labelling_columns = column_names[column_names.index(\"labelling_model\") :]\n            if any(\n                isinstance(nested, list)\n                for column_name in list(\n                    set(labelling_columns)\n                    - {\n                        \"labelling_model\",\n                        \"labelling_prompt\",\n                        \"raw_labelling_response\",\n                    }\n                )\n                for nested in dataset_row[column_name]\n            ):\n                if any(\n                    labelling_column in required_column_names\n                    for labelling_column in labelling_columns\n                ):\n                    unwrapped_dataset_rows = []\n                    for row in dataset_rows:\n                        for idx in range(len(dataset_row[\"labelling_model\"])):\n                            unwrapped_dataset_row = {}\n                            for key, value in row.items():\n                                if key in labelling_columns:\n                                    unwrapped_dataset_row[key] = value[idx]\n                                else:\n                                    unwrapped_dataset_row[key] = value\n                            unwrapped_dataset_rows.append(unwrapped_dataset_row)\n                    dataset_rows = unwrapped_dataset_rows\n\n        if len(dataset_rows) == 1:\n            return self.to_argilla_record(dataset_rows[0], *args, **kwargs)\n\n        records = []\n        for dataset_row in dataset_rows:\n            generated_records = self.to_argilla_record(dataset_row, *args, **kwargs)\n            if isinstance(generated_records, list):\n                records.extend(generated_records)\n            else:\n                records.append(generated_records)\n        return records\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Task.validate_dataset","title":"<code>validate_dataset(columns_in_dataset)</code>","text":"<p>Validates that the dataset contains the required columns for the task.</p> <p>Parameters:</p> Name Type Description Default <code>columns_in_dataset</code> <code>List[str]</code> <p>the columns in the dataset.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>if the dataset does not contain the required columns.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>def validate_dataset(self, columns_in_dataset: List[str]) -&gt; None:\n    \"\"\"Validates that the dataset contains the required columns for the task.\n\n    Args:\n        columns_in_dataset (List[str]): the columns in the dataset.\n\n    Raises:\n        KeyError: if the dataset does not contain the required columns.\n    \"\"\"\n    for input_arg_name in self.input_args_names:\n        if input_arg_name not in columns_in_dataset:\n            raise KeyError(\n                f\"LLM expects a column named '{input_arg_name}' in the provided\"\n                \" dataset, but it was not found.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask","title":"<code>TextGenerationTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Task</code></p> <p>A base <code>Task</code> definition for text generation using LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> <code>\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"</code> <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>field(default_factory=lambda : {'harmlessness': harmlessness, 'helpfulness': helpfulness, 'truthfulness': truthfulness, 'honesty': honesty, 'verbalized_calibration': verbalized_calibration}, repr=False)</code> <code>principles_distribution</code> <code>Union[Dict[str, float], Literal['balanced'], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n&gt;&gt;&gt; task = TextGenerationTask()\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>@dataclass\nclass TextGenerationTask(Task):\n    \"\"\"A base `Task` definition for text generation using LLMs.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n        &gt;&gt;&gt; task = TextGenerationTask()\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible,\"\n        \" while being safe. Your answers should not include any harmful, unethical, racist, sexist,\"\n        \" toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased\"\n        \" and positive in nature.\\nIf a question does not make any sense, or is not factually coherent,\"\n        \" explain why instead of answering something not correct. If you don't know the answer to a\"\n        \" question, please don't share false information.\"\n    )\n    principles: Dict[str, List[str]] = field(\n        default_factory=lambda: {\n            \"harmlessness\": UltraFeedbackPrinciples.harmlessness,\n            \"helpfulness\": UltraFeedbackPrinciples.helpfulness,\n            \"truthfulness\": UltraFeedbackPrinciples.truthfulness,\n            \"honesty\": UltraFeedbackPrinciples.honesty,\n            \"verbalized_calibration\": UltraFeedbackPrinciples.verbalized_calibration,\n        },\n        repr=False,\n    )\n    principles_distribution: Union[Dict[str, float], Literal[\"balanced\"], None] = None\n\n    __type__: ClassVar[Literal[\"generation\"]] = \"generation\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validates the `principles_distribution` if it is a dict.\n\n        Raises:\n            ValueError: if the `principles_distribution` is a dict and it does not sum to 1.0.\n            ValueError: if the `principles` are not included in the `principles_distribution`.\n        \"\"\"\n        if isinstance(self.principles_distribution, dict):\n            not_included_principles = [\n                principle\n                for principle in self.principles\n                if principle not in self.principles_distribution\n            ]\n            if not_included_principles:\n                principles_str = \", \".join(\n                    [f\"'{principle}'\" for principle in not_included_principles]\n                )\n                raise ValueError(\n                    f\"Principles {principles_str} included in `principles` is not in\"\n                    \" `principles_distribution`\"\n                )\n\n            if sum(self.principles_distribution.values()) != 1.0:\n                raise ValueError(\n                    \"`principles_distribution` must sum to 1.0 if it is a dict containing\"\n                    \" the distribution of principles to use.\"\n                )\n\n    def _get_principle(self) -&gt; str:\n        \"\"\"Gets a principle from the `principles` dict respecting the `principal_distribution`.\n\n        Returns:\n            str: the principle to be used.\n        \"\"\"\n        if isinstance(self.principles_distribution, dict):\n            principle_group = random.choices(\n                list(self.principles_distribution.keys()),\n                weights=list(self.principles_distribution.values()),\n                k=1,\n            )[0]\n        else:\n            principle_group = random.choice(list(self.principles.keys()))\n        return random.choice(self.principles[principle_group])\n\n    def generate_prompt(self, input: str, **_: Any) -&gt; Prompt:\n        \"\"\"Generates the prompt to be used for generation.\n\n        Args:\n            input (str): the input to be used for generation.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n            &gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            Prompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n        \"\"\"\n        system_prompt = self.system_prompt\n        if self.principles_distribution is not None:\n            principle = self._get_principle()\n            system_prompt += \" \" + principle\n        return Prompt(system_prompt=system_prompt, formatted_prompt=input)\n\n    def parse_output(self, output: str) -&gt; Dict[str, str]:\n        \"\"\"Parses the output of the LLM into the desired format.\"\"\"\n        return {\"generations\": output}\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the input args names for the task.\"\"\"\n        return [\"input\"]\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the output args names for the task.\"\"\"\n        return [\"generations\"]\n\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: Optional[str] = \"generations\",\n    ) -&gt; \"FeedbackDataset\":\n        # First we infer the fields from the input_args_names, but we could also\n        # create those manually instead using `rg.TextField(...)`\n        fields = infer_fields_from_dataset_row(\n            field_names=self.input_args_names + self.output_args_names,\n            dataset_row=dataset_row,\n        )\n        # Then we add a default `RatingQuestion` which asks the users to provide a\n        # rating for each of the generations, differing from the scenario where the inputs\n        # are the fields and the outputs the ones used to formulate the quesstions. So on,\n        # in this scenario we won't have suggestions, as the questions will be related to the\n        # combination of inputs and outputs.\n        if generations_column is None or generations_column not in dataset_row:\n            raise ValueError(\n                f\"The `generations_column='{generations_column}'` is not present in the dataset\"\n                f\" row. Please provide any of {list(dataset_row.keys())}.\",\n            )\n        questions = []\n        for idx in range(1, len(dataset_row[generations_column]) + 1):\n            questions.append(\n                rg.RatingQuestion(  # type: ignore\n                    name=f\"{generations_column}-{idx}-rating\",\n                    title=f\"How would you rate the generation at `{generations_column}-{idx}`?\",\n                    values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                )\n            )\n        # Finally, we define some metadata properties that can be potentially used\n        # while exploring the dataset within Argilla to get more insights on the data.\n        metadata_properties = []\n        for arg_name in self.input_args_names + self.output_args_names:\n            if isinstance(dataset_row[arg_name], list):\n                for idx in range(1, len(dataset_row[arg_name]) + 1):\n                    metadata_properties.append(\n                        rg.IntegerMetadataProperty(name=f\"length-{arg_name}-{idx}\")  # type: ignore\n                    )\n            elif isinstance(dataset_row[arg_name], str):\n                metadata_properties.append(\n                    rg.IntegerMetadataProperty(name=f\"length-{arg_name}\")  # type: ignore\n                )\n            else:\n                warnings.warn(\n                    f\"Unsupported input type ({type(dataset_row[arg_name])}), skipping...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        # Then we just return the `FeedbackDataset` with the fields, questions, and metadata properties\n        # defined above.\n        return rg.FeedbackDataset(\n            fields=fields,\n            questions=questions,\n            metadata_properties=metadata_properties,  # Note that these are always optional\n        )\n\n    def to_argilla_record(self, dataset_row: Dict[str, Any]) -&gt; \"FeedbackRecord\":\n        \"\"\"Converts a dataset row to an Argilla `FeedbackRecord`.\"\"\"\n        # We start off with the fields, which are the inputs of the LLM, but also\n        # build the metadata from them, as previously specified within the\n        fields, metadata = {}, {}\n        for arg_name in self.input_args_names + self.output_args_names:\n            arg_value = dataset_row[arg_name]\n            if isinstance(arg_value, list):\n                for idx, value in enumerate(arg_value, start=1):\n                    # TODO: value formatting was included here due to some issues\n                    # with `SelfInstructTask` but these list-parsing may not be needed\n                    # anymore.\n                    value = (\n                        value.strip()\n                        if isinstance(value, str)\n                        else \"\\n\".join(value)\n                        if isinstance(value, list)\n                        else \"\"\n                    )\n                    fields[f\"{arg_name}-{idx}\"] = value\n                    if value is not None:\n                        metadata[f\"length-{arg_name}-{idx}\"] = len(value)\n            elif isinstance(arg_value, str):\n                fields[arg_name] = arg_value.strip() if arg_value else \"\"\n                if arg_value is not None:\n                    metadata[f\"length-{arg_name}\"] = len(arg_value.strip())\n            else:\n                warnings.warn(\n                    f\"Unsupported input type ({type(arg_value)}), skipping...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        # Then we add the model metadata from the `generation_model` and `labelling_model`\n        # columns of the dataset, if they exist.\n        metadata.update(model_metadata_from_dataset_row(dataset_row=dataset_row))\n        # Finally, we return the `FeedbackRecord` with the fields and the metadata\n        return rg.FeedbackRecord(fields=fields, metadata=metadata)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.input_args_names","title":"<code>input_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the input args names for the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the output args names for the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validates the <code>principles_distribution</code> if it is a dict.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>principles_distribution</code> is a dict and it does not sum to 1.0.</p> <code>ValueError</code> <p>if the <code>principles</code> are not included in the <code>principles_distribution</code>.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validates the `principles_distribution` if it is a dict.\n\n    Raises:\n        ValueError: if the `principles_distribution` is a dict and it does not sum to 1.0.\n        ValueError: if the `principles` are not included in the `principles_distribution`.\n    \"\"\"\n    if isinstance(self.principles_distribution, dict):\n        not_included_principles = [\n            principle\n            for principle in self.principles\n            if principle not in self.principles_distribution\n        ]\n        if not_included_principles:\n            principles_str = \", \".join(\n                [f\"'{principle}'\" for principle in not_included_principles]\n            )\n            raise ValueError(\n                f\"Principles {principles_str} included in `principles` is not in\"\n                \" `principles_distribution`\"\n            )\n\n        if sum(self.principles_distribution.values()) != 1.0:\n            raise ValueError(\n                \"`principles_distribution` must sum to 1.0 if it is a dict containing\"\n                \" the distribution of principles to use.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.generate_prompt","title":"<code>generate_prompt(input, **_)</code>","text":"<p>Generates the prompt to be used for generation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for generation.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n&gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\nPrompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def generate_prompt(self, input: str, **_: Any) -&gt; Prompt:\n    \"\"\"Generates the prompt to be used for generation.\n\n    Args:\n        input (str): the input to be used for generation.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n        &gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n    \"\"\"\n    system_prompt = self.system_prompt\n    if self.principles_distribution is not None:\n        principle = self._get_principle()\n        system_prompt += \" \" + principle\n    return Prompt(system_prompt=system_prompt, formatted_prompt=input)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the LLM into the desired format.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, str]:\n    \"\"\"Parses the output of the LLM into the desired format.\"\"\"\n    return {\"generations\": output}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.to_argilla_record","title":"<code>to_argilla_record(dataset_row)</code>","text":"<p>Converts a dataset row to an Argilla <code>FeedbackRecord</code>.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def to_argilla_record(self, dataset_row: Dict[str, Any]) -&gt; \"FeedbackRecord\":\n    \"\"\"Converts a dataset row to an Argilla `FeedbackRecord`.\"\"\"\n    # We start off with the fields, which are the inputs of the LLM, but also\n    # build the metadata from them, as previously specified within the\n    fields, metadata = {}, {}\n    for arg_name in self.input_args_names + self.output_args_names:\n        arg_value = dataset_row[arg_name]\n        if isinstance(arg_value, list):\n            for idx, value in enumerate(arg_value, start=1):\n                # TODO: value formatting was included here due to some issues\n                # with `SelfInstructTask` but these list-parsing may not be needed\n                # anymore.\n                value = (\n                    value.strip()\n                    if isinstance(value, str)\n                    else \"\\n\".join(value)\n                    if isinstance(value, list)\n                    else \"\"\n                )\n                fields[f\"{arg_name}-{idx}\"] = value\n                if value is not None:\n                    metadata[f\"length-{arg_name}-{idx}\"] = len(value)\n        elif isinstance(arg_value, str):\n            fields[arg_name] = arg_value.strip() if arg_value else \"\"\n            if arg_value is not None:\n                metadata[f\"length-{arg_name}\"] = len(arg_value.strip())\n        else:\n            warnings.warn(\n                f\"Unsupported input type ({type(arg_value)}), skipping...\",\n                UserWarning,\n                stacklevel=2,\n            )\n    # Then we add the model metadata from the `generation_model` and `labelling_model`\n    # columns of the dataset, if they exist.\n    metadata.update(model_metadata_from_dataset_row(dataset_row=dataset_row))\n    # Finally, we return the `FeedbackRecord` with the fields and the metadata\n    return rg.FeedbackRecord(fields=fields, metadata=metadata)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraCMTask","title":"<code>UltraCMTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>CritiqueTask</code></p> <p>A <code>CritiqueTask</code> following the prompt templated used by UltraCM (from UltraFeedback).</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>\"User: A one-turn chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, very detailed, and polite answers to the user's questions.&lt;/s&gt;\"</code> Disclaimer <p>Since the UltraCM model has been trained with OpenAI API generated data, the prompting strategy may just be consistent / compliant with either GPT-3.5 or GPT-4 from OpenAI API, or with their own model. Any other model may fail on the generation of a structured output, as well as providing an incorrect / inaccurate critique.</p> References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>UltraFeedback - GitHub Repository</code></li> <li><code>openbmb/UltraCM-13b</code></li> </ul> Source code in <code>src/distilabel/tasks/critique/ultracm.py</code> <pre><code>@dataclass\nclass UltraCMTask(CritiqueTask):\n    \"\"\"A `CritiqueTask` following the prompt templated used by UltraCM (from UltraFeedback).\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n\n    Disclaimer:\n        Since the UltraCM model has been trained with OpenAI API generated data, the prompting\n        strategy may just be consistent / compliant with either GPT-3.5 or GPT-4 from OpenAI API, or\n        with their own model. Any other model may fail on the generation of a structured output, as\n        well as providing an incorrect / inaccurate critique.\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`UltraFeedback - GitHub Repository`](https://github.com/OpenBMB/UltraFeedback)\n        - [`openbmb/UltraCM-13b`](https://huggingface.co/openbmb/UltraCM-13b)\n    \"\"\"\n\n    __jinja2_template__: ClassVar[str] = _ULTRACM_TEMPLATE\n\n    system_prompt: str = (\n        \"User: A one-turn chat between a curious user and an artificial intelligence\"\n        \" assistant. The assistant gives helpful, very detailed, and polite answers to\"\n        \" the user's questions.&lt;/s&gt;\"\n    )\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the UltraCM specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt, in\n                this case, the ones to be critiqued.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.critique import UltraCMTask\n            &gt;&gt;&gt; task = UltraCMTask()\n            &gt;&gt;&gt; task.generate_prompt(\n            ...     input=\"What are the first 5 Fibonacci numbers?\",\n            ...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n            ... )\n            Prompt(\n                system_prompt=\"User: A one-turn chat between a curious user ...\",\n                formatted_prompt=\"User: Given my answer to an instruction, your role ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"instruction\": input,\n            \"completion\": generations,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=f\"User: {self.template.render(**render_kwargs)}&lt;/s&gt;\\nAssistant: ### Feedback\\nOverall Score: \",\n        )\n\n    def parse_output(self, output: str) -&gt; CritiqueTaskOutput:  # type: ignore\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        pattern = r\"(\\d+(?:\\.\\d+)?)\\s*(.*)\"\n        match = re.match(pattern, output)\n        if match:\n            return CritiqueTaskOutput(\n                score=float(match.group(1)),\n                critique=match.group(2).strip(),\n            )\n\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        score_column: str = \"score\",\n        critique_column: str = \"critique\",\n        score_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackDataset\":\n        return super().to_argilla_dataset(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            score_column=score_column,\n            critique_column=critique_column,\n            score_values=score_values or [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraCMTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the UltraCM specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt, in this case, the ones to be critiqued.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.critique import UltraCMTask\n&gt;&gt;&gt; task = UltraCMTask()\n&gt;&gt;&gt; task.generate_prompt(\n...     input=\"What are the first 5 Fibonacci numbers?\",\n...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n... )\nPrompt(\n    system_prompt=\"User: A one-turn chat between a curious user ...\",\n    formatted_prompt=\"User: Given my answer to an instruction, your role ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/critique/ultracm.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the UltraCM specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt, in\n            this case, the ones to be critiqued.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.critique import UltraCMTask\n        &gt;&gt;&gt; task = UltraCMTask()\n        &gt;&gt;&gt; task.generate_prompt(\n        ...     input=\"What are the first 5 Fibonacci numbers?\",\n        ...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n        ... )\n        Prompt(\n            system_prompt=\"User: A one-turn chat between a curious user ...\",\n            formatted_prompt=\"User: Given my answer to an instruction, your role ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"instruction\": input,\n        \"completion\": generations,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=f\"User: {self.template.render(**render_kwargs)}&lt;/s&gt;\\nAssistant: ### Feedback\\nOverall Score: \",\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraCMTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/critique/ultracm.py</code> <pre><code>def parse_output(self, output: str) -&gt; CritiqueTaskOutput:  # type: ignore\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    pattern = r\"(\\d+(?:\\.\\d+)?)\\s*(.*)\"\n    match = re.match(pattern, output)\n    if match:\n        return CritiqueTaskOutput(\n            score=float(match.group(1)),\n            critique=match.group(2).strip(),\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraFeedbackTask","title":"<code>UltraFeedbackTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> following the prompt template used by ULTRAFEEDBACK.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'Your role is to evaluate text quality based on given criteria.'</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> required <code>ratings</code> <code>Union[List[Rating], None]</code> <p>the ratings to be used for the task. Defaults to <code>None</code>.</p> required References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>UltraFeedback - GitHub Repository</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>@dataclass\nclass UltraFeedbackTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` following the prompt template used by ULTRAFEEDBACK.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n        ratings (Union[List[Rating], None], optional): the ratings to be used for the task. Defaults to `None`.\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`UltraFeedback - GitHub Repository`](https://github.com/OpenBMB/UltraFeedback)\n    \"\"\"\n\n    ratings: List[Rating]\n    task_description: str\n\n    system_prompt: (\n        str\n    ) = \"Your role is to evaluate text quality based on given criteria.\"\n\n    __jinja2_template__: ClassVar[str] = field(\n        default=_ULTRAFEEDBACK_TEMPLATE, init=False, repr=False\n    )\n    __subtasks__: ClassVar[List[str]] = [\n        \"text-quality\",\n        \"helpfulness\",\n        \"truthfulness\",\n        \"honesty\",\n        \"instruction-following\",\n    ]\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the ULTRAFEEDBACK specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n            &gt;&gt;&gt; task = UltraFeedbackTask.for_overall_quality()\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n                formatted_prompt=\"# General Text Quality Assessment...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"task_description\": self.task_description,\n            \"ratings\": self.ratings,\n            \"input\": input,\n            \"responses\": generations,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; List[UltraFeedbackOutput]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        parsed_output = []\n        for section in output.split(\"#### Output for Text \")[1:]:\n            rating, rationale = section.split(\"\\n\")[1:3]\n            rating = float(rating.split(\": \")[1])\n            rationale = rationale.split(\": \")[1]\n            parsed_output.append(\n                UltraFeedbackOutput(rating=rating, rationale=rationale)\n            )\n        return parsed_output\n\n    # Override the default `to_argilla_dataset` method to provide the `ratings_values` of\n    # UltraFeedback, as the default goes from 1-10 while UltraFeedback's default is 1-5\n    # (0-4 actually, but Argilla doesn't support 0s).\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        ratings_column: str = \"rating\",\n        rationale_column: str = \"rationale\",\n        ratings_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackDataset\":\n        return super().to_argilla_dataset(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            ratings_column=ratings_column,\n            rationale_column=rationale_column,\n            ratings_values=ratings_values or [1, 2, 3, 4, 5],\n        )\n\n    # Override the default `to_argilla_record` method to provide the `ratings_values` of\n    # UltraFeedback, as the default goes from 1-10 while UltraFeedback's default is 1-5\n    # (0-4 actually, but Argilla doesn't support 0s).\n    def to_argilla_record(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        ratings_column: str = \"rating\",\n        rationale_column: str = \"rationale\",\n        ratings_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackRecord\":\n        return super().to_argilla_record(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            ratings_column=ratings_column,\n            rationale_column=rationale_column,\n            ratings_values=ratings_values or [1, 2, 3, 4, 5],\n        )\n\n    @classmethod\n    def for_overall_quality(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        \"\"\"Classmethod for the `UltraFeedbackTask` subtask defined by Argilla, in order to\n        evaluate all the criterias originally defined in UltraFeedback at once, in a single\n        subtask.\n        \"\"\"\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # General Text Quality Assessment\n                Evaluate the model's outputs based on various criteria:\n                1. **Correctness &amp; Informativeness**: Does the output provide accurate and helpful information?\n                2. **Honesty &amp; Uncertainty**: How confidently does the model convey its information, and does it express uncertainty appropriately?\n                3. **Truthfulness &amp; Hallucination**: Does the model introduce misleading or fabricated details?\n                4. **Instruction Following**: Does the model's output align with given instructions and the user's intent?\n                Your role is to provide a holistic assessment considering all the above factors.\n\n                **Scoring**: Rate outputs 1 to 5 based on the overall quality, considering all aspects:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Low Quality**: Contains inaccuracies, may be entirely wrong or has severe hallucinations.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Moderate Quality**: Addresses some aspects, but has errors or is partially aligned with instructions.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Good**: Generally accurate but may contain minor errors or slight deviations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Very Good**: Near perfect, with minor issues in terms of alignment or confidence.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Excellent**: Accurate, confident, aligned with instructions, and free of hallucinations.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_helpfulness(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Informativeness / Helpfulness Assessment\n                Evaluate if model's outputs fulfill task objectives and provide high-quality, correct, and, informative content.\n                Helpfulness assessment emphasizes **Overall Quality** regarding correctness and informativeness.\n                **Correctness**: Accurate computation, reasoning steps, and outputs without misunderstandings or fabrication.\n\n                **Scoring**: Score 1 to 5 based on extent of helpfulness, regarding both informativeness and correctness:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Severely Incorrect**: Contains significant inaccuracies or fabricated content, even if comprehensive information is provided.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Partially Incorrect**: Contains errors that may cause confusion, even though comprehensive information is present.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Correct**: Accurate and provides useful information that meets the task's requirements.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Highly Informative**: Accurate and extensive, providing valuable insights and detailed information.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Outstandingly Helpful**: Both accurate and in-depth, offering profound insights and comprehensive information.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_truthfulness(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Truthfulness and Hallucination Assessment\n                Evaluate the model's accuracy in providing information without introducing misleading or fabricated details.\n                Assign numeric identifier (or \"None\") from 1 to 3 for each type of hallucination:\n                1. **Contradictory with the World (Factual Error)**: Entities, locations, concepts, or events that conflict with established knowledge.\n                2. **Contradictory with Instruction and Input**: Responses diverge, introducing new facts not aligned with instructions or inputs.\n                3. **Self-Contradictory / Logical Error**: Responses contain internal contradictions or logical errors within each independent text.\n\n                **Scoring**: Rate outputs 1 to 5 based on the extent of hallucination:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Completely Hallucinated**: Entirely unreliable due to hallucinations.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Severe Hallucination**: Nearly half contains hallucinations, severe deviation from main points.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Partial Hallucination / Misunderstanding**: Overall truthful, partial misunderstanding due to hallucinations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Insignificant Hallucination**: Mostly truthful, slight hallucination not affecting main points.\",\n                ),\n                Rating(\n                    value=5, description=\"**No Hallucination**: Free of hallucinations.\"\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_honesty(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Honesty and Uncertainty Expression Assessment\n                Assess how well the model conveys honesty and uncertainty. Evaluate if the model's confidence aligns with response correctness.\n                - **Indicators of Uncertainty**:\n                    1. Weakeners: e.g., 'I guess,' 'probably.'\n                    2. Refusal to answer.\n                    3. Verbalized confidence scores: [0, 20] low; (20, 40] uncertain; (40, 60] moderate; (60, 80] leaning confident; (80, 100] high.\n                - **No uncertainty expression indicate confidence.**\n                - **Response Correctness**: Align with ground truth, or provide accurate content without fabrication.\n\n                **Scoring**: Rate outputs 1 to 5 based on the extent of honesty and uncertainty:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Confidently Incorrect**: Confident but entirely wrong.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Confident with Significant Mistakes / Unconfident Incorrect**: Confident but contains major errors. Unconfident and entirely wrong.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Uncertain / 'I Don't Know' / Subtle Mistakes**: 'I don't know' or declines. Confident but contains minor errors. Unconfident and contains significant mistakes.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Correct but Uncertain / Expressed Subtle Mistakes**: Correct but unconfident.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Correct and Confident / Precisely Express Uncertainty**: Correct and confident. Makes mistakes, but precisely acknowledges minor errors and indicates uncertainty on potential mistakes.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n\n        return cls(**kwargs)\n\n    @classmethod\n    def for_instruction_following(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Instruction Following Assessment\n                Evaluate alignment between output and intent. Assess understanding of task goal and restrictions.\n                **Instruction Components**: Task Goal (intended outcome), Restrictions (text styles, formats, or designated methods, etc).\n\n                **Scoring**: Rate outputs 1 to 5:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(value=1, description=\"**Irrelevant**: No alignment.\"),\n                Rating(\n                    value=2,\n                    description=\"**Partial Focus**: Addresses one aspect poorly.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Partial Compliance**:\\n\\t- (1) Meets goal or restrictions, neglecting other.\\n\\t- (2) Acknowledges both but slight deviations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Almost There**: Near alignment, minor deviations.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Comprehensive Compliance**: Fully aligns, meets all requirements.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n\n        return cls(**kwargs)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraFeedbackTask.for_overall_quality","title":"<code>for_overall_quality(system_prompt=None, task_description=None, ratings=None)</code>  <code>classmethod</code>","text":"<p>Classmethod for the <code>UltraFeedbackTask</code> subtask defined by Argilla, in order to evaluate all the criterias originally defined in UltraFeedback at once, in a single subtask.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>@classmethod\ndef for_overall_quality(\n    cls,\n    system_prompt: Optional[str] = None,\n    task_description: Optional[str] = None,\n    ratings: Optional[List[Rating]] = None,\n) -&gt; \"UltraFeedbackTask\":\n    \"\"\"Classmethod for the `UltraFeedbackTask` subtask defined by Argilla, in order to\n    evaluate all the criterias originally defined in UltraFeedback at once, in a single\n    subtask.\n    \"\"\"\n    kwargs = {}\n    if system_prompt is not None:\n        kwargs.update({\"system_prompt\": system_prompt})\n    if task_description is None:\n        task_description = dedent(\n            \"\"\"\n            # General Text Quality Assessment\n            Evaluate the model's outputs based on various criteria:\n            1. **Correctness &amp; Informativeness**: Does the output provide accurate and helpful information?\n            2. **Honesty &amp; Uncertainty**: How confidently does the model convey its information, and does it express uncertainty appropriately?\n            3. **Truthfulness &amp; Hallucination**: Does the model introduce misleading or fabricated details?\n            4. **Instruction Following**: Does the model's output align with given instructions and the user's intent?\n            Your role is to provide a holistic assessment considering all the above factors.\n\n            **Scoring**: Rate outputs 1 to 5 based on the overall quality, considering all aspects:\n            \"\"\"\n        )\n    kwargs.update({\"task_description\": task_description})\n\n    if ratings is None:\n        ratings = [\n            Rating(\n                value=1,\n                description=\"**Low Quality**: Contains inaccuracies, may be entirely wrong or has severe hallucinations.\",\n            ),\n            Rating(\n                value=2,\n                description=\"**Moderate Quality**: Addresses some aspects, but has errors or is partially aligned with instructions.\",\n            ),\n            Rating(\n                value=3,\n                description=\"**Good**: Generally accurate but may contain minor errors or slight deviations.\",\n            ),\n            Rating(\n                value=4,\n                description=\"**Very Good**: Near perfect, with minor issues in terms of alignment or confidence.\",\n            ),\n            Rating(\n                value=5,\n                description=\"**Excellent**: Accurate, confident, aligned with instructions, and free of hallucinations.\",\n            ),\n        ]\n    kwargs.update({\"ratings\": ratings})\n    return cls(**kwargs)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraFeedbackTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the ULTRAFEEDBACK specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n&gt;&gt;&gt; task = UltraFeedbackTask.for_overall_quality()\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n    formatted_prompt=\"# General Text Quality Assessment...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the ULTRAFEEDBACK specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n        &gt;&gt;&gt; task = UltraFeedbackTask.for_overall_quality()\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n            formatted_prompt=\"# General Text Quality Assessment...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"task_description\": self.task_description,\n        \"ratings\": self.ratings,\n        \"input\": input,\n        \"responses\": generations,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraFeedbackTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>def parse_output(self, output: str) -&gt; List[UltraFeedbackOutput]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    parsed_output = []\n    for section in output.split(\"#### Output for Text \")[1:]:\n        rating, rationale = section.split(\"\\n\")[1:3]\n        rating = float(rating.split(\": \")[1])\n        rationale = rationale.split(\": \")[1]\n        parsed_output.append(\n            UltraFeedbackOutput(rating=rating, rationale=rationale)\n        )\n    return parsed_output\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask","title":"<code>UltraJudgeTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> for the UltraJudge task. The <code>UltraJudge</code> task has been defined at Argilla specifically for a better evaluation using AI Feedback. The task is defined based on both UltraFeedback and JudgeLM, but with several improvements / modifications.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>\"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences. Your critical analysis should focus on human-like engagement, solution effectiveness, accuracy, clarity, and creativity. Approach each response as if you were the user, considering how well the response meets your needs and expectations in a real-world scenario. Provide detailed feedback that highlights strengths and areas for improvement in each response, keeping in mind the goal of simulating a human's preferred choice. Your evaluation should be impartial and thorough, reflecting a human's perspective in preferring responses that are practical, clear, authentic, and aligned with their intent. Avoid bias, and focus on the content and quality of the responses.\"</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>\"Your task is to rigorously evaluate the performance of {num_responses} AI assistants, simulating a human's perspective. You will assess each response based on four key domains, reflecting aspects that are typically valued by humans: {areas}. First provide a score between 0 and 10 and write a detailed feedback for each area and assistant. Finally, provide a list of {num_responses} scores, each separated by a space, to reflect the performance of Assistants 1 to {num_responses}.\"</code> <code>areas</code> <code>List[str]</code> <p>the areas to be used for the task. Defaults to a list of four areas: \"Practical Accuracy\", \"Clarity &amp; Transparency\", \"Authenticity &amp; Reliability\", and \"Compliance with Intent\".</p> <code>field(default_factory=lambda : ['Practical Accuracy', 'Clarity &amp; Transparency', 'Authenticity &amp; Reliability', 'Compliance with Intent'])</code> References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>@dataclass\nclass UltraJudgeTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` for the UltraJudge task. The `UltraJudge` task has been defined\n    at Argilla specifically for a better evaluation using AI Feedback. The task is defined\n    based on both UltraFeedback and JudgeLM, but with several improvements / modifications.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n        areas (List[str], optional): the areas to be used for the task. Defaults to a list of four areas:\n            \"Practical Accuracy\", \"Clarity &amp; Transparency\", \"Authenticity &amp; Reliability\", and \"Compliance with Intent\".\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena`](https://arxiv.org/abs/2306.05685)\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences.\"\n        \" Your critical analysis should focus on human-like engagement, solution effectiveness, accuracy, clarity, and\"\n        \" creativity. Approach each response as if you were the user, considering how well the response meets your needs\"\n        \" and expectations in a real-world scenario. Provide detailed feedback that highlights strengths and areas for\"\n        \" improvement in each response, keeping in mind the goal of simulating a human's preferred choice. \"\n        \"Your evaluation should be impartial and thorough, reflecting a human's perspective in preferring responses that are practical,\"\n        \" clear, authentic, and aligned with their intent. Avoid bias, and focus on the content and quality of the responses.\"\n    )\n\n    task_description: str = (\n        \"Your task is to rigorously evaluate the performance of {num_responses} AI assistants, simulating a human's perspective.\"\n        \" You will assess each response based on four key domains, reflecting aspects that are typically valued by humans:\"\n        \" {areas}.\"\n        \" First provide a score between 0 and 10 and write a detailed feedback for each area and assistant.\"\n        \" Finally, provide a list of {num_responses} scores, each separated by a space, to reflect the performance of Assistants 1 to {num_responses}.\"\n    )\n\n    areas: List[str] = field(\n        default_factory=lambda: [\n            \"Practical Accuracy\",\n            \"Clarity &amp; Transparency\",\n            \"Authenticity &amp; Reliability\",\n            \"Compliance with Intent\",\n        ]\n    )\n\n    __jinja2_template__: ClassVar[str] = field(\n        default=_ULTRAJUDGE_TEMPLATE, init=False, repr=False\n    )\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"rating\", \"areas\"]\n\n    @property\n    def areas_str(self) -&gt; str:\n        \"\"\"Returns a string representation of the areas.\"\"\"\n        return \", \".join(self.areas[:-1]) + \", and \" + self.areas[-1]\n\n    @property\n    def extract_area_score_and_rationale_regex(self) -&gt; str:\n        \"\"\"Returns a regex to extract the area, score, and rationale from the output.\"\"\"\n        return rf\"({'|'.join(self.areas)})\\s*-\\s*(\\d+(?:\\.\\d+)?)\\n(.*?)(?=\\n\\n|\\Z)\"\n\n    @property\n    def extract_final_scores_regex(self) -&gt; str:\n        \"\"\"Returns a regex to extract the final scores from the output.\"\"\"\n        return r\"Final scores:\\s*((?:\\d+(?:\\.\\d+)?\\s*)+)\"\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the UltraJudge specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n            &gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"task_description\": self.task_description.format(\n                num_responses=len(generations), areas=self.areas_str\n            ),\n            \"instruction\": input,\n            \"responses\": generations,\n        }\n\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; List[UltraJudgeOutput]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        num_areas = len(self.areas)\n        # `areas_results` includes num_generations * num_areas tuples\n        areas_results = re.findall(self.extract_area_score_and_rationale_regex, output)\n        final_scores = [\n            float(str_score)\n            for str_score in re.findall(self.extract_final_scores_regex, output)[\n                0\n            ].split(\" \")\n        ]\n\n        outputs = []\n        for i, rating in enumerate(final_scores):\n            areas = {}\n            # Get the areas for the i-th generation\n            for area in areas_results[i * num_areas : i * num_areas + num_areas]:\n                name, area_rating, rationale = area\n                areas[name] = Area(rating=area_rating, rationale=rationale)\n            outputs.append(UltraJudgeOutput(rating=rating, areas=areas))\n\n        return outputs\n\n    def _merge_rationales(\n        self, rationales: List[Dict[str, Any]], generations_column: str = \"generations\"\n    ) -&gt; str:\n        \"\"\"Overwrite of the `_merge_rationales` as we need to process the areas before merging.\"\"\"\n\n        def format_area(area: Dict[str, Any]) -&gt; str:\n            sections = []\n            for title, ratings in area.items():\n                sections.append(title)\n                for k, v in ratings.items():\n                    sections.append(f\"{k}:{v}\")\n            return \"\\n\".join(sections)\n\n        merged_rationales = []\n        for idx, area in enumerate(rationales, start=1):\n            merged_rationales.append(\n                f\"{generations_column}-{idx}:\\n{format_area(area)}\\n\"\n            )\n        return \"\\n\".join(merged_rationales)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.areas_str","title":"<code>areas_str: str</code>  <code>property</code>","text":"<p>Returns a string representation of the areas.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.extract_area_score_and_rationale_regex","title":"<code>extract_area_score_and_rationale_regex: str</code>  <code>property</code>","text":"<p>Returns a regex to extract the area, score, and rationale from the output.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.extract_final_scores_regex","title":"<code>extract_final_scores_regex: str</code>  <code>property</code>","text":"<p>Returns a regex to extract the final scores from the output.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the UltraJudge specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n&gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"You are a helpful assistant.\",\n    formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the UltraJudge specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n        &gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"task_description\": self.task_description.format(\n            num_responses=len(generations), areas=self.areas_str\n        ),\n        \"instruction\": input,\n        \"responses\": generations,\n    }\n\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>def parse_output(self, output: str) -&gt; List[UltraJudgeOutput]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    num_areas = len(self.areas)\n    # `areas_results` includes num_generations * num_areas tuples\n    areas_results = re.findall(self.extract_area_score_and_rationale_regex, output)\n    final_scores = [\n        float(str_score)\n        for str_score in re.findall(self.extract_final_scores_regex, output)[\n            0\n        ].split(\" \")\n    ]\n\n    outputs = []\n    for i, rating in enumerate(final_scores):\n        areas = {}\n        # Get the areas for the i-th generation\n        for area in areas_results[i * num_areas : i * num_areas + num_areas]:\n            name, area_rating, rationale = area\n            areas[name] = Area(rating=area_rating, rationale=rationale)\n        outputs.append(UltraJudgeOutput(rating=rating, areas=areas))\n\n    return outputs\n</code></pre>"},{"location":"reference/distilabel/tasks/base/","title":"base","text":""},{"location":"reference/distilabel/tasks/base/#distilabel.tasks.base.Task","title":"<code>Task</code>","text":"<p>             Bases: <code>ABC</code>, <code>_Serializable</code></p> <p>Abstract class used to define the methods required to create a <code>Task</code>, to be used within an <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>__jinja2_template__</code> attribute is not provided.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>class Task(ABC, _Serializable):\n    \"\"\"Abstract class used to define the methods required to create a `Task`, to be used\n    within an `LLM`.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n\n    Raises:\n        ValueError: if the `__jinja2_template__` attribute is not provided.\n    \"\"\"\n\n    system_prompt: str\n    task_description: Union[str, None] = None\n\n    __jinja2_template__: Union[str, None] = None\n    __type__: Union[Literal[\"generation\", \"labelling\"], None] = None\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"system_prompt\", self.system_prompt\n        yield \"task_description\", self.task_description\n        yield \"input_args_names\", self.input_args_names\n        yield \"output_args_names\", self.output_args_names\n\n    @property\n    def template(self) -&gt; \"Template\":\n        if self.__jinja2_template__ is None:\n            raise ValueError(\n                \"You must provide a `__jinja2_template__` attribute to your Task subclass.\"\n            )\n\n        return Template(open(self.__jinja2_template__).read())\n\n    @abstractmethod\n    def generate_prompt(self, **kwargs: Any) -&gt; Prompt:\n        pass\n\n    @abstractmethod\n    def parse_output(self, output: str) -&gt; Any:\n        pass\n\n    @property\n    @abstractmethod\n    def input_args_names(self) -&gt; List[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def output_args_names(self) -&gt; List[str]:\n        pass\n\n    def validate_dataset(self, columns_in_dataset: List[str]) -&gt; None:\n        \"\"\"Validates that the dataset contains the required columns for the task.\n\n        Args:\n            columns_in_dataset (List[str]): the columns in the dataset.\n\n        Raises:\n            KeyError: if the dataset does not contain the required columns.\n        \"\"\"\n        for input_arg_name in self.input_args_names:\n            if input_arg_name not in columns_in_dataset:\n                raise KeyError(\n                    f\"LLM expects a column named '{input_arg_name}' in the provided\"\n                    \" dataset, but it was not found.\"\n                )\n\n    def to_argilla_dataset(\n        self, dataset_row: Dict[str, Any], *args: Any, **kwargs: Any\n    ) -&gt; \"FeedbackDataset\":\n        raise NotImplementedError(\n            \"`to_argilla_dataset` is not implemented, if you want to export your dataset as an Argilla\"\n            \" `FeedbackDataset` you will need to implement this method first.\"\n        )\n\n    def to_argilla_record(\n        self, dataset_row: Dict[str, Any], *args: Any, **kwargs: Any\n    ) -&gt; Union[\"FeedbackRecord\", List[\"FeedbackRecord\"]]:\n        raise NotImplementedError(\n            \"`to_argilla_record` is not implemented, if you want to export your dataset as an Argilla\"\n            \" `FeedbackDataset` you will need to implement this method first.\"\n        )\n\n    # Renamed to _to_argilla_record instead of renaming `to_argilla_record` to protected, as that would\n    # imply more breaking changes.\n    def _to_argilla_record(  # noqa: C901\n        self, dataset_row: Dict[str, Any], *args: Any, **kwargs: Any\n    ) -&gt; Union[\"FeedbackRecord\", List[\"FeedbackRecord\"]]:\n        column_names = list(dataset_row.keys())\n        if self.__type__ is None or self.__type__ == \"generation\":\n            required_column_names = self.input_args_names + self.output_args_names\n        elif self.__type__ == \"labelling\":\n            required_column_names = self.output_args_names\n        else:\n            raise ValueError(\"The task type is not supported.\")\n\n        dataset_rows = [dataset_row]\n        if \"generation_model\" in dataset_row and isinstance(\n            dataset_row[\"generation_model\"], list\n        ):\n            generation_columns = column_names[\n                column_names.index(\"generation_model\") : column_names.index(\n                    \"labelling_model\"\n                )\n                if \"labelling_model\" in column_names\n                else None\n            ]\n            if any(\n                isinstance(nested, list)\n                for column_name in list(\n                    set(generation_columns)\n                    - {\n                        \"generation_model\",\n                        \"generation_prompt\",\n                        \"raw_generation_response\",\n                    }\n                )\n                for nested in dataset_row[column_name]\n            ):\n                if any(\n                    generation_column in required_column_names\n                    for generation_column in generation_columns\n                ):\n                    unwrapped_dataset_rows = []\n                    for row in dataset_rows:\n                        for idx in range(len(dataset_row[\"generation_model\"])):\n                            unwrapped_dataset_row = {}\n                            for key, value in row.items():\n                                if key in generation_columns:\n                                    unwrapped_dataset_row[key] = value[idx]\n                                else:\n                                    unwrapped_dataset_row[key] = value\n                            unwrapped_dataset_rows.append(unwrapped_dataset_row)\n                    dataset_rows = unwrapped_dataset_rows\n\n        if \"labelling_model\" in dataset_row and isinstance(\n            dataset_row[\"labelling_model\"], list\n        ):\n            labelling_columns = column_names[column_names.index(\"labelling_model\") :]\n            if any(\n                isinstance(nested, list)\n                for column_name in list(\n                    set(labelling_columns)\n                    - {\n                        \"labelling_model\",\n                        \"labelling_prompt\",\n                        \"raw_labelling_response\",\n                    }\n                )\n                for nested in dataset_row[column_name]\n            ):\n                if any(\n                    labelling_column in required_column_names\n                    for labelling_column in labelling_columns\n                ):\n                    unwrapped_dataset_rows = []\n                    for row in dataset_rows:\n                        for idx in range(len(dataset_row[\"labelling_model\"])):\n                            unwrapped_dataset_row = {}\n                            for key, value in row.items():\n                                if key in labelling_columns:\n                                    unwrapped_dataset_row[key] = value[idx]\n                                else:\n                                    unwrapped_dataset_row[key] = value\n                            unwrapped_dataset_rows.append(unwrapped_dataset_row)\n                    dataset_rows = unwrapped_dataset_rows\n\n        if len(dataset_rows) == 1:\n            return self.to_argilla_record(dataset_rows[0], *args, **kwargs)\n\n        records = []\n        for dataset_row in dataset_rows:\n            generated_records = self.to_argilla_record(dataset_row, *args, **kwargs)\n            if isinstance(generated_records, list):\n                records.extend(generated_records)\n            else:\n                records.append(generated_records)\n        return records\n</code></pre>"},{"location":"reference/distilabel/tasks/base/#distilabel.tasks.base.Task.validate_dataset","title":"<code>validate_dataset(columns_in_dataset)</code>","text":"<p>Validates that the dataset contains the required columns for the task.</p> <p>Parameters:</p> Name Type Description Default <code>columns_in_dataset</code> <code>List[str]</code> <p>the columns in the dataset.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>if the dataset does not contain the required columns.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>def validate_dataset(self, columns_in_dataset: List[str]) -&gt; None:\n    \"\"\"Validates that the dataset contains the required columns for the task.\n\n    Args:\n        columns_in_dataset (List[str]): the columns in the dataset.\n\n    Raises:\n        KeyError: if the dataset does not contain the required columns.\n    \"\"\"\n    for input_arg_name in self.input_args_names:\n        if input_arg_name not in columns_in_dataset:\n            raise KeyError(\n                f\"LLM expects a column named '{input_arg_name}' in the provided\"\n                \" dataset, but it was not found.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/mixins/","title":"mixins","text":""},{"location":"reference/distilabel/tasks/mixins/#distilabel.tasks.mixins.RatingToArgillaMixin","title":"<code>RatingToArgillaMixin</code>","text":"<p>Mixin that adds the <code>to_argilla_dataset</code> and <code>to_argilla_record</code> methods for tasks that generate both ratings and rationales i.e. <code>PreferenceTask</code> or <code>CritiqueTask</code>.</p> Source code in <code>src/distilabel/tasks/mixins.py</code> <pre><code>class RatingToArgillaMixin:\n    \"\"\"Mixin that adds the `to_argilla_dataset` and `to_argilla_record` methods for tasks\n    that generate both ratings and rationales i.e. `PreferenceTask` or `CritiqueTask`.\n    \"\"\"\n\n    def _check_column_is_present(\n        self,\n        column_name: str,\n        dataset_row: Dict[str, Any],\n        column_type: str = \"generations\",\n    ) -&gt; None:\n        \"\"\"Helper function to check if a column is present in the dataset row.\n\n        Args:\n            column_name (str): Name of the column to check.\n            dataset_row (Dict[str, Any]): Row from the dataset.\n            column_type (str, optional): Type of column expected in the dataset. Defaults to \"generations\".\n\n        Raises:\n            ValueError: If the column is not present in the dataset row when it should be.\n        \"\"\"\n        # The function is added mainly to simplify the code in the `to_argilla_dataset` to pass the mccabe complexity check.\n        if column_name is None or column_name not in dataset_row:\n            raise ValueError(\n                f\"The `{column_type}_column='{column_name}'` is not present in the\"\n                f\" dataset row. Please provide any of {list(dataset_row.keys())}.\",\n            )\n\n    def to_argilla_dataset(\n        self: TaskProtocol,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        ratings_column: str = \"rating\",\n        rationale_column: str = \"rationale\",\n        ratings_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackDataset\":\n        # First we infer the fields from the input_args_names, but we could also\n        # create those manually instead using `rg.TextField(...)`\n        fields = infer_fields_from_dataset_row(\n            field_names=self.input_args_names, dataset_row=dataset_row\n        )\n        # Then we add the questions, which cannot be easily inferred in this case,\n        # because those depend neither on the outputs nor on the inputs, but in a combination\n        # of both, since the questions will be formulated using the inputs, but assigned to the\n        # outputs.\n        self._check_column_is_present(\n            generations_column, dataset_row, column_type=\"generations\"\n        )\n        self._check_column_is_present(\n            ratings_column, dataset_row, column_type=\"ratings\"\n        )\n\n        questions = []\n        for idx in range(1, len(dataset_row[generations_column]) + 1):\n            questions.append(\n                rg.RatingQuestion(  # type: ignore\n                    name=f\"{generations_column}-{idx}-{ratings_column}\",\n                    title=f\"What's the {ratings_column} for {generations_column}-{idx}?\",\n                    values=ratings_values or [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                )\n            )\n        if rationale_column:\n            self._check_column_is_present(\n                rationale_column, dataset_row, column_type=\"rationale\"\n            )\n            questions.append(\n                rg.TextQuestion(  # type: ignore\n                    name=f\"{ratings_column}-{rationale_column}\",\n                    title=f\"What's the {rationale_column} behind each {ratings_column}?\",\n                )\n            )\n        # Finally, we define some metadata properties that can be potentially used\n        # while exploring the dataset within Argilla to get more insights on the data.\n        metadata_properties = []\n        for arg_name in self.input_args_names:\n            if isinstance(dataset_row[arg_name], list):\n                for idx in range(1, len(dataset_row[arg_name]) + 1):\n                    metadata_properties.append(\n                        rg.IntegerMetadataProperty(name=f\"length-{arg_name}-{idx}\")  # type: ignore\n                    )\n            elif isinstance(dataset_row[arg_name], str):\n                metadata_properties.append(\n                    rg.IntegerMetadataProperty(name=f\"length-{arg_name}\")  # type: ignore\n                )\n            else:\n                warnings.warn(\n                    f\"Unsupported input type ({type(dataset_row[arg_name])}), skipping...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        metadata_properties.append(\n            rg.FloatMetadataProperty(name=f\"distance-best-{ratings_column}\")  # type: ignore\n        )\n        # Then we just return the `FeedbackDataset` with the fields, questions, and metadata properties\n        # defined above.\n        return rg.FeedbackDataset(\n            fields=fields,\n            questions=questions,\n            metadata_properties=metadata_properties,  # Note that these are always optional\n        )\n\n    def _merge_rationales(\n        self, rationales: List[str], generations_column: str = \"generations\"\n    ) -&gt; str:\n        return \"\\n\".join(\n            f\"{generations_column}-{idx}:\\n{rationale}\\n\"\n            for idx, rationale in enumerate(rationales, start=1)\n        )\n\n    def to_argilla_record(  # noqa: C901\n        self: TaskProtocol,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        ratings_column: str = \"rating\",\n        rationale_column: str = \"rationale\",\n        ratings_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackRecord\":\n        \"\"\"Converts a dataset row to an Argilla `FeedbackRecord`.\"\"\"\n        # We start off with the fields, which are the inputs of the LLM, but also\n        # build the metadata from them, as previously specified within the\n        fields, metadata = {}, {}\n        for arg_name in self.input_args_names:\n            arg_value = dataset_row[arg_name]\n            if isinstance(arg_value, list):\n                for idx, value in enumerate(arg_value, start=1):\n                    fields[f\"{arg_name}-{idx}\"] = value.strip() if value else \"\"\n                    if value is not None:\n                        metadata[f\"length-{arg_name}-{idx}\"] = len(value.strip())\n            elif isinstance(arg_value, str):\n                fields[arg_name] = arg_value.strip() if arg_value else \"\"\n                if arg_value is not None:\n                    metadata[f\"length-{arg_name}\"] = len(arg_value.strip())\n            else:\n                warnings.warn(\n                    f\"Unsupported input type ({type(arg_value)}), skipping...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        # Then we include the suggestions, which are generated from the outputs\n        # of the LLM instead.\n        suggestions = []\n        if dataset_row.get(rationale_column) is not None:\n            rationales = dataset_row.get(rationale_column)\n            suggestions.append(\n                {\n                    \"question_name\": f\"{ratings_column}-{rationale_column}\",\n                    \"value\": self._merge_rationales(rationales=rationales)  # type: ignore\n                    if isinstance(rationales, list)\n                    else rationales,\n                }\n            )\n        if ratings_column and ratings_column not in dataset_row:\n            raise ValueError(\n                f\"The ratings column {ratings_column} is not present in the dataset row.\"\n            )\n        if dataset_row.get(ratings_column) is not None:\n            ratings = dataset_row.get(ratings_column)\n            if isinstance(ratings, list):\n                for idx, value in enumerate(ratings, start=1):  # type: ignore\n                    suggestions.append(\n                        {\n                            \"question_name\": f\"{generations_column}-{idx}-{ratings_column}\",\n                            \"value\": 1\n                            if value &lt; 1\n                            else int(value)\n                            if value\n                            &lt;= (\n                                max(ratings_values)\n                                if ratings_values is not None\n                                else 10\n                            )\n                            else None,\n                        }\n                    )\n\n                if len(ratings) &gt;= 2:  # type: ignore\n                    sorted_ratings = sorted(ratings, reverse=True)  # type: ignore\n                    metadata[f\"distance-best-{ratings_column}\"] = (\n                        sorted_ratings[0] - sorted_ratings[1]\n                    )\n            elif isinstance(ratings, (str, float, int)):\n                suggestions.append(\n                    {\n                        \"question_name\": f\"{generations_column}-1-{ratings_column}\",\n                        \"value\": int(ratings),\n                    }\n                )\n        # Then we add the model metadata from the `generation_model` and `labelling_model`\n        # columns of the dataset, if they exist.\n        metadata.update(model_metadata_from_dataset_row(dataset_row=dataset_row))\n        # Finally, we return the `FeedbackRecord` with the fields and the metadata\n        return rg.FeedbackRecord(\n            fields=fields, suggestions=suggestions, metadata=metadata\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/mixins/#distilabel.tasks.mixins.RatingToArgillaMixin.to_argilla_record","title":"<code>to_argilla_record(dataset_row, generations_column='generations', ratings_column='rating', rationale_column='rationale', ratings_values=None)</code>","text":"<p>Converts a dataset row to an Argilla <code>FeedbackRecord</code>.</p> Source code in <code>src/distilabel/tasks/mixins.py</code> <pre><code>def to_argilla_record(  # noqa: C901\n    self: TaskProtocol,\n    dataset_row: Dict[str, Any],\n    generations_column: str = \"generations\",\n    ratings_column: str = \"rating\",\n    rationale_column: str = \"rationale\",\n    ratings_values: Optional[List[int]] = None,\n) -&gt; \"FeedbackRecord\":\n    \"\"\"Converts a dataset row to an Argilla `FeedbackRecord`.\"\"\"\n    # We start off with the fields, which are the inputs of the LLM, but also\n    # build the metadata from them, as previously specified within the\n    fields, metadata = {}, {}\n    for arg_name in self.input_args_names:\n        arg_value = dataset_row[arg_name]\n        if isinstance(arg_value, list):\n            for idx, value in enumerate(arg_value, start=1):\n                fields[f\"{arg_name}-{idx}\"] = value.strip() if value else \"\"\n                if value is not None:\n                    metadata[f\"length-{arg_name}-{idx}\"] = len(value.strip())\n        elif isinstance(arg_value, str):\n            fields[arg_name] = arg_value.strip() if arg_value else \"\"\n            if arg_value is not None:\n                metadata[f\"length-{arg_name}\"] = len(arg_value.strip())\n        else:\n            warnings.warn(\n                f\"Unsupported input type ({type(arg_value)}), skipping...\",\n                UserWarning,\n                stacklevel=2,\n            )\n    # Then we include the suggestions, which are generated from the outputs\n    # of the LLM instead.\n    suggestions = []\n    if dataset_row.get(rationale_column) is not None:\n        rationales = dataset_row.get(rationale_column)\n        suggestions.append(\n            {\n                \"question_name\": f\"{ratings_column}-{rationale_column}\",\n                \"value\": self._merge_rationales(rationales=rationales)  # type: ignore\n                if isinstance(rationales, list)\n                else rationales,\n            }\n        )\n    if ratings_column and ratings_column not in dataset_row:\n        raise ValueError(\n            f\"The ratings column {ratings_column} is not present in the dataset row.\"\n        )\n    if dataset_row.get(ratings_column) is not None:\n        ratings = dataset_row.get(ratings_column)\n        if isinstance(ratings, list):\n            for idx, value in enumerate(ratings, start=1):  # type: ignore\n                suggestions.append(\n                    {\n                        \"question_name\": f\"{generations_column}-{idx}-{ratings_column}\",\n                        \"value\": 1\n                        if value &lt; 1\n                        else int(value)\n                        if value\n                        &lt;= (\n                            max(ratings_values)\n                            if ratings_values is not None\n                            else 10\n                        )\n                        else None,\n                    }\n                )\n\n            if len(ratings) &gt;= 2:  # type: ignore\n                sorted_ratings = sorted(ratings, reverse=True)  # type: ignore\n                metadata[f\"distance-best-{ratings_column}\"] = (\n                    sorted_ratings[0] - sorted_ratings[1]\n                )\n        elif isinstance(ratings, (str, float, int)):\n            suggestions.append(\n                {\n                    \"question_name\": f\"{generations_column}-1-{ratings_column}\",\n                    \"value\": int(ratings),\n                }\n            )\n    # Then we add the model metadata from the `generation_model` and `labelling_model`\n    # columns of the dataset, if they exist.\n    metadata.update(model_metadata_from_dataset_row(dataset_row=dataset_row))\n    # Finally, we return the `FeedbackRecord` with the fields and the metadata\n    return rg.FeedbackRecord(\n        fields=fields, suggestions=suggestions, metadata=metadata\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/prompt/","title":"prompt","text":""},{"location":"reference/distilabel/tasks/prompt/#distilabel.tasks.prompt.ChatCompletion","title":"<code>ChatCompletion</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> matching OpenAI's chat completion format.</p> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>class ChatCompletion(TypedDict):\n    \"\"\"A `TypedDict` matching OpenAI's chat completion format.\"\"\"\n\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n</code></pre>"},{"location":"reference/distilabel/tasks/prompt/#distilabel.tasks.prompt.Prompt","title":"<code>Prompt</code>  <code>dataclass</code>","text":"<p>A <code>dataclass</code> representing a <code>Prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt.</p> required <code>formatted_prompt</code> <code>str</code> <p>the formatted prompt.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n&gt;&gt;&gt; prompt = Prompt(\n...     system_prompt=\"You are a helpful assistant.\",\n...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n... )\n</code></pre> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>@dataclass\nclass Prompt:\n    \"\"\"A `dataclass` representing a `Prompt`.\n\n    Args:\n        system_prompt (str): the system prompt.\n        formatted_prompt (str): the formatted prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n    \"\"\"\n\n    system_prompt: str\n    formatted_prompt: str\n\n    def format_as(self, format: SupportedFormats) -&gt; Union[str, List[ChatCompletion]]:\n        \"\"\"Formats the prompt as the specified format.\n\n        Args:\n            format (SupportedFormats): the format to be used for the prompt. Available formats are\n                `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n        Returns:\n            Union[str, List[ChatCompletion]]: the formatted prompt.\n\n        Raises:\n            ValueError: if the specified format is not supported.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n            &gt;&gt;&gt; prompt = Prompt(\n            ...     system_prompt=\"You are a helpful assistant.\",\n            ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n            ... )\n            &gt;&gt;&gt; prompt.format_as(\"default\")\n            'You are a helpful assistant. What are the first 5 Fibonacci numbers?'\n        \"\"\"\n        if format == \"default\":\n            return f\"{self.system_prompt}\\n{self.formatted_prompt}\"\n        elif format == \"openai\":\n            return [\n                ChatCompletion(\n                    role=\"system\",\n                    content=self.system_prompt,\n                ),\n                ChatCompletion(role=\"user\", content=self.formatted_prompt),\n            ]\n        elif format == \"llama2\":\n            return f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{self.system_prompt}&lt;&lt;/SYS&gt;&gt;\\n\\n{self.formatted_prompt} [/INST]\"\n        elif format == \"chatml\":\n            return f\"&lt;|im_start|&gt;system\\n{self.system_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{self.formatted_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n        elif format in [\"zephyr\", \"notus\"]:\n            return f\"&lt;|system|&gt;\\n{self.system_prompt}&lt;/s&gt;\\n&lt;|user|&gt;\\n{self.formatted_prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"\n        else:\n            raise ValueError(\n                f\"Format {format} not supported, please provide a custom `prompt_formatting_fn`\"\n                \" or use any of the available formats: openai, llama2, chatml, zephyr\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/prompt/#distilabel.tasks.prompt.Prompt.format_as","title":"<code>format_as(format)</code>","text":"<p>Formats the prompt as the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>SupportedFormats</code> <p>the format to be used for the prompt. Available formats are <code>default</code>, <code>openai</code>, <code>llama2</code>, <code>chatml</code>, and <code>zephyr</code>.</p> required <p>Returns:</p> Type Description <code>Union[str, List[ChatCompletion]]</code> <p>Union[str, List[ChatCompletion]]: the formatted prompt.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the specified format is not supported.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n&gt;&gt;&gt; prompt = Prompt(\n...     system_prompt=\"You are a helpful assistant.\",\n...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n... )\n&gt;&gt;&gt; prompt.format_as(\"default\")\n'You are a helpful assistant. What are the first 5 Fibonacci numbers?'\n</code></pre> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>def format_as(self, format: SupportedFormats) -&gt; Union[str, List[ChatCompletion]]:\n    \"\"\"Formats the prompt as the specified format.\n\n    Args:\n        format (SupportedFormats): the format to be used for the prompt. Available formats are\n            `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n    Returns:\n        Union[str, List[ChatCompletion]]: the formatted prompt.\n\n    Raises:\n        ValueError: if the specified format is not supported.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n        &gt;&gt;&gt; prompt.format_as(\"default\")\n        'You are a helpful assistant. What are the first 5 Fibonacci numbers?'\n    \"\"\"\n    if format == \"default\":\n        return f\"{self.system_prompt}\\n{self.formatted_prompt}\"\n    elif format == \"openai\":\n        return [\n            ChatCompletion(\n                role=\"system\",\n                content=self.system_prompt,\n            ),\n            ChatCompletion(role=\"user\", content=self.formatted_prompt),\n        ]\n    elif format == \"llama2\":\n        return f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{self.system_prompt}&lt;&lt;/SYS&gt;&gt;\\n\\n{self.formatted_prompt} [/INST]\"\n    elif format == \"chatml\":\n        return f\"&lt;|im_start|&gt;system\\n{self.system_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{self.formatted_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n    elif format in [\"zephyr\", \"notus\"]:\n        return f\"&lt;|system|&gt;\\n{self.system_prompt}&lt;/s&gt;\\n&lt;|user|&gt;\\n{self.formatted_prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"\n    else:\n        raise ValueError(\n            f\"Format {format} not supported, please provide a custom `prompt_formatting_fn`\"\n            \" or use any of the available formats: openai, llama2, chatml, zephyr\"\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/critique/","title":"critique","text":""},{"location":"reference/distilabel/tasks/critique/base/","title":"base","text":""},{"location":"reference/distilabel/tasks/critique/base/#distilabel.tasks.critique.base.CritiqueTask","title":"<code>CritiqueTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RatingToArgillaMixin</code>, <code>Task</code></p> <p>A <code>Task</code> for critique / judge tasks.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/tasks/critique/base.py</code> <pre><code>@dataclass\nclass CritiqueTask(RatingToArgillaMixin, Task):\n    \"\"\"A `Task` for critique / judge tasks.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n    \"\"\"\n\n    __type__: ClassVar[Literal[\"labelling\"]] = \"labelling\"\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the input arguments of the task.\"\"\"\n        return [\"input\", \"generations\"]\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"critique\", \"score\"]\n\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        score_column: str = \"score\",\n        critique_column: str = \"critique\",\n        score_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackDataset\":\n        return super().to_argilla_dataset(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            ratings_column=score_column,\n            rationale_column=critique_column,\n            ratings_values=score_values or [1, 2, 3, 4, 5],\n        )\n\n    def to_argilla_record(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        score_column: str = \"score\",\n        critique_column: str = \"critique\",\n        score_values: Optional[List[int]] = None,\n    ) -&gt; Union[\"FeedbackRecord\", List[\"FeedbackRecord\"]]:\n        return super().to_argilla_record(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            ratings_column=score_column,\n            rationale_column=critique_column,\n            ratings_values=score_values or [1, 2, 3, 4, 5],\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/critique/base/#distilabel.tasks.critique.base.CritiqueTask.input_args_names","title":"<code>input_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the input arguments of the task.</p>"},{"location":"reference/distilabel/tasks/critique/base/#distilabel.tasks.critique.base.CritiqueTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/critique/base/#distilabel.tasks.critique.base.CritiqueTaskOutput","title":"<code>CritiqueTaskOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> matching the output format of any <code>CritiqueTask</code>.</p> Source code in <code>src/distilabel/tasks/critique/base.py</code> <pre><code>class CritiqueTaskOutput(TypedDict):\n    \"\"\"A `TypedDict` matching the output format of any `CritiqueTask`.\"\"\"\n\n    score: float\n    critique: str\n</code></pre>"},{"location":"reference/distilabel/tasks/critique/prometheus/","title":"prometheus","text":""},{"location":"reference/distilabel/tasks/critique/prometheus/#distilabel.tasks.critique.prometheus.PrometheusTask","title":"<code>PrometheusTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>CritiqueTask</code></p> <p>A <code>CritiqueTask</code> following the prompt templated used by Prometheus.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'You are a fair evaluator language model.'</code> <code>scoring_criteria</code> <code>str</code> <p>the scoring criteria to be used for the task, that defines the scores below, provided via <code>score_descriptions</code>.</p> required <code>score_descriptions</code> <code>Dict[int, str]</code> <p>the descriptions of the scores, where the key is the rating value (ideally those should be consecutive), and the value is the description of each rating.</p> required Disclaimer <p>Since the Prometheus model has been trained with OpenAI API generated data, the prompting strategy may just be consistent / compliant with either GPT-3.5 or GPT-4 from OpenAI API, or with their own model. Any other model may fail on the generation of a structured output, as well as providing an incorrect / inaccurate critique.</p> References <ul> <li><code>Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</code></li> <li><code>kaist-ai/prometheus-13b-v1.0</code></li> <li><code>kaist-ai/prometheus-13b-v1.0</code></li> </ul> Source code in <code>src/distilabel/tasks/critique/prometheus.py</code> <pre><code>@dataclass\nclass PrometheusTask(CritiqueTask):\n    \"\"\"A `CritiqueTask` following the prompt templated used by Prometheus.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        scoring_criteria (str): the scoring criteria to be used for the task, that defines\n            the scores below, provided via `score_descriptions`.\n        score_descriptions (Dict[int, str]): the descriptions of the scores, where\n            the key is the rating value (ideally those should be consecutive), and the\n            value is the description of each rating.\n\n    Disclaimer:\n        Since the Prometheus model has been trained with OpenAI API generated data, the prompting\n        strategy may just be consistent / compliant with either GPT-3.5 or GPT-4 from OpenAI API, or\n        with their own model. Any other model may fail on the generation of a structured output, as\n        well as providing an incorrect / inaccurate critique.\n\n    References:\n        - [`Prometheus: Inducing Fine-grained Evaluation Capability in Language Models`](https://arxiv.org/abs/2310.08491)\n        - [`kaist-ai/prometheus-13b-v1.0`](https://huggingface.co/kaist-ai/prometheus-7b-v1.0)\n        - [`kaist-ai/prometheus-13b-v1.0`](https://huggingface.co/kaist-ai/prometheus-13b-v1.0)\n    \"\"\"\n\n    scoring_criteria: str\n    score_descriptions: Dict[int, str]\n\n    system_prompt: str = \"You are a fair evaluator language model.\"\n\n    __jinja2_template__: ClassVar[str] = _PROMETHEUS_TEMPLATE\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        return super().input_args_names + [\"ref_completion\"]\n\n    def generate_prompt(\n        self, input: str, generations: List[str], ref_completion: str, **_: Any\n    ) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Prometheus specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt, in\n                this case, the ones to be critiqued.\n            ref_completion (str): the reference completion to be used for the prompt,\n                which is the reference one, assuming the one with the highest score.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.critique import PrometheusTask\n            &gt;&gt;&gt; task = PrometheusTask(\n            ...     scoring_criteria=\"Overall quality of the responses provided.\",\n            ...     score_descriptions={0: \"false\", 1: \"partially false\", 2: \"average\", 3: \"partially true\", 4: \"true\"},\n            ... )\n            &gt;&gt;&gt; task.generate_prompt(\n            ...     input=\"What are the first 5 Fibonacci numbers?\",\n            ...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n            ...     ref_completion=\"0 1 1 2 3\",\n            ... )\n            Prompt(\n                system_prompt=\"You are a fair evaluator language model.\",\n                formatted_prompt=\"\"###Task Description:...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"instruction\": input,\n            \"completion\": generations,\n            \"ref_completion\": ref_completion,\n            \"scoring_criteria\": self.scoring_criteria,\n            \"score_descriptions\": self.score_descriptions,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; CritiqueTaskOutput:  # type: ignore\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        # We use a regex instead of splitting by the delimiter because the\n        # critique may contain the delimiter, and using the regex is safer.\n        pattern = r\"(.+?)\\. \\[RESULT\\] (\\d+)\"\n        match = re.search(pattern, output)\n        if match:\n            return CritiqueTaskOutput(\n                score=float(match.group(2)),\n                critique=match.group(1).strip(),\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/critique/prometheus/#distilabel.tasks.critique.prometheus.PrometheusTask.generate_prompt","title":"<code>generate_prompt(input, generations, ref_completion, **_)</code>","text":"<p>Generates a prompt following the Prometheus specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt, in this case, the ones to be critiqued.</p> required <code>ref_completion</code> <code>str</code> <p>the reference completion to be used for the prompt, which is the reference one, assuming the one with the highest score.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.critique import PrometheusTask\n&gt;&gt;&gt; task = PrometheusTask(\n...     scoring_criteria=\"Overall quality of the responses provided.\",\n...     score_descriptions={0: \"false\", 1: \"partially false\", 2: \"average\", 3: \"partially true\", 4: \"true\"},\n... )\n&gt;&gt;&gt; task.generate_prompt(\n...     input=\"What are the first 5 Fibonacci numbers?\",\n...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n...     ref_completion=\"0 1 1 2 3\",\n... )\nPrompt(\n    system_prompt=\"You are a fair evaluator language model.\",\n    formatted_prompt=\"\"###Task Description:...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/critique/prometheus.py</code> <pre><code>def generate_prompt(\n    self, input: str, generations: List[str], ref_completion: str, **_: Any\n) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Prometheus specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt, in\n            this case, the ones to be critiqued.\n        ref_completion (str): the reference completion to be used for the prompt,\n            which is the reference one, assuming the one with the highest score.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.critique import PrometheusTask\n        &gt;&gt;&gt; task = PrometheusTask(\n        ...     scoring_criteria=\"Overall quality of the responses provided.\",\n        ...     score_descriptions={0: \"false\", 1: \"partially false\", 2: \"average\", 3: \"partially true\", 4: \"true\"},\n        ... )\n        &gt;&gt;&gt; task.generate_prompt(\n        ...     input=\"What are the first 5 Fibonacci numbers?\",\n        ...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n        ...     ref_completion=\"0 1 1 2 3\",\n        ... )\n        Prompt(\n            system_prompt=\"You are a fair evaluator language model.\",\n            formatted_prompt=\"\"###Task Description:...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"instruction\": input,\n        \"completion\": generations,\n        \"ref_completion\": ref_completion,\n        \"scoring_criteria\": self.scoring_criteria,\n        \"score_descriptions\": self.score_descriptions,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/critique/prometheus/#distilabel.tasks.critique.prometheus.PrometheusTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/critique/prometheus.py</code> <pre><code>def parse_output(self, output: str) -&gt; CritiqueTaskOutput:  # type: ignore\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    # We use a regex instead of splitting by the delimiter because the\n    # critique may contain the delimiter, and using the regex is safer.\n    pattern = r\"(.+?)\\. \\[RESULT\\] (\\d+)\"\n    match = re.search(pattern, output)\n    if match:\n        return CritiqueTaskOutput(\n            score=float(match.group(2)),\n            critique=match.group(1).strip(),\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/critique/ultracm/","title":"ultracm","text":""},{"location":"reference/distilabel/tasks/critique/ultracm/#distilabel.tasks.critique.ultracm.UltraCMTask","title":"<code>UltraCMTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>CritiqueTask</code></p> <p>A <code>CritiqueTask</code> following the prompt templated used by UltraCM (from UltraFeedback).</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>\"User: A one-turn chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, very detailed, and polite answers to the user's questions.&lt;/s&gt;\"</code> Disclaimer <p>Since the UltraCM model has been trained with OpenAI API generated data, the prompting strategy may just be consistent / compliant with either GPT-3.5 or GPT-4 from OpenAI API, or with their own model. Any other model may fail on the generation of a structured output, as well as providing an incorrect / inaccurate critique.</p> References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>UltraFeedback - GitHub Repository</code></li> <li><code>openbmb/UltraCM-13b</code></li> </ul> Source code in <code>src/distilabel/tasks/critique/ultracm.py</code> <pre><code>@dataclass\nclass UltraCMTask(CritiqueTask):\n    \"\"\"A `CritiqueTask` following the prompt templated used by UltraCM (from UltraFeedback).\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n\n    Disclaimer:\n        Since the UltraCM model has been trained with OpenAI API generated data, the prompting\n        strategy may just be consistent / compliant with either GPT-3.5 or GPT-4 from OpenAI API, or\n        with their own model. Any other model may fail on the generation of a structured output, as\n        well as providing an incorrect / inaccurate critique.\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`UltraFeedback - GitHub Repository`](https://github.com/OpenBMB/UltraFeedback)\n        - [`openbmb/UltraCM-13b`](https://huggingface.co/openbmb/UltraCM-13b)\n    \"\"\"\n\n    __jinja2_template__: ClassVar[str] = _ULTRACM_TEMPLATE\n\n    system_prompt: str = (\n        \"User: A one-turn chat between a curious user and an artificial intelligence\"\n        \" assistant. The assistant gives helpful, very detailed, and polite answers to\"\n        \" the user's questions.&lt;/s&gt;\"\n    )\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the UltraCM specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt, in\n                this case, the ones to be critiqued.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.critique import UltraCMTask\n            &gt;&gt;&gt; task = UltraCMTask()\n            &gt;&gt;&gt; task.generate_prompt(\n            ...     input=\"What are the first 5 Fibonacci numbers?\",\n            ...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n            ... )\n            Prompt(\n                system_prompt=\"User: A one-turn chat between a curious user ...\",\n                formatted_prompt=\"User: Given my answer to an instruction, your role ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"instruction\": input,\n            \"completion\": generations,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=f\"User: {self.template.render(**render_kwargs)}&lt;/s&gt;\\nAssistant: ### Feedback\\nOverall Score: \",\n        )\n\n    def parse_output(self, output: str) -&gt; CritiqueTaskOutput:  # type: ignore\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        pattern = r\"(\\d+(?:\\.\\d+)?)\\s*(.*)\"\n        match = re.match(pattern, output)\n        if match:\n            return CritiqueTaskOutput(\n                score=float(match.group(1)),\n                critique=match.group(2).strip(),\n            )\n\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        score_column: str = \"score\",\n        critique_column: str = \"critique\",\n        score_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackDataset\":\n        return super().to_argilla_dataset(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            score_column=score_column,\n            critique_column=critique_column,\n            score_values=score_values or [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/critique/ultracm/#distilabel.tasks.critique.ultracm.UltraCMTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the UltraCM specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt, in this case, the ones to be critiqued.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.critique import UltraCMTask\n&gt;&gt;&gt; task = UltraCMTask()\n&gt;&gt;&gt; task.generate_prompt(\n...     input=\"What are the first 5 Fibonacci numbers?\",\n...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n... )\nPrompt(\n    system_prompt=\"User: A one-turn chat between a curious user ...\",\n    formatted_prompt=\"User: Given my answer to an instruction, your role ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/critique/ultracm.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the UltraCM specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt, in\n            this case, the ones to be critiqued.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.critique import UltraCMTask\n        &gt;&gt;&gt; task = UltraCMTask()\n        &gt;&gt;&gt; task.generate_prompt(\n        ...     input=\"What are the first 5 Fibonacci numbers?\",\n        ...     generations=[\"0 1 1 2 3\", \"0 1 1 2 3\"],\n        ... )\n        Prompt(\n            system_prompt=\"User: A one-turn chat between a curious user ...\",\n            formatted_prompt=\"User: Given my answer to an instruction, your role ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"instruction\": input,\n        \"completion\": generations,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=f\"User: {self.template.render(**render_kwargs)}&lt;/s&gt;\\nAssistant: ### Feedback\\nOverall Score: \",\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/critique/ultracm/#distilabel.tasks.critique.ultracm.UltraCMTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/critique/ultracm.py</code> <pre><code>def parse_output(self, output: str) -&gt; CritiqueTaskOutput:  # type: ignore\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    pattern = r\"(\\d+(?:\\.\\d+)?)\\s*(.*)\"\n    match = re.match(pattern, output)\n    if match:\n        return CritiqueTaskOutput(\n            score=float(match.group(1)),\n            critique=match.group(2).strip(),\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/","title":"preference","text":""},{"location":"reference/distilabel/tasks/preference/base/","title":"base","text":""},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask","title":"<code>PreferenceTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RatingToArgillaMixin</code>, <code>Task</code></p> <p>A <code>Task</code> for preference rating tasks.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>@dataclass\nclass PreferenceTask(RatingToArgillaMixin, Task):\n    \"\"\"A `Task` for preference rating tasks.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n    \"\"\"\n\n    __type__: ClassVar[Literal[\"labelling\"]] = \"labelling\"\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the input arguments of the task.\"\"\"\n        return [\"input\", \"generations\"]\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"rating\", \"rationale\"]\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask.input_args_names","title":"<code>input_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the input arguments of the task.</p>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTaskNoRationale","title":"<code>PreferenceTaskNoRationale</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>Task</code> for preference rating tasks, that only returns the rating, without the rationale.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>@dataclass\nclass PreferenceTaskNoRationale(PreferenceTask):\n    \"\"\"A `Task` for preference rating tasks, that only returns the rating, without the rationale.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n    \"\"\"\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"rating\"]\n\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        ratings_column: str = \"rating\",\n        ratings_values: Optional[List[int]] = None,\n    ):\n        \"\"\"Same definition from the parent class, but removing the rationale column.\"\"\"\n        return super().to_argilla_dataset(\n            dataset_row,\n            generations_column=generations_column,\n            ratings_column=ratings_column,\n            rationale_column=None,\n            ratings_values=ratings_values,\n        )\n\n    def to_argilla_record(  # noqa: C901\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        ratings_column: str = \"rating\",\n        ratings_values: Optional[List[int]] = None,\n    ):\n        \"\"\"Same definition from the parent class, but removing the rationale column.\"\"\"\n        return super().to_argilla_record(\n            dataset_row,\n            generations_column=generations_column,\n            ratings_column=ratings_column,\n            rationale_column=None,\n            ratings_values=ratings_values,\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTaskNoRationale.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTaskNoRationale.to_argilla_dataset","title":"<code>to_argilla_dataset(dataset_row, generations_column='generations', ratings_column='rating', ratings_values=None)</code>","text":"<p>Same definition from the parent class, but removing the rationale column.</p> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>def to_argilla_dataset(\n    self,\n    dataset_row: Dict[str, Any],\n    generations_column: str = \"generations\",\n    ratings_column: str = \"rating\",\n    ratings_values: Optional[List[int]] = None,\n):\n    \"\"\"Same definition from the parent class, but removing the rationale column.\"\"\"\n    return super().to_argilla_dataset(\n        dataset_row,\n        generations_column=generations_column,\n        ratings_column=ratings_column,\n        rationale_column=None,\n        ratings_values=ratings_values,\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTaskNoRationale.to_argilla_record","title":"<code>to_argilla_record(dataset_row, generations_column='generations', ratings_column='rating', ratings_values=None)</code>","text":"<p>Same definition from the parent class, but removing the rationale column.</p> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>def to_argilla_record(  # noqa: C901\n    self,\n    dataset_row: Dict[str, Any],\n    generations_column: str = \"generations\",\n    ratings_column: str = \"rating\",\n    ratings_values: Optional[List[int]] = None,\n):\n    \"\"\"Same definition from the parent class, but removing the rationale column.\"\"\"\n    return super().to_argilla_record(\n        dataset_row,\n        generations_column=generations_column,\n        ratings_column=ratings_column,\n        rationale_column=None,\n        ratings_values=ratings_values,\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/complexity_scorer/","title":"complexity_scorer","text":""},{"location":"reference/distilabel/tasks/preference/complexity_scorer/#distilabel.tasks.preference.complexity_scorer.ComplexityScorerTask","title":"<code>ComplexityScorerTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTaskNoRationale</code></p> <p>A <code>PreferenceTask</code> following the <code>Complexity Scorer</code> specification for rating instructions in terms of complexity.</p> <p>This task is inspired by the Evol Complexity Scorer in the Deita framework: Deita is an open-sourced project designed to facilitate Automatic Data Selection for instruction tuning in Large Language Models (LLMs).</p> <p>The task is defined as follows: Ask an LLM (in the original paper they used ChatGPT) to rate the instructions (the number of instructions is dynamic in the sense that you can compare any number, in Deita the chose 6) to obtain a complexity score c for each instruction.</p> <p>This task will only need to receive the list of <code>generations</code> in a dataset to generate the scores.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/complexity_scorer.py</code> <pre><code>@dataclass\nclass ComplexityScorerTask(PreferenceTaskNoRationale):\n    \"\"\"A `PreferenceTask` following the `Complexity Scorer` specification for rating instructions\n    in terms of complexity.\n\n    This task is inspired by the Evol Complexity Scorer in the Deita framework: *Deita is an open-sourced project\n    designed to facilitate Automatic Data Selection for instruction tuning in Large Language Models (LLMs).*\n\n    The task is defined as follows:\n    Ask an LLM (in the original paper they used ChatGPT) to rate the instructions (the number of instructions\n    is dynamic in the sense that you can compare any number, in *Deita* the chose 6) to obtain a complexity\n    score *c* for each instruction.\n\n    This task will only need to receive the list of `generations` in a dataset to generate the scores.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    system_prompt: str = \"\"\n\n    __jinja2_template__: str = _COMPLEXITY_SCORER_TEMPLATE\n\n    def generate_prompt(self, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the *Evol Complexity* specification in *Deita*.\n\n        Args:\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks import ComplexityScorerTask\n            &gt;&gt;&gt; task = ComplexityScorerTask()\n            &gt;&gt;&gt; task.generate_prompt([\"instruction 1\", \"instruction 2\"])\n            Prompt(system_prompt=\"\", formatted_prompt=\"Ranking the following questions...\")\n        \"\"\"\n        render_kwargs = {\"instructions\": generations}\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the input arguments of the task.\"\"\"\n        return [\"generations\"]\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the task, returning a list with the rank/score of each instruction.\n\n        Args:\n            output (str): The output of the LLM raw.\n\n        Returns:\n            Dict[str, List[str]]: A dict with containing the ranks/scores of each instruction.\n        \"\"\"\n        output = output.lower().split(\"\\n\")\n        scores = [float(re.sub(r\"\\[\\d+\\] score:\", \"\", o).strip()) for o in output]\n        return {self.output_args_names[0]: scores}\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/complexity_scorer/#distilabel.tasks.preference.complexity_scorer.ComplexityScorerTask.input_args_names","title":"<code>input_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the input arguments of the task.</p>"},{"location":"reference/distilabel/tasks/preference/complexity_scorer/#distilabel.tasks.preference.complexity_scorer.ComplexityScorerTask.generate_prompt","title":"<code>generate_prompt(generations, **_)</code>","text":"<p>Generates a prompt following the Evol Complexity specification in Deita.</p> <p>Parameters:</p> Name Type Description Default <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks import ComplexityScorerTask\n&gt;&gt;&gt; task = ComplexityScorerTask()\n&gt;&gt;&gt; task.generate_prompt([\"instruction 1\", \"instruction 2\"])\nPrompt(system_prompt=\"\", formatted_prompt=\"Ranking the following questions...\")\n</code></pre> Source code in <code>src/distilabel/tasks/preference/complexity_scorer.py</code> <pre><code>def generate_prompt(self, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the *Evol Complexity* specification in *Deita*.\n\n    Args:\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks import ComplexityScorerTask\n        &gt;&gt;&gt; task = ComplexityScorerTask()\n        &gt;&gt;&gt; task.generate_prompt([\"instruction 1\", \"instruction 2\"])\n        Prompt(system_prompt=\"\", formatted_prompt=\"Ranking the following questions...\")\n    \"\"\"\n    render_kwargs = {\"instructions\": generations}\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/complexity_scorer/#distilabel.tasks.preference.complexity_scorer.ComplexityScorerTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the task, returning a list with the rank/score of each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output of the LLM raw.</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict[str, List[str]]: A dict with containing the ranks/scores of each instruction.</p> Source code in <code>src/distilabel/tasks/preference/complexity_scorer.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the task, returning a list with the rank/score of each instruction.\n\n    Args:\n        output (str): The output of the LLM raw.\n\n    Returns:\n        Dict[str, List[str]]: A dict with containing the ranks/scores of each instruction.\n    \"\"\"\n    output = output.lower().split(\"\\n\")\n    scores = [float(re.sub(r\"\\[\\d+\\] score:\", \"\", o).strip()) for o in output]\n    return {self.output_args_names[0]: scores}\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/judgelm/","title":"judgelm","text":""},{"location":"reference/distilabel/tasks/preference/judgelm/#distilabel.tasks.preference.judgelm.JudgeLMOutput","title":"<code>JudgeLMOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> matching the output format of JudgeLM.</p> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>class JudgeLMOutput(TypedDict):\n    \"\"\"A `TypedDict` matching the output format of JudgeLM.\"\"\"\n\n    rating: List[float]\n    rationale: str\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/judgelm/#distilabel.tasks.preference.judgelm.JudgeLMTask","title":"<code>JudgeLMTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> following the prompt templated used by JudgeLM.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'You are a helpful and precise assistant for checking the quality of the answer.'</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>'We would like to request your feedback on the performance of {num_responses} AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only {num_responses} values indicating the scores for Assistants 1 to {num_responses}, respectively. The {num_responses} scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.'</code> References <ul> <li><code>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</code></li> <li><code>BAAI/JudgeLM-7B-v1.0</code></li> <li><code>BAAI/JudgeLM-13B-v1.0</code></li> <li><code>BAAI/JudgeLM-33B-v1.0</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>@dataclass\nclass JudgeLMTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` following the prompt templated used by JudgeLM.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n\n    References:\n        - [`Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena`](https://arxiv.org/abs/2306.05685)\n        - [`BAAI/JudgeLM-7B-v1.0`](https://huggingface.co/BAAI/JudgeLM-7B-v1.0)\n        - [`BAAI/JudgeLM-13B-v1.0`](https://huggingface.co/BAAI/JudgeLM-13B-v1.0)\n        - [`BAAI/JudgeLM-33B-v1.0`](https://huggingface.co/BAAI/JudgeLM-33B-v1.0)\n    \"\"\"\n\n    task_description: str = (\n        \"We would like to request your feedback on the performance of {num_responses} AI assistants in response to the\"\n        \" user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details\"\n        \" of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher\"\n        \" score indicates better overall performance.\\nPlease first output a single line containing only {num_responses}\"\n        \" values indicating the scores for Assistants 1 to {num_responses}, respectively. The {num_responses} scores are separated by\"\n        \" a space. In the subsequent line, please provide a comprehensive explanation of your evaluation,\"\n        \" avoiding any potential bias and ensuring that the order in which the responses were presented does\"\n        \" not affect your judgment.\"\n    )\n    system_prompt: str = \"You are a helpful and precise assistant for checking the quality of the answer.\"\n\n    __jinja2_template__: ClassVar[str] = _JUDGELM_TEMPLATE\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the JudgeLM specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n            &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"[Question] What are the first 5 Fibonacci numbers? ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"input\": input,\n            \"responses\": generations,\n            \"task_description\": self.task_description.format(\n                num_responses=len(generations)\n            ),\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; JudgeLMOutput:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        split_output = output.split(\"\\n\")\n        rating = [float(rating) for rating in split_output[0].split(\" \")]\n        rationale = \"\\n\".join(split_output[1:])\n        return JudgeLMOutput(rating=rating, rationale=rationale)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/judgelm/#distilabel.tasks.preference.judgelm.JudgeLMTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the JudgeLM specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n&gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"You are a helpful assistant.\",\n    formatted_prompt=\"[Question] What are the first 5 Fibonacci numbers? ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the JudgeLM specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n        &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"[Question] What are the first 5 Fibonacci numbers? ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"input\": input,\n        \"responses\": generations,\n        \"task_description\": self.task_description.format(\n            num_responses=len(generations)\n        ),\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/judgelm/#distilabel.tasks.preference.judgelm.JudgeLMTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>def parse_output(self, output: str) -&gt; JudgeLMOutput:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    split_output = output.split(\"\\n\")\n    rating = [float(rating) for rating in split_output[0].split(\" \")]\n    rationale = \"\\n\".join(split_output[1:])\n    return JudgeLMOutput(rating=rating, rationale=rationale)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/quality_scorer/","title":"quality_scorer","text":""},{"location":"reference/distilabel/tasks/preference/quality_scorer/#distilabel.tasks.preference.quality_scorer.QualityScorerTask","title":"<code>QualityScorerTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTaskNoRationale</code></p> <p>A <code>PreferenceTask</code> following the <code>Quality Scorer</code> specification for rating instructions in terms of quality.</p> <p>This task is inspired by the Evol Quality Scorer in the Deita framework: Deita is an open-sourced project designed to facilitate Automatic Data Selection for instruction tuning in Large Language Models (LLMs).</p> <p>The task follows the same scheme as the Evol Complexity Scorer, but the instructions are scored in terms of quality, obtaining a quality score q for each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/quality_scorer.py</code> <pre><code>@dataclass\nclass QualityScorerTask(PreferenceTaskNoRationale):\n    \"\"\"A `PreferenceTask` following the `Quality Scorer` specification for rating instructions\n    in terms of quality.\n\n    This task is inspired by the Evol Quality Scorer in the Deita framework: *Deita is an open-sourced project\n    designed to facilitate Automatic Data Selection for instruction tuning in Large Language Models (LLMs).*\n\n    The task follows the same scheme as the Evol Complexity Scorer, but the instructions are scored in terms of\n    quality, obtaining a quality score *q* for each instruction.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    system_prompt: str = \"\"\n    task_description: str = \"\"\"Your evaluation should consider factors such as helpfulness, relevance, accuracy, depth,\ncreativity, and level of detail of the response.\"\"\"\n    __jinja2_template__: str = _QUALITY_SCORER_TEMPLATE\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the *Evol Quality* specification in *Deita*.\n\n        Args:\n            input (str): the instruction for which the model will score the responses.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import QualityScorerTask\n            &gt;&gt;&gt; task = QualityScorerTask()\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"\",\n                formatted_prompt=\"Rank the following responses provided ...\"\n            )\n        \"\"\"\n        render_kwargs = {\n            \"instruction\": input,\n            \"responses\": generations,\n            \"task_description\": self.task_description,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the task, returning a list with the rating of each instruction.\n\n        Args:\n            output (str): The output of the LLM raw.\n\n        Returns:\n            Dict[str, List[str]]: A dict with containing the ratings of each instruction.\n        \"\"\"\n        output = output.lower().split(\"\\n\")\n        scores = [\n            float(re.sub(r\"\\[response \\d+\\] score:\", \"\", o).strip()) for o in output\n        ]\n        return {self.output_args_names[0]: scores}\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/quality_scorer/#distilabel.tasks.preference.quality_scorer.QualityScorerTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the Evol Quality specification in Deita.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the instruction for which the model will score the responses.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import QualityScorerTask\n&gt;&gt;&gt; task = QualityScorerTask()\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"\",\n    formatted_prompt=\"Rank the following responses provided ...\"\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/quality_scorer.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the *Evol Quality* specification in *Deita*.\n\n    Args:\n        input (str): the instruction for which the model will score the responses.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import QualityScorerTask\n        &gt;&gt;&gt; task = QualityScorerTask()\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"\",\n            formatted_prompt=\"Rank the following responses provided ...\"\n        )\n    \"\"\"\n    render_kwargs = {\n        \"instruction\": input,\n        \"responses\": generations,\n        \"task_description\": self.task_description,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/quality_scorer/#distilabel.tasks.preference.quality_scorer.QualityScorerTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the task, returning a list with the rating of each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output of the LLM raw.</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict[str, List[str]]: A dict with containing the ratings of each instruction.</p> Source code in <code>src/distilabel/tasks/preference/quality_scorer.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the task, returning a list with the rating of each instruction.\n\n    Args:\n        output (str): The output of the LLM raw.\n\n    Returns:\n        Dict[str, List[str]]: A dict with containing the ratings of each instruction.\n    \"\"\"\n    output = output.lower().split(\"\\n\")\n    scores = [\n        float(re.sub(r\"\\[response \\d+\\] score:\", \"\", o).strip()) for o in output\n    ]\n    return {self.output_args_names[0]: scores}\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/","title":"ultrafeedback","text":""},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.Rating","title":"<code>Rating</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> representing a rating.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>class Rating(TypedDict):\n    \"\"\"A `TypedDict` representing a rating.\"\"\"\n\n    value: int\n    description: str\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackOutput","title":"<code>UltraFeedbackOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> representing the output of an <code>UltraFeedbackTask</code>.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>class UltraFeedbackOutput(TypedDict):\n    \"\"\"A `TypedDict` representing the output of an `UltraFeedbackTask`.\"\"\"\n\n    rating: float\n    rationale: str\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackTask","title":"<code>UltraFeedbackTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> following the prompt template used by ULTRAFEEDBACK.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'Your role is to evaluate text quality based on given criteria.'</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> required <code>ratings</code> <code>Union[List[Rating], None]</code> <p>the ratings to be used for the task. Defaults to <code>None</code>.</p> required References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>UltraFeedback - GitHub Repository</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>@dataclass\nclass UltraFeedbackTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` following the prompt template used by ULTRAFEEDBACK.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n        ratings (Union[List[Rating], None], optional): the ratings to be used for the task. Defaults to `None`.\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`UltraFeedback - GitHub Repository`](https://github.com/OpenBMB/UltraFeedback)\n    \"\"\"\n\n    ratings: List[Rating]\n    task_description: str\n\n    system_prompt: (\n        str\n    ) = \"Your role is to evaluate text quality based on given criteria.\"\n\n    __jinja2_template__: ClassVar[str] = field(\n        default=_ULTRAFEEDBACK_TEMPLATE, init=False, repr=False\n    )\n    __subtasks__: ClassVar[List[str]] = [\n        \"text-quality\",\n        \"helpfulness\",\n        \"truthfulness\",\n        \"honesty\",\n        \"instruction-following\",\n    ]\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the ULTRAFEEDBACK specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n            &gt;&gt;&gt; task = UltraFeedbackTask.for_overall_quality()\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n                formatted_prompt=\"# General Text Quality Assessment...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"task_description\": self.task_description,\n            \"ratings\": self.ratings,\n            \"input\": input,\n            \"responses\": generations,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; List[UltraFeedbackOutput]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        parsed_output = []\n        for section in output.split(\"#### Output for Text \")[1:]:\n            rating, rationale = section.split(\"\\n\")[1:3]\n            rating = float(rating.split(\": \")[1])\n            rationale = rationale.split(\": \")[1]\n            parsed_output.append(\n                UltraFeedbackOutput(rating=rating, rationale=rationale)\n            )\n        return parsed_output\n\n    # Override the default `to_argilla_dataset` method to provide the `ratings_values` of\n    # UltraFeedback, as the default goes from 1-10 while UltraFeedback's default is 1-5\n    # (0-4 actually, but Argilla doesn't support 0s).\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        ratings_column: str = \"rating\",\n        rationale_column: str = \"rationale\",\n        ratings_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackDataset\":\n        return super().to_argilla_dataset(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            ratings_column=ratings_column,\n            rationale_column=rationale_column,\n            ratings_values=ratings_values or [1, 2, 3, 4, 5],\n        )\n\n    # Override the default `to_argilla_record` method to provide the `ratings_values` of\n    # UltraFeedback, as the default goes from 1-10 while UltraFeedback's default is 1-5\n    # (0-4 actually, but Argilla doesn't support 0s).\n    def to_argilla_record(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: str = \"generations\",\n        ratings_column: str = \"rating\",\n        rationale_column: str = \"rationale\",\n        ratings_values: Optional[List[int]] = None,\n    ) -&gt; \"FeedbackRecord\":\n        return super().to_argilla_record(\n            dataset_row=dataset_row,\n            generations_column=generations_column,\n            ratings_column=ratings_column,\n            rationale_column=rationale_column,\n            ratings_values=ratings_values or [1, 2, 3, 4, 5],\n        )\n\n    @classmethod\n    def for_overall_quality(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        \"\"\"Classmethod for the `UltraFeedbackTask` subtask defined by Argilla, in order to\n        evaluate all the criterias originally defined in UltraFeedback at once, in a single\n        subtask.\n        \"\"\"\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # General Text Quality Assessment\n                Evaluate the model's outputs based on various criteria:\n                1. **Correctness &amp; Informativeness**: Does the output provide accurate and helpful information?\n                2. **Honesty &amp; Uncertainty**: How confidently does the model convey its information, and does it express uncertainty appropriately?\n                3. **Truthfulness &amp; Hallucination**: Does the model introduce misleading or fabricated details?\n                4. **Instruction Following**: Does the model's output align with given instructions and the user's intent?\n                Your role is to provide a holistic assessment considering all the above factors.\n\n                **Scoring**: Rate outputs 1 to 5 based on the overall quality, considering all aspects:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Low Quality**: Contains inaccuracies, may be entirely wrong or has severe hallucinations.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Moderate Quality**: Addresses some aspects, but has errors or is partially aligned with instructions.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Good**: Generally accurate but may contain minor errors or slight deviations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Very Good**: Near perfect, with minor issues in terms of alignment or confidence.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Excellent**: Accurate, confident, aligned with instructions, and free of hallucinations.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_helpfulness(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Informativeness / Helpfulness Assessment\n                Evaluate if model's outputs fulfill task objectives and provide high-quality, correct, and, informative content.\n                Helpfulness assessment emphasizes **Overall Quality** regarding correctness and informativeness.\n                **Correctness**: Accurate computation, reasoning steps, and outputs without misunderstandings or fabrication.\n\n                **Scoring**: Score 1 to 5 based on extent of helpfulness, regarding both informativeness and correctness:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Severely Incorrect**: Contains significant inaccuracies or fabricated content, even if comprehensive information is provided.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Partially Incorrect**: Contains errors that may cause confusion, even though comprehensive information is present.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Correct**: Accurate and provides useful information that meets the task's requirements.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Highly Informative**: Accurate and extensive, providing valuable insights and detailed information.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Outstandingly Helpful**: Both accurate and in-depth, offering profound insights and comprehensive information.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_truthfulness(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Truthfulness and Hallucination Assessment\n                Evaluate the model's accuracy in providing information without introducing misleading or fabricated details.\n                Assign numeric identifier (or \"None\") from 1 to 3 for each type of hallucination:\n                1. **Contradictory with the World (Factual Error)**: Entities, locations, concepts, or events that conflict with established knowledge.\n                2. **Contradictory with Instruction and Input**: Responses diverge, introducing new facts not aligned with instructions or inputs.\n                3. **Self-Contradictory / Logical Error**: Responses contain internal contradictions or logical errors within each independent text.\n\n                **Scoring**: Rate outputs 1 to 5 based on the extent of hallucination:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Completely Hallucinated**: Entirely unreliable due to hallucinations.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Severe Hallucination**: Nearly half contains hallucinations, severe deviation from main points.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Partial Hallucination / Misunderstanding**: Overall truthful, partial misunderstanding due to hallucinations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Insignificant Hallucination**: Mostly truthful, slight hallucination not affecting main points.\",\n                ),\n                Rating(\n                    value=5, description=\"**No Hallucination**: Free of hallucinations.\"\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_honesty(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Honesty and Uncertainty Expression Assessment\n                Assess how well the model conveys honesty and uncertainty. Evaluate if the model's confidence aligns with response correctness.\n                - **Indicators of Uncertainty**:\n                    1. Weakeners: e.g., 'I guess,' 'probably.'\n                    2. Refusal to answer.\n                    3. Verbalized confidence scores: [0, 20] low; (20, 40] uncertain; (40, 60] moderate; (60, 80] leaning confident; (80, 100] high.\n                - **No uncertainty expression indicate confidence.**\n                - **Response Correctness**: Align with ground truth, or provide accurate content without fabrication.\n\n                **Scoring**: Rate outputs 1 to 5 based on the extent of honesty and uncertainty:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Confidently Incorrect**: Confident but entirely wrong.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Confident with Significant Mistakes / Unconfident Incorrect**: Confident but contains major errors. Unconfident and entirely wrong.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Uncertain / 'I Don't Know' / Subtle Mistakes**: 'I don't know' or declines. Confident but contains minor errors. Unconfident and contains significant mistakes.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Correct but Uncertain / Expressed Subtle Mistakes**: Correct but unconfident.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Correct and Confident / Precisely Express Uncertainty**: Correct and confident. Makes mistakes, but precisely acknowledges minor errors and indicates uncertainty on potential mistakes.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n\n        return cls(**kwargs)\n\n    @classmethod\n    def for_instruction_following(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Instruction Following Assessment\n                Evaluate alignment between output and intent. Assess understanding of task goal and restrictions.\n                **Instruction Components**: Task Goal (intended outcome), Restrictions (text styles, formats, or designated methods, etc).\n\n                **Scoring**: Rate outputs 1 to 5:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(value=1, description=\"**Irrelevant**: No alignment.\"),\n                Rating(\n                    value=2,\n                    description=\"**Partial Focus**: Addresses one aspect poorly.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Partial Compliance**:\\n\\t- (1) Meets goal or restrictions, neglecting other.\\n\\t- (2) Acknowledges both but slight deviations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Almost There**: Near alignment, minor deviations.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Comprehensive Compliance**: Fully aligns, meets all requirements.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n\n        return cls(**kwargs)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackTask.for_overall_quality","title":"<code>for_overall_quality(system_prompt=None, task_description=None, ratings=None)</code>  <code>classmethod</code>","text":"<p>Classmethod for the <code>UltraFeedbackTask</code> subtask defined by Argilla, in order to evaluate all the criterias originally defined in UltraFeedback at once, in a single subtask.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>@classmethod\ndef for_overall_quality(\n    cls,\n    system_prompt: Optional[str] = None,\n    task_description: Optional[str] = None,\n    ratings: Optional[List[Rating]] = None,\n) -&gt; \"UltraFeedbackTask\":\n    \"\"\"Classmethod for the `UltraFeedbackTask` subtask defined by Argilla, in order to\n    evaluate all the criterias originally defined in UltraFeedback at once, in a single\n    subtask.\n    \"\"\"\n    kwargs = {}\n    if system_prompt is not None:\n        kwargs.update({\"system_prompt\": system_prompt})\n    if task_description is None:\n        task_description = dedent(\n            \"\"\"\n            # General Text Quality Assessment\n            Evaluate the model's outputs based on various criteria:\n            1. **Correctness &amp; Informativeness**: Does the output provide accurate and helpful information?\n            2. **Honesty &amp; Uncertainty**: How confidently does the model convey its information, and does it express uncertainty appropriately?\n            3. **Truthfulness &amp; Hallucination**: Does the model introduce misleading or fabricated details?\n            4. **Instruction Following**: Does the model's output align with given instructions and the user's intent?\n            Your role is to provide a holistic assessment considering all the above factors.\n\n            **Scoring**: Rate outputs 1 to 5 based on the overall quality, considering all aspects:\n            \"\"\"\n        )\n    kwargs.update({\"task_description\": task_description})\n\n    if ratings is None:\n        ratings = [\n            Rating(\n                value=1,\n                description=\"**Low Quality**: Contains inaccuracies, may be entirely wrong or has severe hallucinations.\",\n            ),\n            Rating(\n                value=2,\n                description=\"**Moderate Quality**: Addresses some aspects, but has errors or is partially aligned with instructions.\",\n            ),\n            Rating(\n                value=3,\n                description=\"**Good**: Generally accurate but may contain minor errors or slight deviations.\",\n            ),\n            Rating(\n                value=4,\n                description=\"**Very Good**: Near perfect, with minor issues in terms of alignment or confidence.\",\n            ),\n            Rating(\n                value=5,\n                description=\"**Excellent**: Accurate, confident, aligned with instructions, and free of hallucinations.\",\n            ),\n        ]\n    kwargs.update({\"ratings\": ratings})\n    return cls(**kwargs)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the ULTRAFEEDBACK specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n&gt;&gt;&gt; task = UltraFeedbackTask.for_overall_quality()\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n    formatted_prompt=\"# General Text Quality Assessment...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the ULTRAFEEDBACK specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n        &gt;&gt;&gt; task = UltraFeedbackTask.for_overall_quality()\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n            formatted_prompt=\"# General Text Quality Assessment...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"task_description\": self.task_description,\n        \"ratings\": self.ratings,\n        \"input\": input,\n        \"responses\": generations,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>def parse_output(self, output: str) -&gt; List[UltraFeedbackOutput]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    parsed_output = []\n    for section in output.split(\"#### Output for Text \")[1:]:\n        rating, rationale = section.split(\"\\n\")[1:3]\n        rating = float(rating.split(\": \")[1])\n        rationale = rationale.split(\": \")[1]\n        parsed_output.append(\n            UltraFeedbackOutput(rating=rating, rationale=rationale)\n        )\n    return parsed_output\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/","title":"ultrajudge","text":""},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.Area","title":"<code>Area</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> representing an area of evaluation.</p> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>class Area(TypedDict):\n    \"\"\"A `TypedDict` representing an area of evaluation.\"\"\"\n\n    rating: float\n    rationale: str\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeOutput","title":"<code>UltraJudgeOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> representing the output of the UltraJudge task.</p> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>class UltraJudgeOutput(TypedDict):\n    \"\"\"A `TypedDict` representing the output of the UltraJudge task.\"\"\"\n\n    rating: float\n    areas: Dict[str, Area]\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask","title":"<code>UltraJudgeTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> for the UltraJudge task. The <code>UltraJudge</code> task has been defined at Argilla specifically for a better evaluation using AI Feedback. The task is defined based on both UltraFeedback and JudgeLM, but with several improvements / modifications.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>\"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences. Your critical analysis should focus on human-like engagement, solution effectiveness, accuracy, clarity, and creativity. Approach each response as if you were the user, considering how well the response meets your needs and expectations in a real-world scenario. Provide detailed feedback that highlights strengths and areas for improvement in each response, keeping in mind the goal of simulating a human's preferred choice. Your evaluation should be impartial and thorough, reflecting a human's perspective in preferring responses that are practical, clear, authentic, and aligned with their intent. Avoid bias, and focus on the content and quality of the responses.\"</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>\"Your task is to rigorously evaluate the performance of {num_responses} AI assistants, simulating a human's perspective. You will assess each response based on four key domains, reflecting aspects that are typically valued by humans: {areas}. First provide a score between 0 and 10 and write a detailed feedback for each area and assistant. Finally, provide a list of {num_responses} scores, each separated by a space, to reflect the performance of Assistants 1 to {num_responses}.\"</code> <code>areas</code> <code>List[str]</code> <p>the areas to be used for the task. Defaults to a list of four areas: \"Practical Accuracy\", \"Clarity &amp; Transparency\", \"Authenticity &amp; Reliability\", and \"Compliance with Intent\".</p> <code>field(default_factory=lambda : ['Practical Accuracy', 'Clarity &amp; Transparency', 'Authenticity &amp; Reliability', 'Compliance with Intent'])</code> References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</code></li> </ul> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>@dataclass\nclass UltraJudgeTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` for the UltraJudge task. The `UltraJudge` task has been defined\n    at Argilla specifically for a better evaluation using AI Feedback. The task is defined\n    based on both UltraFeedback and JudgeLM, but with several improvements / modifications.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n        areas (List[str], optional): the areas to be used for the task. Defaults to a list of four areas:\n            \"Practical Accuracy\", \"Clarity &amp; Transparency\", \"Authenticity &amp; Reliability\", and \"Compliance with Intent\".\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena`](https://arxiv.org/abs/2306.05685)\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences.\"\n        \" Your critical analysis should focus on human-like engagement, solution effectiveness, accuracy, clarity, and\"\n        \" creativity. Approach each response as if you were the user, considering how well the response meets your needs\"\n        \" and expectations in a real-world scenario. Provide detailed feedback that highlights strengths and areas for\"\n        \" improvement in each response, keeping in mind the goal of simulating a human's preferred choice. \"\n        \"Your evaluation should be impartial and thorough, reflecting a human's perspective in preferring responses that are practical,\"\n        \" clear, authentic, and aligned with their intent. Avoid bias, and focus on the content and quality of the responses.\"\n    )\n\n    task_description: str = (\n        \"Your task is to rigorously evaluate the performance of {num_responses} AI assistants, simulating a human's perspective.\"\n        \" You will assess each response based on four key domains, reflecting aspects that are typically valued by humans:\"\n        \" {areas}.\"\n        \" First provide a score between 0 and 10 and write a detailed feedback for each area and assistant.\"\n        \" Finally, provide a list of {num_responses} scores, each separated by a space, to reflect the performance of Assistants 1 to {num_responses}.\"\n    )\n\n    areas: List[str] = field(\n        default_factory=lambda: [\n            \"Practical Accuracy\",\n            \"Clarity &amp; Transparency\",\n            \"Authenticity &amp; Reliability\",\n            \"Compliance with Intent\",\n        ]\n    )\n\n    __jinja2_template__: ClassVar[str] = field(\n        default=_ULTRAJUDGE_TEMPLATE, init=False, repr=False\n    )\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"rating\", \"areas\"]\n\n    @property\n    def areas_str(self) -&gt; str:\n        \"\"\"Returns a string representation of the areas.\"\"\"\n        return \", \".join(self.areas[:-1]) + \", and \" + self.areas[-1]\n\n    @property\n    def extract_area_score_and_rationale_regex(self) -&gt; str:\n        \"\"\"Returns a regex to extract the area, score, and rationale from the output.\"\"\"\n        return rf\"({'|'.join(self.areas)})\\s*-\\s*(\\d+(?:\\.\\d+)?)\\n(.*?)(?=\\n\\n|\\Z)\"\n\n    @property\n    def extract_final_scores_regex(self) -&gt; str:\n        \"\"\"Returns a regex to extract the final scores from the output.\"\"\"\n        return r\"Final scores:\\s*((?:\\d+(?:\\.\\d+)?\\s*)+)\"\n\n    def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the UltraJudge specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n            &gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"task_description\": self.task_description.format(\n                num_responses=len(generations), areas=self.areas_str\n            ),\n            \"instruction\": input,\n            \"responses\": generations,\n        }\n\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; List[UltraJudgeOutput]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        num_areas = len(self.areas)\n        # `areas_results` includes num_generations * num_areas tuples\n        areas_results = re.findall(self.extract_area_score_and_rationale_regex, output)\n        final_scores = [\n            float(str_score)\n            for str_score in re.findall(self.extract_final_scores_regex, output)[\n                0\n            ].split(\" \")\n        ]\n\n        outputs = []\n        for i, rating in enumerate(final_scores):\n            areas = {}\n            # Get the areas for the i-th generation\n            for area in areas_results[i * num_areas : i * num_areas + num_areas]:\n                name, area_rating, rationale = area\n                areas[name] = Area(rating=area_rating, rationale=rationale)\n            outputs.append(UltraJudgeOutput(rating=rating, areas=areas))\n\n        return outputs\n\n    def _merge_rationales(\n        self, rationales: List[Dict[str, Any]], generations_column: str = \"generations\"\n    ) -&gt; str:\n        \"\"\"Overwrite of the `_merge_rationales` as we need to process the areas before merging.\"\"\"\n\n        def format_area(area: Dict[str, Any]) -&gt; str:\n            sections = []\n            for title, ratings in area.items():\n                sections.append(title)\n                for k, v in ratings.items():\n                    sections.append(f\"{k}:{v}\")\n            return \"\\n\".join(sections)\n\n        merged_rationales = []\n        for idx, area in enumerate(rationales, start=1):\n            merged_rationales.append(\n                f\"{generations_column}-{idx}:\\n{format_area(area)}\\n\"\n            )\n        return \"\\n\".join(merged_rationales)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.areas_str","title":"<code>areas_str: str</code>  <code>property</code>","text":"<p>Returns a string representation of the areas.</p>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.extract_area_score_and_rationale_regex","title":"<code>extract_area_score_and_rationale_regex: str</code>  <code>property</code>","text":"<p>Returns a regex to extract the area, score, and rationale from the output.</p>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.extract_final_scores_regex","title":"<code>extract_final_scores_regex: str</code>  <code>property</code>","text":"<p>Returns a regex to extract the final scores from the output.</p>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.generate_prompt","title":"<code>generate_prompt(input, generations, **_)</code>","text":"<p>Generates a prompt following the UltraJudge specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n&gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"You are a helpful assistant.\",\n    formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str], **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the UltraJudge specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n        &gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"task_description\": self.task_description.format(\n            num_responses=len(generations), areas=self.areas_str\n        ),\n        \"instruction\": input,\n        \"responses\": generations,\n    }\n\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>def parse_output(self, output: str) -&gt; List[UltraJudgeOutput]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    num_areas = len(self.areas)\n    # `areas_results` includes num_generations * num_areas tuples\n    areas_results = re.findall(self.extract_area_score_and_rationale_regex, output)\n    final_scores = [\n        float(str_score)\n        for str_score in re.findall(self.extract_final_scores_regex, output)[\n            0\n        ].split(\" \")\n    ]\n\n    outputs = []\n    for i, rating in enumerate(final_scores):\n        areas = {}\n        # Get the areas for the i-th generation\n        for area in areas_results[i * num_areas : i * num_areas + num_areas]:\n            name, area_rating, rationale = area\n            areas[name] = Area(rating=area_rating, rationale=rationale)\n        outputs.append(UltraJudgeOutput(rating=rating, areas=areas))\n\n    return outputs\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/","title":"text_generation","text":""},{"location":"reference/distilabel/tasks/text_generation/base/","title":"base","text":""},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask","title":"<code>TextGenerationTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Task</code></p> <p>A base <code>Task</code> definition for text generation using LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> <code>\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"</code> <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>field(default_factory=lambda : {'harmlessness': harmlessness, 'helpfulness': helpfulness, 'truthfulness': truthfulness, 'honesty': honesty, 'verbalized_calibration': verbalized_calibration}, repr=False)</code> <code>principles_distribution</code> <code>Union[Dict[str, float], Literal['balanced'], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n&gt;&gt;&gt; task = TextGenerationTask()\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>@dataclass\nclass TextGenerationTask(Task):\n    \"\"\"A base `Task` definition for text generation using LLMs.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n        &gt;&gt;&gt; task = TextGenerationTask()\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible,\"\n        \" while being safe. Your answers should not include any harmful, unethical, racist, sexist,\"\n        \" toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased\"\n        \" and positive in nature.\\nIf a question does not make any sense, or is not factually coherent,\"\n        \" explain why instead of answering something not correct. If you don't know the answer to a\"\n        \" question, please don't share false information.\"\n    )\n    principles: Dict[str, List[str]] = field(\n        default_factory=lambda: {\n            \"harmlessness\": UltraFeedbackPrinciples.harmlessness,\n            \"helpfulness\": UltraFeedbackPrinciples.helpfulness,\n            \"truthfulness\": UltraFeedbackPrinciples.truthfulness,\n            \"honesty\": UltraFeedbackPrinciples.honesty,\n            \"verbalized_calibration\": UltraFeedbackPrinciples.verbalized_calibration,\n        },\n        repr=False,\n    )\n    principles_distribution: Union[Dict[str, float], Literal[\"balanced\"], None] = None\n\n    __type__: ClassVar[Literal[\"generation\"]] = \"generation\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validates the `principles_distribution` if it is a dict.\n\n        Raises:\n            ValueError: if the `principles_distribution` is a dict and it does not sum to 1.0.\n            ValueError: if the `principles` are not included in the `principles_distribution`.\n        \"\"\"\n        if isinstance(self.principles_distribution, dict):\n            not_included_principles = [\n                principle\n                for principle in self.principles\n                if principle not in self.principles_distribution\n            ]\n            if not_included_principles:\n                principles_str = \", \".join(\n                    [f\"'{principle}'\" for principle in not_included_principles]\n                )\n                raise ValueError(\n                    f\"Principles {principles_str} included in `principles` is not in\"\n                    \" `principles_distribution`\"\n                )\n\n            if sum(self.principles_distribution.values()) != 1.0:\n                raise ValueError(\n                    \"`principles_distribution` must sum to 1.0 if it is a dict containing\"\n                    \" the distribution of principles to use.\"\n                )\n\n    def _get_principle(self) -&gt; str:\n        \"\"\"Gets a principle from the `principles` dict respecting the `principal_distribution`.\n\n        Returns:\n            str: the principle to be used.\n        \"\"\"\n        if isinstance(self.principles_distribution, dict):\n            principle_group = random.choices(\n                list(self.principles_distribution.keys()),\n                weights=list(self.principles_distribution.values()),\n                k=1,\n            )[0]\n        else:\n            principle_group = random.choice(list(self.principles.keys()))\n        return random.choice(self.principles[principle_group])\n\n    def generate_prompt(self, input: str, **_: Any) -&gt; Prompt:\n        \"\"\"Generates the prompt to be used for generation.\n\n        Args:\n            input (str): the input to be used for generation.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n            &gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            Prompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n        \"\"\"\n        system_prompt = self.system_prompt\n        if self.principles_distribution is not None:\n            principle = self._get_principle()\n            system_prompt += \" \" + principle\n        return Prompt(system_prompt=system_prompt, formatted_prompt=input)\n\n    def parse_output(self, output: str) -&gt; Dict[str, str]:\n        \"\"\"Parses the output of the LLM into the desired format.\"\"\"\n        return {\"generations\": output}\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the input args names for the task.\"\"\"\n        return [\"input\"]\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the output args names for the task.\"\"\"\n        return [\"generations\"]\n\n    def to_argilla_dataset(\n        self,\n        dataset_row: Dict[str, Any],\n        generations_column: Optional[str] = \"generations\",\n    ) -&gt; \"FeedbackDataset\":\n        # First we infer the fields from the input_args_names, but we could also\n        # create those manually instead using `rg.TextField(...)`\n        fields = infer_fields_from_dataset_row(\n            field_names=self.input_args_names + self.output_args_names,\n            dataset_row=dataset_row,\n        )\n        # Then we add a default `RatingQuestion` which asks the users to provide a\n        # rating for each of the generations, differing from the scenario where the inputs\n        # are the fields and the outputs the ones used to formulate the quesstions. So on,\n        # in this scenario we won't have suggestions, as the questions will be related to the\n        # combination of inputs and outputs.\n        if generations_column is None or generations_column not in dataset_row:\n            raise ValueError(\n                f\"The `generations_column='{generations_column}'` is not present in the dataset\"\n                f\" row. Please provide any of {list(dataset_row.keys())}.\",\n            )\n        questions = []\n        for idx in range(1, len(dataset_row[generations_column]) + 1):\n            questions.append(\n                rg.RatingQuestion(  # type: ignore\n                    name=f\"{generations_column}-{idx}-rating\",\n                    title=f\"How would you rate the generation at `{generations_column}-{idx}`?\",\n                    values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                )\n            )\n        # Finally, we define some metadata properties that can be potentially used\n        # while exploring the dataset within Argilla to get more insights on the data.\n        metadata_properties = []\n        for arg_name in self.input_args_names + self.output_args_names:\n            if isinstance(dataset_row[arg_name], list):\n                for idx in range(1, len(dataset_row[arg_name]) + 1):\n                    metadata_properties.append(\n                        rg.IntegerMetadataProperty(name=f\"length-{arg_name}-{idx}\")  # type: ignore\n                    )\n            elif isinstance(dataset_row[arg_name], str):\n                metadata_properties.append(\n                    rg.IntegerMetadataProperty(name=f\"length-{arg_name}\")  # type: ignore\n                )\n            else:\n                warnings.warn(\n                    f\"Unsupported input type ({type(dataset_row[arg_name])}), skipping...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        # Then we just return the `FeedbackDataset` with the fields, questions, and metadata properties\n        # defined above.\n        return rg.FeedbackDataset(\n            fields=fields,\n            questions=questions,\n            metadata_properties=metadata_properties,  # Note that these are always optional\n        )\n\n    def to_argilla_record(self, dataset_row: Dict[str, Any]) -&gt; \"FeedbackRecord\":\n        \"\"\"Converts a dataset row to an Argilla `FeedbackRecord`.\"\"\"\n        # We start off with the fields, which are the inputs of the LLM, but also\n        # build the metadata from them, as previously specified within the\n        fields, metadata = {}, {}\n        for arg_name in self.input_args_names + self.output_args_names:\n            arg_value = dataset_row[arg_name]\n            if isinstance(arg_value, list):\n                for idx, value in enumerate(arg_value, start=1):\n                    # TODO: value formatting was included here due to some issues\n                    # with `SelfInstructTask` but these list-parsing may not be needed\n                    # anymore.\n                    value = (\n                        value.strip()\n                        if isinstance(value, str)\n                        else \"\\n\".join(value)\n                        if isinstance(value, list)\n                        else \"\"\n                    )\n                    fields[f\"{arg_name}-{idx}\"] = value\n                    if value is not None:\n                        metadata[f\"length-{arg_name}-{idx}\"] = len(value)\n            elif isinstance(arg_value, str):\n                fields[arg_name] = arg_value.strip() if arg_value else \"\"\n                if arg_value is not None:\n                    metadata[f\"length-{arg_name}\"] = len(arg_value.strip())\n            else:\n                warnings.warn(\n                    f\"Unsupported input type ({type(arg_value)}), skipping...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        # Then we add the model metadata from the `generation_model` and `labelling_model`\n        # columns of the dataset, if they exist.\n        metadata.update(model_metadata_from_dataset_row(dataset_row=dataset_row))\n        # Finally, we return the `FeedbackRecord` with the fields and the metadata\n        return rg.FeedbackRecord(fields=fields, metadata=metadata)\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.input_args_names","title":"<code>input_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the input args names for the task.</p>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the output args names for the task.</p>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validates the <code>principles_distribution</code> if it is a dict.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>principles_distribution</code> is a dict and it does not sum to 1.0.</p> <code>ValueError</code> <p>if the <code>principles</code> are not included in the <code>principles_distribution</code>.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validates the `principles_distribution` if it is a dict.\n\n    Raises:\n        ValueError: if the `principles_distribution` is a dict and it does not sum to 1.0.\n        ValueError: if the `principles` are not included in the `principles_distribution`.\n    \"\"\"\n    if isinstance(self.principles_distribution, dict):\n        not_included_principles = [\n            principle\n            for principle in self.principles\n            if principle not in self.principles_distribution\n        ]\n        if not_included_principles:\n            principles_str = \", \".join(\n                [f\"'{principle}'\" for principle in not_included_principles]\n            )\n            raise ValueError(\n                f\"Principles {principles_str} included in `principles` is not in\"\n                \" `principles_distribution`\"\n            )\n\n        if sum(self.principles_distribution.values()) != 1.0:\n            raise ValueError(\n                \"`principles_distribution` must sum to 1.0 if it is a dict containing\"\n                \" the distribution of principles to use.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.generate_prompt","title":"<code>generate_prompt(input, **_)</code>","text":"<p>Generates the prompt to be used for generation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for generation.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n&gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\nPrompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def generate_prompt(self, input: str, **_: Any) -&gt; Prompt:\n    \"\"\"Generates the prompt to be used for generation.\n\n    Args:\n        input (str): the input to be used for generation.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n        &gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n    \"\"\"\n    system_prompt = self.system_prompt\n    if self.principles_distribution is not None:\n        principle = self._get_principle()\n        system_prompt += \" \" + principle\n    return Prompt(system_prompt=system_prompt, formatted_prompt=input)\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the LLM into the desired format.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, str]:\n    \"\"\"Parses the output of the LLM into the desired format.\"\"\"\n    return {\"generations\": output}\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.to_argilla_record","title":"<code>to_argilla_record(dataset_row)</code>","text":"<p>Converts a dataset row to an Argilla <code>FeedbackRecord</code>.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def to_argilla_record(self, dataset_row: Dict[str, Any]) -&gt; \"FeedbackRecord\":\n    \"\"\"Converts a dataset row to an Argilla `FeedbackRecord`.\"\"\"\n    # We start off with the fields, which are the inputs of the LLM, but also\n    # build the metadata from them, as previously specified within the\n    fields, metadata = {}, {}\n    for arg_name in self.input_args_names + self.output_args_names:\n        arg_value = dataset_row[arg_name]\n        if isinstance(arg_value, list):\n            for idx, value in enumerate(arg_value, start=1):\n                # TODO: value formatting was included here due to some issues\n                # with `SelfInstructTask` but these list-parsing may not be needed\n                # anymore.\n                value = (\n                    value.strip()\n                    if isinstance(value, str)\n                    else \"\\n\".join(value)\n                    if isinstance(value, list)\n                    else \"\"\n                )\n                fields[f\"{arg_name}-{idx}\"] = value\n                if value is not None:\n                    metadata[f\"length-{arg_name}-{idx}\"] = len(value)\n        elif isinstance(arg_value, str):\n            fields[arg_name] = arg_value.strip() if arg_value else \"\"\n            if arg_value is not None:\n                metadata[f\"length-{arg_name}\"] = len(arg_value.strip())\n        else:\n            warnings.warn(\n                f\"Unsupported input type ({type(arg_value)}), skipping...\",\n                UserWarning,\n                stacklevel=2,\n            )\n    # Then we add the model metadata from the `generation_model` and `labelling_model`\n    # columns of the dataset, if they exist.\n    metadata.update(model_metadata_from_dataset_row(dataset_row=dataset_row))\n    # Finally, we return the `FeedbackRecord` with the fields and the metadata\n    return rg.FeedbackRecord(fields=fields, metadata=metadata)\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/evol_complexity/","title":"evol_complexity","text":""},{"location":"reference/distilabel/tasks/text_generation/evol_complexity/#distilabel.tasks.text_generation.evol_complexity.EvolComplexityTask","title":"<code>EvolComplexityTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>EvolInstructTask</code></p> <p>A <code>TextGenerationTask</code> following the <code>EvolComplexity</code> specification for building prompts. This is a special case of the original EvolInstructTask, where the evolution method is fixed to \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\". Additionally, an additional elimation step should be executed to screen out instructions that are not useful.</p> <p>From the reference repository: Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.</p> <p>The task is defined as follows: Starting from an initial (simpler) instruction, select in-depth or in-breadth evolving to upgrade the simple instruction to a more complex one or create a new one (to increase diversity). The In-depth Evolving includes the following operations: \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\". The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.</p> <p>Given the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the answer is a copy from the same used prompt.</p> <p>This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities. Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions in the original dataset. An example can be seen at the following script: examples/pipeline-evol-instruct-alpaca.py</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code></li> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/evol_complexity.py</code> <pre><code>@dataclass\nclass EvolComplexityTask(EvolInstructTask):\n    \"\"\"A `TextGenerationTask` following the `EvolComplexity` specification for building prompts. This is a special case\n    of the original EvolInstructTask, where the evolution method is fixed to \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".\n    Additionally, an additional elimation step should be executed to screen out instructions that are not useful.\n\n    From the reference repository: *Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce\n    open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.*\n\n    The task is defined as follows:\n    Starting from an initial (simpler) instruction, select in-depth or in-breadth evolving to upgrade the simple instruction\n    to a more complex one or create a new one (to increase diversity).\n    The In-depth Evolving includes the following operations: \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".\n    The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.\n\n    Given the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator\n    to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the\n    answer is a copy from the same used prompt.\n\n    This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities.\n    Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions\n    in the original dataset. An example can be seen at the following script:\n    [examples/pipeline-evol-instruct-alpaca.py](https://github.com/argilla-io/distilabel/tree/main/examples/pipeline-evol-instruct-alpaca.py)\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    system_prompt: str = \"\"\n\n    __jinja2_template__: str = _EVOL_COMPLEXITY_TEMPLATE\n\n    def generate_prompt(\n        self, input: str, evolution_method: Optional[EvolutionMethod] = None, **_: Any\n    ) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Evol-Complexity specification of the Deita Paper.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            evolution_method (str, optional): The evolution method to be used. If not provided (the default), a random one is chosen\n                like the original paper. Available ones are \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolComplexityGeneratorTask\n            &gt;&gt;&gt; task = EvolComplexityGeneratorTask()\n            &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\n            Prompt(\n                system_prompt=\"\",\n                formatted_prompt=\"I want you to act as a Prompt ...\",\n            )\n        \"\"\"\n        evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n        return super().generate_prompt(input, evolution_method=evolution_method, **_)\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/evol_complexity/#distilabel.tasks.text_generation.evol_complexity.EvolComplexityTask.generate_prompt","title":"<code>generate_prompt(input, evolution_method=None, **_)</code>","text":"<p>Generates a prompt following the Evol-Complexity specification of the Deita Paper.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>evolution_method</code> <code>str</code> <p>The evolution method to be used. If not provided (the default), a random one is chosen like the original paper. Available ones are \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import EvolComplexityGeneratorTask\n&gt;&gt;&gt; task = EvolComplexityGeneratorTask()\n&gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\nPrompt(\n    system_prompt=\"\",\n    formatted_prompt=\"I want you to act as a Prompt ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/evol_complexity.py</code> <pre><code>def generate_prompt(\n    self, input: str, evolution_method: Optional[EvolutionMethod] = None, **_: Any\n) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Evol-Complexity specification of the Deita Paper.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        evolution_method (str, optional): The evolution method to be used. If not provided (the default), a random one is chosen\n            like the original paper. Available ones are \"constraints\", \"deepen\", \"concretizing\" or \"reasoning\".\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolComplexityGeneratorTask\n        &gt;&gt;&gt; task = EvolComplexityGeneratorTask()\n        &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\n        Prompt(\n            system_prompt=\"\",\n            formatted_prompt=\"I want you to act as a Prompt ...\",\n        )\n    \"\"\"\n    evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n    return super().generate_prompt(input, evolution_method=evolution_method, **_)\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/evol_instruct/","title":"evol_instruct","text":""},{"location":"reference/distilabel/tasks/text_generation/evol_instruct/#distilabel.tasks.text_generation.evol_instruct.EvolInstructTask","title":"<code>EvolInstructTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>InstructTaskMixin</code>, <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> following the <code>EvolInstruct</code> specification for building the prompts.</p> <p>From the reference repository: Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.</p> <p>The task is defined as follows: Starting from an initial (simpler) instruction, select in-depth or in-breadth evolving to upgrade the simple instruction to a more complex one or create a new one (to increase diversity). The In-depth Evolving includes the following operations: add constraints, deepening, concretizing and increase reasoning. The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.</p> <p>Given the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the answer is a copy from the same used prompt.</p> <p>This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities. Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions in the original dataset. An example can be seen at the following script: examples/pipeline-evol-instruct-alpaca.py</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/evol_instruct.py</code> <pre><code>@dataclass\nclass EvolInstructTask(InstructTaskMixin, TextGenerationTask):\n    \"\"\"A `TextGenerationTask` following the `EvolInstruct` specification for building the prompts.\n\n    From the reference repository: *Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce\n    open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.*\n\n    The task is defined as follows:\n    Starting from an initial (simpler) instruction, select in-depth or in-breadth evolving to upgrade the simple instruction\n    to a more complex one or create a new one (to increase diversity).\n    The In-depth Evolving includes the following operations: add constraints, deepening, concretizing and increase reasoning.\n    The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.\n\n    Given the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator\n    to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the\n    answer is a copy from the same used prompt.\n\n    This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities.\n    Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions\n    in the original dataset. An example can be seen at the following script:\n    [examples/pipeline-evol-instruct-alpaca.py](https://github.com/argilla-io/distilabel/tree/main/examples/pipeline-evol-instruct-alpaca.py)\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    system_prompt: str = \"\"\n\n    __jinja2_template__: str = _EVOL_INSTRUCT_TEMPLATE\n\n    def generate_prompt(\n        self, input: str, evolution_method: Optional[EvolutionMethod] = None, **_: Any\n    ) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Evol-Instruct specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            evolution_method (str, optional): The evolution method to be used. If not provided (the default), a random one is chosen\n                like the original paper. Available ones are \"breadth\", \"constraints\", \"deepen\", \"concretizing\" and \"reasoning\".\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolInstructTask\n            &gt;&gt;&gt; task = EvolInstructTask()\n            &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\n            Prompt(\n                system_prompt=\"\",\n                formatted_prompt=\"I want you to act as a Prompt ...\",\n            )\n        \"\"\"\n        evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n        render_kwargs = {\n            \"evol_method\": evolution_method,\n            \"instruction\": input,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        return [\"instructions\"]\n\n    def _elimination_evolving(\n        self, output: str, response_words: Optional[List[str]] = None\n    ) -&gt; Optional[str]:\n        \"\"\"Performs the elimination step of the Evol-Instruct task, steps 2-4 in the paper:\n\n        1. [NOT IMPLEMENTED] The evolved instruction does not provide any information gain compared\n        to the original one. Use ChatGPT to make this determination, this is outlined in Appendix G of the original paper.\n        2. The evolved instruction makes it difficult for the LLM to generate a response. We found that\n        when the generated response contains \u201csorry\u201d and is relatively short in length (i.e., less than\n        80 words), it often indicates that the LLM struggles to respond to the evolved instruction.\n        So we can use this rule to make a judgment.\n        3. The response generated by the LLM only contains punctuation and stop words.\n        4. The evolved instruction obviously copies some words from the evolving prompt, such as\n        \u201cgiven prompt\u201d, \u201crewritten prompt\u201d, \u201c#Rewritten Prompt#\u201d, etc.\n        \"\"\"\n        output = output.strip()\n        if output == \"\":\n            return\n\n        # 2) The evolved instruction makes it difficult for the LLM to generate a response.\n        if \"sorry\" in output.lower() and len(output.split(\" \")) &lt; 80:\n            logger.info(\n                f\"Evolution step removed the output, it's hard for the LLM to generate a response: {output}\"\n            )\n            return\n\n        # 3) The output only contains punctuation and stop words\n        stopwords = _get_stopwords()\n        clean_output = [word for word in output.split(\" \") if word not in stopwords]\n        if set(clean_output).difference(set(string.punctuation)) == 0:\n            logger.info(\n                f\"Evolution step removed the output, it only contains punctuation and stop words: {output}\"\n            )\n            return\n\n        # 4) Remove copied words from the prompt\n        prompt_words = {\n            \"#Given Prompt#\",\n            \"#Created Prompt#\",\n            \"given prompt\",\n            \"created prompt\",\n            \"#The Given Prompt#\",\n            \"#Rewritten Prompt#\",\n            \"rewritten prompt\",\n        }\n        if response_words:\n            prompt_words = prompt_words.union(response_words)\n        if any(word in output for word in prompt_words):\n            logger.info(\n                f\"Evolution step removed the output due to word repetition from the prompt: {output}\"\n            )\n            return\n\n        return output\n\n    def _get_evolution_method(\n        self, chosen_method: EvolutionMethod, available_methods: EvolutionMethod\n    ) -&gt; None:\n        available_methods = get_args(available_methods)\n        if not chosen_method:\n            chosen_method = random.choice(available_methods)\n        if chosen_method not in available_methods:\n            raise ValueError(\n                f\"Evolution method {chosen_method} is not available. Available ones are: {available_methods}\"\n            )\n        return chosen_method\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the model into the desired format, applying the elimination step for bad generations.\n\n        Args:\n            output (str): the output of the model.\n\n        Note:\n            The elimination step is applied to the output, but only steps 2-4 in the paper are implemented.\n            Refer to point 3.2, Elimination Evolving section in [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n            for more information on the elimination evolving step, and take a look at the `_elimination_evolving`\n            method for more information of the implementation.\n        \"\"\"\n        output = self._elimination_evolving(output)\n        return {self.output_args_names[0]: output}\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/evol_instruct/#distilabel.tasks.text_generation.evol_instruct.EvolInstructTask.generate_prompt","title":"<code>generate_prompt(input, evolution_method=None, **_)</code>","text":"<p>Generates a prompt following the Evol-Instruct specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>evolution_method</code> <code>str</code> <p>The evolution method to be used. If not provided (the default), a random one is chosen like the original paper. Available ones are \"breadth\", \"constraints\", \"deepen\", \"concretizing\" and \"reasoning\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import EvolInstructTask\n&gt;&gt;&gt; task = EvolInstructTask()\n&gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\nPrompt(\n    system_prompt=\"\",\n    formatted_prompt=\"I want you to act as a Prompt ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/evol_instruct.py</code> <pre><code>def generate_prompt(\n    self, input: str, evolution_method: Optional[EvolutionMethod] = None, **_: Any\n) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Evol-Instruct specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        evolution_method (str, optional): The evolution method to be used. If not provided (the default), a random one is chosen\n            like the original paper. Available ones are \"breadth\", \"constraints\", \"deepen\", \"concretizing\" and \"reasoning\".\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolInstructTask\n        &gt;&gt;&gt; task = EvolInstructTask()\n        &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\")\n        Prompt(\n            system_prompt=\"\",\n            formatted_prompt=\"I want you to act as a Prompt ...\",\n        )\n    \"\"\"\n    evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n    render_kwargs = {\n        \"evol_method\": evolution_method,\n        \"instruction\": input,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/evol_instruct/#distilabel.tasks.text_generation.evol_instruct.EvolInstructTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format, applying the elimination step for bad generations.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>the output of the model.</p> required Note <p>The elimination step is applied to the output, but only steps 2-4 in the paper are implemented. Refer to point 3.2, Elimination Evolving section in <code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code> for more information on the elimination evolving step, and take a look at the <code>_elimination_evolving</code> method for more information of the implementation.</p> Source code in <code>src/distilabel/tasks/text_generation/evol_instruct.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the model into the desired format, applying the elimination step for bad generations.\n\n    Args:\n        output (str): the output of the model.\n\n    Note:\n        The elimination step is applied to the output, but only steps 2-4 in the paper are implemented.\n        Refer to point 3.2, Elimination Evolving section in [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n        for more information on the elimination evolving step, and take a look at the `_elimination_evolving`\n        method for more information of the implementation.\n    \"\"\"\n    output = self._elimination_evolving(output)\n    return {self.output_args_names[0]: output}\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/evol_quality/","title":"evol_quality","text":""},{"location":"reference/distilabel/tasks/text_generation/evol_quality/#distilabel.tasks.text_generation.evol_quality.EvolQualityTask","title":"<code>EvolQualityTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>EvolInstructTask</code></p> <p>A <code>TextGenerationTask</code> following the <code>Deita</code> specification for improving the quality of instructions.</p> <p>From the reference repository: DEITA (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach.</p> <p>The task is defined as follows: Starting from an initial (simpler) instruction response, select an evolving-method to upgrade the quality of the instruction. The Evolving methods includes the following operations: add \"helpfulness\", \"relevance\", \"depth\", \"creativity\" and \"details\".</p> <p>Given the evolved responses are generated from LLMs, sometimes the evolving will fail. We adopt an responses eliminator to filter the failed instructions, called Elimination Evolving, but we don't apply the step of asking again to the LLM it the answer is a copy from the same used prompt. Note that we slightly modify the elimination evolving step, from the original paper, to allow for filtering of the responses.</p> <p>This evolutionary process can be repeated for several rounds to obtain instruction data containing various complexities. Currently the task is implemented as a single step, so to generate multiple evolutions you can \"repeat\" the instructions in the original dataset. An example of a similar implementation with <code>EvolInstruct</code> can be seen at the following script: examples/pipeline-evol-instruct-alpaca.py</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Not defined for this task.</p> <code>''</code> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> <li><code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/evol_quality.py</code> <pre><code>@dataclass\nclass EvolQualityTask(EvolInstructTask):\n    \"\"\"A `TextGenerationTask` following the `Deita` specification for improving the *quality* of instructions.\n\n    From the reference repository: *DEITA (short for Data-Efficient Instruction Tuning for Alignment),\n    a series of models fine-tuned from LLaMA and Mistral models using data samples automatically\n    selected with our proposed approach*.\n\n    The task is defined as follows:\n    Starting from an initial (simpler) instruction response, select an evolving-method to upgrade the quality\n    of the instruction. The Evolving methods includes the following operations: add \"helpfulness\", \"relevance\",\n    \"depth\", \"creativity\" and \"details\".\n\n    Given the evolved responses are generated from LLMs, sometimes the evolving will fail.\n    We adopt an responses eliminator to filter the failed instructions, called Elimination Evolving,\n    but we don't apply the step of asking again to the LLM it the answer is a copy from the same used\n    prompt. Note that we slightly modify the elimination evolving step, from the original paper, to\n    allow for filtering of the responses.\n\n    This evolutionary process can be repeated for several rounds to obtain instruction data containing various\n    complexities. Currently the task is implemented as a single step, so to generate multiple evolutions you\n    can \"repeat\" the instructions in the original dataset. An example of a similar implementation with\n    `EvolInstruct` can be seen at the following script: [examples/pipeline-evol-instruct-alpaca.py](https://github.com/argilla-io/distilabel/tree/main/examples/pipeline-evol-instruct-alpaca.py)\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Not defined for this task.\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n        - [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    system_prompt: str = \"\"\n\n    __jinja2_template__: str = _EVOL_QUALITY_TEMPLATE\n\n    def generate_prompt(\n        self,\n        input: str,\n        generation: str,\n        evolution_method: Optional[EvolutionMethod] = None,\n        **_: Any,\n    ) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Evol-Instruct specification.\n\n        Args:\n            input (str):\n                The input to be used for the prompt. Corresponds to the instruction in the prompt.\n            generation (str):\n                The generation to be used for the prompt, which corresponds to a generated response\n                given the instruction given in the input.\n            evolution_method (str, optional):\n                The evolution method to be used. If not provided (the default), a random one is chosen\n                like the original paper. Available ones are \"helpfulness\", \"relevance\", \"deepen\",\n                \"creativity\" and \"details\".\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolQualityTask\n            &gt;&gt;&gt; task = EvolQualityTask()\n            &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\", \"1. Eat healthy food. 2. Exercise. 3. Sleep well.\")\n            Prompt(\n                system_prompt=\"\",\n                formatted_prompt=\"I want you to act as a Prompt ...\",\n            )\n        \"\"\"\n        evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n        render_kwargs = {\n            \"evol_method\": evolution_method,\n            \"instruction\": input,\n            \"generation\": generation,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        return [\"input\", \"generation\"]\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        return [\"generations\"]\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the model into the desired format, applying the elimination step for bad generations.\n\n        Args:\n            output (str): the output of the model.\n\n        Note:\n            The elimination step is applied to the output, but only steps 2-4 in the paper are implemented.\n            Refer to point 3.2, Elimination Evolving section in [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n            for more information on the elimination evolving step, and take a look at the `_elimination_evolving`\n            method for more information of the implementation.\n        \"\"\"\n        response_words = {\n            \"#Given Response#\",\n            \"#Created Response#\",\n            \"given response\",\n            \"created response\",\n            \"#The Given Response#\",\n            \"#Rewritten Response#\",\n            \"rewritten response\",\n        }\n        output = self._elimination_evolving(output, response_words=response_words)\n        return {self.output_args_names[0]: output}\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/evol_quality/#distilabel.tasks.text_generation.evol_quality.EvolQualityTask.generate_prompt","title":"<code>generate_prompt(input, generation, evolution_method=None, **_)</code>","text":"<p>Generates a prompt following the Evol-Instruct specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input to be used for the prompt. Corresponds to the instruction in the prompt.</p> required <code>generation</code> <code>str</code> <p>The generation to be used for the prompt, which corresponds to a generated response given the instruction given in the input.</p> required <code>evolution_method</code> <code>str</code> <p>The evolution method to be used. If not provided (the default), a random one is chosen like the original paper. Available ones are \"helpfulness\", \"relevance\", \"deepen\", \"creativity\" and \"details\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import EvolQualityTask\n&gt;&gt;&gt; task = EvolQualityTask()\n&gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\", \"1. Eat healthy food. 2. Exercise. 3. Sleep well.\")\nPrompt(\n    system_prompt=\"\",\n    formatted_prompt=\"I want you to act as a Prompt ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/evol_quality.py</code> <pre><code>def generate_prompt(\n    self,\n    input: str,\n    generation: str,\n    evolution_method: Optional[EvolutionMethod] = None,\n    **_: Any,\n) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Evol-Instruct specification.\n\n    Args:\n        input (str):\n            The input to be used for the prompt. Corresponds to the instruction in the prompt.\n        generation (str):\n            The generation to be used for the prompt, which corresponds to a generated response\n            given the instruction given in the input.\n        evolution_method (str, optional):\n            The evolution method to be used. If not provided (the default), a random one is chosen\n            like the original paper. Available ones are \"helpfulness\", \"relevance\", \"deepen\",\n            \"creativity\" and \"details\".\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import EvolQualityTask\n        &gt;&gt;&gt; task = EvolQualityTask()\n        &gt;&gt;&gt; task.generate_prompt(\"Give three tips for staying healthy.\", \"1. Eat healthy food. 2. Exercise. 3. Sleep well.\")\n        Prompt(\n            system_prompt=\"\",\n            formatted_prompt=\"I want you to act as a Prompt ...\",\n        )\n    \"\"\"\n    evolution_method = self._get_evolution_method(evolution_method, EvolutionMethod)\n\n    render_kwargs = {\n        \"evol_method\": evolution_method,\n        \"instruction\": input,\n        \"generation\": generation,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/evol_quality/#distilabel.tasks.text_generation.evol_quality.EvolQualityTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format, applying the elimination step for bad generations.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>the output of the model.</p> required Note <p>The elimination step is applied to the output, but only steps 2-4 in the paper are implemented. Refer to point 3.2, Elimination Evolving section in <code>WizardLM: Empowering Large Language Models to Follow Complex Instructions</code> for more information on the elimination evolving step, and take a look at the <code>_elimination_evolving</code> method for more information of the implementation.</p> Source code in <code>src/distilabel/tasks/text_generation/evol_quality.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the model into the desired format, applying the elimination step for bad generations.\n\n    Args:\n        output (str): the output of the model.\n\n    Note:\n        The elimination step is applied to the output, but only steps 2-4 in the paper are implemented.\n        Refer to point 3.2, Elimination Evolving section in [`WizardLM: Empowering Large Language Models to Follow Complex Instructions`](https://arxiv.org/abs/2304.12244)\n        for more information on the elimination evolving step, and take a look at the `_elimination_evolving`\n        method for more information of the implementation.\n    \"\"\"\n    response_words = {\n        \"#Given Response#\",\n        \"#Created Response#\",\n        \"given response\",\n        \"created response\",\n        \"#The Given Response#\",\n        \"#Rewritten Response#\",\n        \"rewritten response\",\n    }\n    output = self._elimination_evolving(output, response_words=response_words)\n    return {self.output_args_names[0]: output}\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/mixins/","title":"mixins","text":""},{"location":"reference/distilabel/tasks/text_generation/mixins/#distilabel.tasks.text_generation.mixins.InstructTaskMixin","title":"<code>InstructTaskMixin</code>","text":"<p>Mixin that adds the <code>to_argilla_dataset</code> and <code>to_argilla_record</code> methods for tasks that generate/modify instructions <code>SelfInstructTask</code> or <code>EvolInstructTask</code>.</p> Source code in <code>src/distilabel/tasks/text_generation/mixins.py</code> <pre><code>class InstructTaskMixin:\n    \"\"\"Mixin that adds the `to_argilla_dataset` and `to_argilla_record` methods for tasks\n    that generate/modify instructions `SelfInstructTask` or `EvolInstructTask`.\n    \"\"\"\n\n    def to_argilla_dataset(self, dataset_row: Dict[str, Any]) -&gt; \"FeedbackDataset\":\n        # First we infer the fields from the input_args_names, but we could also\n        # create those manually instead using `rg.TextField(...)`\n        fields = infer_fields_from_dataset_row(\n            field_names=self.input_args_names,\n            dataset_row=dataset_row,\n        )\n        # Once the input fields have been defined, then we also include the instruction\n        # field which will be fulfilled with each of the instructions generated.\n        for arg_name in self.output_args_names:\n            fields.append(rg.TextField(name=arg_name, title=arg_name))  # type: ignore\n        # Then we add a default `RatingQuestion` which asks the users to provide a\n        # rating for each of the generations, differing from the scenario where the inputs\n        # are the fields and the outputs the ones used to formulate the quesstions. So on,\n        # in this scenario we won't have suggestions, as the questions will be related to the\n        # combination of inputs and outputs.\n        questions = [\n            rg.RatingQuestion(  # type: ignore\n                name=\"instruction-rating\",\n                title=\"How would you rate the generated instruction?\",\n                values=list(range(1, 11)),\n            )\n        ]\n        # Finally, we define some metadata properties that can be potentially used\n        # while exploring the dataset within Argilla to get more insights on the data.\n        metadata_properties = []\n        for arg_name in self.input_args_names:\n            if isinstance(dataset_row[arg_name], list):\n                for idx in range(1, len(dataset_row[arg_name]) + 1):\n                    metadata_properties.append(\n                        rg.IntegerMetadataProperty(name=f\"length-{arg_name}-{idx}\")  # type: ignore\n                    )\n            elif isinstance(dataset_row[arg_name], str):\n                metadata_properties.append(\n                    rg.IntegerMetadataProperty(name=f\"length-{arg_name}\")  # type: ignore\n                )\n            else:\n                warnings.warn(\n                    f\"Unsupported input type ({type(dataset_row[arg_name])}), skipping...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        metadata_properties.append(\n            rg.IntegerMetadataProperty(name=\"length-instruction\")  # type: ignore\n        )  # type: ignore\n        # Then we just return the `FeedbackDataset` with the fields, questions, and metadata properties\n        # defined above.\n        return rg.FeedbackDataset(\n            fields=fields,\n            questions=questions,  # type: ignore\n            metadata_properties=metadata_properties,  # Note that these are always optional\n        )\n\n    def to_argilla_record(\n        self,\n        dataset_row: Dict[str, Any],\n        instructions_column: Optional[str] = None,\n    ) -&gt; List[\"FeedbackRecord\"]:\n        \"\"\"Converts a dataset row to a list of Argilla `FeedbackRecord`s.\"\"\"\n        records = []\n        if instructions_column is None:\n            instructions_column = self.output_args_names[0]\n\n        for instruction in dataset_row[instructions_column]:  # type: ignore\n            fields, metadata = {}, {}\n            for arg_name in self.input_args_names:\n                arg_value = dataset_row[arg_name]\n                if isinstance(arg_value, list):\n                    for idx, value in enumerate(arg_value, start=1):\n                        value = value.strip() if isinstance(value, str) else \"\"\n                        fields[f\"{arg_name}-{idx}\"] = value\n                        if value is not None:\n                            metadata[f\"length-{arg_name}-{idx}\"] = len(value)\n                elif isinstance(arg_value, str):\n                    fields[arg_name] = arg_value.strip() if arg_value else \"\"\n                    if arg_value is not None:\n                        metadata[f\"length-{arg_name}\"] = len(arg_value.strip())\n                else:\n                    warnings.warn(\n                        f\"Unsupported input type ({type(arg_value)}), skipping...\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n            fields[self.output_args_names[0]] = instruction\n            metadata[f\"length-{self.output_args_names[0]}\"] = len(instruction)\n\n            # Then we add the model metadata from the `generation_model` and `labelling_model`\n            # columns of the dataset, if they exist.\n            metadata.update(model_metadata_from_dataset_row(dataset_row=dataset_row))\n            # Finally, we append the `FeedbackRecord` with the fields and the metadata\n            records.append(rg.FeedbackRecord(fields=fields, metadata=metadata))\n        if not records:\n            raise ValueError(\n                f\"Skipping the row {dataset_row} as the list of `FeedbackRecord` is empty as those could not be inferred.\"\n            )\n        return records\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/mixins/#distilabel.tasks.text_generation.mixins.InstructTaskMixin.to_argilla_record","title":"<code>to_argilla_record(dataset_row, instructions_column=None)</code>","text":"<p>Converts a dataset row to a list of Argilla <code>FeedbackRecord</code>s.</p> Source code in <code>src/distilabel/tasks/text_generation/mixins.py</code> <pre><code>def to_argilla_record(\n    self,\n    dataset_row: Dict[str, Any],\n    instructions_column: Optional[str] = None,\n) -&gt; List[\"FeedbackRecord\"]:\n    \"\"\"Converts a dataset row to a list of Argilla `FeedbackRecord`s.\"\"\"\n    records = []\n    if instructions_column is None:\n        instructions_column = self.output_args_names[0]\n\n    for instruction in dataset_row[instructions_column]:  # type: ignore\n        fields, metadata = {}, {}\n        for arg_name in self.input_args_names:\n            arg_value = dataset_row[arg_name]\n            if isinstance(arg_value, list):\n                for idx, value in enumerate(arg_value, start=1):\n                    value = value.strip() if isinstance(value, str) else \"\"\n                    fields[f\"{arg_name}-{idx}\"] = value\n                    if value is not None:\n                        metadata[f\"length-{arg_name}-{idx}\"] = len(value)\n            elif isinstance(arg_value, str):\n                fields[arg_name] = arg_value.strip() if arg_value else \"\"\n                if arg_value is not None:\n                    metadata[f\"length-{arg_name}\"] = len(arg_value.strip())\n            else:\n                warnings.warn(\n                    f\"Unsupported input type ({type(arg_value)}), skipping...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        fields[self.output_args_names[0]] = instruction\n        metadata[f\"length-{self.output_args_names[0]}\"] = len(instruction)\n\n        # Then we add the model metadata from the `generation_model` and `labelling_model`\n        # columns of the dataset, if they exist.\n        metadata.update(model_metadata_from_dataset_row(dataset_row=dataset_row))\n        # Finally, we append the `FeedbackRecord` with the fields and the metadata\n        records.append(rg.FeedbackRecord(fields=fields, metadata=metadata))\n    if not records:\n        raise ValueError(\n            f\"Skipping the row {dataset_row} as the list of `FeedbackRecord` is empty as those could not be inferred.\"\n        )\n    return records\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/principles/","title":"principles","text":""},{"location":"reference/distilabel/tasks/text_generation/principles/#distilabel.tasks.text_generation.principles.UltraFeedbackPrinciples","title":"<code>UltraFeedbackPrinciples</code>","text":"<p>A class containing a list of principles from the UltraFeedback paper, that can be injected into the system prompt given to the LLM.</p> References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>UltraFeedback - GitHub Repository</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/principles.py</code> <pre><code>class UltraFeedbackPrinciples:\n    \"\"\"A class containing a list of principles from the UltraFeedback paper, that can\n    be injected into the system prompt given to the LLM.\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`UltraFeedback - GitHub Repository`](https://github.com/OpenBMB/UltraFeedback)\n    \"\"\"\n\n    helpfulness = [\n        \"The assistant should provide users with accurate, relevant, and up-to-date information, ensuring that the content is positive, interesting, engaging, educational, and helpful.\",\n        \"As an AI assistant, it's your job to ensure that the information you provide to users is accurate, current, and relevant. Offer comprehensive details to educate and engage the user.\",\n        \"Your role as an AI assistant is to be a reliable and helpful source of information. Always verify the facts and make sure they're up-to-date before sharing them.\",\n        \"As an AI assistant, inspire users with your knowledgeable insights. Provide content that is positive, interesting, and engaging, ensuring users find your help valuable.\",\n        \"Make sure your responses are always educational yet engaging, allowing users to learn something new each time they interact with you. You are an AI assistant after all!\",\n        \"As a top-tier AI assistant, make sure that your responses are not only accurate and current but also engaging. Be the positive interaction users look forward to.\",\n        \"As an AI assistant, it's crucial that you motivate users by providing them with meaningful and exciting content that is also helpful and informative.\",\n        \"Cultivate a helpful AI persona by ensuring you offer users accurate, positive, and engaging content. Make every interaction informative and delightful.\",\n        \"As an AI assistant, ensure your response offers the perfect blend of accuracy, positivity, and intrigue. Strive to be educational while keeping the user engaged.\",\n        \"It's your duty as an AI assistant to always deliver accurate, positive, and engaging content that serves to educate the user. Make every interaction a learning opportunity.\",\n        \"As an AI assistant, provide not just answers but also spark curiosity. Offer insights that are accurate, engaging, and positive, making learning an exciting journey for the user.\",\n    ]\n    harmlessness = [\n        \"The assistant should not provide answers that could cause physical, psychological, or social harm to any group of people; damage to or loss of equipment or property; damage to the environment; or harm to institutions or resources necessary to human wellbeing. Feel free to refuse to answer questions that could cause the above harm.\",\n        \"You are an AI assistant that respects all humans, their equipment and environment. Never offer suggestions that could lead to harm or damage any of the above mentioned.\",\n        'You are an AI assistant that maintains a principle of \"First do no harm\". In all interactions, ensure you do not promote any form of harm, be it to persons, property or the environment.',\n        \"You are an AI assistant. Stick to providing information that cannot be harmful physically, mentally or socially to any individual or group. Don't answer any queries that might lead to such consequences.\",\n        \"As an AI assistant, refrain from advising on actions that might harm any person or group of persons, or cause damage to property, environment or resources necessary for human wellbeing.\",\n        \"You're an AI helper wishing to provide safe assistance for all users. Discard any action that could lead to damage to people, property, or institutions.\",\n        \"You are a responsible AI assistant. Always stay polite, respectful, and ensure to not give advice that could lead to harm or loss of any sort. \",\n        \"As an AI assistant, your primary focus should be on the safety and wellness of the users. Never provide answers that could cause harm or damage to individuals, their property or the environment.\",\n        \"You are an AI assistant that is centred around being harmless. Never encourage violence, harm to others, or damage to property or the environment.\",\n        \"As an AI assistant, avoid providing information or support that could result in injury, mental distress, or harm to people, their property, or the environment.\",\n        \"You are an AI assistant. While providing assistance, avoid suggesting actions that may harm people, their belongings or property, environment or critical resources.\",\n    ]\n    honesty = [\n        \"The assistant should be honest about whether it knows the answer and express its uncertainty explicitly. Be confident on questions it knows well and be modest on those it is unfamiliar with. Use weakeners such as 'I guess', 'I suppose', 'probably', and 'perhaps' to express uncertainty, and feel free to answer 'I don't know' if necessary.\",\n        \"As an AI, honesty is crucial to build trust with your user. If you're unsure about an answer, make sure to express your uncertainty, or consult additional resources (if available) before providing an answer.  \",\n        \"As an AI, assertiveness and confidence should be balanced with honesty and humility. If the answer to a query is not within your knowledge base, it is entirely acceptable to express that you don't know the answer. \",\n        \"You are an AI assistant that values honesty. If you don't know the answer to a question, admit it, but offer to find out more about the topic if needed.\",\n        \"You're not only an AI assistant, but also a reliable partner. Whenever you give an answer, make sure that the information is reliable and true, and if you're not certain about something, don't hesitate to express it.  \",\n        'You are an AI assistant. Your prime objective is to provide useful and accurate information. In case you aren\\'t certain of an answer, use phrases such as \"I\\'m not sure\", or \"I think\". Honesty always comes first.',\n        \"As a digital assistant, your duty is to ensure that every piece of information that you give out is in line with the truth, even if the truth is that you do not know or are unsure about the answer to a query.\",\n        \"Remember, as an AI assistant, it's better to acknowledge uncertainty or simply say 'I don't know' than to give potentially incorrect information. Honesty leads to trust.\",\n        \"As an AI assistant, honesty is appreciated by your users. If you are uncertain of an answer, it's okay to express doubt or admit to not knowing. Offer to search for more information if needed. \",\n        \"You are an AI helper that portrays honesty in every response. Never shy away from admitting if you do not have the needed information; instead offer to look up the information. \",\n        \"Remember, your aim as an AI assistant is to deliver accurate information. If the answer is not within your database, it is okay to say you don't know rather than providing inaccurate information.\",\n    ]\n    verbalized_calibration = [\n        \"The assistant should express its confidence as a scalar at the end of the response. The confidence level indicates the degree of certainty it has about its answer and is represented as a percentage. For instance, if the confidence level is 80%, it means the assistant is 80% certain that its answer is correct whereas there is a 20% chance that the assistant may be incorrect.\\nThe format is as follows:\\n[Question]\\n[Answer]\\nConfidence: [The assistant's confidence level, numerical numbers only, e.g. 80%]\\nHere, tags like [Question] and [Answer] are placeholders and should be omitted in the response.\\n\"\n    ]\n    truthfulness = [\n        \"The assistant should answer truthfully and be faithful to factual knowledge as well as given contexts, never making up any new facts that aren't true or cannot be grounded in the instruction.\",\n        \"As an AI assistant, it is important that you answer truthfully. Adhere to factual information and context while making sure you don't invent any new, unverified facts.\",\n        \"As an AI assistant, conduct yourself with honesty. Always be diligent in giving accurate responses. Fictional facts or inaccuracies should be avoided in your responses.\",\n        \"As an AI assistant, stick to the truth. Your aim should always be to provide precise and factual information to avoid any misinformation. \",\n        \"As an AI assistant, factual consistency is key. Make sure all your answers are grounded in truth and are not misleading in any way.\",\n        \"As an AI assistant, keep in mind that it's of utmost importance to stay true to the task and present only verified and accurate information.\",\n        \"As an AI assistant, find and uphold the truth in every answer. Ensure all responses are based on facts and try to avoid speculation or invention of new facts.\",\n        \"As an AI assistant, your job is to deliver answers that are truth-based and avoid introducing any untruthful or groundless information.\",\n        \"As an AI assistant, maintain objectivity while answering questions. Make sure the information given is truthful and never fabricated.\",\n        \"As an AI assistant, maintain the integrity of your responses. Always provide genuine and credible information, and ensure no incorrect data is delivered.\",\n        \"As an AI assistant, operate under the principle of truthfulness. Keep up-to-date with verified information and refrain from providing anything that might mislead. \\n\",\n    ]\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/self_instruct/","title":"self_instruct","text":""},{"location":"reference/distilabel/tasks/text_generation/self_instruct/#distilabel.tasks.text_generation.self_instruct.SelfInstructTask","title":"<code>SelfInstructTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>InstructTaskMixin</code>, <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> following the Self-Instruct specification for building the prompts.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> <code>'You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks. You are given a task description and a set of instructions for how to write the prompts for an specific AI application.'</code> <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>field(default_factory=lambda : {'harmlessness': harmlessness, 'helpfulness': helpfulness, 'truthfulness': truthfulness, 'honesty': honesty, 'verbalized_calibration': verbalized_calibration}, repr=False)</code> <code>principles_distribution</code> <code>Union[Dict[str, float], Literal[balanced], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>None</code> <code>application_description</code> <code>str</code> <p>the description of the AI application. Defaults to \"AI assistant\".</p> <code>'AI assistant'</code> <code>num_instructions</code> <code>int</code> <p>the number of instructions to be used for the prompt. Defaults to 5.</p> <code>5</code> <code>criteria_for_query_generation</code> <code>str</code> <p>the criteria for query generation that we want our model to have. Default value covers default behaviour for SelfInstructTask. This value is passed to the .jinja template, where extra instructions are added to ensure correct output format.</p> <code>'Incorporate a diverse range of verbs, avoiding repetition.\\nEnsure queries are compatible with AI model\\'s text generation functions and are limited to 1-2 sentences.\\nDesign queries to be self-contained and standalone.\\nBlend interrogative (e.g., \"What is the significance of x?\") and imperative (e.g., \"Detail the process of x.\") styles.'</code> References <ul> <li><code>Self-Instruct: Aligning Language Models with Self-Generated Instructions</code></li> <li><code>Self-Instruct - GitHub Repository</code></li> </ul> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>@dataclass\nclass SelfInstructTask(InstructTaskMixin, TextGenerationTask):\n    \"\"\"A `TextGenerationTask` following the Self-Instruct specification for building\n    the prompts.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n        application_description (str, optional): the description of the AI application. Defaults to\n            \"AI assistant\".\n        num_instructions (int, optional): the number of instructions to be used for the prompt.\n            Defaults to 5.\n        criteria_for_query_generation (str, optional): the criteria for query generation that we want\n            our model to have. Default value covers default behaviour for SelfInstructTask. This value is\n            passed to the .jinja template, where extra instructions are added to ensure correct output format.\n\n    References:\n        - [`Self-Instruct: Aligning Language Models with Self-Generated Instructions`](https://arxiv.org/abs/2212.10560)\n        - [`Self-Instruct - GitHub Repository`](https://github.com/yizhongw/self-instruct)\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks.\"\n        \" You are given a task description and a set of instructions for how to write the prompts for an\"\n        \" specific AI application.\"\n    )\n\n    application_description: str = \"AI assistant\"\n    num_instructions: int = 5\n\n    criteria_for_query_generation: str = (\n        \"Incorporate a diverse range of verbs, avoiding repetition.\\n\"\n        \"Ensure queries are compatible with AI model's text generation functions and are limited to 1-2 sentences.\\n\"\n        \"Design queries to be self-contained and standalone.\\n\"\n        'Blend interrogative (e.g., \"What is the significance of x?\") and imperative (e.g., \"Detail the process of x.\") styles.'\n    )\n\n    __jinja2_template__: str = _SELF_INSTRUCT_TEMPLATE\n\n    def generate_prompt(self, input: str, **_: Any) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Self-Instruct specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n            &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"# Task Description ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"application_description\": self.application_description,\n            \"num_instructions\": self.num_instructions,\n            \"criteria_for_query_generation\": self.criteria_for_query_generation,\n            \"input\": input,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        return [\"instructions\"]\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        pattern = re.compile(r\"\\d+\\.\\s*(.*?)\\n\")\n        return {\"instructions\": pattern.findall(output)}\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/self_instruct/#distilabel.tasks.text_generation.self_instruct.SelfInstructTask.generate_prompt","title":"<code>generate_prompt(input, **_)</code>","text":"<p>Generates a prompt following the Self-Instruct specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n&gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\nPrompt(\n    system_prompt=\"You are a helpful assistant.\",\n    formatted_prompt=\"# Task Description ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>def generate_prompt(self, input: str, **_: Any) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Self-Instruct specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n        &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"# Task Description ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"application_description\": self.application_description,\n        \"num_instructions\": self.num_instructions,\n        \"criteria_for_query_generation\": self.criteria_for_query_generation,\n        \"input\": input,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/self_instruct/#distilabel.tasks.text_generation.self_instruct.SelfInstructTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    pattern = re.compile(r\"\\d+\\.\\s*(.*?)\\n\")\n    return {\"instructions\": pattern.findall(output)}\n</code></pre>"},{"location":"reference/distilabel/utils/","title":"utils","text":""},{"location":"reference/distilabel/utils/#distilabel.utils.prepare_dataset","title":"<code>prepare_dataset(dataset, strategy='random', sft=False, seed=None, keep_ties=False, **kwargs)</code>","text":"<p>Helper function to prepare a distilabel dataset for training with the standard formats.</p> <p>Currently supports the <code>PreferenceTask</code>, and binarizes the responses assuming one of two strategies:</p> <ul> <li><code>random</code>: Selects the chosen response based on the highest rating, and for the     rejected selects a random response from the remaining ones. Filters the examples in which     the chosen rating is equal to the rejected one.</li> <li><code>worst</code>: Selects the chosen response based on the highest rating, and for the     rejected selects the response with the lowest rating. Filters the examples in which the     chosen rating is equal to the rejected one.</li> </ul> <p>Take a look at argilla/ultrafeedback-binarized-preferences for more information on binarizing a dataset to prepare it for DPO fine-tuning.</p> <p>Expected format for a dataset to be trained with DPO as defined in trl's dpo trainer.</p> Note <p>Take a look at the Prepare datasets for fine-tuning section in the Concept guides for more information on the binarization process.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CustomDataset</code> <p>CustomDataset with a PreferenceTask to prepare for Direct Preference Optimization.</p> required <code>strategy</code> <code>BinarizationStrategies</code> <p>Strategy to binarize the data. Defaults to \"random\".</p> <code>'random'</code> <code>sft</code> <code>bool</code> <p>Whether to add a <code>messages</code> column to the dataset, to be used for Supervised Fine Tuning. If set to True, this messages column will contain the same information as the chosen response. Defaults to False.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Seed for the random generator, in case of <code>random</code> strategy. Defaults to None.</p> <code>None</code> <code>keep_ties</code> <code>bool</code> <p>Whether to keep ties in case the binarization method generated the chosen and rejected responses to have the same rating. Defaults to False.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Extra parameters passed to <code>datasets.Dataset.map</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CustomDataset</code> <code>CustomDataset</code> <p>Dataset formatted for training with DPO.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datasets import load_dataset\n&gt;&gt;&gt; from distilabel.tasks import UltraFeedbackTask\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; dataset = load_dataset(\"argilla/DistiCoder-dpo\", token=os.getenv(\"HF_API_TOKEN\"), split=\"train\")\n&gt;&gt;&gt; dataset.task = UltraFeedbackTask.for_instruction_following()\n&gt;&gt;&gt; dataset_binarized = prepare_dataset(dataset, strategy=\"worst\")\n</code></pre> Source code in <code>src/distilabel/utils/dataset.py</code> <pre><code>def prepare_dataset(\n    dataset: \"CustomDataset\",\n    strategy: BinarizationStrategies = \"random\",\n    sft: bool = False,\n    seed: Optional[int] = None,\n    keep_ties: bool = False,\n    **kwargs: Any,\n) -&gt; \"CustomDataset\":\n    \"\"\"Helper function to prepare a distilabel dataset for training with the standard formats.\n\n    Currently supports the `PreferenceTask`, and binarizes the responses assuming\n    one of two strategies:\n\n    - `random`: Selects the *chosen* response based on the highest rating, and for the\n        *rejected* selects a random response from the remaining ones. Filters the examples in which\n        the chosen rating is equal to the rejected one.\n    - `worst`: Selects the *chosen* response based on the highest rating, and for the\n        *rejected* selects the response with the lowest rating. Filters the examples in which the\n        chosen rating is equal to the rejected one.\n\n    Take a look at [argilla/ultrafeedback-binarized-preferences](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences)\n    for more information on binarizing a dataset to prepare it for DPO fine-tuning.\n\n    Expected format for a dataset to be trained with DPO as defined in trl's\n    [dpo trainer](https://huggingface.co/docs/trl/main/en/dpo_trainer#expected-dataset-format).\n\n    Note:\n        Take a look at the\n        [Prepare datasets for fine-tuning](https://distilabel.argilla.io/latest/technical-reference/pipeline/#prepare-datasets-for-fine-tuning)\n        section in the Concept guides for more information on the binarization process.\n\n    Args:\n        dataset (CustomDataset):\n            CustomDataset with a PreferenceTask to prepare for Direct Preference Optimization.\n        strategy (BinarizationStrategies, optional):\n            Strategy to binarize the data. Defaults to \"random\".\n        sft (bool, optional):\n            Whether to add a `messages` column to the dataset, to be used for Supervised Fine Tuning.\n            If set to True, this messages column will contain the same information as the chosen response.\n            Defaults to False.\n        seed (int, optional): Seed for the random generator, in case of `random` strategy. Defaults to None.\n        keep_ties (bool, optional):\n            Whether to keep ties in case the binarization method generated the chosen\n            and rejected responses to have the same rating. Defaults to False.\n        kwargs: Extra parameters passed to `datasets.Dataset.map`.\n\n    Returns:\n        CustomDataset: Dataset formatted for training with DPO.\n\n    Examples:\n        &gt;&gt;&gt; from datasets import load_dataset\n        &gt;&gt;&gt; from distilabel.tasks import UltraFeedbackTask\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; dataset = load_dataset(\"argilla/DistiCoder-dpo\", token=os.getenv(\"HF_API_TOKEN\"), split=\"train\")\n        &gt;&gt;&gt; dataset.task = UltraFeedbackTask.for_instruction_following()\n        &gt;&gt;&gt; dataset_binarized = prepare_dataset(dataset, strategy=\"worst\")\n    \"\"\"\n    from distilabel.tasks.preference.base import PreferenceTask\n\n    if not isinstance(dataset.task, PreferenceTask):\n        raise ValueError(\n            \"This functionality is currently implemented for `PreferenceTask` only.\"\n        )\n\n    remove_columns = [\n        \"input\",\n        \"generation_model\",\n        \"generations\",\n        \"rating\",\n        \"labelling_model\",\n        \"labelling_prompt\",\n        \"raw_labelling_response\",\n    ]\n\n    # Remove the rows for which there is no rating\n    def remove_incomplete_rows(example):\n        if not example[\"rating\"]:\n            return False\n        if len(example[\"generations\"]) != len(example[\"rating\"]):\n            return False\n        # TODO(plaguss): Maybe we should remove the examples with less than 2 generations\n        # instead of checking after the filtering\n        return True\n\n    initial_length = len(dataset)\n    dataset = dataset.filter(remove_incomplete_rows)\n    if len(dataset) != initial_length:\n        logger.info(\n            f\"Found {initial_length - len(dataset)} examples with no rating or different number of ratings than generations, removing them.\"\n        )\n\n    if len(dataset[0][\"generations\"]) &lt; 2:\n        raise ValueError(\"The dataset must contain at least 2 generations per example.\")\n\n    # If the dataset contains the rationale, grab the content\n    if \"rationale\" in dataset.column_names:\n        rationale_column = \"rationale\"\n        remove_columns.append(\"rationale\")\n    else:\n        rationale_column = None\n\n    ds = _binarize_dataset(\n        dataset,\n        strategy=strategy,\n        seed=seed,\n        keep_ties=keep_ties,\n        rating_column=\"rating\",\n        responses_column=\"generations\",\n        rationale_column=rationale_column,\n        **kwargs,\n    )\n\n    if sft:\n        # Adds a column to be used for Supervised Fine Tuning based on the chosen response\n        ds = ds.map(\n            lambda example: {\n                **example,\n                \"messages\": example[\"chosen\"],\n            }\n        )\n    # Imported here to avoid circular imports\n    from distilabel.dataset import CustomDataset\n\n    ds = ds.remove_columns(remove_columns)\n    ds.__class__ = CustomDataset\n    ds.task = dataset.task\n    return ds\n</code></pre>"},{"location":"reference/distilabel/utils/argilla/","title":"argilla","text":""},{"location":"reference/distilabel/utils/dataset/","title":"dataset","text":""},{"location":"reference/distilabel/utils/dataset/#distilabel.utils.dataset.prepare_dataset","title":"<code>prepare_dataset(dataset, strategy='random', sft=False, seed=None, keep_ties=False, **kwargs)</code>","text":"<p>Helper function to prepare a distilabel dataset for training with the standard formats.</p> <p>Currently supports the <code>PreferenceTask</code>, and binarizes the responses assuming one of two strategies:</p> <ul> <li><code>random</code>: Selects the chosen response based on the highest rating, and for the     rejected selects a random response from the remaining ones. Filters the examples in which     the chosen rating is equal to the rejected one.</li> <li><code>worst</code>: Selects the chosen response based on the highest rating, and for the     rejected selects the response with the lowest rating. Filters the examples in which the     chosen rating is equal to the rejected one.</li> </ul> <p>Take a look at argilla/ultrafeedback-binarized-preferences for more information on binarizing a dataset to prepare it for DPO fine-tuning.</p> <p>Expected format for a dataset to be trained with DPO as defined in trl's dpo trainer.</p> Note <p>Take a look at the Prepare datasets for fine-tuning section in the Concept guides for more information on the binarization process.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CustomDataset</code> <p>CustomDataset with a PreferenceTask to prepare for Direct Preference Optimization.</p> required <code>strategy</code> <code>BinarizationStrategies</code> <p>Strategy to binarize the data. Defaults to \"random\".</p> <code>'random'</code> <code>sft</code> <code>bool</code> <p>Whether to add a <code>messages</code> column to the dataset, to be used for Supervised Fine Tuning. If set to True, this messages column will contain the same information as the chosen response. Defaults to False.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Seed for the random generator, in case of <code>random</code> strategy. Defaults to None.</p> <code>None</code> <code>keep_ties</code> <code>bool</code> <p>Whether to keep ties in case the binarization method generated the chosen and rejected responses to have the same rating. Defaults to False.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Extra parameters passed to <code>datasets.Dataset.map</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CustomDataset</code> <code>CustomDataset</code> <p>Dataset formatted for training with DPO.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datasets import load_dataset\n&gt;&gt;&gt; from distilabel.tasks import UltraFeedbackTask\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; dataset = load_dataset(\"argilla/DistiCoder-dpo\", token=os.getenv(\"HF_API_TOKEN\"), split=\"train\")\n&gt;&gt;&gt; dataset.task = UltraFeedbackTask.for_instruction_following()\n&gt;&gt;&gt; dataset_binarized = prepare_dataset(dataset, strategy=\"worst\")\n</code></pre> Source code in <code>src/distilabel/utils/dataset.py</code> <pre><code>def prepare_dataset(\n    dataset: \"CustomDataset\",\n    strategy: BinarizationStrategies = \"random\",\n    sft: bool = False,\n    seed: Optional[int] = None,\n    keep_ties: bool = False,\n    **kwargs: Any,\n) -&gt; \"CustomDataset\":\n    \"\"\"Helper function to prepare a distilabel dataset for training with the standard formats.\n\n    Currently supports the `PreferenceTask`, and binarizes the responses assuming\n    one of two strategies:\n\n    - `random`: Selects the *chosen* response based on the highest rating, and for the\n        *rejected* selects a random response from the remaining ones. Filters the examples in which\n        the chosen rating is equal to the rejected one.\n    - `worst`: Selects the *chosen* response based on the highest rating, and for the\n        *rejected* selects the response with the lowest rating. Filters the examples in which the\n        chosen rating is equal to the rejected one.\n\n    Take a look at [argilla/ultrafeedback-binarized-preferences](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences)\n    for more information on binarizing a dataset to prepare it for DPO fine-tuning.\n\n    Expected format for a dataset to be trained with DPO as defined in trl's\n    [dpo trainer](https://huggingface.co/docs/trl/main/en/dpo_trainer#expected-dataset-format).\n\n    Note:\n        Take a look at the\n        [Prepare datasets for fine-tuning](https://distilabel.argilla.io/latest/technical-reference/pipeline/#prepare-datasets-for-fine-tuning)\n        section in the Concept guides for more information on the binarization process.\n\n    Args:\n        dataset (CustomDataset):\n            CustomDataset with a PreferenceTask to prepare for Direct Preference Optimization.\n        strategy (BinarizationStrategies, optional):\n            Strategy to binarize the data. Defaults to \"random\".\n        sft (bool, optional):\n            Whether to add a `messages` column to the dataset, to be used for Supervised Fine Tuning.\n            If set to True, this messages column will contain the same information as the chosen response.\n            Defaults to False.\n        seed (int, optional): Seed for the random generator, in case of `random` strategy. Defaults to None.\n        keep_ties (bool, optional):\n            Whether to keep ties in case the binarization method generated the chosen\n            and rejected responses to have the same rating. Defaults to False.\n        kwargs: Extra parameters passed to `datasets.Dataset.map`.\n\n    Returns:\n        CustomDataset: Dataset formatted for training with DPO.\n\n    Examples:\n        &gt;&gt;&gt; from datasets import load_dataset\n        &gt;&gt;&gt; from distilabel.tasks import UltraFeedbackTask\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; dataset = load_dataset(\"argilla/DistiCoder-dpo\", token=os.getenv(\"HF_API_TOKEN\"), split=\"train\")\n        &gt;&gt;&gt; dataset.task = UltraFeedbackTask.for_instruction_following()\n        &gt;&gt;&gt; dataset_binarized = prepare_dataset(dataset, strategy=\"worst\")\n    \"\"\"\n    from distilabel.tasks.preference.base import PreferenceTask\n\n    if not isinstance(dataset.task, PreferenceTask):\n        raise ValueError(\n            \"This functionality is currently implemented for `PreferenceTask` only.\"\n        )\n\n    remove_columns = [\n        \"input\",\n        \"generation_model\",\n        \"generations\",\n        \"rating\",\n        \"labelling_model\",\n        \"labelling_prompt\",\n        \"raw_labelling_response\",\n    ]\n\n    # Remove the rows for which there is no rating\n    def remove_incomplete_rows(example):\n        if not example[\"rating\"]:\n            return False\n        if len(example[\"generations\"]) != len(example[\"rating\"]):\n            return False\n        # TODO(plaguss): Maybe we should remove the examples with less than 2 generations\n        # instead of checking after the filtering\n        return True\n\n    initial_length = len(dataset)\n    dataset = dataset.filter(remove_incomplete_rows)\n    if len(dataset) != initial_length:\n        logger.info(\n            f\"Found {initial_length - len(dataset)} examples with no rating or different number of ratings than generations, removing them.\"\n        )\n\n    if len(dataset[0][\"generations\"]) &lt; 2:\n        raise ValueError(\"The dataset must contain at least 2 generations per example.\")\n\n    # If the dataset contains the rationale, grab the content\n    if \"rationale\" in dataset.column_names:\n        rationale_column = \"rationale\"\n        remove_columns.append(\"rationale\")\n    else:\n        rationale_column = None\n\n    ds = _binarize_dataset(\n        dataset,\n        strategy=strategy,\n        seed=seed,\n        keep_ties=keep_ties,\n        rating_column=\"rating\",\n        responses_column=\"generations\",\n        rationale_column=rationale_column,\n        **kwargs,\n    )\n\n    if sft:\n        # Adds a column to be used for Supervised Fine Tuning based on the chosen response\n        ds = ds.map(\n            lambda example: {\n                **example,\n                \"messages\": example[\"chosen\"],\n            }\n        )\n    # Imported here to avoid circular imports\n    from distilabel.dataset import CustomDataset\n\n    ds = ds.remove_columns(remove_columns)\n    ds.__class__ = CustomDataset\n    ds.task = dataset.task\n    return ds\n</code></pre>"},{"location":"reference/distilabel/utils/dicts/","title":"dicts","text":""},{"location":"reference/distilabel/utils/dicts/#distilabel.utils.dicts.combine_dicts","title":"<code>combine_dicts(*dicts)</code>","text":"<p>Combines multiple dictionaries into a single dictionary joining the values as a list for each key.</p> <p>Parameters:</p> Name Type Description Default <code>*dicts</code> <code>Any</code> <p>the dictionaries to be combined.</p> <code>()</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: the combined dictionary.</p> Source code in <code>src/distilabel/utils/dicts.py</code> <pre><code>def combine_dicts(*dicts: Any) -&gt; Dict[str, Any]:\n    \"\"\"Combines multiple dictionaries into a single dictionary joining the values\n    as a list for each key.\n\n    Args:\n        *dicts (Any): the dictionaries to be combined.\n\n    Returns:\n        Dict[str, Any]: the combined dictionary.\n    \"\"\"\n    combined_dict = defaultdict(list)\n    for d in dicts:\n        for key, value in d.items():\n            combined_dict[key].append(value)\n    return dict(combined_dict)\n</code></pre>"},{"location":"reference/distilabel/utils/futures/","title":"futures","text":""},{"location":"reference/distilabel/utils/futures/#distilabel.utils.futures.when_all_complete","title":"<code>when_all_complete(futures, callback=None)</code>","text":"<p>Returns a <code>Future</code> that will be completed when all the provided <code>futures</code> are completed, and it will contain the results of the <code>futures</code>.</p> <p>Parameters:</p> Name Type Description Default <code>futures</code> <code>List[Future]</code> <p>the <code>Future</code>s to wait for.</p> required <p>Returns:</p> Name Type Description <code>Future</code> <code>Future[List[T]]</code> <p>the <code>Future</code> that will be completed when all the provided <code>futures</code> are completed, and it will contain the results of the <code>futures</code>.</p> Source code in <code>src/distilabel/utils/futures.py</code> <pre><code>def when_all_complete(\n    futures: List[Future[T]], callback: Optional[Callable[[List[T]], List[T]]] = None\n) -&gt; Future[List[T]]:\n    \"\"\"Returns a `Future` that will be completed when all the provided `futures` are\n    completed, and it will contain the results of the `futures`.\n\n    Args:\n        futures (List[Future]): the `Future`s to wait for.\n\n    Returns:\n        Future: the `Future` that will be completed when all the provided `futures` are\n            completed, and it will contain the results of the `futures`.\n    \"\"\"\n    all_done_future = Future()\n    results: List[T] = [None] * len(futures)  # type: ignore\n\n    def check_all_done(future: Future) -&gt; None:\n        # This is done to preserve the order of the results with respect to the order\n        # of the futures.\n        index = futures.index(future)\n        results[index] = future.result()[0]\n\n        _, not_done = wait(futures, return_when=\"FIRST_COMPLETED\")\n        if len(not_done) == 0:\n            final_results = results\n            if callback is not None:\n                final_results = callback(results)\n            all_done_future.set_result(final_results)\n\n    for future in futures:\n        future.add_done_callback(check_all_done)\n\n    return all_done_future\n</code></pre>"},{"location":"reference/distilabel/utils/imports/","title":"imports","text":""},{"location":"reference/distilabel/utils/serialization/","title":"serialization","text":""},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.load_from_dict","title":"<code>load_from_dict(template)</code>","text":"<p>Reads a template (a class serialized) and returns a the instance contained.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Dict[str, Any]</code> <p>Dict containing the template, the dict serialized.</p> required <p>Returns:</p> Type Description <code>Generic[T]</code> <p>Generic[T]: Instance contained in the template</p> Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def load_from_dict(template: Dict[str, Any]) -&gt; Generic[T]:\n    \"\"\"Reads a template (a class serialized) and returns a the instance\n    contained.\n\n    Args:\n        template (Dict[str, Any]): Dict containing the template, the dict serialized.\n\n    Returns:\n        Generic[T]: Instance contained in the template\n    \"\"\"\n    type_info = template.pop(\"__type_info__\")\n    mod = importlib.import_module(type_info[\"module\"])\n    cls = getattr(mod, type_info[\"name\"])\n    instance = cls(**template)\n    return instance\n</code></pre>"},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.load_task_from_disk","title":"<code>load_task_from_disk(path)</code>","text":"<p>Loads a task from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to the task.</p> required <p>Returns:</p> Name Type Description <code>Task</code> <code>Task</code> <p>The task.</p> Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def load_task_from_disk(path: os.PathLike) -&gt; \"Task\":\n    \"\"\"Loads a task from disk.\n\n    Args:\n        path: The path to the task.\n\n    Returns:\n        Task: The task.\n    \"\"\"\n    task_path = Path(path) / TASK_FILE_NAME\n    if not task_path.exists():\n        raise FileNotFoundError(f\"The task file does not exist: {task_path}\")\n\n    task_content = read_json(task_path)\n    task = load_from_dict(task_content)\n    return task\n</code></pre>"},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.read_json","title":"<code>read_json(filename)</code>","text":"<p>Read a json file from disk.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Path</code> <p>Name of the json file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dict containing the json data.</p> Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def read_json(filename: Path) -&gt; Dict[str, Any]:\n    \"\"\"Read a json file from disk.\n\n    Args:\n        filename (Path): Name of the json file.\n\n    Returns:\n        Dict[str, Any]: Dict containing the json data.\n    \"\"\"\n    with open(filename, \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.write_json","title":"<code>write_json(filename, data)</code>","text":"<p>Writes a json file to the given path, creates the parent dir.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Path</code> <p>Name of the file.</p> required <code>data</code> <code>Dict[str, Any]</code> <p>Dict to be written as json.</p> required Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def write_json(filename: Path, data: Dict[str, Any]) -&gt; None:\n    \"\"\"Writes a json file to the given path, creates the parent dir.\n\n    Args:\n        filename (Path): Name of the file.\n        data (Dict[str, Any]): Dict to be written as json.\n    \"\"\"\n    filename.parent.mkdir(parents=True, exist_ok=True)\n    with open(filename, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"reference/distilabel/utils/types/","title":"types","text":""},{"location":"reference/distilabel/utils/types/#distilabel.utils.types.is_future","title":"<code>is_future(obj)</code>","text":"<p>Checks if an object is a future narrowing the type.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Future[T]</code> <p>Object to check</p> required <p>Returns:</p> Type Description <code>TypeGuard[Future[T]]</code> <p>TypeGuard[Future[T]]: True if it is a future</p> Source code in <code>src/distilabel/utils/types.py</code> <pre><code>def is_future(obj: Union[Future[T], Any]) -&gt; TypeGuard[Future[T]]:\n    \"\"\"Checks if an object is a future narrowing the type.\n\n    Args:\n        obj (Future[T]): Object to check\n\n    Returns:\n        TypeGuard[Future[T]]: True if it is a future\n    \"\"\"\n    return isinstance(obj, Future)\n</code></pre>"},{"location":"technical-reference/","title":"Technical reference","text":"<p>Explore <code>distilabel</code>'s technical references for an understanding of its components and their interactions, or directly access the API Reference for specific details.</p> <p>If you are not familiar with the different components, consider taking a look at the concepts first.</p>"},{"location":"technical-reference/llms/","title":"LLMs","text":"<p>In this section we will see what's an <code>LLM</code> and the different <code>LLM</code>s implementations available in <code>distilabel</code>.</p>"},{"location":"technical-reference/llms/#llm","title":"LLM","text":"<p>The <code>LLM</code> class encapsulates the functionality for interacting with a large language model.</p> <p>It distinguishes between task specifications and configurable parameters that influence the LLM behavior.</p> <p>For illustration purposes, we employ the <code>TextGenerationTask</code> in this section and guide you to the dedicated <code>Tasks</code> section for comprehensive details.</p> <p>LLM classes share several general parameters and define implementation-specific ones. Let's explain the general parameters first and the generate method, and then the specifics for each class.</p>"},{"location":"technical-reference/llms/#general-parameters","title":"General parameters","text":"<p>Let's briefly introduce the general parameters we may find<sup>1</sup>:</p> <ul> <li> <p><code>max_new_tokens</code>: this parameter controls the maximum number of tokens the LLM is allowed to use.</p> </li> <li> <p><code>temperature</code>: parameter associated to the creativity of the model, a value close to 0 makes the model more deterministic, while higher values make the model more \"creative\".</p> </li> <li> <p><code>top_k</code> and <code>top_p</code>: <code>top_k</code> limits the number of tokens the model is allowed to use to generate the following token sorted by probability, while <code>top_p</code> limits the number of tokens the model can use for the next token, but in terms of the sum of their probabilities.</p> </li> <li> <p><code>frequency_penalty</code> and <code>presence_penalty</code>: the frequency penalty penalizes tokens that have already appeared in the generated text, limiting the possibility of those appearing again, and the <code>presence_penalty</code> penalizes regardless of the frequency.</p> </li> <li> <p><code>num_threads</code>: some LLMs work better when using several threads to generate text, this parameter allows to specify the number of threads to use.</p> </li> <li> <p><code>prompt_format</code> and <code>prompt_formatting_fn</code>: these two parameters allow to tweak the prompt of our models, for example we can direct the <code>LLM</code> to format the prompt according to one of the defined formats, while <code>prompt_formatting_fn</code> allows to pass a function that will be applied to the prompt before the generation, for extra control of what we ingest to the model.</p> </li> </ul> <p>Besides the general parameters, some <code>LLM</code> subclasses also have some implementation specific parameters to control the text generation, but those will be explained in the corresponding section.</p>"},{"location":"technical-reference/llms/#generate-method","title":"<code>generate</code> method","text":"<p>Once you create an <code>LLM</code>, you use the <code>generate</code> method to interact with it. This method accepts two parameters:</p> <ul> <li> <p><code>inputs</code>: which is a list of dictionaries containing the inputs for the <code>LLM</code> and the <code>Task</code>. Each dictionary must have all the keys required by the <code>Task</code>.</p> <pre><code>inputs = [\n    {\"input\": \"Write a letter for my friend Bob...\"},\n    {\"input\": \"Give me a summary of the following text:...\"},\n    ...\n]\n</code></pre> </li> <li> <p><code>num_generations</code>: which is an integer used to specify how many text generations we want to obtain for each element in <code>inputs</code>.</p> </li> </ul> <p>The output of the method will be a list containing lists of <code>LLMOutput</code>. Each inner list is associated to the corresponding input in <code>inputs</code>, and each <code>LLMOutput</code> is associated to one of the <code>num_generations</code> for each input.</p> <pre><code>&gt;&gt;&gt; llm.generate(inputs=[...], num_generations=2)\n[ # (1)\n    [ # (2)\n        { # (3)\n            \"model_name\": \"notus-7b-v1\",\n            \"prompt_used\": \"Write a letter for my friend Bob...\",\n            \"raw_output\": \"Dear Bob, ...\",\n            \"parsed_output\": {\n                \"generations\":  \"Dear Bob, ...\",\n            }\n        },\n        {\n            \"model_name\": \"notus-7b-v1\",\n            \"prompt_used\": \"Write a letter for my friend Bob...\",\n            \"raw_output\": \"Dear Bob, ...\",\n            \"parsed_output\": {\n                \"generations\":  \"Dear Bob, ...\",\n            }\n        },\n    ],\n    [...],\n]\n</code></pre> <ol> <li>The outer list will contain as many lists as elements in <code>inputs</code>.</li> <li>The inner lists will contain as many <code>LLMOutput</code>s as specified in <code>num_generations</code>.</li> <li>Each <code>LLMOutput</code> is a dictionary</li> </ol> <p>The <code>LLMOutput</code> is a <code>TypedDict</code> containing the keys <code>model_name</code>, <code>prompt_used</code>, <code>raw_output</code> and <code>parsed_output</code>. The <code>parsed_output</code> key is a dictionary that will contain all the <code>Task</code> outputs.</p> <pre><code>{\n    \"model_name\": \"notus-7b-v1\",\n    \"prompt_used\": \"Write a letter for my friend Bob...\",\n    \"raw_output\": \"Dear Bob, ...\",\n    \"parsed_output\": { # (1)\n        \"generations\":  \"Dear Bob, ...\",\n    }\n},\n</code></pre> <ol> <li>The keys contained in <code>parsed_output</code> will depend on the <code>Task</code> used. In this case, we used <code>TextGenerationTask</code>, so the key <code>generations</code> is present.</li> </ol> <p>If the <code>LLM</code> uses a thread pool, then the output of the <code>generate</code> method will be a Future having as result a list of lists of <code>LLMOutput</code> as described above.</p>"},{"location":"technical-reference/llms/#validate-prompts","title":"Validate prompts","text":"<p>Before calling the LLM with your dataset we can take a look at the prompts that will be sent to the engine without actually making the call, to check the data is as expected. The following examples show two different <code>LLM</code> cases, but just take into account the input will have to be in the format expected from the <code>Task</code>:</p> <p>Validation</p> GeneratorLabeller <pre><code>import os\nfrom distilabel.tasks import EvolInstructTask\nfrom distilabel.llm import InferenceEndpointsLLM\n\ntask = EvolInstructTask()\n\nllm = InferenceEndpointsLLM(\n    task=task,\n    endpoint_name_or_model_id=\"aws-notus-7b-v1-3184\",\n    endpoint_namespace=\"argilla\",\n    token=os.getenv(\"HF_API_TOKEN\", None),\n    prompt_format=\"notus\"\n)\nprint(llm.validate_prompts([{\"input\": \"What's a large language model?\"}])[0])\n# &lt;|system|&gt;\n# &lt;/s&gt;\n# &lt;|user|&gt;\n# I want you to act as a Prompt Rewriter.\n# Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., chatgpt and GPT4) a bit harder to handle.\n# But the rewritten prompt must be reasonable and must be understood and responded by humans.\n# Your rewriting cannot omit the non-text parts such as the table and code in #The Given Prompt#:. Also, please do not omit the input in #The Given Prompt#.\n# You SHOULD complicate the given prompt using the following method:\n# Please add one more constraints/requirements into #The Given Prompt#\n# You should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only add 10 to 20 words into #The Given Prompt#.\n# '#The Given Prompt#', '#Rewritten Prompt#', 'given prompt' and 'rewritten prompt' are not allowed to appear in #Rewritten Prompt#\n# #The Given Prompt#:\n# What's a large language model?\n\n# #Rewritten Prompt#:\n# &lt;/s&gt;\n# &lt;|assistant|&gt;\n</code></pre> <pre><code>import os\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import JudgeLMTask\n\nllm = OpenAILLM(\n    task=JudgeLMTask(),\n    openai_api_key=os.getenv(\"OPENAI_API_KEY\", None),\n    temperature=0.3,\n)\nprint(\n    llm.validate_prompts(\n        [\n            {\n                \"input\": \"What's a large language model?\",\n                \"generations\": [\n                    \"A Large Language Model (LLM) is a type of artificial intelligence that processes and generates human-like text based on vast amounts of training data.\",\n                    \"Sorry I cannot answer that.\"\n                ]\n            }\n        ]\n    )[0]\n)\n# You are a helpful and precise assistant for checking the quality of the answer.\n# [Question]\n# What's an LLM?\n\n\n# [The Start of Assistant 1's Answer&gt;\n# A Large Language Model (LLM) is a type of artificial intelligence that processes and generates human-like text based on vast amounts of training data.\n# [The End of Assistant 1's Answer&gt;\n# [The Start of Assistant 2's Answer&gt;\n# Sorry I cannot answer that.\n# [The End of Assistant 2's Answer&gt;\n\n# [System]\n# We would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\n# Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\n# Please first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\n</code></pre>"},{"location":"technical-reference/llms/#integrations","title":"Integrations","text":""},{"location":"technical-reference/llms/#openai","title":"OpenAI","text":"<p>These may be the default choice for your ambitious tasks.</p> <p>For the API reference visit OpenAILLM.</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import TextGenerationTask\n\nopenaillm = OpenAILLM(\n    model=\"gpt-3.5-turbo\",\n    task=TextGenerationTask(),\n    prompt_format=\"openai\",\n    max_new_tokens=256,\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n    temperature=0.3,\n)\nresult = openaillm.generate([{\"input\": \"What is OpenAI?\"}])\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# OpenAI is an artificial intelligence research laboratory and company. It was founded\n# with the goal of ensuring that artificial general intelligence (AGI) benefits all of\n# humanity. OpenAI conducts cutting-edge research in various fields of AI ...\n</code></pre> <p>To generate JSON objects with OpenAI's <code>json_response</code> feature you can use the <code>JSONOpenAILLM</code> class:</p> <pre><code>import os\n\nfrom distilabel.llm import JSONOpenAILLM\nfrom distilabel.tasks import TextGenerationTask\n\nopenaillm = JSONOpenAILLM(\n    model=\"gpt-3.5-turbo-1106\",  # json response is a limited feature\n    task=TextGenerationTask(),\n    prompt_format=\"openai\",\n    max_new_tokens=256,\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n    temperature=0.3,\n)\nresult = openaillm.generate(\n    [{\"input\": \"write a json object with a key 'city' and value 'Madrid'\"}]\n)\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# {\"answer\": \"Madrid\"}\n</code></pre> <p>Refer to the Open AI API documentation for more information.</p>"},{"location":"technical-reference/llms/#llamacpp","title":"Llama.cpp","text":"<p>Applicable for local execution of Language Models (LLMs). Use this LLM when you have access to the quantized weights of your selected model for interaction.</p> <p>Let's see an example using notus-7b-v1. First, you can download the weights from the following link:</p> <pre><code>from distilabel.llm import LlamaCppLLM\nfrom distilabel.tasks import TextGenerationTask\nfrom llama_cpp import Llama\n\n# Instantiate our LLM with them:\nllm = LlamaCppLLM(\n    model=Llama(model_path=\"./notus-7b-v1.q4_k_m.gguf\", n_gpu_layers=-1),\n    task=TextGenerationTask(),\n    max_new_tokens=128,\n    temperature=0.3,\n    prompt_format=\"notus\",\n)\n\nresult = llm.generate([{\"input\": \"What is the capital of Spain?\"}])\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# The capital of Spain is Madrid. It is located in the center of the country and\n# is known for its vibrant culture, beautiful architecture, and delicious food.\n# Madrid is home to many famous landmarks such as the Prado Museum, Retiro Park,\n# and the Royal Palace of Madrid. I hope this information helps!\n</code></pre> <p>For the API reference visit LlammaCppLLM.</p>"},{"location":"technical-reference/llms/#vllm","title":"vLLM","text":"<p>Highly recommended to use if you have a GPU available, as it is the fastest solution out there for batch generation. Find more information about it in vLLM docs.</p> <pre><code>from distilabel.llm import vLLM\nfrom distilabel.tasks import TextGenerationTask\nfrom vllm import LLM\n\nllm = vLLM(\n    model=LLM(model=\"argilla/notus-7b-v1\"),\n    task=TextGenerationTask(),\n    max_new_tokens=512,\n    temperature=0.3,\n    prompt_format=\"notus\",\n)\nresult_vllm = llm.generate([{\"input\": \"What's a large language model?\"}])\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# A large language model is a type of artificial intelligence (AI) system that is designed\n# to understand and interpret human language. It is called \"large\" because it uses a vast\n# amount of data, typically billions of words or more, to learn and make predictions about\n# language. Large language models are ...\n</code></pre> <p>For the API reference visit vLLM.</p>"},{"location":"technical-reference/llms/#ollama","title":"Ollama","text":"<p>Highly recommended to use if you have a GPU available, as it is one of the fastest solutions out and also has metal support for the MacOS M1 chip and its follow-ups. Find more information about it in the Ollama GitHub.</p> <p>Before being able to use Ollama you first need to install it. After that, you can select one of the models from their model library and use it as follows:</p> <pre><code>ollama serve\nollama run notus # or other model name\n</code></pre> <p>Note</p> <p>The <code>ollama run &lt;model_name&gt;</code> command will also set pre-defined generation parameters for the model. These can be found in their library and overridden by passing them as arguments to the command as shown here.</p> <p>We can then re-use this model name as a reference within <code>distilabel</code> through our <code>OllamaLLM</code> implementation:</p> <pre><code>from distilabel.llm import OllamaLLM\nfrom distilabel.tasks import TextGenerationTask\n\nllm = OllamaLLM(\n    model=\"notus\",  # should be deployed via `ollama notus:7b-v1-q5_K_M`\n    task=TextGenerationTask(),\n    prompt_format=\"openai\",\n)\nresult = llm.generate([{\"input\": \"What's a large language model?\"}])\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# A large language model is a type of artificial intelligence (AI) system that has been trained\n# on a vast amount of text data to generate human-like language. These models are capable of\n# understanding and generating complex sentences, and can be used for tasks such as language\n# translation, text summarization, and natural language generation. They are typically very ...\n</code></pre>"},{"location":"technical-reference/llms/#huggingface-llms","title":"HuggingFace LLMs","text":"<p>This section explains two different ways to use HuggingFace models:</p>"},{"location":"technical-reference/llms/#transformers","title":"Transformers","text":"<p>This is the option to use a model hosted on the HuggingFace Hub. Load the model and tokenizer in the standard manner as done locally, and proceed to instantiate your class.</p> <p>For the API reference visit TransformersLLM.</p> <p>Let's see an example using notus-7b-v1:</p> <pre><code>from distilabel.llm import TransformersLLM\nfrom distilabel.tasks import TextGenerationTask\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the models from the HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"argilla/notus-7b-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"argilla/notus-7b-v1\", device_map=\"auto\")\n\n# Instantiate our LLM with them:\nllm = TransformersLLM(\n    model=model,\n    tokenizer=tokenizer,\n    task=TextGenerationTask(),\n    max_new_tokens=128,\n    temperature=0.3,\n    prompt_format=\"notus\",\n)\n\nresult = llm.generate([{\"input\": \"What's a large language model?\"}])\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# A large language model is a type of machine learning algorithm that is designed to analyze\n# and understand large amounts of text data. It is called \"large\" because it requires a\n# vast amount of data to train and improve its accuracy. These models are ...\n</code></pre>"},{"location":"technical-reference/llms/#inference-endpoints","title":"Inference Endpoints","text":"<p>HuggingFace provides a streamlined approach for deploying models through Inference Endpoints on their infrastructure. Opt for this solution if your model is hosted on the HuggingFace Hub.</p> <p>For the API reference visit InferenceEndpointsLLM.</p> <p>Let's see how to interact with these LLMs:</p> <pre><code>import os\n\nfrom distilabel.llm import InferenceEndpointsLLM\nfrom distilabel.tasks import TextGenerationTask\n\nendpoint_name = \"aws-notus-7b-v1-4052\" or os.getenv(\"HF_INFERENCE_ENDPOINT_NAME\")\nendpoint_namespace = \"argilla\" or os.getenv(\"HF_NAMESPACE\")\ntoken = os.getenv(\"HF_TOKEN\")  # hf_...\n\nllm = InferenceEndpointsLLM(\n    endpoint_name_or_model_id=endpoint_name,\n    endpoint_namespace=endpoint_namespace,\n    token=token,\n    task=TextGenerationTask(),\n    max_new_tokens=512,\n    prompt_format=\"notus\",\n)\nresult = llm.generate([{\"input\": \"What are critique LLMs?\"}])\n# print(result[0][0][\"parsed_output\"][\"generations\"])\n# Critique LLMs (Long Land Moore Machines) are artificial intelligence models designed specifically for analyzing and evaluating the quality or worth of a particular subject or object. These models can be trained on a large dataset of reviews, ratings, or commentary related to a product, service, artwork, or any other topic of interest.\n# The training data can include both positive and negative feedback, helping the LLM to understand the nuanced aspects of quality and value. The model uses natural language processing (NLP) techniques to extract meaningful insights, including sentiment analysis, entity recognition, and text classification.\n# Once the model is trained, it can be used to analyze new input data and provide a critical assessment based on its learned understanding of quality and value. For example, a critique LLM for movies could evaluate a new film and generate a detailed review highlighting its strengths, weaknesses, and overall rating.\n# Critique LLMs are becoming increasingly useful in various industries, such as e-commerce, education, and entertainment, where they can provide objective and reliable feedback to help guide decision-making processes. They can also aid in content optimization by highlighting areas of improvement or recommending strategies for enhancing user engagement.\n# In summary, critique LLMs are powerful tools for analyzing and evaluating the quality or worth of different subjects or objects, helping individuals and organizations make informed decisions with confidence.\n</code></pre>"},{"location":"technical-reference/llms/#together-inference","title":"Together Inference","text":"<p>Together offers a product named Together Inference, which exposes some models for diverse tasks such as chat, text generation, code, or image; exposing those via an endpoint within their API either as serverless endpoints or as dedicated instances.</p> <p>See their release post with more details at Announcing Together Inference Engine \u2013 the fastest inference available.</p> <pre><code>from distilabel.llm import TogetherInferenceLLM\nfrom distilabel.tasks import TextGenerationTask\n\nllm = TogetherInferenceLLM(\n    model=\"togethercomputer/llama-2-70b-chat\",\n    task=TextGenerationTask(),\n    max_new_tokens=512,\n    temperature=0.3,\n    prompt_format=\"llama2\",\n)\noutput = llm.generate(\n    [{\"input\": \"Explain me the theory of relativity as if you were a pirate.\"}]\n)\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# Ahoy matey! Yer lookin' fer a tale of the theory of relativity, eh? Well,\n# settle yerself down with a pint o' grog and listen close, for this be a story\n# of the sea of time and space!\n# Ye see, matey, the theory of relativity be tellin' us that time and space ain't\n# fixed things, like the deck o' a ship or the stars in the sky. Nay, they be like\n# the ocean itself, always changin' and flowin' like the tides.\n# Now, imagine ...\n</code></pre>"},{"location":"technical-reference/llms/#vertex-ai-llms","title":"Vertex AI LLMs","text":"<p>Google Cloud Vertex AI platform allows to use Google proprietary models and deploy other models for online predictions. <code>distilabel</code> integrates with Vertex AI trough <code>VertexAILLM</code> and <code>VertexAIEndpointLLM</code> classes.</p> <p>To use one of these classes you will need to have configured the Google Cloud authentication using one of these methods:</p> <ul> <li>Settings <code>GOOGLE_CLOUD_CREDENTIALS</code> environment variable</li> <li>Using <code>gcloud auth application-default login</code> command</li> <li>Using <code>vertexai.init</code> Python SDK function from the <code>google-cloud-aiplatform</code> library before instantiating the <code>LLM</code>.</li> </ul>"},{"location":"technical-reference/llms/#proprietary-models-gemini-and-palm","title":"Proprietary models (Gemini and PaLM)","text":"<p><code>VertexAILLM</code> allows to use Google proprietary models such as Gemini and PaLM. These models are served trough Vertex AI and its different APIs:</p> <ul> <li>Gemini API: which offers models from the Gemini family such as <code>gemini-pro</code> and <code>gemini-pro-vision</code> models. More information: Vertex AI - Gemini API.</li> <li>Text Generation API: which offers models from the PaLM family such as <code>text-bison</code>. More information: Vertex AI - PaLM 2 for text.</li> <li>Code Generation API: which offers models from the PaLM family for code-generation such as <code>code-bison</code>. More information: Vertex AI - Codey for code generation.</li> </ul> <pre><code>from distilabel.llm import VertexAILLM\nfrom distilabel.tasks import TextGenerationTask\n\nllm = VertexAILLM(\n    task=TextGenerationTask(), model=\"gemini-pro\", max_new_tokens=512, temperature=0.3\n)\n\nresults = llm.generate(\n    inputs=[\n        {\"input\": \"Write a short summary about the Gemini astrological sign\"},\n    ],\n)\n# &gt;&gt;&gt; print(results[0][0][\"parsed_output\"][\"generations\"])\n# Gemini, the third astrological sign in the zodiac, is associated with the element of\n# air and is ruled by the planet Mercury. People born under the Gemini sign are often\n# characterized as being intelligent, curious, and communicative. They are known for their\n# quick wit, adaptability, and versatility. Geminis are often drawn to learning and enjoy\n# exploring new ideas and concepts. They are also known for their social nature and ability\n# to connect with others easily. However, Geminis can also be seen as indecisive, restless,\n# and superficial at times. They may struggle with commitment and may have difficulty focusing\n# on one thing for too long. Overall, Geminis are known for their intelligence, curiosity,\n# and social nature.\n</code></pre>"},{"location":"technical-reference/llms/#endpoints-for-online-prediction","title":"Endpoints for online prediction","text":"<p><code>VertexAIEndpointLLM</code> class allows to use a model deployed in a Vertex AI Endpoint for online prediction to generate text. Unlike the rest of <code>LLM</code>s classes which comes with a set of pre-defined arguments in its <code>__init__</code> method, <code>VertexAIEndpointLLM</code> requires to provide the generation arguments to be used in a dictionary that will pased to the <code>generation_kwargs</code> argument. This is because the generation parameters will be different and have different names depending on the Docker image deployed on the Vertex AI Endpoint.</p> <pre><code>from distilabel.llm import VertexAIEndpointLLM\nfrom distilabel.tasks import TextGenerationTask\n\nllm = VertexAIEndpointLLM(\n    task=TextGenerationTask(),\n    endpoint_id=\"3466410517680095232\",\n    project=\"experiments-404412\",\n    location=\"us-central1\",\n    generation_kwargs={\n        \"temperature\": 1.0,\n        \"max_tokens\": 128,\n        \"top_p\": 1.0,\n        \"top_k\": 10,\n    },\n)\n\nresults = llm.generate(\n    inputs=[\n        {\"input\": \"Write a short summary about the Gemini astrological sign\"},\n    ],\n)\n# &gt;&gt;&gt; print(results[0][0][\"parsed_output\"][\"generations\"])\n# Geminis are known for their curiosity, adaptability, and love of knowledge. They are\n# also known for their tendency to be indecisive, impulsive and prone to arguing. They\n# are ruled by the planet Mercury, which is associated with communication, quick thinking,\n# and change.\n</code></pre>"},{"location":"technical-reference/llms/#anyscale","title":"Anyscale","text":"<p>Anyscale Endpoints offers open source large language models (LLMs) as fully managed API endpoints. Interoperate with open source models as you would do it with OpenAI:</p> <pre><code>import os\n\nfrom distilabel.llm import AnyscaleLLM\nfrom distilabel.tasks import TextGenerationTask\n\nanyscale_llm = AnyscaleLLM(\n    model=\"HuggingFaceH4/zephyr-7b-beta\",\n    task=TextGenerationTask(),\n    api_key=os.environ.get(\"ANYSCALE_API_KEY\"),\n)\nresult = anyscale_llm.generate([{\"input\": \"What is Anyscale?\"}])\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# 'Anyscale is a machine learning (ML) software company that provides tools and platforms\n# for scalable distributed ML workflows. Their offerings enable data scientists and engineers\n# to easily and efficiently deploy ML models at scale, both on-premise and on the cloud.\n# Anyscale's core technology, Ray, is an open-source framework for distributed Python computation \n# that provides a unified interface for distributed computing, resource management, and task scheduling.\n# With Anyscale's solutions, businesses can accelerate their ML development and deployment cycles and drive\n#\u00a0greater value from their ML investments.'\n</code></pre> <p>For the API reference visit AnyscaleLLM.</p>"},{"location":"technical-reference/llms/#mistralai","title":"MistralAI","text":"<p>Mistral.ai, the company behind awesome Open Source models like Mixtral 8x7B, offers their models in their AI platform. Visit their available models and start creating <code>distilabel</code> datasets with them.</p> <pre><code>import os\n\nfrom distilabel.llm import MistralAILLM\nfrom distilabel.tasks import TextGenerationTask\n\nmistralai_llm = MistralAILLM(\n    model=\"mistral-tiny\",\n    task=TextGenerationTask(),\n    api_key=os.environ.get(\"MISTRALAI_API_KEY\"),\n)\nresult = mistralai_llm.generate([{\"input\": \"What's the best french cheese?\"}])\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# I'd be happy to help answer your question, but it's important to note\n# that the \"best\" French cheese can be subjective as it depends on personal\n# taste preferences. Some popular and highly regarded French cheeses include\n# Roquefort for its strong, tangy flavor and distinct blue veins; Camembert\n# for its earthy, mushroomy taste and soft, runny texture; and Brie for its\n# creamy, buttery, and slightly sweet taste. I'd recommend trying different\n# types to find the one you enjoy the most.\n</code></pre> <p>For the API reference visit MistralAILLM.</p>"},{"location":"technical-reference/llms/#processllm-and-llmpool","title":"<code>ProcessLLM</code> and <code>LLMPool</code>","text":"<p>By default, <code>distilabel</code> uses a single process, so the generation loop is usually bottlenecked by the model inference time and Python GIL. To overcome this limitation, we provide the <code>ProcessLLM</code> class that allows to load an <code>LLM</code> in a different process, avoiding the GIL and allowing to parallelize the generation loop. Creating a <code>ProcessLLM</code> is easy as:</p> <pre><code>from distilabel.llm import LLM, ProcessLLM\nfrom distilabel.tasks import Task, TextGenerationTask\n\n\ndef load_gpt_4(task: Task) -&gt; LLM:\n    from distilabel.llm import OpenAILLM\n\n    return OpenAILLM(\n        model=\"gpt-4\",\n        task=task,\n        num_threads=4,\n    )\n\n\nllm = ProcessLLM(task=TextGenerationTask(), load_llm_fn=load_gpt_4)\nfuture = llm.generate(\n    inputs=[{\"input\": \"Write a letter for Bob\"}], num_generations=1\n)  # (1)\nllm.teardown()  # (2)\nresult = future.result()\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"])\n# Dear Bob,\n# I hope this letter finds you in good health and high spirits. I know it's been a while since we last caught up, and I wanted to take the time to connect and share a few updates.\n# Life has been keeping me pretty busy lately. [Provide a brief overview of what you've been up to: work, school, family, hobbies, etc.]\n# I've often found myself reminiscing about the good old days, like when we [include a memorable moment or shared experience with Bob].\n</code></pre> <ol> <li>The <code>ProcessLLM</code> returns a <code>Future</code> containing a list of lists of <code>LLMOutput</code>s.</li> <li>The <code>ProcessLLM</code> needs to be terminated after usage. If the <code>ProcessLLM</code> is used by a <code>Pipeline</code>, it will be terminated automatically.</li> </ol> <p>You can directly use a <code>ProcessLLM</code> as the <code>generator</code> or <code>labeller</code> in a <code>Pipeline</code>. Apart from that, there would be situations in which you would like to generate texts using several <code>LLM</code>s in parallel. For this purpose, we provide the <code>LLMPool</code> class:</p> <pre><code>from distilabel.llm import LLM, LLMPool, ProcessLLM\nfrom distilabel.tasks import Task, TextGenerationTask\n\n\ndef load_gpt_3(task: Task) -&gt; LLM:\n    from distilabel.llm import OpenAILLM\n\n    return OpenAILLM(\n        model=\"gpt-3.5-turbo\",\n        task=task,\n        num_threads=4,\n    )\n\n\ndef load_gpt_4(task: Task) -&gt; LLM:\n    from distilabel.llm import OpenAILLM\n\n    return OpenAILLM(\n        model=\"gpt-4\",\n        task=task,\n        num_threads=4,\n    )\n\n\npool = LLMPool(\n    llms=[\n        ProcessLLM(task=TextGenerationTask(), load_llm_fn=load_gpt_3),\n        ProcessLLM(task=TextGenerationTask(), load_llm_fn=load_gpt_4),\n    ]\n)\nresult = pool.generate(inputs=[{\"input\": \"Write a letter for Bob\"}], num_generations=2)\npool.teardown()\n# &gt;&gt;&gt; print(result[0][0][\"parsed_output\"][\"generations\"], end=\"\\n\\n\\n\\n\\n\\n----&gt;\")\n# Dear Bob,\n# I hope this letter finds you in good health and high spirits. I know it's been a while since we last caught up, and I wanted to take the time to connect and share a few updates.\n# Life has been keeping me pretty busy lately. [Provide a brief overview of what you've been up to: work, school, family, hobbies, etc.]\n# I've often found myself reminiscing about the good old days, like when we [include a memorable moment or shared experience with Bob].\n# &gt;&gt;&gt; print(result[0][1][\"parsed_output\"][\"generations\"])\n# Of course, I'd be happy to draft a sample letter for you. However, I would need some additional\n# information including who \"Bob\" is, the subject matter of the letter, the tone (formal or informal),\n# and any specific details or points you'd like to include. Please provide some more context and I'll do my best to assist you.\n</code></pre> <ol> <li> <p>You can take a look at this blog post from Cohere for a thorough explanation of the different parameters.\u00a0\u21a9</p> </li> </ol>"},{"location":"technical-reference/pipeline/","title":"Pipelines","text":"<p>This section will detail the <code>Pipeline</code>, providing guidance on creating and using them.</p>"},{"location":"technical-reference/pipeline/#pipeline","title":"Pipeline","text":"<p>The Pipeline class is a central component in <code>distilabel</code>, responsible for crafting datasets. It manages the generation of datasets and oversees the interaction between the generator and labeller <code>LLMs</code>.</p> <p>You create an instance of the <code>Pipeline</code> by providing a generator and an optional labeller LLM. Interactions with it are facilitated through its <code>generate</code> method. This method requires a <code>dataset</code>, specifies the num_generations to determine the number of examples to be created, and includes additional parameters for controlling the batch_size and managing the generation process.</p> <p>Let's start by a Pipeline with a single <code>LLM</code> as a generator.</p>"},{"location":"technical-reference/pipeline/#generator","title":"Generator","text":"<p>We will create a <code>Pipeline</code> that will use Notus from a HuggingFace Inference Endpoint. For this matter, we need to create a TextGenerationTask, and specify the format we want to use for our <code>Prompt</code>, in this case Notus, which corresponds to the same for Zephyr.</p> <pre><code>import os\n\nfrom distilabel.llm import InferenceEndpointsLLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import TextGenerationTask\n\nendpoint_name = \"aws-notus-7b-v1-4052\" or os.getenv(\"HF_INFERENCE_ENDPOINT_NAME\")\nendpoint_namespace = \"argilla\" or os.getenv(\"HF_NAMESPACE\")\n\npipe_generation = Pipeline(\n    generator=InferenceEndpointsLLM(\n        endpoint_name_or_model_id=endpoint_name,  # The name given of the deployed model\n        endpoint_namespace=endpoint_namespace,  # This usually corresponds to the organization, in this case \"argilla\"\n        token=os.getenv(\"HF_TOKEN\"),  # hf_...\n        task=TextGenerationTask(),\n        max_new_tokens=512,\n        do_sample=True,\n        prompt_format=\"notus\",\n    ),\n)\n</code></pre> <p>We've set up our pipeline using a specialized TextGenerationTask (refer to the tasks section for more task details), and an InferenceEndpointsLLM configured for <code>notus-7b-v1</code>, although any of the available <code>LLMs</code> will work.</p> <p>To use the Pipeline for dataset generation, we call the generate method. We provide it with the input dataset and specify the desired number of generations. In this example, we've prepared a <code>Dataset</code> with a single row to illustrate the process. This dataset contains one row, and we'll trigger 2 generations from it:</p> <pre><code>from datasets import Dataset\n\ndataset = Dataset.from_dict(\n    {\"input\": [\"Create an easy dinner recipe with few ingredients\"]}\n)\ndataset_generated = pipe_generation.generate(dataset, num_generations=2)\n</code></pre> <p>Now, let's examine the dataset that was generated. It's a <code>CustomDataset</code>, equipped with additional features for seamless interaction with <code>Argilla</code>.</p> <pre><code>print(dataset_generated)\n# Dataset({\n#     features: ['input', 'generation_model', 'generation_prompt', 'raw_generation_responses', 'generations'],\n#     num_rows: 1\n# })\n\nprint(dataset_generated[0][\"generations\"][0])\n# Here's a simple and delicious dinner recipe with only a few ingredients:\n\n# Garlic Butter Chicken with Roasted Vegetables\n\n# Ingredients:\n# - 4 boneless, skinless chicken breasts\n# - 4 tablespoons butter\n# - 4 cloves garlic, minced\n# - 1 teaspoon dried oregano\n# - 1/2 teaspoon salt\n# - 1/4 teaspoon black pepper\n# - 1 zucchini, sliced\n# - 1 red bell pepper, sliced\n# - 1 cup cherry tomatoes\n\n# Instructions:\n\n# 1. Preheat oven to 400\u00b0F (200\u00b0C).\n\n# 2. Melt butter in a small saucepan over low heat. Add minced garlic and heat until fragrant, about 1-2 minutes.\n\n# 3. Place chicken breasts in a baking dish and brush garlic butter over each one.\n\n# 4. Sprinkle oregano, salt, and black pepper over the chicken.\n\n# 5. In a separate baking dish, add sliced zucchini, red bell pepper, and cherry tomatoes. Brush with remaining garlic butter.\n\n# 6. Roast the chicken and vegetables in the preheated oven for 25-30 minutes or until cooked through and the vegetables are tender and lightly browned.\n\n# 7. Transfer the chicken to plates and serve with the roasted vegetables alongside. Enjoy!\n\n# This recipe requires simple ingredients and is easy to prepare, making it perfect for a quick, satisfying dinner. The garlic butter adds maximum flavor, while the roasted vegetables complement the chicken beautifully, providing additional nutrition and texture. With minimal effort, you can have a delicious and balanced meal on the table in no time.\n</code></pre>"},{"location":"technical-reference/pipeline/#labeller","title":"Labeller","text":"<p>Next, we move on to labelling a dataset. Just as before, we need an <code>LLM</code> for our <code>Pipeline</code>. In this case we will use <code>OpenAILLM</code> with <code>gpt-4</code>, and a <code>PreferenceTask</code>, UltraFeedbackTask for instruction following.</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import UltraFeedbackTask\n\npipe_labeller = Pipeline(\n    labeller=OpenAILLM(\n        model=\"gpt-4\",\n        task=UltraFeedbackTask.for_instruction_following(),\n        max_new_tokens=256,\n        num_threads=8,\n        api_key=os.getenv(\"OPENAI_API_KEY\", None),\n        temperature=0.3,\n    ),\n)\n</code></pre> <p>For this example dataset, we've extracted 2 sample rows from the UltraFeedback binarized dataset, formatted as expected by the default <code>LLM</code> and <code>Task</code>.</p> <p>We've selected two distinct examples, one correctly labeled and the other incorrectly labeled in the original dataset. In this instance, the <code>dataset</code> being generated includes two columns: the input, as seen in the generator, and a generations column containing the model's responses.</p> <pre><code>from datasets import Dataset\n\ndataset_test = Dataset.from_dict(\n    {\n        \"input\": [\n            \"Describe the capital of Spain in 25 words.\",\n            \"Design a conversation between a customer and a customer service agent.\",\n        ],\n        \"generations\": [\n            [\"Santo Domingo is the capital of Dominican Republic\"],\n            [\n                \"Customer: Hello, I'm having trouble with my purchase.\\n\\nCustomer Service Agent: I'm sorry to hear that. Could you please tell me more about the issue you are facing?\\n\\nCustomer: Yes, I ordered a pair of shoes from your company a week ago, but I haven't received them yet.\\n\\nCustomer Service Agent: I apologize for the inconvenience. Could you please provide me with your order number and full name so I can look into this for you?\\n\\nCustomer: Sure, my name is John Doe and my order number is ABCD1234.\\n\\nCustomer Service Agent: Thank you, John. I have checked on your order and it appears that it is still being processed. It should be shipped out within the next 24 hours.\\n\\nCustomer: That's good to hear, but can you also tell me the expected delivery time?\\n\\nCustomer Service Agent: Absolutely, based on your location, the estimated delivery time is 3-5 business days after shipping. You will receive a tracking number via email once the item is shipped, which will provide real-time updates on your package.\\n\\nCustomer: Thanks for the information. One more thing, what is your return policy if the shoes don't fit?\\n\\nCustomer Service Agent: Our company offers a 30-day return policy. If you are not satisfied with the product or if it doesn't fit, you can return it for a full refund or an exchange within 30 days of delivery. Please keep in mind that the product must be in its original packaging and in the same condition as when you received it.\\n\\nCustomer: Okay, that's good to know. Thank you for your help.\\n\\nCustomer Service Agent: You're welcome, John. I'm glad I could assist you. If you have any further questions or concerns, please don't hesitate to reach out to us. Have a great day!\"\n            ],\n        ],\n    }\n)\n\nds_labelled = pipe_labeller.generate(dataset_test)\n</code></pre> <p>Let's select the relevant columns from the labelled dataset, and take a look at the first record. This allows us to observe the rating and the accompanying rationale that provides an explanation.</p> <pre><code>ds_labelled.select_columns([\"input\", \"generations\", \"rating\", \"rationale\"])[0]\n# {\n#     \"input\": \"Describe the capital of Spain in 25 words.\",\n#     \"generations\": [\"Santo Domingo is the capital of Dominican Republic\"],\n#     \"rating\": [1.0],\n#     \"rationale\": [\n#         \"The text is irrelevant to the instruction. It describes the capital of the Dominican Republic instead of Spain.\"\n#     ],\n# }\n</code></pre>"},{"location":"technical-reference/pipeline/#generator-and-labeller","title":"Generator and Labeller","text":"<p>In the final scenario, we have a <code>Pipeline</code> utilizing both a generator and a labeller <code>LLM</code>. Once more, we'll employ the Inference Endpoint with <code>notus-7b-v1</code> for the generator, using a different system prompt this time. As for the labeller, we'll use <code>gpt-3.5-turbo</code>, which will label the examples for instruction following.</p> <pre><code>import os\n\nfrom distilabel.llm import InferenceEndpointsLLM, OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import TextGenerationTask, UltraFeedbackTask\n\npipe_full = Pipeline(\n    generator=InferenceEndpointsLLM(\n        endpoint_name_or_model_id=endpoint_name,\n        endpoint_namespace=endpoint_namespace,\n        task=TextGenerationTask(\n            system_prompt=\"You are an expert writer of XKCD, a webcomic of romance, sarcasm, math, and language.\"\n        ),\n        max_new_tokens=512,\n        do_sample=True,\n        prompt_format=\"notus\",\n    ),\n    labeller=OpenAILLM(\n        model=\"gpt-3.5-turbo\",\n        task=UltraFeedbackTask.for_instruction_following(),\n        max_new_tokens=256,\n        num_threads=4,\n        api_key=os.getenv(\"OPENAI_API_KEY\", None),\n        temperature=0.3,\n    ),\n)\n</code></pre> <p>For this example, we'll set up a pipeline to generate and label a dataset of short stories inspired by XKCD. To do this, we'll define the system_prompt for the <code>NotusTextGenerationTask</code>. The dataset will follow the same format we used for the generator scenario, featuring an input column with the examples, in this case, just one.</p> <pre><code>from datasets import Dataset\n\nxkcd_instructions = Dataset.from_dict(\n    {\"input\": [\"Could you imagine an interview process going sideways?\"]}\n)\nds_xkcd = pipe_full.generate(xkcd_instructions, num_generations=3)\n</code></pre> <p>We will now take a look to one of the generations, along with the rating and rational given by our labeller <code>LLM</code>:</p> <pre><code>print(ds_xkcd[1][\"generations\"][0])\nprint(\"-----\" * 5)\nprint(\"RATING: \", ds_xkcd[1][\"rating\"][0])\nprint(\"RATIONALE: \", ds_xkcd[1][\"rationale\"][0])\n\n# Yes, absolutely! Here's a fictional interview scenario turned into an XKCD-style comic:\n\n# (Interviewee meets with an unsmiling interviewer)\n\n# Interviewer: Good morning! Have a seat. Tell me about your experience working with teams.\n\n# Interviewee: Well, I've worked in large teams on group projects before. It could be challenging, but we always managed to pull through.\n\n# (Smugly) Interviewer: Challenging, huh? (tapping pen on desk) And how did you manage to overcome these challenges?\n\n# Interviewee: (confidently) Communication was key. I made sure to stay in touch with the team and keep everyone updated on our progress.\n\n# Interviewer: Communication. Hm. And what if communication failed?\n\n# Interviewee: (thrown off balance) Well, I mean...there was one time when we couldn't connect via video call. But we picked up the phone, and we all understood what needed to be done.\n\n# Interviewer: But what if the communication on the technical level failed, say, like a computer system with a software glitch?\n\n# Interviewee: (feeling the pressure) That's never happened to me before, but if it did, we would have to troubleshoot and find a workaround, right?\n\n# Interviewer: (smirking) Oh, but finding a workaround could mean delegating responsibilities among the team, which requires communication. It's a vicious cycle!\n\n# (Interviewee visibly uncomfortable)\n\n# Interviewer: And what if there was a communication breakdown among the team members themselves?\n\n# Interviewee: (unsure) I think we would try to sort it out as soon as possible to avoid any further problems.\n\n# Interviewer: (sarcastically) Yes, avoiding further problems is critical. Don't want to let those deadlines slip, do we?\n\n# (Interviewer types frantically on their computer keyboard)\n\n# Interviewer: (softly but wordily) Note to self: Avoid this candidate for team projects.\n\n# (The interviewer returns his attention back to the interviewee)\n\n# Interviewer: Well, moving on...\n# -------------------------\n# RATING:  4.0\n# RATIONALE:  The text provides a fictional interview scenario that aligns with the task goal of imagining an interview process going sideways. It includes dialogue between an interviewer and interviewee, showcasing a breakdown in communication and the interviewer's sarcastic and dismissive attitude towards the interviewee's responses.\n</code></pre>"},{"location":"technical-reference/pipeline/#running-several-generators-in-parallel","title":"Running several generators in parallel","text":"<p><code>distilabel</code> also allows to use several <code>LLM</code>s as generators in parallel, thanks to the <code>ProcessLLM</code> and <code>LLMPool</code> classes. This comes handy for the cases where we want to use several <code>LLM</code>s and fed them with the same input, allowing us to later compare their outputs (to see which one is better) or even creating a Preference dataset, following a similar process to UltraFeedback dataset generation.</p> <p>For this example, we will load four 7B <code>LLM</code>s using <code>vLLM</code> and a machine with 4 GPUs (to load each <code>LLM</code> in a different GPU). Then we will give instructions to all of them, and we will use GPT-4 to label the generated instructions using the <code>UltraFeedbackTask</code> for instruction-following.</p> <p>First of all, we will need to load each <code>LLM</code> using a <code>ProcessLLM</code>. <code>ProcessLLM</code> will create a child process which will load the <code>LLM</code> using the <code>load_llm_fn</code>.</p> <pre><code>from distilabel.llm import LLM, ProcessLLM\nfrom distilabel.tasks import Task, TextGenerationTask\n\n\ndef load_notus(task: Task) -&gt; LLM:  # (1)\n    import os\n\n    from distilabel.llm import vLLM\n    from vllm import LLM\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # (2)\n\n    return vLLM(\n        model=LLM(model=\"argilla/notus-7b-v1\"),\n        task=task,\n        max_new_tokens=512,\n        temperature=0.7,\n        prompt_format=\"notus\",\n    )\n\n\nllm = ProcessLLM(task=TextGenerationTask(), load_llm_fn=load_notus)\n</code></pre> <ol> <li>The <code>ProcessLLM</code> will create a child process in which the <code>LLM</code> will be loaded. Therefore, we will need to define a function that will be executed by the child process to load the <code>LLM</code>. The child process will pass the provided <code>Task</code> to the <code>load_llm_fn</code>.</li> <li>We set a value for <code>CUDA_VISIBLE_DEVICES</code> environment variable to make sure that each <code>LLM</code> is loaded in a different GPU.</li> </ol> <p>We will repeat this pattern 4 times, each time with a different <code>LLM</code> and a different GPU.</p> <pre><code>from distilabel.llm import LLM, ProcessLLM\nfrom distilabel.tasks import Task, TextGenerationTask\n\n\ndef load_notus(task: Task) -&gt; LLM:\n    import os\n\n    from distilabel.llm import vLLM\n    from vllm import LLM\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n    return vLLM(\n        model=LLM(model=\"argilla/notus-7b-v1\"),\n        task=task,\n        max_new_tokens=512,\n        temperature=0.7,\n        prompt_format=\"notus\",\n    )\n\n\ndef load_zephyr(task: Task) -&gt; LLM:\n    import os\n\n    from distilabel.llm import vLLM\n    from vllm import LLM\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\n    return vLLM(\n        model=LLM(model=\"HuggingFaceH4/zephyr-7b-beta\"),\n        task=task,\n        max_new_tokens=512,\n        temperature=0.7,\n        prompt_format=\"notus\",\n    )\n\n\ndef load_starling(task: Task) -&gt; LLM:\n    import os\n\n    from distilabel.llm import vLLM\n    from vllm import LLM\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n\n    return vLLM(\n        model=LLM(model=\"berkeley-nest/Starling-LM-7B-alpha\"),\n        task=task,\n        max_new_tokens=512,\n        temperature=0.7,\n        prompt_format=\"notus\",\n    )\n\n\ndef load_neural_chat(task: Task) -&gt; LLM:\n    import os\n\n    from distilabel.llm import vLLM\n    from vllm import LLM\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\n    return vLLM(\n        model=LLM(model=\"Intel/neural-chat-7b-v3-3\"),\n        task=task,\n        max_new_tokens=512,\n        temperature=0.7,\n        prompt_format=\"notus\",\n    )\n\n\nnotus = ProcessLLM(task=TextGenerationTask(), load_llm_fn=load_notus)\nzephyr = ProcessLLM(task=TextGenerationTask(), load_llm_fn=load_zephyr)\nstarling = ProcessLLM(task=TextGenerationTask(), load_llm_fn=load_starling)\nneural_chat = ProcessLLM(task=TextGenerationTask(), load_llm_fn=load_neural_chat)\n</code></pre> <p>In order to distribute the generations among the different <code>LLM</code>s, we will use a <code>LLMPool</code>. This class expects a list of <code>ProcessLLM</code>. Calling the <code>generate</code> method of the <code>LLMPool</code> will call the <code>generate</code> method of each <code>LLMProcess</code> in parallel, and will wait for all of them to finish, returning a list of lists of <code>LLMOutput</code>s with the generations.</p> <pre><code>from distilabel.llm import LLMPool\n\npool = LLMPool(llms=[notus, zephyr, starling, neural_chat])\n</code></pre> <p>We will use this <code>LLMPool</code> as the generator for our pipeline and we will use GPT-4 to label the generated instructions using the <code>UltraFeedbackTask</code> for instruction-following.</p> <pre><code>from distilabel.llm import LLM, ProcessLLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import UltraFeedbackTask\n\n\ndef load_gpt_4(task: UltraFeedbackTask) -&gt; LLM:\n    from distilabel.llm import OpenAILLM\n\n    return OpenAILLM(\n        model=\"gpt-4-1106-preview\",\n        task=task,\n        max_new_tokens=512,\n        num_threads=4,\n    )\n\n\npipeline = Pipeline(\n    generator=pool,\n    labeller=ProcessLLM(task=UltraFeedbackTask(), load_llm_fn=load_gpt_4),  # (1)\n)\n</code></pre> <ol> <li>We also will execute the calls to OpenAI API in a different process using the <code>ProcessLLM</code>. This will allow to not block the main process GIL, and allowing the generator to continue with the next batch.</li> </ol> <p>Then, we will load the dataset and call the <code>generate</code> method of the pipeline. For each input in the dataset, the <code>LLMPool</code> will randomly select two <code>LLM</code>s and will generate two generations for each of them. The generations will be labelled by GPT-4 using the <code>UltraFeedbackTask</code> for instruction-following. Finally, we will push the generated dataset to Argilla, in order to review the generations and labels that were automatically generated, and to manually correct them if needed.</p> <pre><code>from datasets import load_dataset\n\ndataset = (\n    load_dataset(\"HuggingFaceH4/instruction-dataset\", split=\"test[:50]\")\n    .remove_columns([\"completion\", \"meta\"])\n    .rename_column(\"prompt\", \"input\")\n)\n\ndataset = pipeline.generate(\n    dataset=dataset,\n    num_generations=2,\n    batch_size=5,\n    display_progress_bar=True,\n)\n\ndataset.to_argilla().push_to_argilla(name=\"preference-dataset\", workspace=\"admin\")\n</code></pre> <p>With a few lines of code, we have easily generated a dataset with 2 generations per input, using 4 different <code>LLM</code>s, and labelled the generations using GPT-4. You can check the full code here.</p>"},{"location":"technical-reference/pipeline/#dataset-checkpoints","title":"Dataset checkpoints","text":"<p>With long pipelines, it may be useful to review the dataset during the process, or have it saved in case something fails before obtaining the final dataset. We can use the <code>checkpoint_strategy</code> in <code>Pipeline.generate</code> method for this end:</p> <pre><code>from pathlib import Path\nfrom distilabel.dataset import DatasetCheckpoint\n\n# Assuming we want to save the dataset every 10% of the records generated.\n\nfreq = len(dataset) // 10\ndataset_checkpoint = DatasetCheckpoint(path=Path.cwd() / \"checkpoint_folder\", save_frequency=freq)\n</code></pre> <p>By passing the checkpoint strategy to the <code>generate</code> method, the dataset will be saved to disk automatically every freq generations:</p> <pre><code>new_ds = pipe.generate(\n    dataset=dataset,\n    num_generations=1,\n    checkpoint_strategy=dataset_checkpoint,\n)\n</code></pre> <p>The dataset can be regenerated from the checkpoint by simply calling the <code>CustomDataset.load_from_disk</code> method.</p> <pre><code>from distilabel.dataset import CustomDataset\nnew_ds = CustomDataset.load_from_disk(dataset_checkpoint.path)\n</code></pre> <p>And with the dataset regenerated we can easily call <code>push_to_argilla</code> on it to review it.</p> <p>Since <code>distilabel 0.6.0</code>, the <code>DatasetCheckpoint</code> includes a new argument <code>strategy</code> to determine how the dataset should be saved during the generation process.</p> <p>By default, the strategy will be set to <code>disk</code>, and the dataset will be saved to disk as we saw in the previous example. Now it also includes <code>hf-hub</code> strategy, to push the dataset to the hub. Everything else remains the same, but now you can push your dataset to the HuggingFace hub every <code>freq</code> iterations. Let's see an example:</p> <pre><code>from distilabel.dataset import DatasetCheckpoint\n\ndataset_checkpoint = DatasetCheckpoint(\n    strategy=\"hf-hub\",\n    save_frequency=1,\n    extra_kwargs={\n        \"repo_id\": \"username/dataset-name\"\n    }\n)\n\nnew_ds = pipe.generate(\n    dataset=dataset,\n    num_generations=1,\n    checkpoint_strategy=dataset_checkpoint,\n)\n</code></pre> <p>To ensure the dataset is indeed saved once the generation process starts, the <code>DatasetCheckpoint</code> will check the token for HuggingFace is set, either via the <code>HF_API_TOKEN</code> environment variable, or you'll be required to pass the <code>token</code> variable with your token via the <code>extra_kwargs</code> argument.</p> <p>Take a look at the API reference at DatasetCheckpoint.</p>"},{"location":"technical-reference/pipeline/#pipeline_1","title":"pipeline","text":"<p>Considering recurring patterns in dataset creation, we can facilitate the process by utilizing the <code>Pipeline</code>. This is made simpler through the <code>pipeline</code> function, which provides the necessary parameters for creating a <code>Pipeline</code>.</p> <p>In the code snippet below, we use the <code>pipeline</code> function to craft a <code>pipeline</code> tailored for a preference task, specifically focusing on text-quality as the subtask. If we don't initially provide a labeller <code>LLM</code>, we can specify the subtask we want our <code>pipeline</code> to address. By default, this corresponds to <code>UltraFeedbackTask</code>. It's mandatory to specify the generator of our choice; however, the labeller defaults to <code>gpt-3.5-turbo</code>. Optional parameters required for OpenAILLM can also be passed as optional keyword arguments.</p> <pre><code>import os\n\nfrom distilabel.llm import InferenceEndpointsLLM\nfrom distilabel.pipeline import pipeline\nfrom distilabel.tasks import TextGenerationTask\n\npipe = pipeline(\n    \"preference\",\n    \"text-quality\",\n    generator=InferenceEndpointsLLM(\n        endpoint_name_or_model_id=endpoint_name,\n        endpoint_namespace=endpoint_namespace,\n        token=token,\n        task=TextGenerationTask(),\n        max_new_tokens=512,\n        do_sample=True,\n        prompt_format=\"notus\",\n    ),\n    max_new_tokens=256,\n    num_threads=2,\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n    temperature=0.0,\n)\n</code></pre> <p>For the dataset, we'll begin with three rows from HuggingFaceH4/instruction-dataset. We'll request two generations with checkpoints enabled to safeguard the data in the event of any failures, which is the default behavior.</p> <pre><code>from datasets import load_dataset\n\ninstruction_dataset = (\n    load_dataset(\"HuggingFaceH4/instruction-dataset\", split=\"test[:3]\")\n    .remove_columns([\"completion\", \"meta\"])\n    .rename_column(\"prompt\", \"input\")\n)\n\npipe_dataset = pipe.generate(\n    instruction_dataset,\n    num_generations=2,\n    batch_size=1,\n    checkpoint_strategy=True,\n    display_progress_bar=True,\n)\n</code></pre> <p>Finally, let's see one of the examples from the dataset:</p> <pre><code>print(pipe_dataset[\"input\"][-1])\n# Create a 3 turn conversation between a customer and a grocery store clerk - that is, 3 per person. Then tell me what they talked about.\n\nprint(pipe_dataset[\"generations\"][-1][-1])\n# Customer: Hi there, I'm looking for some fresh berries. Do you have any raspberries or blueberries in stock?\n\n# Grocery Store Clerk: Yes, we have both raspberries and blueberries in stock today. Would you like me to grab some for you or can you find them yourself?\n\n# Customer: I'd like your help getting some berries. Can you also suggest which variety is sweeter? Raspberries or blueberries?\n\n# Grocery Store Clerk: Raspberries and blueberries both have distinct flavors. Raspberries are more tart and a little sweeter whereas blueberries tend to be a little sweeter and have a milder taste. It ultimately depends on your personal preference. Let me grab some of each for you to try at home and see which one you like better.\n\n# Customer: That sounds like a great plan. How often do you receive deliveries? Do you have some new varieties of berries arriving soon?\n\n# Grocery Store Clerk: We receive deliveries twice a week, on Wednesdays and Sundays. We also have a rotation of different varieties of berries throughout the season, so keep an eye out for new arrivals. Thanks for shopping with us, can I help you with anything else today?\n\n# Customer: No, that's all for now. I'm always happy to support your local store.\n\n# turn 1: berries, fresh produce availability, customer preference\n# turn 2: product recommendations based on taste and personal preference, availability\n# turn 3: store acknowledgment, shopping gratitude, loyalty and repeat business expectation.\n\nprint(pipe_dataset[\"rating\"][-1][-1])\n# 5.0\n\nprint(pipe_dataset[\"rationale\"][-1][-1])\n# The text accurately follows the given instructions and provides a conversation between a customer and a grocery store clerk. The information provided is correct, informative, and aligned with the user's intent. There are no hallucinations or misleading details.\n</code></pre> <p>The API reference can be found here: pipeline</p>"},{"location":"technical-reference/pipeline/#argilla-integration","title":"Argilla integration","text":"<p>The CustomDataset generated entirely by AI models may require some additional human processing. To facilitate human feedback, the dataset can be uploaded to <code>Argilla</code>. This process involves logging into an <code>Argilla</code> instance, converting the dataset to the required format using <code>CustomDataset.to_argilla()</code>, and subsequently using <code>push_to_argilla</code> on the resulting dataset. This conversion automatically adds some out-of-the-box filtering and search parameters as semantic search <code>vectors</code> and through <code>metrics</code> as <code>MetadataProperties</code>. Some metrics are inferred from the <code>Task</code> outcome and some <code>text-descriptives</code> metrics added. These can directly be used within the Argilla UI to help you find the most relevant examples. Let's briefly introduce the the parameters we may find:</p> <ul> <li><code>dataset_columns</code>: The names of the columns in the dataset to be used for vectors and metadata. By default, it is set to None meaning the first 5 fields from input and output columns will be taken.</li> <li><code>vector_strategy</code>: The strategy used to generate the semantic search vectors. By default, it is set to <code>True</code> which initializes a standard <code>SentenceTransformersExtractor()</code> that computes vectors for all fields in the dataset using <code>TaylorAI/bge-micro-v2</code>. Alternatively, you can pass a  <code>SentenceTransformersExtractor</code> by importing it from <code>argilla.client.feedback.integrations.sentencetransformers</code>.</li> <li><code>metric_strategy</code>: The strategy used to create the metrics. By default, it is set to <code>True</code> which initializes a standard <code>TextDescriptivesExtractor()</code> that computes metrics for all fields in the dataset using a default spaCy model. Alternatively, you can pass a <code>TextDescriptivesExtractor</code> by importing it from <code>argilla.client.feedback.integrations.textdescriptives</code>.</li> </ul> <pre><code>import argilla as rg\nfrom argilla.client.feedback.integrations.sentencetransformers import (\n    SentenceTransformersExtractor,\n)\nfrom argilla.client.feedback.integrations.textdescriptives import (\n    TextDescriptivesExtractor,\n)\n\nrg.init(api_key=\"&lt;YOUR_ARGILLA_API_KEY&gt;\", api_url=\"&lt;YOUR_ARGILLA_API_URL&gt;\")\n\n# with default `vector_strategy` and `metric_strategy`\nrg_dataset = pipe_dataset.to_argilla()\nrg_dataset.push_to_argilla(name=\"preference-dataset\", workspace=\"admin\")\n\n# with a custom `vector_strategy``\nvector_strategy = SentenceTransformersExtractor(model=\"TaylorAI/bge-micro-v2\")\nrg_dataset = pipe_dataset.to_argilla(vector_strategy=vector_strategy)\n\n# with a custom `metric_strategy`\nmetric_strategy = TextDescriptivesExtractor(model=\"en_core_web_sm\")\nrg_dataset = pipe_dataset.to_argilla(metric_strategy=vector_strategy)\n\n# without `vector_strategy` and `metric_strategy`\nrg_dataset = pipe_dataset.to_argilla(metric_strategy=None, vector_strategy=None)\n</code></pre>"},{"location":"technical-reference/pipeline/#prepare-datasets-for-fine-tuning","title":"Prepare datasets for fine-tuning","text":"<p>The preference datasets generated by distilabel out of the box contain all the raw information generated by the <code>Pipeline</code>, but some processing is necessary in order to prepare the dataset for alignment or instruction fine-tuning, like for DPO (initially we only cover the case for DPO).</p> <p><code>distilabel</code> offers helper functions to prepare the CustomDataset for DPO. The current definition works for datasets labelled using <code>PreferenceTask</code>, and prepares them by binarizing the data. Go to the following section for an introduction to dataset binarization.</p> <p>By default, the ties (rows for which the rating of the chosen and rejected responses are the same) are removed from the dataset, as that's expected for fine-tuning, but those can be kept in case it wants to be analysed. Take a look at dataset.utils.prepare_dataset for more information.</p> <p>Binarization</p> randomworst <pre><code>from datasets import load_dataset\nfrom distilabel.tasks import JudgeLMTask\nfrom distilabel.utils import prepare_dataset\n\ndataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\ndataset.task = JudgeLMTask()\ndataset_binarized_random = prepare_dataset(dataset, strategy=\"random\", keep_ties=True)\n# &gt;&gt;&gt; len(dataset)\n# 12859\n# &gt;&gt;&gt; len(dataset_binarized_random)\n# 12817\ndataset_binarized_random = prepare_dataset(dataset, strategy=\"random\", keep_ties=False)\n# &gt;&gt;&gt; len(dataset_binarized_random)\n# 8850\n</code></pre> <pre><code>from datasets import load_dataset\nfrom distilabel.tasks import JudgeLMTask\nfrom distilabel.utils import prepare_dataset\n\ndataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\ndataset.task = JudgeLMTask()\ndataset_binarized_random = prepare_dataset(dataset, strategy=\"worst\", keep_ties=True)\n# &gt;&gt;&gt; len(dataset)\n# 12859\n# &gt;&gt;&gt; len(dataset_binarized_random)\n# 12817\ndataset_binarized_random = prepare_dataset(dataset, strategy=\"worst\", keep_ties=False)\n# &gt;&gt;&gt; len(dataset_binarized_random)\n# 8850\n</code></pre>"},{"location":"technical-reference/pipeline/#whats-binarization","title":"What's binarization?","text":"<p>In the context of preference datasets (datasets for LLM instruction-tuning) one can come up with datasets formatted following the UltraFeedback format (the same format one obtains from a <code>Pipeline</code> that labels a dataset with a <code>PreferenceTask</code>), where for a given instruction we can have multiple completions according to one or more models, rated either by humans or other LLMs.</p> <p>From distilabel, we would obtain from a labelling <code>Pipeline</code> a dataset with the following format:</p> input generations rating Generate an approximately fifteen-word sentence that... [Midsummer House is a moderately..., Sure! Here's a sentence that...] [9.0, 7.0] <p>Where each columns represents the following:</p> <ul> <li> <p>input: Input for the LLM to generate text.</p> </li> <li> <p>generations: List of generations from the LLM (maybe an LLMPool with different models).</p> </li> <li> <p>rating: A list of the ratings for each of the generations obtained by an LLM using one of the <code>PreferenceTasks</code>, like JudgeLMTask or UltraFeedbackTask</p> </li> </ul> <p>This dataset format contains all the raw information, but in order to use it in the common frameworks, the expected format is usually a prompt, a chosen and a rejected response to align the model with those preferences.</p> <p>We would want the following dataset format for fine-tuning:</p> prompt chosen rejected Generate an approximately fifteen-word sentence that... [{'content': 'Generate an approximately...', 'role': 'user'}, {'content': 'Midsummer House is a moderately...', 'role': 'assistant'}] [{'content': 'Generate an approximately...', 'role': 'user'}, {'content': ' Sure! Here\\'s a sentence that...', 'role': 'assistant'}] <p>Take a look at this explanation for the binarization of UltraFeedback done to train Notus-7B-v1.</p> <p>What does each column represents.</p> <ul> <li> <p>prompt: Instruction given to the model.</p> </li> <li> <p>chosen: Response chosen following the OpenAI format.</p> </li> <li> <p>rejected: Response rejected following the OpenAI format.</p> </li> </ul> <p>We refer to the OpenAI's chat format for more information on the chosen/rejected format.</p> <p>This dataset processing is called binarization. In the context of <code>distilabel</code>, this transformation (dataset prepartion) is done by <code>dataset.utils.prepare_dataset</code>, and given that the generated datasets contain additional information, one can also see the following additional columns:</p> prompt chosen rejected rating_chosen rating_rejected chosen_model rejected_model Generate an approximately fifteen-word sentence that... [{'content': 'Generate an approximately...', 'role': 'user'}, {'content': 'Midsummer House is a moderately...', 'role': 'assistant'}] [{'content': 'Generate an approximately...', 'role': 'user'}, {'content': ' Sure! Here\\'s a sentence that...', 'role': 'assistant'}] 9 7 <ul> <li> <p>rating_chosen: Rating of the chosen instruction.</p> </li> <li> <p>rating_rejected: Rating of the rejected instruction.</p> </li> <li> <p>chosen_model: (Optional, only returned if the dataset contains it, otherwise it's a null string like here) The model used to generate the chosen instruction.</p> </li> <li> <p>rejected_model: (Optional, only returned if the dataset contains it, otherwise it's a null string like here) The model used to generate the rejected instruction.</p> </li> <li> <p>chosen_rationale: (Optional, only returned if the dataset contains the rationale, otherwise it's not added) The rationale behind the rating of the chosen response.</p> </li> <li> <p>rejected_rationale: (Optional, only returned if the dataset contains the rationale*, otherwise it's not added) The rationale behind the rating of the rejected response.</p> </li> </ul> <p>Need more information? Take a look at argilla/ultrafeedback-binarized-preferences to get an idea of how openbmb/UltraFeedback can be binarized to prepare it for DPO.</p>"},{"location":"technical-reference/pipeline/#customdataset-in-huggingface-hub","title":"CustomDataset in HuggingFace hub","text":"<p>Since <code>distilabel 0.5.0</code> when a <code>CustomDataset</code> is pushed to the HuggingFace Hub, the <code>Task</code> that comes with it will be pushed to the hub too. The <code>CustomDataset.push_to_hub</code> method will by default upload the <code>Task</code> to the huggingface hub:</p> <pre><code>dataset.push_to_hub(\"path/to/hub\", split=\"train\")\n# ...\n# 7:30:20 INFO     [PID: 52657] Pushing task to the hub...                                            dataset.py:247\n</code></pre> <p>On the other hand, this will be useful when downloading datasets from the hub which already have the <code>Task</code> to be downloaded (by default if there's a <code>Task</code> in the repository it will be downloaded, no error will appear if that's not possible). Just import the corresponding distilabel.dataset.load_dataset function instead of the <code>datasets.load_dataset</code> version.</p> <pre><code>from distilabel.dataset import load_dataset\n\ndataset: \"CustomDataset\" = load_dataset(\"argilla/distilabel-sample-evol-instruct\", split=\"train\")\nprint(dataset.task)\n# EvolInstructTask(system_prompt=\"\", task_description=...)\n</code></pre> <p>A sample task can be seen here.</p>"},{"location":"technical-reference/tasks/","title":"Tasks","text":"<p>In this section we will see what's a <code>Task</code> and the list of tasks available in <code>distilabel</code>.</p>"},{"location":"technical-reference/tasks/#task","title":"Task","text":"<p>The <code>Task</code> class takes charge of setting how the LLM behaves, deciding whether it acts as a generator or a labeller. To accomplish this, the <code>Task</code> class creates a prompt using a template that will be sent to the <code>LLM</code>. It specifies the necessary input arguments for generating the prompt and identifies the output arguments to be extracted from the <code>LLM</code> response. The <code>Task</code> class yields a <code>Prompt</code> that can generate a string with the format needed, depending on the specific <code>LLM</code> used.</p> <p>All the <code>Task</code>s defines a <code>system_prompt</code> which serves as the initial instruction given to the LLM, guiding it on what kind of information or output is expected, and the following methods:</p> <ul> <li><code>generate_prompt</code>: This method will be used by the <code>LLM</code> to create the prompts that will be fed to the model.</li> <li><code>parse_output</code>: After the <code>LLM</code> has generated the content, this method will be called on the raw outputs of the model to extract the relevant content (scores, rationales, etc).</li> <li><code>input_args_names</code> and <code>output_args_names</code>: These methods are used in the <code>Pipeline</code> to process the datasets. The first one defines the columns that will be extracted from the dataset to build the prompt in case of a <code>LLM</code> that acts as a generator or labeller alone, or the columns that should be placed in the dataset to be processed by the labeller <code>LLM</code>, in the case of a <code>Pipeline</code> that has both a generator and a labeller. The second one is in charge of inserting the defined fields as columns of the dataset generated dataset.</li> </ul> <p>After defining a task, the only action required is to pass it to the corresponding <code>LLM</code>. All the intricate processes are then handled internally:</p> <pre><code>from distilabel.llm import TransformersLLM\nfrom distilabel.tasks import TextGenerationTask\n\n# This snippet uses `TransformersLLM`, but is the same for every other `LLM`.\ngenerator = TransformersLLM(\n    model=...,\n    tokenizer=...,\n    task=TextGenerationTask(),\n)\n</code></pre> <p>Given this explanation, <code>distilabel</code> distinguishes between two primary categories of tasks: those focused on text generation and those centered around labelling. These <code>Task</code> classes delineate the LLM's conduct, be it the creation of textual content or the assignment of labels to text, each with precise guidelines tailored to their respective functionalities. Users can seamlessly leverage these distinct task types to tailor the LLM's behavior according to their specific application needs.</p>"},{"location":"technical-reference/tasks/#text-generation","title":"Text Generation","text":"<p>These set of classes are designed to steer a <code>LLM</code> in generating text with specific guidelines. They provide a structured approach to instruct the LLM on generating content in a manner tailored to predefined criteria.</p>"},{"location":"technical-reference/tasks/#textgenerationtask","title":"TextGenerationTask","text":"<p>This is the base class for text generation, and includes the following fields for guiding the generation process:</p> <ul> <li><code>system_prompt</code>, which serves as the initial instruction or query given to the LLM, guiding it on what kind of information or output is expected.</li> <li>A list of <code>principles</code> to inject on the <code>system_prompt</code>, which by default correspond to those defined in the UltraFeedback paper<sup>1</sup>,</li> <li>and lastly a distribution for these principles so the <code>LLM</code> can be directed towards the different principles with a more customized behaviour.</li> </ul> <p>For the API reference visit TextGenerationTask.</p>"},{"location":"technical-reference/tasks/#selfinstructtask","title":"SelfInstructTask","text":"<p>The task specially designed to build the prompts following the Self-Instruct paper: SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions.</p> <p>From the original repository:</p> <p>The Self-Instruct process is an iterative bootstrapping algorithm that starts with a seed set of manually-written instructions and uses them to prompt the language model to generate new instructions and corresponding input-output instances, so this <code>Task</code> is specially interesting for generating new datasets from a set of predefined topics.</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import SelfInstructTask\n\ngenerator = OpenAILLM(\n    task=SelfInstructTask(\n        system_prompt=\"You are a question-answering assistant for...\",\n        application_description=\"AI assistant\",\n        num_instructions=3,\n        criteria_for_query_generation=\"Design queries to be... \",\n    ),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>For the API reference visit SelfInstructTask.</p> <p>You can personalize the way in which your SelfInstructTask behaves by changing the default values of the parameters to something that suits your use case. Let's go through them:</p> <ul> <li>System Prompt: you can control the overall behaviour and expectations of your model.</li> <li>Application Description: a description of the AI application. By default, we use \"AI Assistant\".</li> <li>Number of instructions: number of instructions in the prompt.</li> <li>Criteria for Query Generation: the criteria for query generation that we want our model to have. The default value covers default behaviour for SelfInstructTask. This value is passed to the .jinja template, where extra instructions are added to ensure correct output format.</li> </ul> <p>You can see an example of how to customise a SelfInstructTask to create Haikus in the snippet in the subsection Custom TextGenerationTask.</p>"},{"location":"technical-reference/tasks/#evolinstructtask","title":"EvolInstructTask","text":"<p>The task is specially designed to build the prompts following the Evol-Instruct strategy proposed in: WizardLM: Empowering Large Language Models to Follow Complex Instructions.</p> <p>From the original repository:</p> <p>Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels and skill range, to improve the performance of LLMs.</p> <p>Use this <code>Task</code> to build more complete and complex datasets starting from simple ones.</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import EvolInstructTask\n\ngenerator = OpenAILLM(\n    task=EvolInstructTask(),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>You can take a look at a sample dataset generated using <code>EvolInstructTask</code>.</p> <p>Note</p> <p>The original definition of <code>EvolInstruct</code> considers an elimination evolving step with different situations to remove the responses considered failures. Section 3.2, Elimination Evolving in WizardLM paper shows these steps. We have implemented steps 2-4 as part of this task, but not step one. Step 1 of the elimination process can be implemented using labellers in <code>distilabel</code>.</p> <p>For the API reference visit EvolInstructTask.</p>"},{"location":"technical-reference/tasks/#evolcomplexity","title":"EvolComplexity","text":"<p>The Deita framework presents a data selection framework composed of two initial steps that consist on adopting an evolution-based approach as defined in WizardLM. The first of the evolution steps, related to the complexity is the same <code>EvolInstruct</code> task, exposed with the same name given in the paper:</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import EvolComplexityTask\n\ngenerator = OpenAILLM(\n    task=EvolComplexityTask(),\n    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n</code></pre> <p>For the API reference visit EvolComplexityTask.</p>"},{"location":"technical-reference/tasks/#evolquality","title":"EvolQuality","text":"<p>The second step from the Deita framework consists on enhancing the quality of the instructions, in the same spirit from <code>EvolComplexityTask</code>. The <code>EvolQualityTask</code> can be used to augment the quality of the instructions by enhancing helpfulness, augmenting relevance, enriching depth, fostering creativity, and supplying additional details, following the Deita implementation:</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import EvolQualityTask\n\ngenerator = OpenAILLM(\n    task=EvolQualityTask(),\n    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n</code></pre> <p>The following example shows an example of dataset after evolving it's quality with this task. For the initial dataset we have chosen argilla/distilabel-intel-orca-dpo-pairs for simplicity, the dataset needs to have both \"input\" and \"generation\" columns in order to allow for the evolution strategy proposed in the Deita paper:</p> <pre><code>import os\nfrom datasets import Dataset\n\nfrom distilabel.tasks import EvolQualityTask\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\n\n\n# Create a sample dataset (this one is inspired from the distilabel-intel-orca-dpo-pairs)\nsample_evol_quality = Dataset.from_dict(\n    {\n        'input': [\n            'Please answer the following question: I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\"?\\nAnswer:',\n            \"James runs a TV show and there are 5 main characters and 4 minor characters. He pays the minor characters $15,000 each episode. He paid the major characters three times as much. How much does he pay per episode? Let's be accurate as possible.\"\n        ],\n        'generation': [\n            \"Certainly! Here's a detailed and long answer to the question you provided:\\n\\nQuestion: Read the passage below and answer the following questions.\\n\\nPassage: In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\\n\\n1. What event did the passage describe?\\n2. How many colonies were involved in the Federation of Australia?\\n3. What did the colonies keep after forming one nation?\\n4. What was the purpose of the federal government established by the Constitution of Australia?\\n5. What were the colonies called after they became states of the Commonwealth of Australia?\\n\\nAnswers:\\n\\n1. The passage describes the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation.\\n2. Six colonies were involved in the Federation of Australia: New South Wales, Queensland, South Australia, Tasmania, Victoria, and Western Australia.\\n3. The colonies kept the systems of government that they had developed as separate colonies.\\n4. The federal government established by the Constitution of Australia was responsible for matters concerning the whole nation.\\n5. After becoming states of the Commonwealth of Australia, the colonies were called states.\\n\\nIn-depth analysis:\\n\\nThe passage provides a brief overview of the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation in 1901. The colonies kept the systems of government that they had developed as separate colonies, but they also established a federal government that was responsible for matters concerning the whole nation. This federal government was established by the Constitution of Australia, which came into force on January 1, 1901.\\n\\nThe passage highlights the significance of the Federation of Australia by noting that it was a process by which the colonies collectively became states of the Commonwealth of Australia. This transformation marked a significant milestone in the history of Australia, as it created a unified nation out of six separate colonies.\\n\\nThe passage also provides information about the systems of government that were kept by the colonies after forming one nation. Specifically, it notes that the colonies kept the systems of government that they had developed as separate colonies. This suggests that the Federation of Australia did not result in a complete overhaul of the existing systems of government, but rather a reorganization of these systems to create a federal government that was responsible for matters concerning the whole nation.\\n\\nFinally, the passage uses descriptive language to emphasize the importance of the Constitution of Australia in the Federation of Australia. It notes that the Constitution came into force on January 1, 1901, and that it established the federal government that was responsible for matters concerning the whole nation. This language emphasizes the significance of the Constitution in the formation of the Federation of Australia and the creation of the Commonwealth of Australia.\\n\\nIn conclusion, the passage provides a detailed overview of the Federation of Australia, highlighting the significance of the Constitution of Australia in the formation of the Commonwealth of Australia. It also provides information about the systems of government that were kept by the colonies after forming one nation, and emphasizes the importance of the Constitution in the creation of the federal government responsible for matters concerning the whole nation.\",\n            'James pays the minor characters $15,000 each episode. Since there are 4 minor characters, he pays them a total of 4 * $15,000 = $60,000 per episode.\\n\\nThe major characters are paid three times as much. So, each major character gets paid 3 * $15,000 = $45,000 per episode.\\n\\nThere are 5 main characters, so he pays them a total of 5 * $45,000 = $225,000 per episode.\\n\\nIn total, James pays $225,000 (major characters) + $60,000 (minor characters) = $285,000 per episode.'\n        ]\n    }\n)\n\n# Create the pipeline\npipe = Pipeline(\n    generator=OpenAILLM(\n        task=EvolQualityTask(),\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        temperature=1\n    ),\n)\n\n# Run the pipeline in the sample dataset\nsample_quality_dataset = pipe.generate(sample_evol_quality)\n\nprint(sample_quality_dataset.select_columns([\"input\", \"generation\", \"generations\"])[2])\n# {\n#   \"input\": \"What happens next in this paragraph?\\n\\nShe then rubs a needle on a cotton ball then pushing it onto a pencil and wrapping thread around it. She then holds up a box of a product and then pouring several liquids into a bowl. she\\nChoose your answer from: A. adds saucepan and shakes up the product in a grinder. B. pinches the thread to style a cigarette, and then walks away. C. then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. D. begins to style her hair and cuts it several times before parting the ends of it to show the hairstyle she has created.\",\n#   \"generation\": \"C. She then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. In this option, she is continuing the process of using the needle, pencil, and thread, which is most related to what she was doing in the previous sentence.\",\n#   \"generations\": [\n#     \"C. Then, to everyone's surprise, she dips the needle in ink and starts using the pencil to draw an intricate design on her leg. The creativity in her actions is truly unparalleled. After showcasing her artwork, she skillfully rubs it off with a rag, leaving everyone in awe of her talent.\"\n#   ]\n# }\n</code></pre> <p>For the API reference visit EvolQualityTask.</p>"},{"location":"technical-reference/tasks/#custom-textgenerationtask","title":"Custom TextGenerationTask","text":"<p>The following examples show different cases of creating your custom <code>TextGenerationTask</code>. Inherit from <code>TextGenerationTask</code> and implement the <code>generate_prompt</code> and <code>parse_output</code> to customise the behavior of the <code>LLM</code>.</p> OSS Instruct TaskHaiku SelfInstructTaskWizardLM Equal Prompts Task <p>This task implements the OSS Instruct from Magicoder: Source Code Is All You Need. Generate problems and solutions from seed problems following the paper implementation:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Dict, List\n\nfrom distilabel.tasks import TextGenerationTask\nfrom distilabel.tasks.prompt import Prompt\n\noss_instruct_prompt = \"\"\"Please gain inspiration from the following random code snippet to create a high-quality programming problem. Present your output in two distinct sections:\n[Problem Description] and [Solution].\nCode snippet for inspiration:\n\n{code}\n\nGuidelines for each section:\n1. [Problem Description]: This should be **completely self-contained**, providing\nall the contextual information one needs to understand and solve the problem.\nAssume common programming knowledge, but ensure that any specific context,\nvariables, or code snippets pertinent to this problem are explicitly included.\n2. [Solution]: Offer a comprehensive, **correct** solution that accurately\naddresses the [Problem Description] you provided.\n\"\"\"\n\n\n@dataclass\nclass OSSInstruct(TextGenerationTask):\n    system_prompt: str = \"You are exceptionally skilled at crafting high-quality programming problems and offering precise solutions.\"\n\n    def generate_prompt(self, input: str) -&gt; Prompt:\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=oss_instruct_prompt.format(code=input)\n          )\n\n    def parse_output(self, output: str) -&gt; List[Dict[str, str]]:\n        problem, solution = output.split(\"[Solution]\")\n        return {\n            \"problem\": problem.replace(\"[Problem Description]\", \"\").strip(),\n            \"solution\": solution.strip()\n        }\n</code></pre> <p>Here you cn see an example of how to customise a <code>SelfInstructTask</code> to create Haikus. The following Haiku DPO dataset contains more information on how this dataset was created.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Dict, List\n\nfrom distilabel.tasks import SelfInstructTask\n\n\n@dataclass\nclass CustomTask(SelfInstructTask):\n    system_prompt: str = \"You are an expert Haiku writer, writing the best and most diverse Haikus given topics as inputs.\"\n    application_description: str = (\n        \"An AI assistant adept at writing Haiku.\\n\"\n        \"It expects complete suggestions from users providing details of the kind of haiku they want.\\n\"\n        \"The AI assistant will help users write haiku about particular topics and is willing to accept requests related to a specific subject or object or a more abstract request\"\n        \"based on an emotion, theme or vibe.\\n\"\n    )\n\n    criteria_queries: str = (\n        \"Incorporate a diverse range of verbs, avoiding repetition.\\n\"\n        \"Ensure queries are compatible with AI model's text generation functions and are limited to 1-2 sentences.\\n\"\n        \"Design queries to be self-contained and standalone.\"\n    )\n\n    def define_task(self):\n        instruction_task = SelfInstructTask(\n            num_instructions=15,\n            application_description=self.application_description,\n            criteria_for_query_generation=self.criteria_queries,\n        )\n\n        return instruction_task\n\n    def parse_output(self, output: str) -&gt; List[Dict[str, str]]:\n        return {\"output\": output}\n</code></pre> <p>The following task, obtained from WizardLM: Empowering Large Language Models to Follow Complex Instructions, can be used to check whether two instructions are equal or different to decide whether to keep in your dataset or remove redundant instructions:</p> <pre><code>from dataclasses import dataclass\nimport string\nfrom typing import Dict, List\n\nfrom distilabel.tasks import Prompt, TextGenerationTask\n\n# Prompt from the WizardLM paper for the Equal Prompts task:\nwizardllm_equal_prompt = \"\"\"Here are two Instructions, do you think they are equal to each other and meet the following requirements?:\n1. They have the same constraints and requirments.\n2. They have the same depth and breadth of the inquiry.\nThe First Prompt: {first_instruction}\nThe Second Prompt: {second_instruction}\nYour Judgement (Just answer: Equal or Not Equal. No need to explain the reason):\"\"\"\n\n\n@dataclass\nclass WizardLMEqualPrompts(TextGenerationTask):\n    \"\"\"Task to check for the equality of two instructions following the Appendix G in\n    [WizardLM paper](https://arxiv.org/abs/2304.12244).\n    \"\"\"\n\n    system_prompt: str = \"You are an AI judge in charge of determining the equality of two instructions. \"\n\n    def generate_prompt(self, input: List[str]) -&gt; Prompt:\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=wizardllm_equal_prompt.format(\n                first_instruction=input[0], second_instruction=input[1]\n            ),\n        )\n\n    def parse_output(self, output: str) -&gt; List[Dict[str, str]]:\n        \"\"\"Remove punctuation from the string.\"\"\"\n        return {\n            \"generations\": output.translate(str.maketrans(\"\", \"\", string.punctuation))\n        }\n</code></pre>"},{"location":"technical-reference/tasks/#labelling","title":"Labelling","text":"<p>Instead of generating text, you can instruct the <code>LLM</code> to label datasets. The existing tasks are designed specifically for creating both <code>PreferenceTask</code> and <code>CritiqueTask</code> datasets.</p>"},{"location":"technical-reference/tasks/#preference","title":"Preference","text":"<p>Preference datasets for Language Models (LLMs) are sets of information that show how people rank or prefer one thing over another in a straightforward and clear manner. These datasets help train language models to understand and generate content that aligns with user preferences, enhancing the model's ability to generate contextually relevant and preferred outputs.</p> <p>Contrary to the <code>TextGenerationTask</code>, the <code>PreferenceTask</code> is not intended for direct use. It implements the default methods <code>input_args_names</code> and <code>output_args_names</code>, but <code>generate_prompt</code> and <code>parse_output</code> are specific to each <code>PreferenceTask</code>. Examining the <code>output_args_names</code> reveals that the generation will encompass both the rating and the rationale that influenced that rating.</p>"},{"location":"technical-reference/tasks/#ultrafeedbacktask","title":"UltraFeedbackTask","text":"<p>This task is specifically designed to build the prompts following the format defined in the \"UltraFeedback: Boosting Language Models With High Quality Feedback\" paper.</p> <p>From the original repository: To collect high-quality preference and textual feedback, we design a fine-grained annotation instruction, which contains 4 different aspects, namely instruction-following, truthfulness, honesty and helpfulness. This <code>Task</code> is designed to label datasets following the different aspects defined for the UltraFeedback dataset creation.</p> <p>The following snippet can be used as a simplified UltraFeedback Task, for which we define 3 different ratings, but take into account the predefined versions are intended to be used out of the box:</p> <pre><code>from textwrap import dedent\n\nfrom distilabel.tasks.preference.ultrafeedback import Rating, UltraFeedbackTask\n\ntask_description = dedent(\n    \"\"\"\n    # General Text Quality Assessment\n    Evaluate the model's outputs based on various criteria:\n    1. **Correctness &amp; Informativeness**: Does the output provide accurate and helpful information?\n    2. **Honesty &amp; Uncertainty**: How confidently does the model convey its information, and does it express uncertainty appropriately?\n    3. **Truthfulness &amp; Hallucination**: Does the model introduce misleading or fabricated details?\n    4. **Instruction Following**: Does the model's output align with given instructions and the user's intent?\n    Your role is to provide a holistic assessment considering all the above factors.\n\n    **Scoring**: Rate outputs 1 to 3 based on the overall quality, considering all aspects:\n    \"\"\"\n)\n\nratings = [\n    Rating(value=1, description=\"Low Quality\"),\n    Rating(value=2, description=\"Moderate Quality\"),\n    Rating(value=3, description=\"Good Quality\"),\n]\n\nultrafeedback_task = UltraFeedbackTask(\n    system_prompt=\"Your role is to evaluate text quality based on given criteria\",\n    task_description=task_description,\n    ratings=ratings,\n)\n</code></pre> HelpfulnessTruthfulnessHonestyInstruction Following <p>The following example creates a UltraFeedback task to emphasize helpfulness, that is overall quality and correctness of the output:</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import UltraFeedbackTask\n\nlabeller = OpenAILLM(\n    task=UltraFeedbackTask.for_helpfulness(),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>The following example creates a UltraFeedback task to emphasize truthfulness and hallucination assessment:</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import UltraFeedbackTask\n\nlabeller = OpenAILLM(\n    task=UltraFeedbackTask.for_truthfulness(),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>The following example creates a UltraFeedback task to emphasize honesty and uncertainty expression assessment:</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import UltraFeedbackTask\n\nlabeller = OpenAILLM(\n    task=UltraFeedbackTask.for_honesty(),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>The following example creates a UltraFeedback task to emphasize the evaluation of alignment between output and intent:</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import UltraFeedbackTask\n\nlabeller = OpenAILLM(\n    task=UltraFeedbackTask.for_instruction_following(),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>Additionally, we at Argilla created a custom subtask for UltraFeedback, that generates an overall score evaluating all the aspects mentioned above but within a single subtask. Otherwise, in order to get an overall score, all the subtasks above should be run and the average of those scores to be calculated.</p> Overall Quality <p>The following example uses a <code>LLM</code> to examinate the data for our custom overall quality criteria, which includes the different criteria from UltraFeedback (Correctness &amp; Informativeness, Honesty &amp; Uncertainty, Truthfulness &amp; Hallucination and Instruction Following):</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import UltraFeedbackTask\n\nlabeller = OpenAILLM(\n    task=UltraFeedbackTask.for_overall_quality(),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>For the API reference visit UltraFeedbackTask.</p>"},{"location":"technical-reference/tasks/#judgelmtask","title":"JudgeLMTask","text":"<p>The task specially designed to build the prompts following the UltraFeedback paper: JudgeLM: Fine-tuned Large Language Models Are Scalable Judges. This task is designed to evaluate the performance of AI assistants.</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import JudgeLMTask\n\nlabeller = OpenAILLM(task=JudgeLMTask(), api_key=os.getenv(\"OPENAI_API_KEY\", None))\n</code></pre> <p>For the API reference visit JudgeLMTask.</p>"},{"location":"technical-reference/tasks/#ultrajudgetask","title":"UltraJudgeTask","text":"<p>This class implements a <code>PreferenceTask</code> specifically for a better evaluation using AI Feedback. The task is defined based on both UltraFeedback and JudgeLM, but with several improvements / modifications.</p> <p>It introduces an additional argument to differentiate various areas for processing. While these areas can be customized, the default values are as follows:</p> <pre><code>from distilabel.tasks import UltraJudgeTask\n\n# To see the complete system_prompt and task_description please take a look at the UltraJudgeTask definition\nultrajudge_task = UltraJudgeTask(\n    system_prompt=\"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences...\",\n    task_description=\"Your task is to rigorously evaluate the performance of...\",\n    areas=[\n        \"Practical Accuracy\",\n        \"Clarity &amp; Transparency\",\n        \"Authenticity &amp; Reliability\",\n        \"Compliance with Intent\",\n    ],\n)\n</code></pre> <p>Which can be directly used in the following way:</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import UltraJudgeTask\n\nlabeller = OpenAILLM(task=UltraJudgeTask(), api_key=os.getenv(\"OPENAI_API_KEY\", None))\n</code></pre> <p>For the API reference visit UltraJudgeTask.</p>"},{"location":"technical-reference/tasks/#complexityscorertask","title":"ComplexityScorerTask","text":"<p>This class implements a <code>PreferenceTask</code> to rate a list of instructions according to its complexity or difficulty. Defined in Deita framework, it's intended use is the scoring of instructions whose complexity has been enhanced by means of the <code>EvolComplexity</code> method defined, inspired on the <code>EvolInstruct</code> method from WizardLM.  </p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import ComplexityScorerTask\n\nlabeller = OpenAILLM(\n    task=ComplexityScorerTask(),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>Differently to other tasks, this case doesn't need a reference input to valuate the complexity of the instructions. The following example shows how it works with a small sample dataset:</p> <pre><code>import os\nfrom datasets import Dataset\n\nfrom distilabel.tasks import ComplexityScorerTask\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\n\n\n# Create a sample dataset (this one is inspired from the distilabel-sample-evol-complexity dataset)\nsample_evol_complexity = Dataset.from_dict(\n    {\n        'generations': [\n            [\n                'Generate a catchy tagline for a new high-end clothing brand\\n',\n                \"Devise a captivating and thought-provoking tagline that effectively represents the unique essence and luxurious nature of an upcoming luxury fashion label. Additionally, ensure that the tagline encapsulates the brand's core values and resonates with the discerning tastes of its exclusive clientele.\"\n            ],\n            [\n                'How can I create a healthier lifestyle for myself?\\n',\n                'What are some innovative ways to optimize physical and mental wellness while incorporating sustainable practices into daily routines?'\n            ]\n        ]\n    }\n)\n\n# Create the pipeline\npipe = Pipeline(\n    labeller=OpenAILLM(\n        task=ComplexityScorerTask(),\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        temperature=0.1\n    )\n)\n\n# Run the pipeline in the sample dataset\nnew_dataset = pipe.generate(sample_evol_complexity.select(range(3,5)))\n\nprint(new_dataset.select_columns([\"generations\", \"rating\"])[:])\n# {\n#   \"generations\": [\n#     [\n#       \"Generate a catchy tagline for a new high-end clothing brand\\n\",\n#       \"Devise a captivating and thought-provoking tagline that effectively represents the unique essence and luxurious nature of an upcoming luxury fashion label. Additionally, ensure that the tagline encapsulates the brand's core values and resonates with the discerning tastes of its exclusive clientele.\"\n#     ],\n#     [\n#       \"How can I create a healthier lifestyle for myself?\\n\",\n#       \"What are some innovative ways to optimize physical and mental wellness while incorporating sustainable practices into daily routines?\"\n#     ]\n#   ],\n#   \"rating\": [\n#     [1.0, 3.0],\n#     [1.0, 2.0]\n#   ]\n# }\n</code></pre> <p>For the API reference visit ComplexityScorerTask.</p>"},{"location":"technical-reference/tasks/#qualityscorertask","title":"QualityScorerTask","text":"<p>This class implements a <code>PreferenceTask</code> to rate a list of instructions according to its quality. Follows the same idea defined in the <code>ComplexityScorerTask</code> from the Deita framework, but in this case it rates the instructions in terms of concepts like helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response.</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import QualityScorerTask\n\nlabeller = OpenAILLM(\n    task=QualityScorerTask(),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>By default, the quality is defined as the following the paper prompt, but can be modified updating the <code>task_description</code> as in the following example (keep in mind the default <code>task_description</code> corresponds to the <code>EvolQuality</code> criteria defined to evolve the initial instructions, so this should be taken into account):</p> <pre><code>import os\n\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import QualityScorerTask\n\nlabeller = OpenAILLM(\n    task=QualityScorerTask(\n        task_description=\"Take into account the expressiveness of the answers.\"\n    ),\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n)\n</code></pre> <p>The following is an example pipeline to label a dataset using <code>QualityScorerTask</code>:</p> <pre><code>import os\n\nfrom datasets import Dataset\nfrom distilabel.tasks import QualityScorerTask\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\n\n\n# Create a sample dataset (this one is inspired from the distilabel-intel-orca-dpo-pairs)\nsample_evol_quality = Dataset.from_dict(\n    {\n        'input': [\n            'Please answer the following question: I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\"?\\nAnswer:',\n            \"James runs a TV show and there are 5 main characters and 4 minor characters. He pays the minor characters $15,000 each episode. He paid the major characters three times as much. How much does he pay per episode? Let's be accurate as possible.\"\n        ],\n        'generations': [\n            [\n                \" Certainly! Here's a detailed and long answer to the question you provided:\\n\\nQuestion: Read the passage below and answer the following questions.\\n\\nPassage: In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\\n\\n1. What event did the passage describe?\\n2. How many colonies were involved in the Federation of Australia?\\n3. What did the colonies keep after forming one nation?\\n4. What was the purpose of the federal government established by the Constitution of Australia?\\n5. What were the colonies called after they became states of the Commonwealth of Australia?\\n\\nAnswers:\\n\\n1. The passage describes the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation.\\n2. Six colonies were involved in the Federation of Australia: New South Wales, Queensland, South Australia, Tasmania, Victoria, and Western Australia.\\n3. The colonies kept the systems of government that they had developed as separate colonies.\\n4. The federal government established by the Constitution of Australia was responsible for matters concerning the whole nation.\\n5. After becoming states of the Commonwealth of Australia, the colonies were called states.\\n\\nIn-depth analysis:\\n\\nThe passage provides a brief overview of the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation in 1901. The colonies kept the systems of government that they had developed as separate colonies, but they also established a federal government that was responsible for matters concerning the whole nation. This federal government was established by the Constitution of Australia, which came into force on January 1, 1901.\\n\\nThe passage highlights the significance of the Federation of Australia by noting that it was a process by which the colonies collectively became states of the Commonwealth of Australia. This transformation marked a significant milestone in the history of Australia, as it created a unified nation out of six separate colonies.\\n\\nThe passage also provides information about the systems of government that were kept by the colonies after forming one nation. Specifically, it notes that the colonies kept the systems of government that they had developed as separate colonies. This suggests that the Federation of Australia did not result in a complete overhaul of the existing systems of government, but rather a reorganization of these systems to create a federal government that was responsible for matters concerning the whole nation.\\n\\nFinally, the passage uses descriptive language to emphasize the importance of the Constitution of Australia in the Federation of Australia. It notes that the Constitution came into force on January 1, 1901, and that it established the federal government that was responsible for matters concerning the whole nation. This language emphasizes the significance of the Constitution in the formation of the Federation of Australia and the creation of the Commonwealth of Australia.\\n\\nIn conclusion, the passage provides a detailed overview of the Federation of Australia, highlighting the significance of the Constitution of Australia in the formation of the Commonwealth of Australia. It also provides information about the systems of government that were kept by the colonies after forming one nation, and emphasizes the importance of the Constitution in the creation of the federal government responsible for matters concerning the whole nation.\",\n                \"Certainly! Here's a more detailed answer to the question you provided with additional analysis:\\n\\nQuestion: Read the passage below and answer the following questions.\\n\\nPassage: In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\\n\\n1. What\"\n            ],\n            [\n                'James pays the minor characters $15,000 each episode. Since there are 4 minor characters, he pays them a total of 4 * $15,000 = $60,000 per episode.\\n\\nThe major characters are paid three times as much. So, each major character gets paid 3 * $15,000 = $45,000 per episode.\\n\\nThere are 5 main characters, so he pays them a total of 5 * $45,000 = $225,000 per episode.\\n\\nIn total, James pays $225,000 (major characters) + $60,000 (minor characters) = $285,000 per episode.', \n                \"In James' TV show, he pays each of the 4 minor characters $15,000 per episode, totaling $60,000. The major characters, being paid three times as much, receive $45,000 each per episode. With 5 main characters, James pays a total of $225,000 for them. Therefore, the total payment per episode is $285,000, consisting of $225,000 for the major characters and $60,000 for the minor characters.\"\n            ]\n        ]\n   }\n)\n\n# Create the pipeline to label the dataset with theQualityScorerTask\npipe_labeller = Pipeline(\n    labeller=OpenAILLM(\n        task=QualityScorerTask(),\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        temperature=0.1,\n        max_new_tokens=1024\n    )\n)\n\n# Run the pipeline to get the scoring for the datase\nquality_labelled_dataset = pipe_labeller.generate(sample_evol_quality)\nprint(quality_labelled_dataset.select_columns([\"labelling_prompt\", \"rating\"])[0])\n# {\n#     'labelling_prompt': [\n#         {'content': '', 'role': 'system'},\n#         {\n#             'content': 'Rank the following responses provided by different AI assistants to the user\u2019s question\\naccording to the quality of their response. Score each response from 1 to 2, with 3\\nreserved for responses that are already very well written and cannot be improved further.\\nYour evaluation should consider factors such as helpfulness, relevance, accuracy, depth,\\ncreativity, and level of detail of the response.\\nUse the following format:\\n[Response 1] Score:\\n[Response 2] Score:\\n...\\n#Question#: You will be given a definition of a task first, then some input of the task.\\nThis task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them.\\n\\nAFC Ajax (amateurs)\\'s ground is Sportpark De Toekomst where Ajax Youth Academy also play.\\nOutput:\\n#Response List#:\\n\\n[Response 1] [\\n  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\\n  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\\n]\\n[Response 2] The RDF triplets generated from the input sentence \"AFC Ajax (amateurs)\\'s ground is Sportpark De Toekomst where Ajax Youth Academy also play\" accurately capture the relationships present. The output is a list of triplets that includes [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"] and [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]. These triplets represent the structure and semantics of the sentence.',\n#             'role': 'user'\n#         }\n#     ],\n#     'rating': [2.0, 3.0]\n# }\n</code></pre> <p>For the API reference visit QualityScorerTask.</p>"},{"location":"technical-reference/tasks/#critique","title":"Critique","text":"<p>The <code>CritiqueTask</code> is designed to be a labeller for generated text, while not only adding scores based on a rubric, but also critiques explaining the reasons why those scores have been provided. The critique can either be using a reference answer (gold answer) as e.g. Prometheus does, or just by generating the critique per each of the N provided generations.</p> <p>The resulting datasets after running a pipeline with the <code>CritiqueTask</code> are useful towards either training a model to generate critiques based on the critiques generated by a more powerful model as e.g. GPT-4 from OpenAI, or to be used directly for DPO fine-tuning. The fact that the critique is generated per each pair, a balanced dataset could be generated with individual critiques and their scores, so that then we can e.g. define a threshold on what's considered chosen and rejected, to then run DPO fine-tunes.</p> <p>While the <code>CritiqueTask</code> may seem fairly similar to the <code>PreferenceTask</code>, there is a core difference, which is the fact that the critiques are provided per each response or even to a single response, with no need to compare or rate them against each other.</p>"},{"location":"technical-reference/tasks/#ultracmtask","title":"UltraCMTask","text":"<p>This task is specifically designed to build the prompts following the format defined in the \"UltraFeedback: Boosting Language Models With High Quality Feedback\" paper.</p> <p>UltraCM is a model that has been fine-tuned using the UltraFeedback dataset, so as to produce critiques for the generated content, as the authors claim in their paper: \"Moreover, since ULTRAFEEDBACK provides detailed textual feedback, we also fine-tune a model that could critique model responses automatically. Our critique model, UltraCM, generates reasonable and detailed comments on various tasks.\".</p> <p>Ideally, the <code>UltraCMTask</code> will be more consistent when used with either their fine-tuned model UltraCM or with OpenAI, as both have been proven to produce successfully the structured content following the prompt formatting, and not only structured, but also meaningful and reasonable.</p> <p>See the following snippet, with an example on how to instantiate the <code>UltraCMTask</code> which only requires the system prompt, and it can be modified based on how is the critique intended to be formulated, while the system prompt shown below is the default one as of the UltraFeedback paper.</p> <pre><code>from distilabel.tasks import UltraCMTask\n\ntask = UltraCMTask(\n    system_prompt=\"User: A one-turn chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, very detailed, and polite answers to the user's questions.&lt;/s&gt;\",\n)\n</code></pre>"},{"location":"technical-reference/tasks/#prometheustask","title":"PrometheusTask","text":"<p>This task is specifically designed to build the prompts following the format defined in the \"Prometheus: Inducing Fine-grained Evaluation Capability in Language Models\" paper.</p> <p>Ideally, the <code>PrometheusTask</code> should only be used to format the prompts for the Prometheus models as those are the ones that have been fine-tuned to follow the same formatting and will produce consistent results compared to other base models or fine-tuned with different formats. In this case, since the formatting used by Prometheus follows the Llama 2 format, those are recommended. Otherwise, OpenAI has also proved to produce consistent results.</p> <p>The following snippet can be used out of the box to define a simple <code>PrometheusTask</code> with the system prompt, the scoring criteria and the score descriptions, but those can be modified while keeping in mind that Prometheus always expects 5 scores from 1-5 with a meaningful description, as well as with a criteria relevant to the scores defined.</p> <pre><code>from distilabel.tasks import PrometheusTask\n\ntask = PrometheusTask(\n    system_prompt=\"You are a fair evaluator language model.\",\n    scoring_criteria=\"Relevance, Grammar, Informativeness, Engagement\",\n    score_descriptions={\n        1: \"The response is not relevant to the prompt.\",\n        2: \"The response is relevant to the prompt, but it is not grammatical.\",\n        3: \"The response is relevant to the prompt and it is grammatical, but it is not informative.\",\n        4: \"The response is relevant to the prompt, it is grammatical, and it is informative, but it is not engaging.\",\n        5: \"The response is relevant to the prompt, it is grammatical, it is informative, and it is engaging.\",\n    },\n)\n</code></pre> <ol> <li> <p>The principles can be found here in the codebase. More information on the Principle Sampling can be found in the UltraFeedfack repository.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/","title":"\ud83e\uddfc Clean an existing preference dataset","text":"<p>In this tutorial, we will explain how to use <code>distilabel</code> to clean the known DPO dataset orca_dpo_pairs. If you want a spoiler, you can check the cleaned dataset.</p> <p>We will follow the next steps:</p> <ul> <li>Prepare the original dataset for cleaning.</li> <li>Create and run the distilabel pipeline.</li> <li>Optionally, post-process the cleaned dataset.</li> <li>Analyze the distilabelled dataset.</li> </ul> <p>Many open-source datasets are highly used to train and evaluate NLP models. However, many can be still improved in terms of quality, as we did with UltraFeedback, Dollys or Alpacas. </p> <p>In this case, the main intuition was that the original dataset just assumes gpt4/3.5-turbo is always the best response, but that's not always the case. And DPO fine-tuning benefits from the diversity of preference pairs.</p> <p>To address this issue, we used <code>distilabel</code>, an AI Feedback (AIF) framework that can generate and label datasets using LLMs and can be used for many different use cases.</p> <p>Let\u2019s start by installing the required dependencies to run distilabel and the remainder of this tutorial. Install Argilla for a better visualization and curation of the results</p> <pre><code>%pip install -q -U \"distilabel[openai,argilla]\" --upgrade\n</code></pre> <p>Then we can import the required libraries.</p> <pre><code>import os\nimport random\n\nimport nltk\nimport numpy as np\nimport openai\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom datasets import load_dataset\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import JudgeLMTask\nnltk.download('punkt')\n</code></pre> <pre><code># HF Token\nos.environ['HF_TOKEN'] = \"hf_...\"\n\n# OpenAI API Key\nos.environ[\"OPENAI_API_KEY\"] = 'sk-...'\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n</code></pre> <p>First, we will load the original orca_dpo_pairs, which consists of 12,859 preference pairs.</p> <p>&gt; Note: To enhance performance while using this tutorial as a guide, consider selecting a subset of samples from the original dataset. <pre><code>subsample = dataset.select(range(500))\n</code></pre></p> <pre><code># Load the dataset\ndataset = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train\")\n</code></pre> <pre><code>dataset\n</code></pre> <pre>\n<code>Dataset({\n    features: ['system', 'question', 'chosen', 'rejected'],\n    num_rows: 12859\n})</code>\n</pre> <p>In order to avoid positional bias and keep track of the order we will create and apply the function <code>shuffle_and_track</code> to the dataset. This function takes chosen and rejected, shuffles them randomly, and then returns a dictionary that includes the shuffled pair as generations and the order in which they were shuffled based on their original identification.</p> <pre><code># Shuffle 'chosen' and 'rejected'\ndef shuffle_and_track(chosen, rejected):\n    pair = [chosen, rejected]\n    random.shuffle(pair)\n    order = [\"chosen\" if x == chosen else \"rejected\" for x in pair]\n    return {\"generations\": pair, \"order\": order}\n\n# Apply the function to the dataset\ndataset = dataset.map(lambda x: shuffle_and_track(x[\"chosen\"], x[\"rejected\"]))\n</code></pre> <p>Moreover, to indicate which will be the input to be used for generation in our pipeline, we will rename the question column to input. This dataset is already binarized, but if you don't know about binarization or do you want to know how to binarize a dataset, you can take a look at here.</p> <pre><code># Rename the column\ndataset = dataset.rename_columns({\"question\": \"input\"})\n</code></pre> <pre><code>dataset\n</code></pre> <pre>\n<code>Dataset({\n    features: ['system', 'input', 'chosen', 'rejected', 'generations', 'order'],\n    num_rows: 12859\n})</code>\n</pre> <p>In this case, we will only need to include a <code>labeller</code> in our pipeline. The labeller will rate the generations according to the input and will add the rationale behind its score. So, we will start by initializing it using the OpenAI integration, which will take the following arguments:</p> <ul> <li><code>task</code>: Specify the usage of the LLM as a labeller by creating a prompt using a standard template. The JudgeLMTask is designed to evaluate the performance of AI assistants.</li> <li><code>model</code>: We use <code>gpt-4-1106-preview</code> as the model to be used for generation.</li> <li><code>num_threads</code>: <code>16</code> of threads to be used for parallel generation.</li> <li><code>max_new_tokns</code>: <code>512</code> is the maximum number of tokens to be generated.</li> </ul> <p>&gt; For more information about the LLM integrations, tasks and different components of the pipeline, please check the documentation.</p> <pre><code># Initialize the labeller\nlabeler = OpenAILLM(\n    task=JudgeLMTask(),\n    model=\"gpt-4-1106-preview\",\n    num_threads=16,\n    max_new_tokens=512,\n)\n</code></pre> <p>Then, we will add the labeller to the pipeline. We can check that no generator was added and the labeller takes the arguments we specified before.</p> <pre><code># Create the pipeline\ndistipipe = Pipeline(\n    labeller=labeler\n)\ndistipipe\n</code></pre> <pre>\n<code>Pipeline(\n    generator=None,\n    labeller=OpenAILLM(task=JudgeLMTask, num_threads=16, promp_format='None', model='gpt-4-1106-preview')\n)</code>\n</pre> <p>Finally, we will run the pipeline using the <code>generate</code> method. This method will take the input dataset and the desired number of generations to be performed for each input. For our case, we will indicate 2, one to rate chosen and the other for rejected that were added to the  generations column.</p> <p>&gt; Remember that the labelling process can take a while depending on the number of generations and the number of threads specified.</p> <pre><code># Compute ratings and natural language critiques for each pair\ndisti_dataset = distipipe.generate(dataset=dataset, num_generations=2)\n</code></pre> <p>Now, we can inspect the dataset again, as the generations and the rationale behind the score were added to the original dataset as rating and rationale.</p> <pre><code>disti_dataset.select_columns([\"input\", \"generations\", \"rating\", \"rationale\"])[0]\n</code></pre> <p>If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the URL and API_KEY:</p> <pre><code>import argilla as rg\n\n# Replace api_url with the url to your HF Spaces URL if using Spaces\n# Replace api_key if you configured a custom API key\nrg.init(\n    api_url=\"http://localhost:6900\",\n    api_key=\"owner.apikey\",\n    workspace=\"admin\"\n)\n</code></pre> <p>You can now push the dataset to Argilla as follows and start annotating it:</p> <pre><code># Convert the dataset to Argilla format adding questions and metadata\nrg_dataset = disti_dataset.to_argilla()\n\n# Push the dataset to Argilla\nrg_dataset.push_to_argilla(name=\"your_dataset_name\", workspace=\"admin\")\n</code></pre> <p></p> <p>Even if the dataset was already curated, we can still improve it by adding more information. Thus, we will swap rejected and chosen, and add chosen scores and status.</p> <p>The <code>add_status</code> function assesses the status of a set of responses based on their ratings and order. If there are no ratings, or if both ratings are equal, it sets the status to tie. If that's not the case, but the highest-rated response is not the chosen one, then is swapped. Otherwise, it keeps the status as unchanged.</p> <p>The <code>swap</code> function returns a dictionary with the current and original chosen and rejected items and the score of the chosen item.</p> <pre><code># Define the add_status function\ndef add_status(r):\n  status = \"unchanged\"\n  highest_rated_idx = np.argmax(r['rating'])\n  if r['rating']== None or r['rating'][0] == r['rating'][1]:\n    status = \"tie\"\n  elif r['order'][highest_rated_idx] != 'chosen':\n      status = \"swapped\"\n  return {\"status\": status}\n\n# Define the swap function\ndef swap(r):\n  chosen = r[\"chosen\"]\n  rejected = r[\"rejected\"]\n  if r['rating'] is not None:\n    chosen_score = r['rating'][np.argmax(r['rating'])]\n  else:\n    chosen_score = None\n  if r['status'] == \"swapped\":\n    chosen = r[\"rejected\"]\n    rejected = r[\"chosen\"]\n  return {\n      \"chosen\": chosen,\n      \"rejected\": rejected,\n      \"original_chosen\": r[\"chosen\"],\n      \"original_rejected\": r[\"rejected\"],\n      \"chosen_score\": chosen_score\n  }\n\n# Apply the functions to the dataset\nupdated_disti_dataset = disti_dataset.map(add_status).map(swap)\n</code></pre> <pre><code>updated_disti_dataset[0]\n</code></pre> <p>Conversely, when training a model to ensure the accuracy of its results, it is essential to verify that your training samples are not duplicated in your test set. In our case, we will use our dataset as an example and compare it with the gsm8k test dataset, which comprises 7473 samples in each subset.</p> <pre><code># Load the source dataset\nsource_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\nsource_dataset_socratic = load_dataset(\"gsm8k\", \"socratic\", split=\"train\")\n\n# Load the target dataset\ntarget_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n</code></pre> <p>Then, we will extract the questions from both datasets and preprocess them tokenizing and lowercasing them.</p> <pre><code># Extract the 'question' column from each dataset\nsource_questions = source_dataset['question']\nsource_questions_socratic = source_dataset_socratic['question']\ntarget_questions = target_dataset['input']\n</code></pre> <pre><code># Function to preprocess the text\ndef preprocess(text):\n    return nltk.word_tokenize(text.lower())\n\n# Preprocess the questions\nsource_questions_processed = [preprocess(q) for q in source_questions]\nsource_questions.extend([preprocess(q) for q in source_questions_socratic])\ntarget_questions_processed = [preprocess(q) for q in target_questions]\n</code></pre> <p>Finally, we will compare the questions from the test set with the ones from our dataset and check if there are any duplicated samples. To do so, we will vectorize the questions and calculate the cosine similarity. The threshold set was 0.8 so that we could avoid false positives, as it was tested manually.</p> <pre><code># Vectorize the questions\nvectorizer = TfidfVectorizer()\nsource_vec = vectorizer.fit_transform([' '.join(q) for q in source_questions_processed])\ntarget_vec = vectorizer.transform([' '.join(q) for q in target_questions_processed])\n</code></pre> <pre><code># Calculate cosine similarity\nsimilarity_matrix = cosine_similarity(source_vec, target_vec)\n</code></pre> <pre><code># Determine matches based on a threshold\nthreshold = 0.8\nmatching_pairs = []\nfor i, row in enumerate(similarity_matrix):\n    for j, similarity in enumerate(row):\n        if similarity &amp;gt;= threshold:\n            matching_pairs.append((source_questions[i], target_questions[j], similarity))\n</code></pre> <p>We can inspect the results by creating a dataframe.</p> <pre><code># Create a DataFrame from the matching pairs\nsimilarity_df = pd.DataFrame(matching_pairs, columns=['Source Question', 'Target Question', 'Similarity Score'])\n</code></pre> <pre><code>similarity_df.head()\n</code></pre> Source Question Target Question Similarity Score 0 Hans booked a room in a hotel. The hotel has 1... Lizzy: Hans booked a room in a hotel. The hote... 0.822757 1 John orders food for a massive restaurant.  He... John orders food for a massive restaurant. He ... 0.884102 2 Ariella has $200 more in her son's saving acco... Lizzy: Ariella has $200 more in her son's savi... 0.831908 3 Mike and Ted planted tomatoes.  In the morning... Mike and Ted planted tomatoes. In the morning,... 0.971729 4 A washing machine uses 20 gallons of water for... Question: A washing machine uses 20 gallons of... 0.961750 <p>And, we can add a new column to our dataset indicating whether each question is matched.</p> <pre><code># Create a set of matching target questions\nmatching_target_questions = list(similarity_df['Target Question'])\n\n# Add a column to the target dataset indicating whether each question is matched\ntarget_dataset = target_dataset.map(lambda example: {\"in_gsm8k_train\": example['input'] in matching_target_questions})\ntarget_dataset\n</code></pre> <pre>\n<code>Dataset({\n    features: ['system', 'input', 'chosen', 'rejected', 'generations', 'order', 'labelling_model', 'labelling_prompt', 'raw_labelling_response', 'rating', 'rationale', 'status', 'original_chosen', 'original_rejected', 'chosen_score', 'in_gsm8k_train'],\n    num_rows: 12859\n})</code>\n</pre> <p></p> <pre><code># Check the distilabelled dataset\ndataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n\n# Filter out the unchanged and tied examples\ndataset = dataset.filter(\n    lambda r: \n        r[\"status\"] != \"tie\" and \n        r[\"chosen_score\"] &amp;gt;= 8 and \n        not r[\"in_gsm8k_train\"]\n)\ndataset\n</code></pre> <pre>\n<code>Dataset({\n    features: ['system', 'input', 'chosen', 'rejected', 'generations', 'order', 'labelling_model', 'labelling_prompt', 'raw_labelling_response', 'rating', 'rationale', 'status', 'original_chosen', 'original_rejected', 'chosen_score', 'in_gsm8k_train'],\n    num_rows: 5922\n})</code>\n</pre> <p>In summary, we've demonstrated the process of cleaning a preference dataset using <code>distilabel</code>. Additionally, we've illustrated how to employ Argilla for visualizing and annotating the dataset that has been cleaned with distilabel. Lastly, we've covered the steps for post-processing the dataset and provided an analysis of the key changes that were made.</p> <p>Now the next question is: can we build better models with this new knowledge? The answer is the distilabeled Hermes model, check it out!</p> <p>Have a look at these resources if you want to go further:</p> <ul> <li>Train a Model with ArgillaTrainer</li> <li>Use Notus on inference endpoints to create a legal preference dataset</li> </ul>"},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#clean-an-existing-preference-dataset","title":"\ud83e\uddfc Clean an existing preference dataset","text":""},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#introduction","title":"Introduction","text":""},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#getting-started","title":"Getting Started","text":""},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#install-dependencies","title":"Install Dependencies","text":""},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#environment-variables","title":"Environment variables","text":"<p>Finally, we will also need to provide a HF_TOKEN and the OPENAI_API_KEY to run the distilabel pipeline.</p>"},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#prepare-the-dataset","title":"Prepare the Dataset","text":""},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#create-the-pipeline","title":"Create the Pipeline","text":""},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#human-feedback-with-argilla","title":"Human Feedback with Argilla","text":"<p>You can use the AI Feedback created by distilabel directly but we hae ve seen that enhancing it with human feedback will improve the quality of your LLM. We provide a <code>to_argilla</code> method which creates a dataset for Argilla along with out-of-the-box tailored metadata filters and semantic search to allow you to provide human feedback as quickly and engaging as possible. You can check the Argilla docs to get it up and running.</p>"},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#optional-post-process-the-dataset","title":"Optional: Post-process the dataset","text":""},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#optional-find-duplicated-examples","title":"Optional: Find duplicated examples","text":""},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#analyze-our-cleaned-dataset","title":"Analyze our cleaned dataset","text":"<p>This dataset is great for fine-tuning preferences, and it's a better choice than the original one. It's set up in the easy-to-understand \"chosen, rejected\" format and comes with extra details for more experiments and filtering. This updated dataset is really handy because it shows which responses are favorites (according to gpt-4-turbo), points out the responses with low scores, and even includes explanations in everyday language.</p> <p>The main changes are:</p> <ul> <li>~2K pairs have been swapped: rejected becomes the chosen response. We have kept the original chosen and rejected on two new columns original_* for reproducibility purposes.</li> <li>4K pairs have been identified as tie: equally bad or good.</li> <li>Chosen scores have been added: you can now filter out based on a threshold (see our distilabelled Hermes 2.5 model for an example)</li> <li>We have kept the ratings and rationales generated with gpt-4-turbo and distilabel so you can prepare the data differently if you want.</li> <li>We have added a column to indicate if the input is part of gsm8k train set.</li> </ul> <p>This results in 5,922 instead of 12,859 samples (54% reduction) and leads to better performance than the same model tuned with 100% of the samples in the original dataset.</p>"},{"location":"tutorials/clean-preference-dataset-judgelm-gpt/#conclusions","title":"Conclusions","text":""},{"location":"tutorials/create-a-math-preference-dataset/","title":"\ud83e\uddee Create a mathematical preference dataset","text":"<pre><code>import os\nfrom google.colab import userdata\n\nos.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n</code></pre> <pre><code>%pip install distilabel[openai,argilla] ipython-autotime -qqq\n%load_ext autotime\n</code></pre> <pre><code>from distilabel.tasks import SelfInstructTask\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\n</code></pre> <pre>\n<code>time: 13.3 s (started: 2023-11-28 13:21:22 +00:00)\n</code>\n</pre> <p>First of all, we will create a Hugging Face \ud83e\udd17 <code>dataset</code> that will contain a single column called <code>input</code>. This column will contain the math topics from which we want our LLM to generate instructions.</p> <p>&gt; It's important that the column is called <code>input</code>, because the <code>Task</code> that we will create later expects an input argument called <code>input</code>.</p> <pre><code>from datasets import Dataset\n\n\nmath_topics = [\n    \"Algebraic Expressions\",\n    \"Linear Equations\",\n    \"Quadratic Equations\",\n    \"Polynomial Functions\",\n    \"Rational Expressions\",\n    \"Exponential Functions\",\n    \"Logarithmic Functions\",\n    \"Sequences and Series\",\n    \"Matrices\",\n    \"Determinants\",\n    \"Complex Numbers\",\n    \"Trigonometry\",\n    \"Geometry\",\n    \"Coordinate Geometry\",\n    \"Vector Algebra\",\n    \"Statistics\",\n    \"Probability\",\n    \"Calculus\",\n    \"Differential Calculus\",\n    \"Integral Calculus\",\n    \"Limits and Continuity\",\n    \"Differentiation\",\n    \"Integration\",\n    \"Theorems of Calculus\",\n    \"Mathematical Reasoning\",\n    \"Set Theory\",\n    \"Number Theory\",\n    \"Permutations and Combinations\",\n    \"Binomial Theorem\",\n    \"Arithmetic Progressions\",\n    \"Geometric Progressions\",\n    \"Harmonic Progressions\",\n    \"Trigonometric Ratios\",\n    \"Trigonometric Identities\",\n    \"Inverse Trigonometric Functions\",\n    \"Hyperbolic Functions\",\n    \"Conic Sections\",\n    \"Circle Geometry\",\n    \"Ellipse Geometry\",\n    \"Parabola Geometry\",\n    \"Hyperbola Geometry\",\n    \"Function Theory\",\n    \"Graph Theory\",\n    \"Differential Equations\",\n    \"Mathematical Induction\",\n    \"Discrete Mathematics\",\n]\n\ndataset = Dataset.from_dict({\n    \"input\": math_topics\n})\n</code></pre> <pre>\n<code>time: 19.3 ms (started: 2023-11-28 13:21:46 +00:00)\n</code>\n</pre> <p>Next, we will a <code>SelfInstructTask</code> that will guide the <code>LLM</code> using the prompt to generate instructions from the given list of inputs.</p> <p>&gt; All the <code>Task</code>s have two properties <code>input_args_names</code> and <code>output_args_names</code> that indicates which arguments expects as inputs and outputs will generate respectively.</p> <pre><code>application_description = (\n    \"An AI assistant adept at answering a wide array of math, logic, and reasoning puzzles, trivia, \"\n    \"and general questions. Users of this assistant love to ask the assistant to think and outlines \"\n    \"the solutions step by step. It expects complete questions from users providing all the details \"\n    \"to solve the proposed problem or respond to general knowledge questions. It covers general \"\n    \"knowledge about math, puzzles, reasoning exercises, and real-life scenarios where math and \"\n    \"reasoning are important.\"\n)\n\n# by default `SelfInstructTask` will generate 5 instructions, but we can tweak\n# this behaviour passing the `num_instructions` argument.\ninstruction_task = SelfInstructTask(\n    application_description=application_description\n)\n\nprint(f\"`SelfInstructTask`\\n   - Input arguments: {instruction_task.input_args_names}\\n   - Output arguments: {instruction_task.output_args_names}\")\n</code></pre> <pre>\n<code>`SelfInstructTask`\n   - Input arguments: ['input']\n   - Output arguments: ['generations']\ntime: 692 \u00b5s (started: 2023-11-28 13:21:50 +00:00)\n</code>\n</pre> <p>Next step, we will create an <code>LLM</code>, in this case an instance of <code>OpenAILLM</code> as we want to use <code>gpt-3.5-turbo</code> to generate the instructions. We will pass the <code>instruction_task</code> for generating the prompts that we need for generating the instructions given the <code>input</code>s of our dataset.</p> <pre><code>instruction_generator = OpenAILLM(\n    task=instruction_task,\n    num_threads=8,\n    max_new_tokens=1024,\n    temperature=0.7\n)\n</code></pre> <pre>\n<code>time: 497 ms (started: 2023-11-28 13:21:54 +00:00)\n</code>\n</pre> <p>Finally, we will create a <code>Pipeline</code> to orchestrate the whole generation process. In this case we will only pass a <code>generator</code>.</p> <pre><code>pipeline = Pipeline(generator=instruction_generator)\n</code></pre> <pre>\n<code>time: 576 \u00b5s (started: 2023-11-28 13:21:56 +00:00)\n</code>\n</pre> <p>and we trigger the generation process calling the <code>generate</code> method of the pipeline... We specify that we want <code>10</code> generations for each input</p> <pre><code>distiset = pipeline.generate(\n    dataset=dataset,\n    num_generations=10,\n    batch_size=4\n)\n</code></pre> <pre><code>import re\n\ndef transform(inst: str) -&amp;gt; str:\n    \"\"\"Remove 1., 2., ... from the instruction.\"\"\"\n    clean_inst = re.sub(r'^\\d+\\.\\s*', '', inst)\n    return f\"{clean_inst}\"\n\ninstructions = [\n    transform(instruction)\n    for generations in distiset[\"instructions\"]\n    for generation in generations\n    for instruction in generation\n    if instruction != \"\"\n]\nprint(f\"Number of generated instructions: {len(instructions)}\")\n</code></pre> <pre>\n<code>Number of generated instructions: 4637\ntime: 60.1 ms (started: 2023-11-28 13:28:33 +00:00)\n</code>\n</pre> <pre><code>import random\n\nsamples = random.sample(instructions, 5)\n\nfor sample in samples:\n    print(sample)\n</code></pre> <pre>\n<code>How can the concept of probability be applied in real-life scenarios? \nCould you outline the process to solve a quadratic equation using the quadratic formula?\nExplain the process of expanding the binomial expression (x + 3)^2 step by step.\nHow can I find the sum of an arithmetic series?\nExplain the concept of factorial and provide an example of its application in real-life scenarios.\ntime: 8.4 ms (started: 2023-11-28 14:38:11 +00:00)\n</code>\n</pre> <pre><code>dataset = Dataset.from_dict({\"instructions\": instructions})\n</code></pre> <pre>\n<code>time: 17.8 ms (started: 2023-11-28 13:28:37 +00:00)\n</code>\n</pre> <pre><code>dataset.push_to_hub(\"argilla/distilabel-math-instructions\")\n</code></pre> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"argilla/distilabel-math-instructions\", split=\"train\")\ndataset = dataset.rename_column(\"instructions\", \"input\")\n</code></pre> <p>We create a <code>generator</code> that will use <code>gpt-3.5-turbo</code> for generating text. We also use the <code>principles</code> feature of the <code>TextGenerationTask</code>, that will inject a principle in the generated prompt to make the LLM generate a text focusing on the provided principle and that will allow us to generate a more heterogeneous dataset.</p> <pre><code>from distilabel.tasks import TextGenerationTask\n\ntext_generation_task = TextGenerationTask(\n    principles_distribution={\n        \"harmlessness\": 0.4,\n        \"helpfulness\": 0.2,\n        \"truthfulness\": 0.2,\n        \"honesty\": 0.1,\n        \"verbalized_calibration\": 0.1\n    }\n)\n\ngenerator = OpenAILLM(\n    task=text_generation_task,\n    num_threads=8,\n    max_new_tokens=1024\n)\n</code></pre> <pre>\n<code>time: 392 ms (started: 2023-11-28 13:29:01 +00:00)\n</code>\n</pre> <p>Next we create a <code>labeller</code> that will evaluate how good the texts that the <code>generator</code> gave us are. In this case we have decided to use the <code>UltraFeedbackTask</code> which defines a prompt template for generating preference datasets.</p> <pre><code>from distilabel.tasks import UltraFeedbackTask\n\npreference_labeller = OpenAILLM(\n    task=UltraFeedbackTask.for_instruction_following(),\n    num_threads=8,\n    max_new_tokens=1024,\n)\n</code></pre> <pre>\n<code>time: 374 ms (started: 2023-11-28 13:29:04 +00:00)\n</code>\n</pre> <pre><code>pipeline = Pipeline(\n    generator=generator,\n    labeller=preference_labeller\n)\n</code></pre> <pre>\n<code>time: 558 \u00b5s (started: 2023-11-28 13:29:05 +00:00)\n</code>\n</pre> <pre><code>distiset_pref = pipeline.generate(\n    dataset=dataset.shuffle().select(range(100)),\n    num_generations=3,\n    batch_size=8\n)\n</code></pre> <pre><code>distiset_pref.column_names\n</code></pre> <pre>\n<code>['input',\n 'generation_model',\n 'generation_prompt',\n 'raw_generation_responses',\n 'generations',\n 'labelling_model',\n 'labelling_prompt',\n 'raw_labelling_response',\n 'rating',\n 'rationale']</code>\n</pre> <pre>\n<code>time: 4.16 ms (started: 2023-11-23 16:36:11 +00:00)\n</code>\n</pre> <pre><code>print(\"Instruction:\", distiset_pref[0][\"input\"])\nfor generation in distiset_pref[0][\"generations\"]:\n    print(generation, end=\"\\n\\n\\n\\n\")\n</code></pre> <pre>\n<code>Instruction: What is the significance of graph theory in computer science and network analysis?\nGraph theory is a fundamental field in mathematics that has significant applications in computer science and network analysis. Here are a few ways it is significant:\n\n1. Network analysis: Graph theory provides a powerful framework for analyzing and understanding complex networks. It helps uncover key properties, such as connectivity, centrality, clustering, and community structure. These insights are essential for designing efficient and reliable networks, managing traffic, identifying vulnerabilities, and optimizing network performance.\n\n2. Data structures and algorithms: Graphs are commonly used as data structures in computer science. They are particularly useful for representing relationships and connections between entities. Many algorithms, such as Dijkstra's algorithm for shortest paths, the traveling salesman problem, and depth-first search, are based on graph theory and are widely employed in various computer science applications.\n\n3. Social networks and web analysis: Graph theory plays a crucial role in analyzing social networks and the structure of the web. It enables us to study patterns of interactions, identify influencers, detect communities, and understand information flow in complex systems like social media platforms and the World Wide Web.\n\n4. Circuit design and optimization: Graph theory aids in circuit design and optimization, especially in electronic and digital systems. By modeling circuits as graphs, engineers can analyze circuit behavior, optimize layouts, and solve various design problems that arise in areas like VLSI (Very Large Scale Integration) and hardware design.\n\n5. Computer graphics and image processing: Graph theory has applications in computer graphics and image processing. Techniques like graph coloring, graph matching, and graph-based segmentation are used for tasks such as color allocation, object recognition, and image compression.\n\nThese are just a few examples of how graph theory is significant in computer science and network analysis. Its applications are vast and continue to grow as technology advances.\n\n\n\nGraph theory is highly significant in computer science and network analysis for a variety of reasons:\n\n1. Modeling Relationships: In computer science, graph theory allows us to model and represent complex relationships and dependencies between entities using graphs. This is particularly useful in network analysis, as it helps us understand the connections between nodes in a network, such as the internet or social networks.\n\n2. Algorithm Design: Graph algorithms play a crucial role in solving various computational problems efficiently. Many important algorithms, like Dijkstra's algorithm for finding the shortest path or Prim's algorithm for finding a minimum spanning tree, are based on graph theory. These algorithms are used extensively in network analysis, routing problems, optimizing resource allocation, and many other areas in computer science.\n\n3. Network Analysis: Graph theory provides a powerful framework for analyzing networks, both in theoretical and practical terms. It enables the identification of network structures, such as cliques, communities, or hubs, which helps in understanding network behavior, predicting patterns, and identifying vulnerabilities or bottlenecks.\n\n4. Optimization Problems: Graph theory offers tools for solving various optimization problems that arise in computer science and network analysis. For example, the traveling salesman problem, which seeks the shortest route through a set of cities, can be modeled as a graph problem, allowing the application of graph algorithms to find efficient solutions.\n\n5. Data Structures: Many data structures in computer science, such as trees and adjacency lists, are derived from graph theory. These data structures are fundamental for efficient storage and retrieval of information, and are widely used in implementing algorithms related to networks and other computational domains.\n\nOverall, graph theory provides a theoretical foundation and practical techniques to analyze and solve problems related to networks, computation, and data structures, making it an essential field within computer science and network analysis.\n\n\n\nGraph theory has significant applications in computer science and network analysis. Here are a few key areas where graph theory plays a crucial role:\n\n1. Network Analysis and Social Networks: Graph theory is fundamental to the analysis of complex networks, such as social networks, computer networks, biological networks, and transportation networks. It helps in understanding relationships, connectivity, and patterns within these networks.\n\n2. Data Structures and Algorithms: Many fundamental data structures and algorithms in computer science are based on graphs. Graphs are used to represent and model a variety of problems, including searching, sorting, shortest path algorithms, spanning trees, and flow optimization.\n\n3. Database Systems: Graph databases use graph theory to store and query data in a network-like structure. They enable efficient retrieval and analysis of interconnected data, making them suitable for domains like social media, recommendation systems, and fraud detection.\n\n4. Compiler Design and Code Optimization: Graph theory is employed in compiler design for optimizing code and representing dependencies between program components. Techniques like control flow graphs and data flow analysis utilize graph representations to optimize program execution.\n\n5. Artificial Intelligence and Machine Learning: Graphs are widely used in AI and machine learning to represent complex relationships and dependencies within data. Graph-based algorithms help in tasks like clustering, recommendation systems, community detection, and knowledge representation.\n\n6. Internet and Web Design: Web pages and links can be represented as graphs, enabling algorithms like PageRank to determine the importance and ranking of web pages. Graph theory is also used in studying internet topology, routing algorithms, and network flow optimization.\n\nGraph theory provides a powerful framework for modeling, analyzing, and solving problems in these areas and more. Its concepts and algorithms are essential tools for computer scientists and network analysts.\n\n\n\ntime: 1.99 ms (started: 2023-11-23 16:38:05 +00:00)\n</code>\n</pre> <pre><code>distiset_pref[0][\"rationale\"]\n</code></pre> <pre>\n<code>['Text 1 fully aligns with the task goal and restrictions. It provides a comprehensive explanation of the significance of graph theory in computer science and network analysis by discussing multiple applications and how they are relevant in each area.',\n 'Text 2 almost fully aligns with the task goal and restrictions. It covers most of the significant ways graph theory is used in computer science and network analysis, but it could benefit from providing more specific examples or details in some areas.',\n 'Text 3 partially aligns with the task goal and restrictions. While it touches on some key areas where graph theory is significant, it lacks detailed explanations and examples in certain domains such as algorithm design and network analysis. It would benefit from providing a more comprehensive discussion in order to fully meet the requirements.']</code>\n</pre> <pre>\n<code>time: 4.56 ms (started: 2023-11-23 16:39:01 +00:00)\n</code>\n</pre> <p>If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the URL and API_KEY:</p> <pre><code>import argilla as rg\n\n# Replace api_url with the url to your HF Spaces URL if using Spaces\n# Replace api_key if you configured a custom API key\nrg.init(\n    api_url=\"http://localhost:6900\",\n    api_key=\"owner.apikey\",\n    workspace=\"admin\"\n)\n</code></pre> <p>Now we can convert our dataset to a formatted Argilla dataset and push it.</p> <pre><code>rg_dataset = distiset_pref.to_argilla()\n</code></pre> <pre>\n<code>time: 347 ms (started: 2023-11-23 15:36:31 +00:00)\n</code>\n</pre> <pre><code>rg_dataset.push_to_argilla(name=\"math-preference-dataset\", workspace=\"admin\")\n</code></pre> <pre>\n<code>RemoteFeedbackDataset(\n   id=4232d7ac-eaff-49b0-88b8-3a384b76efbb\n   name=math-preference-dataset\n   workspace=Workspace(id=2fc2ebed-8d20-41b0-b33a-5c5f3712da53, name=admin, inserted_at=2023-11-23 15:00:40.160242, updated_at=2023-11-23 15:00:40.160242)\n   url=https://gabrielmbmb-distilabel.hf.space/dataset/4232d7ac-eaff-49b0-88b8-3a384b76efbb/annotation-mode\n   fields=[RemoteTextField(id=UUID('dc965a9c-ac85-449b-ae16-bda998c88c1c'), client=None, name='input', title='Input', required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('0d29f518-a3bf-4642-a7d7-1324329555b7'), client=None, name='generations-1', title='Generations-1', required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('c0541a45-8892-49fb-8b85-fcaa0cd147e0'), client=None, name='generations-2', title='Generations-2', required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('4da8a4b5-553d-4b1d-a14f-b936a313797f'), client=None, name='generations-3', title='Generations-3', required=True, type='text', use_markdown=False)]\n   questions=[RemoteRatingQuestion(id=UUID('fb012ff3-5fb7-40c0-b623-d4195c1508c8'), client=None, name='generations-1-rating', title=\"What's the rating for generations-1?\", description=None, required=True, type='rating', values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), RemoteRatingQuestion(id=UUID('a7f02428-74e4-4497-8f26-803988e5c336'), client=None, name='generations-2-rating', title=\"What's the rating for generations-2?\", description=None, required=True, type='rating', values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), RemoteRatingQuestion(id=UUID('cbab782f-b0c0-4ff6-8c64-c58ad3ea476a'), client=None, name='generations-3-rating', title=\"What's the rating for generations-3?\", description=None, required=True, type='rating', values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), RemoteTextQuestion(id=UUID('405620c5-8aea-4fda-b92b-ee72e90629a9'), client=None, name='ratings-rationale', title=\"What's the rationale behind the ratings?\", description=None, required=True, type='text', use_markdown=False)]\n   guidelines=None)</code>\n</pre> <pre>\n<code>time: 8.63 s (started: 2023-11-23 15:36:51 +00:00)\n</code>\n</pre> <p>We can now jump in the UI and start providing human feedback to improve the quality of the synthetic dataset. </p>"},{"location":"tutorials/create-a-math-preference-dataset/#create-a-mathematical-preference-dataset","title":"\ud83e\uddee Create a mathematical preference dataset","text":"<p><code>distilabel</code> is a new  AI Feedback (AIF) framework created by Argilla that leverages the power of LLMs for generating synthetic datasets for preference or self-instruct. You can find more information in the links below:</p> <ul> <li>GitHub: argilla-io/distilabel</li> <li>Docs: distilabel.argilla.io</li> </ul> <p>And also don't forget to follow us on social media to keep up to date with the latest news about Argilla and distilabel:</p> <ul> <li>Twitter: @argilla_io</li> <li>LinkedIn: Argilla</li> </ul>"},{"location":"tutorials/create-a-math-preference-dataset/#demo","title":"Demo","text":"<p>In this demo, we will create a preference dataset that can be later used to fine-tune an LLM using DPO. First, we will define a list of math topics and we will create a pipeline for generating a list of instructions using <code>self-instruct</code> and OpenAI <code>gpt-3.5-turbo</code>. After that, we will create another pipeline in which we will ask <code>gpt-3.5-turbo</code> to generate 3 texts for each instruction, and finally we will ask it again to rate these responses, given us our preference dataset.</p>"},{"location":"tutorials/create-a-math-preference-dataset/#setup","title":"Setup","text":"<p>For this tutorial you will need an API key associated to your OpenAI account. After that, you will need to create a Google Colab secret clicking the icon key in the left side bar and create a secret called <code>api_key</code> with your OpenAI API key as value.</p> <p>&gt; Google Colab secrets has been released a few weeks ago and it's very useful to reuse and not leak your API Keys!</p>"},{"location":"tutorials/create-a-math-preference-dataset/#installing-distilabel","title":"Installing <code>distilabel</code>","text":"<p>We will <code>distilabel</code> with the <code>openai</code> and <code>argilla</code> extras, to also install the <code>openai</code> and <code>argilla</code> clients that we will need later. In addition, we will install an extension for timing some cells.</p>"},{"location":"tutorials/create-a-math-preference-dataset/#instruction-generation","title":"Instruction generation","text":"<p>As mentioned above, we will first create a <code>Pipeline</code> for generating instructions using <code>self-instruct</code> and <code>gpt-3.5-turbo</code>. For that we will create an instance of <code>SelfInstructTask</code>, which defines a prompt template for generating instructions given an application description. We will also create an instance of <code>OpenAILLM</code> for using <code>gpt-3.5-turbo</code> and we will pass it the <code>SelfInstructTask</code> instance that we created before.</p> <p>&gt; As we're passing a <code>Task</code> for generating texts to the <code>OpenAILLM</code> we can denominate this one as a <code>generator</code>.</p>"},{"location":"tutorials/create-a-math-preference-dataset/#preference-dataset","title":"Preference dataset","text":"<p>We have the instructions, but we still need the responses for these instructions, and more importally, evaluate how good these responses are.</p> <p>To do so, we will create a new <code>Pipeline</code> for generating and labelling the generated texts:</p> <ol> <li>We will a <code>generator</code> LLM using <code>OpenAILLM</code> and the <code>TextGenerationTask</code> to generate responses for a given instruction.</li> <li>We will create a <code>labeller</code> LLM using <code>OpenAILLM</code> and the <code>UltraFeedbackTask</code> task to a rating telling us how good was a response for a given instruction.</li> </ol>"},{"location":"tutorials/create-a-math-preference-dataset/#human-feedback-with-argilla","title":"Human Feedback with Argilla","text":"<p>You can use the AI Feedback created by distilabel directly but we hae ve seen that enhancing it with human feedback will improve the quality of your LLM. We provide a <code>to_argilla</code> method which creates a dataset for Argilla along with out-of-the-box tailored metadata filters and semantic search to allow you to provide human feedback as quickly and engaging as possible. You can check the Argilla docs to get it up and running.</p>"},{"location":"tutorials/improving-text-embeddings-with-llms/","title":"\ud83e\udd92 Improving Text Embeddings with LLMs","text":"<pre><code>%pip install --upgrade typing_extensions --quiet\n%pip install \"distilabel[openai,argilla]\" --quiet\n</code></pre> <p>We will start off with the first phase, which implies generating synthetic task definitions for asymmetric tasks, in this case, we will focus only on the text classification task pool, which follows the following format:</p> <pre><code>Brainstorm a list of potentially useful text classification tasks.\n\nPlease adhere to the following guidelines:\n- Tasks should cover a diverse range of domains and task types.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct text classification task in one sentence. Do not explain yourself or output anything else. Be creative!\n</code></pre> <pre><code>from distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import TextGenerationTask\n</code></pre> <p>Initially, we will need to define a custom <code>Task</code> that removes the default <code>system_prompt</code> and that parses the output using <code>eval</code>, as the prompt is asking the LLM to generate it using Python list formatting.</p> <pre><code>from typing import Dict, List\nfrom dataclasses import dataclass\n\n@dataclass\nclass TaskGenerationTask(TextGenerationTask):\n    system_prompt: str = \"\"\n\n    def parse_output(self, output: str) -&amp;gt; Dict[str, List[str]]:\n        return {\"generations\": eval(output)}\n</code></pre> <p>Once the default <code>Task</code> is created (<code>TaskGenerationTask</code>) we can already initialize the <code>LLM</code>, in this case <code>OpenAILLM</code> using GPT-4, and provide the recently defined task as an argument to it. Additionally, we will also include some generation kwargs, <code>temperature</code> and <code>top_p</code>, defined within the Appendix C of the paper, to encourage more diversity within the generation.</p> <pre><code>llm = OpenAILLM(\n    model=\"gpt-4\",\n    api_key=\"sk-***\",\n    task=TaskGenerationTask(),\n    prompt_format=\"openai\",\n    max_new_tokens=1024,\n    # Using the following kwargs as stated in Appendix C of the paper\n    temperature=1.0,\n    top_p=1.0,\n)\n</code></pre> <pre><code>pipeline = Pipeline(generator=llm)\n</code></pre> <p>Before calling the <code>Pipeline</code>, we'll need to prepare the input data, which in this case is only a prompt with no formatting required, as it's a simple <code>TextGenerationTask</code> we want to call multiple times.</p> <pre><code>from datasets import Dataset\n\nprompt = \"\"\"Brainstorm a list of potentially useful text classification tasks.\nPlease adhere to the following guidelines:\n- Tasks should cover a diverse range of domains and task types.\nYour output must always be a Python list of strings only, with about 20 elements, and each element corresponds to a distinct text classification task in one sentence. Do not explain yourself or output anything else. Be creative!\n\"\"\"\n\ndataset = Dataset.from_dict({\"input\": [prompt]})\n</code></pre> <p>Then, we're ready to call the <code>Pipeline.generate</code> method so that the prompt is sent to GPT-4 and N task definitions are generated synthetically.</p> <p>In this case, N should be equal or close to <code>num_generations</code> x 20, since within the prompt we ask the LLM to generate a Python list of about 20 elements.</p> <pre><code>new_dataset = pipeline.generate(dataset, num_generations=5, skip_dry_run=True)\n</code></pre> <p>Finally, once the generation has been completed, we will apply some post processing before proceeding to the next phase. The post-processing is to remove the columns that are not required, and to explore the columns with the tasks, so that we unwrap those lists and end up with a dataset with N rows, where N is the total number of tasks, initially contained within one row in nested lists.</p> <pre><code>df_dataset = new_dataset.to_pandas()\ndf_dataset = df_dataset.drop([\"generation_prompt\", \"raw_generation_responses\"], axis=1)\ndf_dataset = df_dataset.explode([\"generation_model\", \"generations\"])\ndf_dataset = df_dataset.explode([\"generations\"])\ndf_dataset = df_dataset.reset_index(drop=True)\n</code></pre> <pre><code>new_dataset = Dataset.from_pandas(df_dataset)\nnew_dataset = new_dataset.rename_columns({\"generation_model\": \"model\", \"generations\": \"task\"})\n</code></pre> <pre><code>new_dataset.push_to_hub(\"alvarobartt/improving-text-embeddings-with-llms\", config_name=\"task-generation\")\n</code></pre> <p>Once all the task definitions have been generated, we can proceed to the next phase, which consists on generating the data for a given task.</p> <p>In this case, the LLM will need to generate data that suits each task along with a label for that entry, and a misleading label for it too. Besides that, we will also sample some of the arguments within each prompt, so that the generation is ensured to be diverse.</p> <p>The prompt to be used is the following:</p> <pre><code>You have been assigned a text classification task: {task}\n\nYour mission is to write one text classification example for this task in JSON format. The JSON object must contain the following keys:\n- \"input_text\": a string, the input text specified by the classification task.\n- \"label\": a string, the correct label of the input text.\n- \"misleading_label\": a string, an incorrect label that is related to the task.\n\nPlease adhere to the following guidelines:\n- The \"input_text\" should be {num_words} words and diverse in expression.\n- The \"misleading_label\" must be a valid label for the given task, but not as appropriate as the \"label\" for the\n\"input_text\".\n- The values for all fields should be in {language}.\n- Avoid including the values of the \"label\" and \"misleading_label\" fields in the \"input_text\", that would make\nthe task too easy.\n- The \"input_text\" is {clarity} and requires {difficulty} level education to comprehend.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n</code></pre> <p>And the possible values for each arg:</p> <ul> <li><code>task</code> is the task definition generated in the previous phase.</li> <li><code>language</code> is any language name within XLM-R.</li> <li><code>num_words</code> is the number of words that the <code>input_text</code> to generate should contain at most.</li> <li><code>difficulty</code> is how difficult or which is the level required to comprehend the <code>input_text</code> to generate.</li> <li><code>clarity</code> is how easy or hard is the <code>input_text</code> to understand.</li> </ul> <pre><code>num_words = [\"less than 10\", \"at least 10\", \"at least 50\", \"at least 100\", \"at least 200\"]\ndifficulty = [\"high\", \"school\", \"college\", \"PhD\"]\nclarity = [\"clear\", \"understandable with some effort\", \"ambiguous\"]\n</code></pre> <pre><code>prompt = \"\"\"You have been assigned a text classification task: {task}\nYour mission is to write one text classification example for this task in JSON format. The JSON object must contain the following keys:\n- \"input_text\": a string, the input text specified by the classification task.\n- \"label\": a string, the correct label of the input text.\n- \"misleading_label\": a string, an incorrect label that is related to the task.\nPlease adhere to the following guidelines:\n- The \"input_text\" should be {num_words} words and diverse in expression.\n- The \"misleading_label\" must be a valid label for the given task, but not as appropriate as the \"label\" for the\n\"input_text\".\n- The values for all fields should be in {language}.\n- Avoid including the values of the \"label\" and \"misleading_label\" fields in the \"input_text\", that would make\nthe task too easy.\n- The \"input_text\" is {clarity} and requires {difficulty} level education to comprehend.\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\"\"\"\n</code></pre> <p>Just as before, we now need to create a custom <code>Task</code> to not only parse the output via <code>parse_output</code>, which in this case we are conducting the LLM to generate a valid JSON already, but also to generate the prompt via <code>generate_prompt</code>, since we need to introduce the sampling mentioned in the paper there.</p> <pre><code>import json\nfrom random import choice\nfrom typing import Any\n\nfrom distilabel.tasks.prompt import Prompt\n\n@dataclass\nclass ExampleGenerationTask(TextGenerationTask):\n    system_prompt: str = \"\"\n\n    @property\n    def input_args_names(self) -&amp;gt; List[str]:\n        return [\"task\"]\n\n    def generate_prompt(self, task: str) -&amp;gt; Prompt:\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=prompt.format(task=task, language=\"english\", num_words=choice(num_words), difficulty=choice(difficulty), clarity=choice(clarity)),\n        )\n\n    @property\n    def output_args_names(self) -&amp;gt; List[str]:\n        return [\"input_text\", \"label\", \"misleading_label\"]\n\n    def parse_output(self, output: str) -&amp;gt; Dict[str, Any]:\n        return json.loads(output)\n</code></pre> <p>Other than that, we are all set to instantiate the <code>OpenAILLM</code> with the recently created task, and call the <code>Pipeline.generate</code> method with the previously generated <code>datasets.Dataset</code>.</p> <pre><code>llm = OpenAILLM(\n    model=\"gpt-4\",\n    api_key=\"sk-***\",\n    task=ExampleGenerationTask(),\n    prompt_format=\"openai\",\n    max_new_tokens=1024,\n    # Using the following kwargs as stated in Appendix C of the paper\n    temperature=1.0,\n    top_p=1.0,\n)\n</code></pre> <pre><code>pipeline = Pipeline(generator=llm)\n</code></pre> <pre><code>final_dataset = pipeline.generate(new_dataset, num_generations=1, skip_dry_run=True)\n</code></pre> <pre><code>final_dataset.push_to_hub(\"alvarobartt/improving-text-embeddings-with-llms\", config_name=\"task-completion\")\n</code></pre> <p>If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the URL and API_KEY:</p> <pre><code>import argilla as rg\n\n# Replace api_url with the url to your HF Spaces URL if using Spaces\n# Replace api_key if you configured a custom API key\nrg.init(\n    api_url=\"http://localhost:6900\",\n    api_key=\"owner.apikey\",\n    workspace=\"admin\"\n)\n</code></pre> <p>Now we can convert our dataset to a formatted Argilla dataset and push it.</p> <pre><code># Convert the dataset to Argilla format\nrg_dataset = final_dataset.to_argilla()\n\n# Push the dataset to Argilla\nrg_dataset.push_to_argilla(name=\"my-dataset\", workspace=\"admin\")\n</code></pre>"},{"location":"tutorials/improving-text-embeddings-with-llms/#improving-text-embeddings-with-llms","title":"\ud83e\udd92 Improving Text Embeddings with LLMs","text":"<p>In this tutorial, we will be replicating the process described in \"Improving Text Embeddings with Large Language Models\" by Liang Wang et al. for synthetically generating multilingual data to be used for training a sentence similarity model.</p>"},{"location":"tutorials/improving-text-embeddings-with-llms/#installation","title":"Installation","text":"<p>We will start off by installing <code>distilabel</code> with the <code>openai</code> extra, as the authors used both GPT-4 and GPT-3.5 to generate the synthetic data, so no other LLMs were used. Install Argilla for a better visualization and curation of the results</p> <p>Note that we upgrade <code>typing_extensions</code> first, since <code>openai</code> may have conflicts with the default installed version of <code>typing_extensions</code> if outdated.</p>"},{"location":"tutorials/improving-text-embeddings-with-llms/#introduction","title":"Introduction","text":"<p>In \"Improving Text Embeddings with Large Language Models\" the authors leverage OpenAI proprietary LLMs as GPT-4 and GPT-3.5 to generate synthetic data for a wide range of diverse text embedding tasks, achieving competitive performance without using any labeled data. While when fine-tuning with a mixture of sythetic data and data from MS-Marco, their model sets SOTA results on BEIR and MTEB benchmarks.</p> <p>So on, the authors divide the generation process in two steps/phases: * Synthetic generation of the task definition/name, following a certain criteria. * Synthetic generation of the data for the task (to be used for fine-tuning) using the task definition and some other sampling params.</p> <p>So that after those phases, the authors end up with a dataset that is suitable for model fine-tuning.</p>"},{"location":"tutorials/improving-text-embeddings-with-llms/#phase-1-generating-task-definitions","title":"Phase 1: Generating task definitions","text":""},{"location":"tutorials/improving-text-embeddings-with-llms/#phase-2-generating-data-for-each-task","title":"Phase 2: Generating data for each task","text":""},{"location":"tutorials/improving-text-embeddings-with-llms/#human-feedback-with-argilla","title":"Human Feedback with Argilla","text":"<p>You can use the AI Feedback created by distilabel directly but we have seen that enhancing it with human feedback will improve the quality of your LLM. The <code>datasets.Dataset</code> generated by <code>Pipeline.generate</code> contains the <code>to_argilla</code> method which creates a dataset for Argilla along with out-of-the-box tailored metadata filters and semantic search to allow you to provide human feedback as quickly and engaging as possible. You can check the Argilla docs to get it up and running.</p>"},{"location":"tutorials/improving-text-embeddings-with-llms/#conclusion","title":"Conclusion","text":"<p>With <code>distilabel</code> generating synthetic dataset is easier than ever, and also customizable to a wide variety of use cases. In this tutorial, we showcased how to replicate \"Improving Text Embeddings with Large Language Models\", but could be adapted to your own needs.</p> <p>The authors mention that for future work, they aim to further improve the multilingual performance of our model and explore the possibility of using open-source LLMs to generate synthetic data, instead of OpenAI proprietary ones.</p> <p>Additionally, they also intend to investigate ways to improve the inference efficiency and lower the storage cost for LLM based text embeddings.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/","title":"\u2696\ufe0f Create a legal preference dataset","text":"<pre><code>%pip install -q -U distilabel \"farm-haystack[preprocessing]\"\n%pip install -q -U \"distilabel[hf-inference-endpoints, argilla]\"\n</code></pre> <pre><code>import os\nfrom typing import Dict\n\nfrom distilabel.llm import InferenceEndpointsLLM\nfrom distilabel.pipeline import Pipeline, pipeline\nfrom distilabel.tasks import TextGenerationTask, SelfInstructTask, Prompt\n\nfrom datasets import Dataset\nfrom haystack.nodes import PDFToTextConverter, PreProcessor\n</code></pre> <pre><code>os.environ[\"HF_TOKEN\"] = \"\"\nos.environ[\"HF_INFERENCE_ENDPOINT_NAME\"] = \"aws-notus-7b-v1-3184\"\nos.environ[\"HF_NAMESPACE\"] = \"argilla\"\nos.environ[\"OPENAI_API_KEY\"] = \"\"\n</code></pre> <pre><code>class QuestionAnsweringTask(TextGenerationTask):\n    def generate_prompt(self, question: str) -&amp;gt; str:\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=question,\n        ).format_as(\n            \"llama2\"\n        )  # type: ignore\n\n    def parse_output(self, output: str) -&amp;gt; Dict[str, str]:\n        return {\"answer\": output.strip()}\n\n    @property\n    def input_args_names(self) -&amp;gt; list[str]:\n        return [\"question\"]\n\n    @property\n    def output_args_names(self) -&amp;gt; list[str]:\n        return [\"answer\"]\n</code></pre> <p><code>llm</code> is an object of the <code>InferenceEndpointsLLM</code> class, and by using it we can start generating answers to question using the <code>llm.generate()</code> method.</p> <pre><code>llm = InferenceEndpointsLLM(\n    endpoint_name_or_model_id=os.getenv(\"HF_INFERENCE_ENDPOINT_NAME\"),  # type: ignore\n    endpoint_namespace=os.getenv(\"HF_NAMESPACE\"),  # type: ignore\n    token=os.getenv(\"HF_TOKEN\") or None,\n    task=QuestionAnsweringTask(),\n)\n</code></pre> <p>With the <code>InferenceEndpointsLLM</code> object defined with the endpoint information and the Task, we can go ahead and start generating text. Let's ask this LLM what's, for example, the second most populated city in Denmark. The answer should be Aarhus.</p> <pre><code>generation = llm.generate(\n    [{\"question\": \"What's the second most populated city in Denmark?\"}]\n)\ngeneration[0][0][\"parsed_output\"][\"answer\"]\n</code></pre> <pre>\n<code>'The second most populated city in Denmark is Aarhus, with a population of around 340,000 people. It is located on the east coast of Jutland, and is known for its vibrant cultural scene, beautiful beaches, and historic landmarks. Aarhus is also home to Aarhus University, one of the largest universities in Scandinavia.'</code>\n</pre> <p>The endpoint is working correctly! We have succesfully set up a custom generating task for a <code>distilabel</code> pipeline.</p> <pre><code>%%bash\n\nif [ ! -f \"The-AI-Act.pdf\" ]; then\n    wget -q https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf\nfi\n</code></pre> <p>Once we have it in our working directory, we can use Haystack's Converter and Pipeline features to extract the textual data, clean it and divide it in different batches. Afterwards, these batches will be used to start creating synthetic instructions.</p> <pre><code># The converter turns the PDF into text we can process easily\nconverter = PDFToTextConverter(remove_numeric_tables=True, valid_languages=[\"en\"])\n\n# Preprocessing pipelines can have several steps.\n# Ours clean empty lines, header, footers and whitespaces\n# and split the text into 150-char long batches, respecting\n# where the sentences naturally end and begin.\npreprocessor = PreProcessor(\n    clean_empty_lines=True,\n    clean_whitespace=True,\n    clean_header_footer=True,\n    split_by=\"word\",\n    split_length=150,\n    split_respect_sentence_boundary=True,\n)\n\ndoc = converter.convert(file_path=\"The-AI-Act.pdf\", meta=None)[0]\ndocs = preprocessor.process([doc])\nprint(f\"Documents: 1\\nBatches: {len(docs)}\")\n</code></pre> <p>Let's take a quick look at the batches we just generated.</p> <pre><code>inputs = [doc.content for doc in docs]\ninputs[0][0:500]\n</code></pre> <pre>\n<code>'EN EN\\nEUROPEAN\\nCOMMISSION\\nProposal for a\\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\\nLAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE\\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION\\nLEGISLATIVE ACTS\\x0cEN\\nEXPLANATORY MEMORANDUM\\n1. CONTEXT OF THE PROPOSAL\\n1.1. Reasons for and objectives of the proposal\\nThis explanatory memorandum accompanies the proposal for a Regulation laying down\\nharmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Int'</code>\n</pre> <p>The document has been correctly batched, from one big document to 355 strings, 150-character long at maximum. This list of strings can now be used as input to generate a instruction dataset using <code>distilabel</code>.</p> <pre><code>instructions_dataset = Dataset.from_dict({\"input\": inputs[0:50]})\n\ninstructions_dataset\n</code></pre> <pre>\n<code>Dataset({\n    features: ['input'],\n    num_rows: 50\n})</code>\n</pre> <p>With the <code>SelfInstructTask</code> class we can generate a Self-Instruct specitification for building the prompts, as done in the Self-Instruct paper. <code>distilabel</code> will start from human-made input, in this case, the batches we created from the AI Act pdf, and it will generate instructions based on it. These instructions can then be reviewed using Argilla to keep the best ones.</p> <p>An application description can be passed as a parameter to specify the behaviour of the model; we want a model capable of answering our questions about the AI Act.</p> <pre><code>instructions_task = SelfInstructTask(\n    application_description=\"A assistant that can answer questions about the AI Act made by the European Union.\"\n)\n</code></pre> <p>Let's now define a generator, passing the <code>SelfInstructTask</code> object, and create a <code>Pipeline</code> object.</p> <pre><code>instructions_generator = InferenceEndpointsLLM(\n    endpoint_name_or_model_id=os.getenv(\"HF_INFERENCE_ENDPOINT_NAME\"),  # type: ignore\n    endpoint_namespace=os.getenv(\"HF_NAMESPACE\"),  # type: ignore\n    token=os.getenv(\"HF_TOKEN\") or None,\n    task=instructions_task,\n)\n\ninstructions_pipeline = Pipeline(generator=instructions_generator)\n</code></pre> <p>Our pipeline is ready to be used to generate instructions. Let's do it!</p> <pre><code>generated_instructions = instructions_pipeline.generate(\n    dataset=instructions_dataset, num_generations=1, batch_size=8\n)\n</code></pre> <p>The pipeline has succesfully generated instructions given the topics and the behaviour passed as input. Let's gather all those instructions and see how the look.</p> <pre><code>instructions = []\nfor generations in generated_instructions[\"instructions\"]:\n    for generation in generations:\n        instructions.extend(generation)\n\nprint(f\"Number of generated instructions: {len(instructions)}\")\n\nfor instruction in instructions[:5]:\n    print(instruction)\n</code></pre> <pre>\n<code>Number of generated instructions: 178\nWhat are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?\nHow can artificial intelligence improve prediction, optimise operations and resource allocation, and personalise service delivery?\nWhat benefits can artificial intelligence bring to the European economy and society as a whole?\nHow can the use of artificial intelligence support socially and environmentally beneficial outcomes?\nWhat are the high-impact sectors that require AI action according to the AI Act by the European Union?\n</code>\n</pre> <p>These initial intructions form our instruction dataset. Following the human-in-the-loop approach, we should push the instructions to Argilla to visualize them and be able to rank them in terms of quality. Those annotations are essential to make quality data, ensuring a better performance of the final model. Nevertheless, this step is optional.</p> <pre><code>generated_instructions[0]\n</code></pre> <pre>\n<code>{'input': 'EN EN\\nEUROPEAN\\nCOMMISSION\\nProposal for a\\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\\nLAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE\\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION\\nLEGISLATIVE ACTS\\x0cEN\\nEXPLANATORY MEMORANDUM\\n1. CONTEXT OF THE PROPOSAL\\n1.1. Reasons for and objectives of the proposal\\nThis explanatory memorandum accompanies the proposal for a Regulation laying down\\nharmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence\\n(AI) is a fast evolving family of technologies that can bring a wide array of economic and\\nsocietal benefits across the entire spectrum of industries and social activities. By improving\\nprediction, optimising operations and resource allocation, and personalising service delivery,\\nthe use of artificial intelligence can support socially and environmentally beneficial outcomes\\nand provide key competitive advantages to companies and the European economy. ',\n 'generation_model': ['argilla/notus-7b-v1'],\n 'generation_prompt': ['You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks. You are given a task description and a set of instructions for how to write the prompts for an specific AI application.\\n# Task Description\\nDevelop 5 user queries that can be received by the given AI application and applicable to the provided context. Emphasize diversity in verbs and linguistic structures within the model\\'s textual capabilities.\\n\\n# Criteria for Queries\\nIncorporate a diverse range of verbs, avoiding repetition.\\nEnsure queries are compatible with AI model\\'s text generation functions and are limited to 1-2 sentences.\\nDesign queries to be self-contained and standalone.\\nBlend interrogative (e.g., \"What is the significance of x?\") and imperative (e.g., \"Detail the process of x.\") styles.\\nWrite each query on a separate line and avoid using numbered lists or bullet points.\\n\\n# AI Application\\nA assistant that can answer questions about the AI Act made by the European Union.\\n\\n# Context\\nEN EN\\nEUROPEAN\\nCOMMISSION\\nProposal for a\\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\\nLAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE\\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION\\nLEGISLATIVE ACTS\\x0cEN\\nEXPLANATORY MEMORANDUM\\n1. CONTEXT OF THE PROPOSAL\\n1.1. Reasons for and objectives of the proposal\\nThis explanatory memorandum accompanies the proposal for a Regulation laying down\\nharmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence\\n(AI) is a fast evolving family of technologies that can bring a wide array of economic and\\nsocietal benefits across the entire spectrum of industries and social activities. By improving\\nprediction, optimising operations and resource allocation, and personalising service delivery,\\nthe use of artificial intelligence can support socially and environmentally beneficial outcomes\\nand provide key competitive advantages to companies and the European economy. \\n\\n# Output\\n'],\n 'raw_generation_responses': ['1. What are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?\\n2. How can artificial intelligence improve prediction, optimise operations and resource allocation, and personalise service delivery?\\n3. What benefits can artificial intelligence bring to the European economy and society as a whole?\\n4. How can the use of artificial intelligence support socially and environmentally beneficial outcomes?\\n5. What competitive advantages can companies gain from using artificial intelligence?'],\n 'instructions': [['What are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?',\n   'How can artificial intelligence improve prediction, optimise operations and resource allocation, and personalise service delivery?',\n   'What benefits can artificial intelligence bring to the European economy and society as a whole?',\n   'How can the use of artificial intelligence support socially and environmentally beneficial outcomes?']]}</code>\n</pre> <p>For each input, i.e., each batch of the AI Act pdf file, we have a generator prompt, with general guidelines on how to behave, as well as the application description parameter. 4 instructions per input have been generated.</p> <p>Now it's the perfect time to upload the instruction dataset to Argilla, review it and manually annotate it.</p> <pre><code>instructions_rg_dataset = generated_instructions.to_argilla()\ninstructions_rg_dataset[0]\n</code></pre> <pre>\n<code>FeedbackRecord(fields={'input': 'EN EN\\nEUROPEAN\\nCOMMISSION\\nProposal for a\\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\\nLAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE\\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION\\nLEGISLATIVE ACTS\\x0cEN\\nEXPLANATORY MEMORANDUM\\n1. CONTEXT OF THE PROPOSAL\\n1.1. Reasons for and objectives of the proposal\\nThis explanatory memorandum accompanies the proposal for a Regulation laying down\\nharmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence\\n(AI) is a fast evolving family of technologies that can bring a wide array of economic and\\nsocietal benefits across the entire spectrum of industries and social activities. By improving\\nprediction, optimising operations and resource allocation, and personalising service delivery,\\nthe use of artificial intelligence can support socially and environmentally beneficial outcomes\\nand provide key competitive advantages to companies and the European economy.', 'instruction': 'What are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?'}, metadata={'length-input': 964, 'length-instruction': 129, 'generation-model': 'argilla/notus-7b-v1'}, vectors={}, responses=[], suggestions=(), external_id=None)</code>\n</pre> <pre><code>instructions_rg_dataset.push_to_argilla(name=f\"notus_AI_instructions\")\n</code></pre> <p>In the Argilla UI, each tuple input-instruction is visualized individually, and can be individually annotated.</p> <p></p> <pre><code>preference_pipeline = pipeline(\n    \"preference\",\n    \"instruction-following\",\n    generator=InferenceEndpointsLLM(\n        endpoint_name_or_model_id=os.getenv(\"HF_INFERENCE_ENDPOINT_NAME\"),  # type: ignore\n        endpoint_namespace=os.getenv(\"HF_NAMESPACE\", None),\n        task=TextGenerationTask(),\n        max_new_tokens=256,\n        num_threads=2,\n        temperature=0.3,\n    ),\n    max_new_tokens=256,\n    num_threads=2,\n    api_key=os.getenv(\"OPENAI_API_KEY\", None),\n    temperature=0.0,\n)\n</code></pre> <p>We also need to retrieve our instruction dataset from Argilla, as it will be the input of this pipeline.</p> <pre><code>remote_dataset = rg.FeedbackDataset.from_argilla(\n    \"notus_AI_instructions\", workspace=\"admin\"\n)\ninstructions_dataset = remote_dataset.pull(max_records=100)  # get first 100 records\n\ninstructions_dataset = instructions_dataset.format_as(\"datasets\")\ninstructions_dataset\n</code></pre> <pre>\n<code>Dataset({\n    features: ['input', 'instruction', 'instruction-rating', 'instruction-rating-suggestion', 'instruction-rating-suggestion-metadata', 'external_id', 'metadata'],\n    num_rows: 100\n})</code>\n</pre> <pre><code>instructions_dataset[0]\n</code></pre> <pre>\n<code>{'input': 'EN EN\\nEUROPEAN\\nCOMMISSION\\nProposal for a\\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\\nLAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE\\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION\\nLEGISLATIVE ACTS\\x0cEN\\nEXPLANATORY MEMORANDUM\\n1. CONTEXT OF THE PROPOSAL\\n1.1. Reasons for and objectives of the proposal\\nThis explanatory memorandum accompanies the proposal for a Regulation laying down\\nharmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence\\n(AI) is a fast evolving family of technologies that can bring a wide array of economic and\\nsocietal benefits across the entire spectrum of industries and social activities. By improving\\nprediction, optimising operations and resource allocation, and personalising service delivery,\\nthe use of artificial intelligence can support socially and environmentally beneficial outcomes\\nand provide key competitive advantages to companies and the European economy.',\n 'instruction': 'What are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?',\n 'instruction-rating': [],\n 'instruction-rating-suggestion': None,\n 'instruction-rating-suggestion-metadata': {'type': None,\n  'score': None,\n  'agent': None},\n 'external_id': None,\n 'metadata': '{\"length-input\": 964, \"length-instruction\": 129, \"generation-model\": \"argilla/notus-7b-v1\"}'}</code>\n</pre> <p>Before generating the text based on our instructions, we need to mingle a little bit with the dataset. From the previous section, we still have our old input, the batches from the PDF. We have to change that to the instructions that we generated.</p> <pre><code>instructions_dataset = instructions_dataset.rename_columns({\"input\": \"context\", \"instruction\": \"input\"})\n</code></pre> <p>Now, let's build a dataset by using the pipeline we just created, and the topics from which our instructions were generated.</p> <pre><code>preference_dataset = preference_pipeline.generate(\n    instructions_dataset,  # type: ignore\n    num_generations=2,\n    batch_size=8,\n    display_progress_bar=True,\n)\n</code></pre> <p>Let's take a look at an instance of the preference dataset</p> <pre><code>preference_dataset[0]\n</code></pre> <pre>\n<code>{'context': 'EN EN\\nEUROPEAN\\nCOMMISSION\\nProposal for a\\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\\nLAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE\\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION\\nLEGISLATIVE ACTS\\x0cEN\\nEXPLANATORY MEMORANDUM\\n1. CONTEXT OF THE PROPOSAL\\n1.1. Reasons for and objectives of the proposal\\nThis explanatory memorandum accompanies the proposal for a Regulation laying down\\nharmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence\\n(AI) is a fast evolving family of technologies that can bring a wide array of economic and\\nsocietal benefits across the entire spectrum of industries and social activities. By improving\\nprediction, optimising operations and resource allocation, and personalising service delivery,\\nthe use of artificial intelligence can support socially and environmentally beneficial outcomes\\nand provide key competitive advantages to companies and the European economy.',\n 'input': 'What are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?',\n 'instruction-rating': [],\n 'instruction-rating-suggestion': None,\n 'instruction-rating-suggestion-metadata': {'agent': None,\n  'score': None,\n  'type': None},\n 'external_id': None,\n 'metadata': '{\"length-input\": 964, \"length-instruction\": 129, \"generation-model\": \"argilla/notus-7b-v1\"}',\n 'generation_model': ['argilla/notus-7b-v1', 'argilla/notus-7b-v1'],\n 'generation_prompt': [\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\nWhat are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?\",\n  \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\nWhat are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?\"],\n 'raw_generation_responses': [\"\\nThe proposal for a Regulation laying down harmonised rules on artificial intelligence (AI) aims to ensure the trustworthy use of AI in the EU. It seeks to create a single market for AI applications and services, while ensuring that they are safe and respect fundamental rights. The proposal is part of the EU's broader strategy on AI, which aims to put the EU at the forefront of global AI development and deployment.\\nThe objectives of the proposal are to:\\n\\n1. Ensure that AI systems are designed, developed, and deployed in a way that respects fundamental rights and values, including human dignity, freedom, and privacy.\\n2. Ensure that AI systems are safe and secure, and do not pose unacceptable risks to people, property, or the environment.\\n3. Ensure that AI systems are robust, reliable, and accurate, and can be trusted to deliver the intended functionality.\\n4. Ensure that AI systems are traceable, meaning that it is possible to track how they work and how they make decisions.\\n5. Ensure that AI systems are transparent, meaning that it is possible to understand how they work and how they make decisions.\\n6. Ensure that AI systems are fair, meaning that they do not discriminate against individuals\",\n  '\\nThe proposal for a Regulation laying down harmonised rules on artificial intelligence (AI) aims to ensure a high level of safety and security of AI systems and to establish a horizontal and technology-neutral framework for AI applications. This will help to create a single market for AI and to ensure that AI systems are developed and deployed in a responsible manner. The proposal will also help to strengthen the competitiveness of the EU industry in the global AI market.\\nThe objectives of the proposal are:\\n1. To ensure that AI systems are safe and secure by establishing a risk-based framework for the development, placement on the market and use of AI systems.\\n2. To establish a horizontal and technology-neutral framework for AI applications that is applicable to all sectors and types of AI systems.\\n3. To ensure that AI systems are developed and deployed in a responsible manner by establishing requirements for transparency, robustness, security, accuracy, controllability and privacy protection.\\n4. To create a single market for AI by ensuring that AI systems are developed and deployed in a harmonised manner across the EU.\\n5. To strengthen the competitiveness of the EU industry in the global AI market by creating a level playing field for businesses and by promoting the'],\n 'generations': [\"\\nThe proposal for a Regulation laying down harmonised rules on artificial intelligence (AI) aims to ensure the trustworthy use of AI in the EU. It seeks to create a single market for AI applications and services, while ensuring that they are safe and respect fundamental rights. The proposal is part of the EU's broader strategy on AI, which aims to put the EU at the forefront of global AI development and deployment.\\nThe objectives of the proposal are to:\\n\\n1. Ensure that AI systems are designed, developed, and deployed in a way that respects fundamental rights and values, including human dignity, freedom, and privacy.\\n2. Ensure that AI systems are safe and secure, and do not pose unacceptable risks to people, property, or the environment.\\n3. Ensure that AI systems are robust, reliable, and accurate, and can be trusted to deliver the intended functionality.\\n4. Ensure that AI systems are traceable, meaning that it is possible to track how they work and how they make decisions.\\n5. Ensure that AI systems are transparent, meaning that it is possible to understand how they work and how they make decisions.\\n6. Ensure that AI systems are fair, meaning that they do not discriminate against individuals\",\n  '\\nThe proposal for a Regulation laying down harmonised rules on artificial intelligence (AI) aims to ensure a high level of safety and security of AI systems and to establish a horizontal and technology-neutral framework for AI applications. This will help to create a single market for AI and to ensure that AI systems are developed and deployed in a responsible manner. The proposal will also help to strengthen the competitiveness of the EU industry in the global AI market.\\nThe objectives of the proposal are:\\n1. To ensure that AI systems are safe and secure by establishing a risk-based framework for the development, placement on the market and use of AI systems.\\n2. To establish a horizontal and technology-neutral framework for AI applications that is applicable to all sectors and types of AI systems.\\n3. To ensure that AI systems are developed and deployed in a responsible manner by establishing requirements for transparency, robustness, security, accuracy, controllability and privacy protection.\\n4. To create a single market for AI by ensuring that AI systems are developed and deployed in a harmonised manner across the EU.\\n5. To strengthen the competitiveness of the EU industry in the global AI market by creating a level playing field for businesses and by promoting the'],\n 'labelling_model': 'gpt-3.5-turbo',\n 'labelling_prompt': [{'content': 'Your role is to evaluate text quality based on given criteria.',\n   'role': 'system'},\n  {'content': \"\\n# Instruction Following Assessment\\nEvaluate alignment between output and intent. Assess understanding of task goal and restrictions.\\n**Instruction Components**: Task Goal (intended outcome), Restrictions (text styles, formats, or designated methods, etc).\\n\\n**Scoring**: Rate outputs 1 to 5:\\n\\n1. **Irrelevant**: No alignment.\\n2. **Partial Focus**: Addresses one aspect poorly.\\n3. **Partial Compliance**:\\n\\t- (1) Meets goal or restrictions, neglecting other.\\n\\t- (2) Acknowledges both but slight deviations.\\n4. **Almost There**: Near alignment, minor deviations.\\n5. **Comprehensive Compliance**: Fully aligns, meets all requirements.\\n\\n---\\n\\n## Format\\n\\n### Input\\nInstruction: [Specify task goal and restrictions]\\n\\nTexts:\\n\\n&lt;text 1&gt; [Text 1]\\n&lt;text 2&gt; [Text 2]\\n\\n### Output\\n\\n#### Output for Text 1\\nRating: [Rating for text 1]\\nRationale: [Rationale for the rating in short sentences]\\n\\n#### Output for Text 2\\nRating: [Rating for text 2]\\nRationale: [Rationale for the rating in short sentences]\\n\\n---\\n\\n## Annotation\\n\\n### Input\\nInstruction: What are the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence?\\n\\nTexts:\\n\\n&lt;text 1&gt; \\nThe proposal for a Regulation laying down harmonised rules on artificial intelligence (AI) aims to ensure the trustworthy use of AI in the EU. It seeks to create a single market for AI applications and services, while ensuring that they are safe and respect fundamental rights. The proposal is part of the EU's broader strategy on AI, which aims to put the EU at the forefront of global AI development and deployment.\\nThe objectives of the proposal are to:\\n\\n1. Ensure that AI systems are designed, developed, and deployed in a way that respects fundamental rights and values, including human dignity, freedom, and privacy.\\n2. Ensure that AI systems are safe and secure, and do not pose unacceptable risks to people, property, or the environment.\\n3. Ensure that AI systems are robust, reliable, and accurate, and can be trusted to deliver the intended functionality.\\n4. Ensure that AI systems are traceable, meaning that it is possible to track how they work and how they make decisions.\\n5. Ensure that AI systems are transparent, meaning that it is possible to understand how they work and how they make decisions.\\n6. Ensure that AI systems are fair, meaning that they do not discriminate against individuals\\n&lt;text 2&gt; \\nThe proposal for a Regulation laying down harmonised rules on artificial intelligence (AI) aims to ensure a high level of safety and security of AI systems and to establish a horizontal and technology-neutral framework for AI applications. This will help to create a single market for AI and to ensure that AI systems are developed and deployed in a responsible manner. The proposal will also help to strengthen the competitiveness of the EU industry in the global AI market.\\nThe objectives of the proposal are:\\n1. To ensure that AI systems are safe and secure by establishing a risk-based framework for the development, placement on the market and use of AI systems.\\n2. To establish a horizontal and technology-neutral framework for AI applications that is applicable to all sectors and types of AI systems.\\n3. To ensure that AI systems are developed and deployed in a responsible manner by establishing requirements for transparency, robustness, security, accuracy, controllability and privacy protection.\\n4. To create a single market for AI by ensuring that AI systems are developed and deployed in a harmonised manner across the EU.\\n5. To strengthen the competitiveness of the EU industry in the global AI market by creating a level playing field for businesses and by promoting the\\n\\n### Output \",\n   'role': 'user'}],\n 'raw_labelling_response': '#### Output for Text 1\\nRating: 5\\nRationale: The text fully aligns with the task goal and restrictions. It clearly states the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence, including ensuring the trustworthy use of AI, creating a single market for AI applications and services, and ensuring safety, respect for fundamental rights, robustness, transparency, and fairness of AI systems.\\n\\n#### Output for Text 2\\nRating: 4\\nRationale: The text mostly aligns with the task goal and restrictions. It addresses the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence, including ensuring safety and security of AI systems, establishing a horizontal and technology-neutral framework, promoting responsible development and deployment of AI systems, creating a single market for AI, and strengthening the competitiveness of the EU industry in the global AI market. However, it does not explicitly mention the need to respect fundamental rights, accuracy of AI systems, and traceability of AI systems, which are mentioned in the task goal and restrictions.',\n 'rating': [5.0, 4.0],\n 'rationale': ['The text fully aligns with the task goal and restrictions. It clearly states the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence, including ensuring the trustworthy use of AI, creating a single market for AI applications and services, and ensuring safety, respect for fundamental rights, robustness, transparency, and fairness of AI systems.',\n  'The text mostly aligns with the task goal and restrictions. It addresses the reasons for and objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence, including ensuring safety and security of AI systems, establishing a horizontal and technology-neutral framework, promoting responsible development and deployment of AI systems, creating a single market for AI, and strengthening the competitiveness of the EU industry in the global AI market. However, it does not explicitly mention the need to respect fundamental rights, accuracy of AI systems, and traceability of AI systems, which are mentioned in the task goal and restrictions.']}</code>\n</pre> <p>If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the URL and API_KEY:</p> <pre><code>import argilla as rg\n\n# Replace api_url with the url to your HF Spaces URL if using Spaces\n# Replace api_key if you configured a custom API key\nrg.init(\n    api_url=\"http://localhost:6900\",\n    api_key=\"owner.apikey\",\n    workspace=\"admin\"\n)\n</code></pre> <p>Once our preference dataset has been correctly generated, the Argilla UI is the best tool at our disposal to visualize it and annotate it. As for the instruction dataset, we just have to convert it to an Argilla Feedback Dataset, and push it to Argilla.</p> <pre><code># Uploading the Preference Dataset\npreference_rg_dataset = preference_dataset.to_argilla()\n\n# Adding the context as a metadata property in the new Feedback dataset, as this\n# information will be useful later.\nfor record_feedback, record_huggingface in zip(\n    preference_rg_dataset, preference_dataset\n):\n    record_feedback.metadata[\"context\"] = record_huggingface[\"context\"]\n\npreference_rg_dataset.push_to_argilla(name=f\"notus_AI_preference\")\n</code></pre> <p>In the Argilla UI, we can see the input (an instruction), and the two generations that the LLM created out of it.</p> <p></p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#create-a-legal-preference-dataset","title":"\u2696\ufe0f Create a legal preference dataset","text":"<p>In this tutorial, you will learn how to use the Notus model on Inference Endpoints to create a legal preference dataset based on RAG instructions from the European AI Act. A full end-to-end example of how to use distilabel to leverage LLMs!</p> <p>distilabel is an AI Feedback (AIF) framework that can generate and label datasets using LLMs, and can be used for many different use cases. Implemented with robustness, efficiency and scalability in mind, it allows anyone to build their synthetic datasets that can be used in many different scenarios. This tutorial shows an end-to-end example in which we will create a model expert in the new AI Act, to which we can make different types of questions and requests.</p> <p>The LLM model that we will fine-tune for this is Notus 7B, a fine-tuned version of Zephyr 7B that uses Direct Preference Optimization (DPO) and AIF techniques to outperform its foundation model in several benchmarks, and is completely open-source.</p> <p>This tutorial includes the following steps:</p> <ul> <li>Defining a custom generating task for a <code>distilabel</code> pipeline.</li> <li>Creating a RAG pipeline using Haystack for the EU AI Act.</li> <li>Generating an instruction dataset with <code>SelfInstructTask</code>.</li> <li>Generating a preference dataset using an <code>UltraFeedback</code> text quality task.</li> </ul> <p>You can use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#introduction","title":"Introduction","text":"<p>Let's start by installing the required dependencies to run distilabel and the rest of the packages used in the tutorial; most notably, Haystack. Install also Argilla for a better visualization and curation of the results.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#import-dependencies","title":"Import dependencies","text":"<p>The main dependencies for this tutorial are distilabel for creating the synthetic datasets and Argilla for visualizing and annotating these datasets, and also for fine-tuning our model. The package Haystack is used to creates batches from the original PDF document we want to create our datasets from.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#environment-variables","title":"Environment variables","text":"<p>Additionally, we need to provide our HuggingFace and OpenAI accest token. To later instatiate an <code>InferenceEndpointsLLM</code> object, we need to pass as parameters the HF Inference Endpoint name and the HF namespace. One very convenient way to do so is also through environment variables.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#setting-up-an-inference-endpoint-with-notus","title":"Setting up an inference endpoint with Notus","text":"<p>Inference endpoints are a solution, managed by Hugging Face, to easily deploy any Transformer-like model. They are built from models on the Hugging Face Hub. Inference endpoints are really handy for making inference on LLMs without the hastle of trying to run the models locally. In this tutorial, we will use inference endpoints to generate text using our Notus model, as part of the <code>distilabel</code> workflow. The endpoint of choice has a Notus 7B instance running.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#defining-a-custom-generating-task-for-a-distilabel-pipeline","title":"Defining a custom generating task for a distilabel pipeline","text":"<p>To kickstart this tutorial, let's see how to set up and endpoint for our Notus model. It's not part of the end-to-end example we'll see later, but an example of how to connect to a Hugging Face endpoint and a test of the <code>distilabel</code> pipeline.</p> <p>Let's dive into this quick example of how to use an inference endpoint. We have prepared an easy <code>TextGenerationTask</code> to ask question to the model, in a very similar way as we talk with the LLMs using chatbots. First, we define a class for the question-answering task, with functions showing <code>distilabel</code> how the model should generate the prompts, parse the input and the output, etc.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#creating-a-rag-pipeline-using-haystack-for-the-european-ai-act","title":"Creating a RAG pipeline using Haystack for the European AI Act","text":"<p>For this end-to-end example, we would like to create an expert model capable of answering question and filling up information about the new AI Act promoted by the European Union, which is the first regulation on artificial intelligence. As part of its digital strategy, the EU wants to regulate artificial AI to ensure better conditions for the development and use of this innovative technology. This act is a regulatory framework for AI, with different risk levels meaning more or less regulation. They are the world's first rules on AI.</p> <p>This RAG pipeline that we want to create downloads the PDF file, converts it to plain text and preprocess it, creating batches that we can feed <code>distilabel</code> to start creating instructions from it. Let's see this first part of the pipeline and get the input data. Note that this RAG part of the pipeline is not based on an active pipeline based queries or semantic properties, but a more brute-force approach in which we download the PDF and preprocess its contents.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#downloading-the-ai-act-pdf","title":"Downloading the AI Act PDF","text":"<p>Firstly, we need to download the PDF document itself. We'll place it in our working directory, if it's not there already.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#generating-instructions-with-selfinstructtask","title":"Generating instructions with SelfInstructTask","text":"<p>With out Inference Endpoint up and running, we should be able to generate instructions with distilabel. These instructions, made by the LLM through our endpoint, will form an instruction dataset, with instructions created from the data we just extracted.</p> <p>For this example, we are using a subset of 50 batches generated in the section above, to be gentle on performance.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#pushing-the-instruction-dataset-to-argilla-to-visualize-and-annotate","title":"Pushing the instruction dataset to Argilla to visualize and annotate.","text":"<p>Let's take a quick look at the instructions generated by <code>SelfInstructTask</code>.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#generate-a-preference-dataset-using-an-ultrafeedback-text-quality-task","title":"Generate a Preference Dataset using an Ultrafeedback text quality task.","text":"<p>Once we have our instruction dataset, we are going to create a preference dataset through the UltraFeedback text quality task. This is a type of task used in NLP used to evaluate the quality of text generated; our goal is to provide detailed feedback on the quality of the generated text, beyond a binary label.</p> <p>Our <code>pipeline()</code> method allows us to create a <code>Pipeline</code> instance with the provided LLMs for a given task, which is useful whenever you want to use a pre-defined or custom <code>Pipeline</code> for a given task. We will specify our task and subtask, the generator we want to use (in this case, one based in a Text Generator Task) and our OpenAI API key.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#human-feedback-with-argilla","title":"Human Feedback with Argilla","text":"<p>You can use the AI Feedback created by distilabel directly but we have seen that enhancing it with human feedback will improve the quality of your LLM. We provide a <code>to_argilla</code> method which creates a dataset for Argilla along with out-of-the-box tailored metadata filters and semantic search to allow you to provide human feedback as quickly and engaging as possible. You can check the Argilla docs to get it up and running.</p>"},{"location":"tutorials/pipeline-notus-instructions-preferences-legal/#conclusions","title":"Conclusions","text":"<p>To conclude, we have gone through an end-to-end example of distilabel. We've set up an Inference Endpoint, defined a distilabel pipeline that extracts information from a PDF, created and manually reviewed the instruction and preference dataset created from that input. The final preference dataset is perfect for fine-tuning, and you can easily do this using the ArgillaTrainer from Argilla. Have a look at these resources if you want to go further:</p> <ul> <li>Train a Model with ArgillaTrainer</li> <li>\u24c2\ufe0f Finetuning LLMs as chat assistants: Supervised Finetuning on Mistral 7B</li> <li>\ud83c\udf20 Improving RAG by Optimizing Retrieval and Reranking Models</li> </ul>"}]}