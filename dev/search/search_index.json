{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"Synthesize data for AI and add feedback on the fly! <p>Distilabel is the framework for synthetic data and AI feedback for AI engineers that require high-quality outputs, full data ownership, and overall efficiency.</p> <p>If you just want to get started, we recommend you check the documentation. Curious, and want to know more? Keep reading!</p>"},{"location":"#why-use-distilabel","title":"Why use Distilabel?","text":"<p>Whether you are working on a predictive model that computes semantic similarity or the next generative model that is going to beat the LLM benchmarks. Our framework ensures that the hard data work pays off. Distilabel is the missing piece that helps you synthesize data and provide AI feedback.</p>"},{"location":"#improve-your-ai-output-quality-through-data-quality","title":"Improve your AI output quality through data quality","text":"<p>Compute is expensive and output quality is important. We help you focus on data quality, which tackles the root cause of both of these problems at once. Distilabel helps you to synthesize and judge data to let you spend your valuable time on achieveing and keeping high-quality standards for your data.</p>"},{"location":"#take-control-of-your-data-and-models","title":"Take control of your data and models","text":"<p>Ownership of data for fine-tuning your own LLMs is not easy but Distilabel can help you to get started. We integrate AI feedback from any LLM provider out there using one unified API.</p>"},{"location":"#improve-efficiency-by-quickly-iterating-on-the-right-research-and-llms","title":"Improve efficiency by quickly iterating on the right research and LLMs","text":"<p>Synthesize and judge data with latest research papers while ensuring flexibility, scalability and fault tolerance. So you can focus on improving your data and training your models.</p>"},{"location":"#community","title":"\ud83c\udfd8\ufe0f Community","text":"<p>We are an open-source community-driven project and we love to hear from you. Here are some ways to get involved:</p> <ul> <li> <p>Community Meetup: listen in or present during one of our bi-weekly events.</p> </li> <li> <p>Slack: get direct support from the community.</p> </li> <li> <p>Roadmap: plans change but we love to discuss those with our community so feel encouraged to participate.</p> </li> </ul>"},{"location":"#what-do-people-build-with-distilabel","title":"What do people build with Distilabel?","text":"<p>Distilabel is a tool that can be used to synthesize data and provide AI feedback. Our community uses Distilabel to create amazing datasets and models, and we love contributions to open-source ourselves too.</p> <ul> <li>The 1M OpenHermesPreference is a dataset of ~1 million AI preferences derived from teknium/OpenHermes-2.5. It shows how we can use Distilabel to synthesize data on an immense scale.</li> <li>Our distilabeled Intel Orca DPO dataset and the improved OpenHermes model,, show how we improve model performance by filtering out 50% of the original dataset through AI feedback.</li> <li>The haiku DPO data outlines how anyone can create a dataset for a specific task and the latest research papers to improve the quality of the dataset.</li> </ul>"},{"location":"#installation","title":"\ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb Installation","text":"<pre><code>pip install distilabel --upgrade\n</code></pre> <p>Requires Python 3.8+</p> <p>In addition, the following extras are available:</p> <ul> <li><code>anthropic</code>: for using models available in Anthropic API via the <code>AnthropicLLM</code> integration.</li> <li><code>cohere</code>: for using models available in Cohere via the <code>CohereLLM</code> integration.</li> <li><code>argilla</code>: for exporting the generated datasets to Argilla.</li> <li><code>groq</code>: for using models available in Groq using <code>groq</code> Python client via the <code>GroqLLM</code> integration.</li> <li><code>hf-inference-endpoints</code>: for using the Hugging Face Inference Endpoints via the <code>InferenceEndpointsLLM</code> integration.</li> <li><code>hf-transformers</code>: for using models available in transformers package via the <code>TransformersLLM</code> integration.</li> <li><code>litellm</code>: for using <code>LiteLLM</code> to call any LLM using OpenAI format via the <code>LiteLLM</code> integration.</li> <li><code>llama-cpp</code>: for using llama-cpp-python Python bindings for <code>llama.cpp</code> via the <code>LlamaCppLLM</code> integration.</li> <li><code>mistralai</code>: for using models available in Mistral AI API via the <code>MistralAILLM</code> integration.</li> <li><code>ollama</code>: for using Ollama and their available models via <code>OllamaLLM</code> integration.</li> <li><code>openai</code>: for using OpenAI API models via the <code>OpenAILLM</code> integration, or the rest of the integrations based on OpenAI and relying on its client as <code>AnyscaleLLM</code>, <code>AzureOpenAILLM</code>, and <code>TogetherLLM</code>.</li> <li><code>vertexai</code>: for using Google Vertex AI proprietary models via the <code>VertexAILLM</code> integration.</li> <li><code>vllm</code>: for using vllm serving engine via the <code>vLLM</code> integration.</li> </ul>"},{"location":"#example","title":"Example","text":"<p>To run the following example you must install <code>distilabel</code> with both <code>openai</code> extra:</p> <pre><code>pip install \"distilabel[openai]\" --upgrade\n</code></pre> <p>Then run:</p> <pre><code>from distilabel.llms import OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\nwith Pipeline(\n    name=\"simple-text-generation-pipeline\",\n    description=\"A simple text generation pipeline\",\n) as pipeline:\n    load_dataset = LoadHubDataset(output_mappings={\"prompt\": \"instruction\"})\n\n    generate_with_openai = TextGeneration(llm=OpenAILLM(model=\"gpt-3.5-turbo\"))\n\n    load_dataset.connect(generate_with_openai)\n\nif __name__ == \"__main__\":\n    distiset = pipeline.run(\n        parameters={\n            load_dataset.name: {\n                \"repo_id\": \"distilabel-internal-testing/instruction-dataset-mini\",\n                \"split\": \"test\",\n            },\n            generate_with_openai.name: {\n                \"llm\": {\n                    \"generation_kwargs\": {\n                        \"temperature\": 0.7,\n                        \"max_new_tokens\": 512,\n                    }\n                }\n            },\n        },\n    )\n</code></pre>"},{"location":"#badges","title":"Badges","text":"<p>If you build something cool with <code>distilabel</code> consider adding one of these badges to your dataset or model card.</p> <pre><code>[&lt;img src=\"https://raw.githubusercontent.com/argilla-io/distilabel/main/docs/assets/distilabel-badge-light.png\" alt=\"Built with Distilabel\" width=\"200\" height=\"32\"/&gt;](https://github.com/argilla-io/distilabel)\n</code></pre> <p></p> <pre><code>[&lt;img src=\"https://raw.githubusercontent.com/argilla-io/distilabel/main/docs/assets/distilabel-badge-dark.png\" alt=\"Built with Distilabel\" width=\"200\" height=\"32\"/&gt;](https://github.com/argilla-io/distilabel)\n</code></pre> <p></p>"},{"location":"#contribute","title":"Contribute","text":"<p>To directly contribute with <code>distilabel</code>, check our good first issues or open a new one.</p>"},{"location":"api/cli/","title":"Command Line Interface (CLI)","text":"<p>This section contains the API reference for the CLI. For more information on how to use the CLI, see Tutorial - CLI.</p>"},{"location":"api/cli/#utility-functions-for-the-distilabel-pipeline-sub-commands","title":"Utility functions for the <code>distilabel pipeline</code> sub-commands","text":"<p>Here are some utility functions to help working with the pipelines in the console.</p>"},{"location":"api/cli/#distilabel.cli.pipeline.utils.display_pipeline_information","title":"<code>display_pipeline_information(pipeline)</code>","text":"<p>Displays the pipeline information to the console.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BasePipeline</code> <p>The pipeline.</p> required Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def display_pipeline_information(pipeline: \"BasePipeline\") -&gt; None:\n    \"\"\"Displays the pipeline information to the console.\n\n    Args:\n        pipeline: The pipeline.\n    \"\"\"\n    from rich.console import Console\n\n    Console().print(_build_pipeline_panel(pipeline))\n</code></pre>"},{"location":"api/cli/#distilabel.cli.pipeline.utils.get_config_from_url","title":"<code>get_config_from_url(url)</code>","text":"<p>Loads the pipeline configuration from a URL pointing to a JSON or YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL pointing to the pipeline configuration file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The pipeline configuration as a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported.</p> Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def get_config_from_url(url: str) -&gt; Dict[str, Any]:\n    \"\"\"Loads the pipeline configuration from a URL pointing to a JSON or YAML file.\n\n    Args:\n        url: The URL pointing to the pipeline configuration file.\n\n    Returns:\n        The pipeline configuration as a dictionary.\n\n    Raises:\n        ValueError: If the file format is not supported.\n    \"\"\"\n    if not url.endswith((\".json\", \".yaml\", \".yml\")):\n        raise ValueError(\n            f\"Unsupported file format for '{url}'. Only JSON and YAML are supported\"\n        )\n    if \"huggingface.co\" in url and \"HF_TOKEN\" in os.environ:\n        headers = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n    else:\n        headers = None\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n\n    if url.endswith((\".yaml\", \".yml\")):\n        content = response.content.decode(\"utf-8\")\n        return yaml.safe_load(content)\n\n    return response.json()\n</code></pre>"},{"location":"api/cli/#distilabel.cli.pipeline.utils.get_pipeline","title":"<code>get_pipeline(config)</code>","text":"<p>Get a pipeline from a configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>The path or URL to the pipeline configuration file.</p> required <p>Returns:</p> Type Description <code>BasePipeline</code> <p>The pipeline.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported.</p> <code>FileNotFoundError</code> <p>If the configuration file does not exist.</p> Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def get_pipeline(config: str) -&gt; \"BasePipeline\":\n    \"\"\"Get a pipeline from a configuration file.\n\n    Args:\n        config: The path or URL to the pipeline configuration file.\n\n    Returns:\n        The pipeline.\n\n    Raises:\n        ValueError: If the file format is not supported.\n        FileNotFoundError: If the configuration file does not exist.\n    \"\"\"\n    if valid_http_url(config):\n        return Pipeline.from_dict(get_config_from_url(config))\n\n    if Path(config).is_file():\n        return Pipeline.from_file(config)\n\n    raise FileNotFoundError(f\"Config file '{config}' does not exist.\")\n</code></pre>"},{"location":"api/cli/#distilabel.cli.pipeline.utils.parse_runtime_parameters","title":"<code>parse_runtime_parameters(params)</code>","text":"<p>Parses the runtime parameters from the CLI format to the format expected by the <code>Pipeline.run</code> method. The CLI format is a list of tuples, where the first element is a list of keys and the second element is the value.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>List[Tuple[List[str], str]]</code> <p>A list of tuples, where the first element is a list of keys and the second element is the value.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary with the runtime parameters in the format expected by the</p> <code>Dict[str, Dict[str, Any]]</code> <p><code>Pipeline.run</code> method.</p> Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def parse_runtime_parameters(\n    params: List[Tuple[List[str], str]],\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Parses the runtime parameters from the CLI format to the format expected by the\n    `Pipeline.run` method. The CLI format is a list of tuples, where the first element is\n    a list of keys and the second element is the value.\n\n    Args:\n        params: A list of tuples, where the first element is a list of keys and the\n            second element is the value.\n\n    Returns:\n        A dictionary with the runtime parameters in the format expected by the\n        `Pipeline.run` method.\n    \"\"\"\n    runtime_params = {}\n    for keys, value in params:\n        current = runtime_params\n        for i, key in enumerate(keys):\n            if i == len(keys) - 1:\n                current[key] = value\n            else:\n                current = current.setdefault(key, {})\n    return runtime_params\n</code></pre>"},{"location":"api/cli/#distilabel.cli.pipeline.utils.valid_http_url","title":"<code>valid_http_url(url)</code>","text":"<p>Check if the URL is a valid HTTP URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code>, if the URL is a valid HTTP URL. <code>False</code>, otherwise.</p> Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def valid_http_url(url: str) -&gt; bool:\n    \"\"\"Check if the URL is a valid HTTP URL.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        `True`, if the URL is a valid HTTP URL. `False`, otherwise.\n    \"\"\"\n    try:\n        TypeAdapter(HttpUrl).validate_python(url)  # type: ignore\n    except ValidationError:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/llm/","title":"LLM","text":"<p>This section contains the API reference for the <code>distilabel</code> LLMs, both for the <code>LLM</code> synchronous implementation, and for the <code>AsyncLLM</code> asynchronous one.</p> <p>For more information and examples on how to use existing LLMs or create custom ones, please refer to Tutorial - LLM.</p>"},{"location":"api/llm/#distilabel.llms.base.AsyncLLM","title":"<code>AsyncLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Abstract class for asynchronous LLMs, so as to benefit from the async capabilities of each LLM implementation. This class is meant to be subclassed by each LLM, and the method <code>agenerate</code> needs to be implemented to provide the asynchronous generation of responses.</p> <p>Attributes:</p> Name Type Description <code>_event_loop</code> <code>AbstractEventLoop</code> <p>the event loop to be used for the asynchronous generation of responses.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>class AsyncLLM(LLM):\n    \"\"\"Abstract class for asynchronous LLMs, so as to benefit from the async capabilities\n    of each LLM implementation. This class is meant to be subclassed by each LLM, and the\n    method `agenerate` needs to be implemented to provide the asynchronous generation of\n    responses.\n\n    Attributes:\n        _event_loop: the event loop to be used for the asynchronous generation of responses.\n    \"\"\"\n\n    _event_loop: \"asyncio.AbstractEventLoop\" = PrivateAttr(default=None)\n    _new_event_loop: bool = PrivateAttr(default=False)\n\n    @property\n    def generate_parameters(self) -&gt; List[inspect.Parameter]:\n        \"\"\"Returns the parameters of the `agenerate` method.\n\n        Returns:\n            A list containing the parameters of the `agenerate` method.\n        \"\"\"\n        return list(inspect.signature(self.agenerate).parameters.values())\n\n    @cached_property\n    def generate_parsed_docstring(self) -&gt; \"Docstring\":\n        \"\"\"Returns the parsed docstring of the `agenerate` method.\n\n        Returns:\n            The parsed docstring of the `agenerate` method.\n        \"\"\"\n        return parse_google_docstring(self.agenerate)\n\n    @property\n    def event_loop(self) -&gt; \"asyncio.AbstractEventLoop\":\n        if self._event_loop is None:\n            try:\n                self._event_loop = asyncio.get_running_loop()\n                if self._event_loop.is_closed():\n                    self._event_loop = asyncio.new_event_loop()  # type: ignore\n                    self._new_event_loop = True\n            except RuntimeError:\n                self._event_loop = asyncio.new_event_loop()\n                self._new_event_loop = True\n        asyncio.set_event_loop(self._event_loop)\n        return self._event_loop\n\n    @abstractmethod\n    async def agenerate(\n        self, input: \"FormattedInput\", num_generations: int = 1, **kwargs: Any\n    ) -&gt; List[Union[str, None]]:\n        \"\"\"Method to generate a `num_generations` responses for a given input asynchronously,\n        and executed concurrently in `generate` method.\n        \"\"\"\n        pass\n\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"FormattedInput\"], **kwargs: Any\n        ) -&gt; List[List[Union[str, None]]]:\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(\n                    self.agenerate(\n                        input=input, num_generations=num_generations, **kwargs\n                    )\n                )\n                for input in inputs\n            ]\n            return await asyncio.gather(*tasks)\n\n        return self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n\n    def __del__(self) -&gt; None:\n        \"\"\"Closes the event loop when the object is deleted.\"\"\"\n        if sys.meta_path is None:\n            return\n\n        if self._new_event_loop:\n            if self._event_loop.is_running():\n                self._event_loop.stop()\n            self._event_loop.close()\n\n    @staticmethod\n    def _prepare_structured_output(\n        structured_output: \"InstructorStructuredOutputType\",\n        client: Any = None,\n        framework: Optional[str] = None,\n    ) -&gt; Dict[str, Union[str, Any]]:\n        \"\"\"Wraps the client and updates the schema to work store it internally as a json schema.\n\n        Args:\n            structured_output: The configuration dict to prepare the structured output.\n            client: The client to wrap to generate structured output. Implemented to work\n                with `instructor`.\n            framework: The name of the framework.\n\n        Returns:\n            A dictionary containing the wrapped client and the schema to update the structured_output\n            variable in case it is a pydantic model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.instructor import (\n            prepare_instructor,\n        )\n\n        result = {}\n        client = prepare_instructor(\n            client,\n            mode=structured_output.get(\"mode\"),\n            framework=framework,\n        )\n        result[\"client\"] = client\n\n        schema = structured_output.get(\"schema\")\n        if not schema:\n            raise ValueError(\n                f\"The `structured_output` argument must contain a schema: {structured_output}\"\n            )\n        if issubclass(schema, BaseModel):\n            # We want a json schema for the serialization, but instructor wants a pydantic BaseModel.\n            structured_output[\"schema\"] = schema.model_json_schema()\n            result[\"structured_output\"] = structured_output\n\n        return result\n\n    @staticmethod\n    def _prepare_kwargs(\n        arguments: Dict[str, Any], structured_output: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Helper method to update the kwargs with the structured output configuration,\n        used in case they are defined.\n\n        Args:\n            arguments: The arguments that would be passed to the LLM as **kwargs.\n                to update with the structured output configuration.\n            structured_outputs: The structured output configuration to update the arguments.\n\n        Returns:\n            kwargs updated with the special arguments used by `instructor`.\n        \"\"\"\n        # We can deal with json schema or BaseModel, but we need to convert it to a BaseModel\n        # for the Instructor client.\n        schema = structured_output.get(\"schema\")\n        if not issubclass(schema, BaseModel):\n            from distilabel.steps.tasks.structured_outputs.utils import (\n                json_schema_to_model,\n            )\n\n            try:\n                schema = json_schema_to_model(schema)\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to convert the schema to a pydantic model, the model is too complex currently: {e}\"\n                ) from e\n\n        arguments.update(\n            **{\n                \"response_model\": schema,\n                \"max_retries\": structured_output.get(\"max_retries\", 1),\n            },\n        )\n        return arguments\n</code></pre>"},{"location":"api/llm/#distilabel.llms.base.AsyncLLM.generate_parameters","title":"<code>generate_parameters: List[inspect.Parameter]</code>  <code>property</code>","text":"<p>Returns the parameters of the <code>agenerate</code> method.</p> <p>Returns:</p> Type Description <code>List[Parameter]</code> <p>A list containing the parameters of the <code>agenerate</code> method.</p>"},{"location":"api/llm/#distilabel.llms.base.AsyncLLM.generate_parsed_docstring","title":"<code>generate_parsed_docstring: Docstring</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the parsed docstring of the <code>agenerate</code> method.</p> <p>Returns:</p> Type Description <code>Docstring</code> <p>The parsed docstring of the <code>agenerate</code> method.</p>"},{"location":"api/llm/#distilabel.llms.base.AsyncLLM.__del__","title":"<code>__del__()</code>","text":"<p>Closes the event loop when the object is deleted.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Closes the event loop when the object is deleted.\"\"\"\n    if sys.meta_path is None:\n        return\n\n    if self._new_event_loop:\n        if self._event_loop.is_running():\n            self._event_loop.stop()\n        self._event_loop.close()\n</code></pre>"},{"location":"api/llm/#distilabel.llms.base.AsyncLLM.agenerate","title":"<code>agenerate(input, num_generations=1, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Method to generate a <code>num_generations</code> responses for a given input asynchronously, and executed concurrently in <code>generate</code> method.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>@abstractmethod\nasync def agenerate(\n    self, input: \"FormattedInput\", num_generations: int = 1, **kwargs: Any\n) -&gt; List[Union[str, None]]:\n    \"\"\"Method to generate a `num_generations` responses for a given input asynchronously,\n    and executed concurrently in `generate` method.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llm/#distilabel.llms.base.AsyncLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"FormattedInput\"], **kwargs: Any\n    ) -&gt; List[List[Union[str, None]]]:\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(\n                self.agenerate(\n                    input=input, num_generations=num_generations, **kwargs\n                )\n            )\n            for input in inputs\n        ]\n        return await asyncio.gather(*tasks)\n\n    return self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n</code></pre>"},{"location":"api/llm/#distilabel.llms.base.LLM","title":"<code>LLM</code>","text":"<p>               Bases: <code>RuntimeParametersMixin</code>, <code>BaseModel</code>, <code>_Serializable</code>, <code>ABC</code></p> <p>Base class for <code>LLM</code>s to be used in <code>distilabel</code> framework.</p> <p>To implement an <code>LLM</code> subclass, you need to subclass this class and implement:     - <code>load</code> method to load the <code>LLM</code> if needed. Don't forget to call <code>super().load()</code>,         so the <code>_logger</code> attribute is initialized.     - <code>model_name</code> property to return the model name used for the LLM.     - <code>generate</code> method to generate <code>num_generations</code> per input in <code>inputs</code>.</p> <p>Attributes:</p> Name Type Description <code>generation_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>the kwargs to be propagated to either <code>generate</code> or <code>agenerate</code> methods within each <code>LLM</code>.</p> <code>structured_output</code> <code>Optional[Any]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>_logger</code> <code>Union[Logger, None]</code> <p>the logger to be used for the <code>LLM</code>. It will be initialized when the <code>load</code> method is called.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>class LLM(RuntimeParametersMixin, BaseModel, _Serializable, ABC):\n    \"\"\"Base class for `LLM`s to be used in `distilabel` framework.\n\n    To implement an `LLM` subclass, you need to subclass this class and implement:\n        - `load` method to load the `LLM` if needed. Don't forget to call `super().load()`,\n            so the `_logger` attribute is initialized.\n        - `model_name` property to return the model name used for the LLM.\n        - `generate` method to generate `num_generations` per input in `inputs`.\n\n    Attributes:\n        generation_kwargs: the kwargs to be propagated to either `generate` or `agenerate`\n            methods within each `LLM`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        _logger: the logger to be used for the `LLM`. It will be initialized when the `load`\n            method is called.\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        protected_namespaces=(),\n        validate_default=True,\n        validate_assignment=True,\n        extra=\"forbid\",\n    )\n\n    generation_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"The kwargs to be propagated to either `generate` or `agenerate`\"\n        \" methods within each `LLM`.\",\n    )\n    structured_output: Optional[Any] = None\n\n    _logger: Union[logging.Logger, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Method to be called to initialize the `LLM`, its logger and optionally the structured output generator.\"\"\"\n        self._logger = logging.getLogger(f\"distilabel.llm.{self.model_name}\")\n\n    @property\n    @abstractmethod\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        pass\n\n    @abstractmethod\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Abstract method to be implemented by each LLM to generate `num_generations`\n        per input in `inputs`.\n\n        Args:\n            inputs: the list of inputs to generate responses for which follows OpenAI's\n                API format:\n\n                ```python\n                [\n                    {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n                    {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n                    {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n                    {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n                ]\n                ```\n            num_generations: the number of generations to generate per input.\n            **kwargs: the additional kwargs to be used for the generation.\n        \"\"\"\n        pass\n\n    @property\n    def generate_parameters(self) -&gt; List[\"inspect.Parameter\"]:\n        \"\"\"Returns the parameters of the `generate` method.\n\n        Returns:\n            A list containing the parameters of the `generate` method.\n        \"\"\"\n        return list(inspect.signature(self.generate).parameters.values())\n\n    @property\n    def runtime_parameters_names(self) -&gt; \"RuntimeParametersNames\":\n        \"\"\"Returns the runtime parameters of the `LLM`, which are combination of the\n        attributes of the `LLM` type hinted with `RuntimeParameter` and the parameters\n        of the `generate` method that are not `input` and `num_generations`.\n\n        Returns:\n            A dictionary with the name of the runtime parameters as keys and a boolean\n            indicating if the parameter is optional or not.\n        \"\"\"\n        runtime_parameters = super().runtime_parameters_names\n        runtime_parameters[\"generation_kwargs\"] = {}\n\n        # runtime parameters from the `generate` method\n        for param in self.generate_parameters:\n            if param.name in [\"input\", \"inputs\", \"num_generations\"]:\n                continue\n            is_optional = param.default != inspect.Parameter.empty\n            runtime_parameters[\"generation_kwargs\"][param.name] = is_optional\n\n        return runtime_parameters\n\n    def get_runtime_parameters_info(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Gets the information of the runtime parameters of the `LLM` such as the name\n        and the description. This function is meant to include the information of the runtime\n        parameters in the serialized data of the `LLM`.\n\n        Returns:\n            A list containing the information for each runtime parameter of the `LLM`.\n        \"\"\"\n        runtime_parameters_info = super().get_runtime_parameters_info()\n\n        generation_kwargs_info = next(\n            runtime_parameter_info\n            for runtime_parameter_info in runtime_parameters_info\n            if runtime_parameter_info[\"name\"] == \"generation_kwargs\"\n        )\n\n        generate_docstring_args = self.generate_parsed_docstring[\"args\"]\n\n        generation_kwargs_info[\"keys\"] = []\n        for key, value in generation_kwargs_info[\"optional\"].items():\n            info = {\"name\": key, \"optional\": value}\n            if description := generate_docstring_args.get(key):\n                info[\"description\"] = description\n            generation_kwargs_info[\"keys\"].append(info)\n\n        generation_kwargs_info.pop(\"optional\")\n\n        return runtime_parameters_info\n\n    @cached_property\n    def generate_parsed_docstring(self) -&gt; \"Docstring\":\n        \"\"\"Returns the parsed docstring of the `generate` method.\n\n        Returns:\n            The parsed docstring of the `generate` method.\n        \"\"\"\n        return parse_google_docstring(self.generate)\n\n    def get_last_hidden_states(\n        self, inputs: List[\"StandardInput\"]\n    ) -&gt; List[\"HiddenState\"]:\n        \"\"\"Method to get the last hidden states of the model for a list of inputs.\n\n        Args:\n            inputs: the list of inputs to get the last hidden states from.\n\n        Returns:\n            A list containing the last hidden state for each sequence using a NumPy array\n                with shape [num_tokens, hidden_size].\n        \"\"\"\n        raise NotImplementedError(\n            f\"Method `get_last_hidden_states` is not implemented for `{self.__class__.__name__}`\"\n        )\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Any, None]:\n        \"\"\"Method in charge of preparing the structured output generator.\n\n        By default will raise a `NotImplementedError`, subclasses that allow it must override this\n        method with the implementation.\n\n        Args:\n            structured_output: the config to prepare the guided generation.\n\n        Returns:\n            The structure to be used for the guided generation.\n        \"\"\"\n        raise NotImplementedError(\n            f\"Guided generation is not implemented for `{type(self).__name__}`\"\n        )\n</code></pre>"},{"location":"api/llm/#distilabel.llms.base.LLM.generate_parameters","title":"<code>generate_parameters: List[inspect.Parameter]</code>  <code>property</code>","text":"<p>Returns the parameters of the <code>generate</code> method.</p> <p>Returns:</p> Type Description <code>List[Parameter]</code> <p>A list containing the parameters of the <code>generate</code> method.</p>"},{"location":"api/llm/#distilabel.llms.base.LLM.generate_parsed_docstring","title":"<code>generate_parsed_docstring: Docstring</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the parsed docstring of the <code>generate</code> method.</p> <p>Returns:</p> Type Description <code>Docstring</code> <p>The parsed docstring of the <code>generate</code> method.</p>"},{"location":"api/llm/#distilabel.llms.base.LLM.model_name","title":"<code>model_name: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/#distilabel.llms.base.LLM.runtime_parameters_names","title":"<code>runtime_parameters_names: RuntimeParametersNames</code>  <code>property</code>","text":"<p>Returns the runtime parameters of the <code>LLM</code>, which are combination of the attributes of the <code>LLM</code> type hinted with <code>RuntimeParameter</code> and the parameters of the <code>generate</code> method that are not <code>input</code> and <code>num_generations</code>.</p> <p>Returns:</p> Type Description <code>RuntimeParametersNames</code> <p>A dictionary with the name of the runtime parameters as keys and a boolean</p> <code>RuntimeParametersNames</code> <p>indicating if the parameter is optional or not.</p>"},{"location":"api/llm/#distilabel.llms.base.LLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be implemented by each LLM to generate <code>num_generations</code> per input in <code>inputs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[FormattedInput]</code> <p>the list of inputs to generate responses for which follows OpenAI's API format:</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n    {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n    {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n    {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n]\n</code></pre> required <code>num_generations</code> <code>int</code> <p>the number of generations to generate per input.</p> <code>1</code> <code>**kwargs</code> <code>Any</code> <p>the additional kwargs to be used for the generation.</p> <code>{}</code> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Abstract method to be implemented by each LLM to generate `num_generations`\n    per input in `inputs`.\n\n    Args:\n        inputs: the list of inputs to generate responses for which follows OpenAI's\n            API format:\n\n            ```python\n            [\n                {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n                {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n                {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n                {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n            ]\n            ```\n        num_generations: the number of generations to generate per input.\n        **kwargs: the additional kwargs to be used for the generation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llm/#distilabel.llms.base.LLM.get_last_hidden_states","title":"<code>get_last_hidden_states(inputs)</code>","text":"<p>Method to get the last hidden states of the model for a list of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>the list of inputs to get the last hidden states from.</p> required <p>Returns:</p> Type Description <code>List[HiddenState]</code> <p>A list containing the last hidden state for each sequence using a NumPy array with shape [num_tokens, hidden_size].</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def get_last_hidden_states(\n    self, inputs: List[\"StandardInput\"]\n) -&gt; List[\"HiddenState\"]:\n    \"\"\"Method to get the last hidden states of the model for a list of inputs.\n\n    Args:\n        inputs: the list of inputs to get the last hidden states from.\n\n    Returns:\n        A list containing the last hidden state for each sequence using a NumPy array\n            with shape [num_tokens, hidden_size].\n    \"\"\"\n    raise NotImplementedError(\n        f\"Method `get_last_hidden_states` is not implemented for `{self.__class__.__name__}`\"\n    )\n</code></pre>"},{"location":"api/llm/#distilabel.llms.base.LLM.get_runtime_parameters_info","title":"<code>get_runtime_parameters_info()</code>","text":"<p>Gets the information of the runtime parameters of the <code>LLM</code> such as the name and the description. This function is meant to include the information of the runtime parameters in the serialized data of the <code>LLM</code>.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list containing the information for each runtime parameter of the <code>LLM</code>.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def get_runtime_parameters_info(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Gets the information of the runtime parameters of the `LLM` such as the name\n    and the description. This function is meant to include the information of the runtime\n    parameters in the serialized data of the `LLM`.\n\n    Returns:\n        A list containing the information for each runtime parameter of the `LLM`.\n    \"\"\"\n    runtime_parameters_info = super().get_runtime_parameters_info()\n\n    generation_kwargs_info = next(\n        runtime_parameter_info\n        for runtime_parameter_info in runtime_parameters_info\n        if runtime_parameter_info[\"name\"] == \"generation_kwargs\"\n    )\n\n    generate_docstring_args = self.generate_parsed_docstring[\"args\"]\n\n    generation_kwargs_info[\"keys\"] = []\n    for key, value in generation_kwargs_info[\"optional\"].items():\n        info = {\"name\": key, \"optional\": value}\n        if description := generate_docstring_args.get(key):\n            info[\"description\"] = description\n        generation_kwargs_info[\"keys\"].append(info)\n\n    generation_kwargs_info.pop(\"optional\")\n\n    return runtime_parameters_info\n</code></pre>"},{"location":"api/llm/#distilabel.llms.base.LLM.load","title":"<code>load()</code>","text":"<p>Method to be called to initialize the <code>LLM</code>, its logger and optionally the structured output generator.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Method to be called to initialize the `LLM`, its logger and optionally the structured output generator.\"\"\"\n    self._logger = logging.getLogger(f\"distilabel.llm.{self.model_name}\")\n</code></pre>"},{"location":"api/llm/anthropic/","title":"AnthropicLLM","text":""},{"location":"api/llm/anthropic/#distilabel.llms.anthropic.AnthropicLLM","title":"<code>AnthropicLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Anthropic LLM implementation running the Async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the name of the model to use for the LLM e.g. \"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", etc. Available models can be checked here: Anthropic: Models overview.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Anthropic API. If not provided, it will be read from <code>ANTHROPIC_API_KEY</code> environment variable.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Anthropic API. Defaults to <code>None</code> which means that <code>https://api.anthropic.com</code> will be used internally.</p> <code>timeout</code> <code>RuntimeParameter[float]</code> <p>the maximum time in seconds to wait for a response. Defaults to <code>600.0</code>.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>The maximum number of times to retry the request before failing. Defaults to <code>6</code>.</p> <code>http_client</code> <code>Optional[AsyncClient]</code> <p>if provided, an alternative HTTP client to use for calling Anthropic API. Defaults to <code>None</code>.</p> <code>structured_output</code> <code>Optional[AsyncClient]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. Defaults to None.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> <code>_aclient</code> <code>Optional[AsyncAnthropic]</code> <p>the <code>AsyncAnthropic</code> client to use for the Anthropic API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>api_key</code>: the API key to authenticate the requests to the Anthropic API. If not     provided, it will be read from <code>ANTHROPIC_API_KEY</code> environment variable.</li> <li><code>base_url</code>: the base URL to use for the Anthropic API. Defaults to <code>\"https://api.anthropic.com\"</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response. Defaults to <code>600.0</code>.</li> <li><code>max_retries</code>: the maximum number of times to retry the request before failing.     Defaults to <code>6</code>.</li> </ul> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>class AnthropicLLM(AsyncLLM):\n    \"\"\"Anthropic LLM implementation running the Async API client.\n\n    Attributes:\n        model: the name of the model to use for the LLM e.g. \"claude-3-opus-20240229\",\n            \"claude-3-sonnet-20240229\", etc. Available models can be checked here:\n            [Anthropic: Models overview](https://docs.anthropic.com/claude/docs/models-overview).\n        api_key: the API key to authenticate the requests to the Anthropic API. If not provided,\n            it will be read from `ANTHROPIC_API_KEY` environment variable.\n        base_url: the base URL to use for the Anthropic API. Defaults to `None` which means\n            that `https://api.anthropic.com` will be used internally.\n        timeout: the maximum time in seconds to wait for a response. Defaults to `600.0`.\n        max_retries: The maximum number of times to retry the request before failing. Defaults\n            to `6`.\n        http_client: if provided, an alternative HTTP client to use for calling Anthropic\n            API. Defaults to `None`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. Defaults to None.\n        _api_key_env_var: the name of the environment variable to use for the API key. It\n            is meant to be used internally.\n        _aclient: the `AsyncAnthropic` client to use for the Anthropic API. It is meant\n            to be used internally. Set in the `load` method.\n\n    Runtime parameters:\n        - `api_key`: the API key to authenticate the requests to the Anthropic API. If not\n            provided, it will be read from `ANTHROPIC_API_KEY` environment variable.\n        - `base_url`: the base URL to use for the Anthropic API. Defaults to `\"https://api.anthropic.com\"`.\n        - `timeout`: the maximum time in seconds to wait for a response. Defaults to `600.0`.\n        - `max_retries`: the maximum number of times to retry the request before failing.\n            Defaults to `6`.\n    \"\"\"\n\n    model: str\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"ANTHROPIC_BASE_URL\", \"https://api.anthropic.com\"\n        ),\n        description=\"The base URL to use for the Anthropic API.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_ANTHROPIC_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Anthropic API.\",\n    )\n    timeout: RuntimeParameter[float] = Field(\n        default=600.0,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    http_client: Optional[AsyncClient] = Field(default=None, exclude=True)\n\n    _api_key_env_var: str = PrivateAttr(default=_ANTHROPIC_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncAnthropic\"] = PrivateAttr(...)\n\n    def _check_model_exists(self) -&gt; None:\n        \"\"\"Checks if the specified model exists in the available models.\"\"\"\n        from anthropic import AsyncAnthropic\n\n        annotation = get_type_hints(AsyncAnthropic().messages.create).get(\"model\", None)\n        models = [\n            value\n            for type_ in get_args(annotation)\n            if get_origin(type_) is Literal\n            for value in get_args(type_)\n        ]\n\n        if self.model not in models:\n            raise ValueError(\n                f\"Model {self.model} does not exist among available models. \"\n                f\"The available models are {', '.join(models)}\"\n            )\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncAnthropic` client to use the Anthropic async API.\"\"\"\n        super().load()\n\n        try:\n            from anthropic import AsyncAnthropic\n        except ImportError as ie:\n            raise ImportError(\n                \"Anthropic Python client is not installed. Please install it using\"\n                \" `pip install anthropic`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._check_model_exists()\n\n        self._aclient = AsyncAnthropic(\n            api_key=self.api_key.get_secret_value(),\n            base_url=self.base_url,\n            timeout=self.timeout,\n            http_client=self.http_client,\n            max_retries=self.max_retries,\n        )\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"anthropic\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        max_tokens: int = 128,\n        stop_sequences: Union[List[str], None] = None,\n        temperature: float = 1.0,\n        top_p: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates a response asynchronously, using the [Anthropic Async API definition](https://github.com/anthropics/anthropic-sdk-python).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_tokens: the maximum number of new tokens that the model will generate. Defaults to `128`.\n            stop_sequences: custom text sequences that will cause the model to stop generating. Defaults to `NOT_GIVEN`.\n            temperature: the temperature to use for the generation. Set only if top_p is None. Defaults to `1.0`.\n            top_p: the top-p value to use for the generation. Defaults to `NOT_GIVEN`.\n            top_k: the top-k value to use for the generation. Defaults to `NOT_GIVEN`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        from anthropic._types import NOT_GIVEN\n\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"system\": (\n                input.pop(0)[\"content\"]\n                if input and input[0][\"role\"] == \"system\"\n                else NOT_GIVEN\n            ),\n            \"max_tokens\": max_tokens,\n            \"stream\": False,\n            \"stop_sequences\": NOT_GIVEN if stop_sequences is None else stop_sequences,\n            \"temperature\": temperature,\n            \"top_p\": NOT_GIVEN if top_p is None else top_p,\n            \"top_k\": NOT_GIVEN if top_k is None else top_k,\n        }\n\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n\n        completion = await self._aclient.messages.create(**kwargs)  # type: ignore\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        if (content := completion.content[0].text) is None:\n            self._logger.warning(\n                f\"Received no response using Anthropic client (model: '{self.model}').\"\n                f\" Finish reason was: {completion.stop_reason}\"\n            )\n        generations.append(content)\n        return generations\n\n    # TODO: remove this function once Anthropic client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"api/llm/anthropic/#distilabel.llms.anthropic.AnthropicLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/anthropic/#distilabel.llms.anthropic.AnthropicLLM.agenerate","title":"<code>agenerate(input, max_tokens=128, stop_sequences=None, temperature=1.0, top_p=None, top_k=None)</code>  <code>async</code>","text":"<p>Generates a response asynchronously, using the Anthropic Async API definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>stop_sequences</code> <code>Union[List[str], None]</code> <p>custom text sequences that will cause the model to stop generating. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Set only if top_p is None. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_p</code> <code>Union[float, None]</code> <p>the top-p value to use for the generation. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <code>top_k</code> <code>Union[int, None]</code> <p>the top-k value to use for the generation. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    max_tokens: int = 128,\n    stop_sequences: Union[List[str], None] = None,\n    temperature: float = 1.0,\n    top_p: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates a response asynchronously, using the [Anthropic Async API definition](https://github.com/anthropics/anthropic-sdk-python).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_tokens: the maximum number of new tokens that the model will generate. Defaults to `128`.\n        stop_sequences: custom text sequences that will cause the model to stop generating. Defaults to `NOT_GIVEN`.\n        temperature: the temperature to use for the generation. Set only if top_p is None. Defaults to `1.0`.\n        top_p: the top-p value to use for the generation. Defaults to `NOT_GIVEN`.\n        top_k: the top-k value to use for the generation. Defaults to `NOT_GIVEN`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    from anthropic._types import NOT_GIVEN\n\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"system\": (\n            input.pop(0)[\"content\"]\n            if input and input[0][\"role\"] == \"system\"\n            else NOT_GIVEN\n        ),\n        \"max_tokens\": max_tokens,\n        \"stream\": False,\n        \"stop_sequences\": NOT_GIVEN if stop_sequences is None else stop_sequences,\n        \"temperature\": temperature,\n        \"top_p\": NOT_GIVEN if top_p is None else top_p,\n        \"top_k\": NOT_GIVEN if top_k is None else top_k,\n    }\n\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n\n    completion = await self._aclient.messages.create(**kwargs)  # type: ignore\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    if (content := completion.content[0].text) is None:\n        self._logger.warning(\n            f\"Received no response using Anthropic client (model: '{self.model}').\"\n            f\" Finish reason was: {completion.stop_reason}\"\n        )\n    generations.append(content)\n    return generations\n</code></pre>"},{"location":"api/llm/anthropic/#distilabel.llms.anthropic.AnthropicLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"api/llm/anthropic/#distilabel.llms.anthropic.AnthropicLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncAnthropic</code> client to use the Anthropic async API.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncAnthropic` client to use the Anthropic async API.\"\"\"\n    super().load()\n\n    try:\n        from anthropic import AsyncAnthropic\n    except ImportError as ie:\n        raise ImportError(\n            \"Anthropic Python client is not installed. Please install it using\"\n            \" `pip install anthropic`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._check_model_exists()\n\n    self._aclient = AsyncAnthropic(\n        api_key=self.api_key.get_secret_value(),\n        base_url=self.base_url,\n        timeout=self.timeout,\n        http_client=self.http_client,\n        max_retries=self.max_retries,\n    )\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"anthropic\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"api/llm/anyscale/","title":"AnyscaleLLM","text":""},{"location":"api/llm/anyscale/#distilabel.llms.anyscale.AnyscaleLLM","title":"<code>AnyscaleLLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>Anyscale LLM implementation running the async API client of OpenAI.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM, e.g., <code>google/gemma-7b-it</code>. See the supported models under the \"Text Generation -&gt; Supported Models\" section here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Anyscale API requests. Defaults to <code>None</code>, which means that the value set for the environment variable <code>ANYSCALE_BASE_URL</code> will be used, or \"https://api.endpoints.anyscale.com/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Anyscale API. Defaults to <code>None</code> which means that the value set for the environment variable <code>ANYSCALE_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> Source code in <code>src/distilabel/llms/anyscale.py</code> <pre><code>class AnyscaleLLM(OpenAILLM):\n    \"\"\"Anyscale LLM implementation running the async API client of OpenAI.\n\n    Attributes:\n        model: the model name to use for the LLM, e.g., `google/gemma-7b-it`. See the\n            supported models under the \"Text Generation -&gt; Supported Models\" section\n            [here](https://docs.endpoints.anyscale.com/).\n        base_url: the base URL to use for the Anyscale API requests. Defaults to `None`, which\n            means that the value set for the environment variable `ANYSCALE_BASE_URL` will be used, or\n            \"https://api.endpoints.anyscale.com/v1\" if not set.\n        api_key: the API key to authenticate the requests to the Anyscale API. Defaults to `None` which\n            means that the value set for the environment variable `ANYSCALE_API_KEY` will be used, or\n            `None` if not set.\n        _api_key_env_var: the name of the environment variable to use for the API key.\n            It is meant to be used internally.\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"ANYSCALE_BASE_URL\", \"https://api.endpoints.anyscale.com/v1\"\n        ),\n        description=\"The base URL to use for the Anyscale API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_ANYSCALE_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Anyscale API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_ANYSCALE_API_KEY_ENV_VAR_NAME)\n</code></pre>"},{"location":"api/llm/azure/","title":"AzureOpenAILLM","text":""},{"location":"api/llm/azure/#distilabel.llms.azure.AzureOpenAILLM","title":"<code>AzureOpenAILLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>Azure OpenAI LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM i.e. the name of the Azure deployment.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Azure OpenAI API can be set with <code>AZURE_OPENAI_ENDPOINT</code>. Defaults to <code>None</code> which means that the value set for the environment variable <code>AZURE_OPENAI_ENDPOINT</code> will be used, or <code>None</code> if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Azure OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>AZURE_OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>api_version</code> <code>Optional[RuntimeParameter[str]]</code> <p>the API version to use for the Azure OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_VERSION</code> will be used, or <code>None</code> if not set.</p> Icon <p><code>:simple-microsoftazure:</code></p> Source code in <code>src/distilabel/llms/azure.py</code> <pre><code>class AzureOpenAILLM(OpenAILLM):\n    \"\"\"Azure OpenAI LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM i.e. the name of the Azure deployment.\n        base_url: the base URL to use for the Azure OpenAI API can be set with `AZURE_OPENAI_ENDPOINT`.\n            Defaults to `None` which means that the value set for the environment variable\n            `AZURE_OPENAI_ENDPOINT` will be used, or `None` if not set.\n        api_key: the API key to authenticate the requests to the Azure OpenAI API. Defaults to `None`\n            which means that the value set for the environment variable `AZURE_OPENAI_API_KEY` will be\n            used, or `None` if not set.\n        api_version: the API version to use for the Azure OpenAI API. Defaults to `None` which means\n            that the value set for the environment variable `OPENAI_API_VERSION` will be used, or\n            `None` if not set.\n\n    Icon:\n        `:simple-microsoftazure:`\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(_AZURE_OPENAI_ENDPOINT_ENV_VAR_NAME),\n        description=\"The base URL to use for the Azure OpenAI API requests i.e. the Azure OpenAI endpoint.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_AZURE_OPENAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Azure OpenAI API.\",\n    )\n\n    api_version: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\"OPENAI_API_VERSION\"),\n        description=\"The API version to use for the Azure OpenAI API.\",\n    )\n\n    _base_url_env_var: str = PrivateAttr(_AZURE_OPENAI_ENDPOINT_ENV_VAR_NAME)\n    _api_key_env_var: str = PrivateAttr(_AZURE_OPENAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncAzureOpenAI\"] = PrivateAttr(...)  # type: ignore\n\n    @override\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncAzureOpenAI` client to benefit from async requests.\"\"\"\n        # This is a workaround to avoid the `OpenAILLM` calling the _prepare_structured_output\n        # in the load method before we have the proper client.\n        with patch(\"OpenAILLM._prepare_structured_output\", lambda x: x):\n            super().load()\n\n        try:\n            from openai import AsyncAzureOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        # TODO: May be worth adding the AD auth too? Also the `organization`?\n        self._aclient = AsyncAzureOpenAI(  # type: ignore\n            azure_endpoint=self.base_url,  # type: ignore\n            azure_deployment=self.model,\n            api_version=self.api_version,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            self._prepare_structured_output(self.structured_output)\n</code></pre>"},{"location":"api/llm/azure/#distilabel.llms.azure.AzureOpenAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncAzureOpenAI</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/azure.py</code> <pre><code>@override\ndef load(self) -&gt; None:\n    \"\"\"Loads the `AsyncAzureOpenAI` client to benefit from async requests.\"\"\"\n    # This is a workaround to avoid the `OpenAILLM` calling the _prepare_structured_output\n    # in the load method before we have the proper client.\n    with patch(\"OpenAILLM._prepare_structured_output\", lambda x: x):\n        super().load()\n\n    try:\n        from openai import AsyncAzureOpenAI\n    except ImportError as ie:\n        raise ImportError(\n            \"OpenAI Python client is not installed. Please install it using\"\n            \" `pip install openai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    # TODO: May be worth adding the AD auth too? Also the `organization`?\n    self._aclient = AsyncAzureOpenAI(  # type: ignore\n        azure_endpoint=self.base_url,  # type: ignore\n        azure_deployment=self.model,\n        api_version=self.api_version,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        self._prepare_structured_output(self.structured_output)\n</code></pre>"},{"location":"api/llm/groq/","title":"GroqLLM","text":""},{"location":"api/llm/groq/#distilabel.llms.groq.GroqLLM","title":"<code>GroqLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Groq API implementation using the async client for concurrent text generation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the name of the model from the Groq API to use for the generation.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Groq API requests. Defaults to <code>\"https://api.groq.com\"</code>.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Groq API. Defaults to the value of the <code>GROQ_API_KEY</code> environment variable.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of times to retry the request to the API before failing. Defaults to <code>2</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response from the API. Defaults to <code>120</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key.</p> <code>_aclient</code> <code>Optional[AsyncGroq]</code> <p>the <code>AsyncGroq</code> client from the <code>groq</code> package.</p> Runtime parameters <ul> <li><code>base_url</code>: the base URL to use for the Groq API requests. Defaults to     <code>\"https://api.groq.com\"</code>.</li> <li><code>api_key</code>: the API key to authenticate the requests to the Groq API. Defaults to     the value of the <code>GROQ_API_KEY</code> environment variable.</li> <li><code>max_retries</code>: the maximum number of times to retry the request to the API before     failing. Defaults to <code>2</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response from the API. Defaults     to <code>120</code>.</li> </ul> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>class GroqLLM(AsyncLLM):\n    \"\"\"Groq API implementation using the async client for concurrent text generation.\n\n    Attributes:\n        model: the name of the model from the Groq API to use for the generation.\n        base_url: the base URL to use for the Groq API requests. Defaults to\n            `\"https://api.groq.com\"`.\n        api_key: the API key to authenticate the requests to the Groq API. Defaults to\n            the value of the `GROQ_API_KEY` environment variable.\n        max_retries: the maximum number of times to retry the request to the API before\n            failing. Defaults to `2`.\n        timeout: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n        _api_key_env_var: the name of the environment variable to use for the API key.\n        _aclient: the `AsyncGroq` client from the `groq` package.\n\n    Runtime parameters:\n        - `base_url`: the base URL to use for the Groq API requests. Defaults to\n            `\"https://api.groq.com\"`.\n        - `api_key`: the API key to authenticate the requests to the Groq API. Defaults to\n            the value of the `GROQ_API_KEY` environment variable.\n        - `max_retries`: the maximum number of times to retry the request to the API before\n            failing. Defaults to `2`.\n        - `timeout`: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n    \"\"\"\n\n    model: str\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            _GROQ_API_BASE_URL_ENV_VAR_NAME, \"https://api.groq.com\"\n        ),\n        description=\"The base URL to use for the Groq API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_GROQ_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Groq API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=2,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_GROQ_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncGroq\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncGroq` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from groq import AsyncGroq\n        except ImportError as ie:\n            raise ImportError(\n                \"Groq Python client is not installed. Please install it using\"\n                ' `pip install groq` or from the extras as `pip install \"distilabel[groq]\"`.'\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = AsyncGroq(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"groq\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        seed: Optional[int] = None,\n        max_new_tokens: int = 128,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        stop: Optional[str] = None,\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Generates `num_generations` responses for the given input using the Groq async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            seed: the seed to use for the generation. Defaults to `None`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: the stop sequence to use for the generation. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n\n        References:\n            - https://console.groq.com/docs/text-chat\n        \"\"\"\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"seed\": seed,\n            \"temperature\": temperature,\n            \"max_tokens\": max_new_tokens,\n            \"top_p\": top_p,\n            \"stream\": False,\n            \"stop\": stop,\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n        completion = await self._aclient.chat.completions.create(**kwargs)  # type: ignore\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(  # type: ignore\n                    f\"Received no response using the Groq client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n\n    # TODO: remove this function once Groq client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"api/llm/groq/#distilabel.llms.groq.GroqLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/groq/#distilabel.llms.groq.GroqLLM.agenerate","title":"<code>agenerate(input, seed=None, max_new_tokens=128, temperature=1.0, top_p=1.0, stop=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the Groq async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>seed</code> <code>Optional[int]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[str]</code> <p>the stop sequence to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> References <ul> <li>https://console.groq.com/docs/text-chat</li> </ul> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    seed: Optional[int] = None,\n    max_new_tokens: int = 128,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stop: Optional[str] = None,\n) -&gt; \"GenerateOutput\":\n    \"\"\"Generates `num_generations` responses for the given input using the Groq async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        seed: the seed to use for the generation. Defaults to `None`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: the stop sequence to use for the generation. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n\n    References:\n        - https://console.groq.com/docs/text-chat\n    \"\"\"\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"seed\": seed,\n        \"temperature\": temperature,\n        \"max_tokens\": max_new_tokens,\n        \"top_p\": top_p,\n        \"stream\": False,\n        \"stop\": stop,\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n    completion = await self._aclient.chat.completions.create(**kwargs)  # type: ignore\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(  # type: ignore\n                f\"Received no response using the Groq client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"api/llm/groq/#distilabel.llms.groq.GroqLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"api/llm/groq/#distilabel.llms.groq.GroqLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncGroq</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncGroq` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from groq import AsyncGroq\n    except ImportError as ie:\n        raise ImportError(\n            \"Groq Python client is not installed. Please install it using\"\n            ' `pip install groq` or from the extras as `pip install \"distilabel[groq]\"`.'\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = AsyncGroq(\n        base_url=self.base_url,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"groq\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"api/llm/huggingface/","title":"Hugging Face","text":"<p>This section contains the reference for Hugging Face integrations:</p>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM","title":"<code>InferenceEndpointsLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>InferenceEndpoints LLM implementation running the async API client.</p> <p>This LLM will internally use <code>huggingface_hub.AsyncInferenceClient</code> or <code>openai.AsyncOpenAI</code> depending on the <code>use_openai_client</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>Optional[str]</code> <p>the model ID to use for the LLM as available in the Hugging Face Hub, which will be used to resolve the base URL for the serverless Inference Endpoints API requests. Defaults to <code>None</code>.</p> <code>endpoint_name</code> <code>Optional[RuntimeParameter[str]]</code> <p>the name of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> <code>endpoint_namespace</code> <code>Optional[RuntimeParameter[str]]</code> <p>the namespace of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Inference Endpoints API requests.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Inference Endpoints API.</p> <code>tokenizer_id</code> <code>Optional[str]</code> <p>the tokenizer ID to use for the LLM as available in the Hugging Face Hub. Defaults to <code>None</code>, but defining one is recommended to properly format the prompt.</p> <code>model_display_name</code> <code>Optional[str]</code> <p>the model display name to use for the LLM. Defaults to <code>None</code>.</p> <code>use_openai_client</code> <code>bool</code> <p>whether to use the OpenAI client instead of the Hugging Face client.</p> Icon <p><code>:hugging:</code></p> <p>Examples:</p> <pre><code>Free serverless Inference API:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n\nDedicated Inference Endpoints:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    endpoint_namespace=\"&lt;USER|ORG&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n\nDedicated Inference Endpoints or TGI:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    base_url=\"&lt;BASE_URL&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n</code></pre> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>class InferenceEndpointsLLM(AsyncLLM):\n    \"\"\"InferenceEndpoints LLM implementation running the async API client.\n\n    This LLM will internally use `huggingface_hub.AsyncInferenceClient` or `openai.AsyncOpenAI`\n    depending on the `use_openai_client` attribute.\n\n    Attributes:\n        model_id: the model ID to use for the LLM as available in the Hugging Face Hub, which\n            will be used to resolve the base URL for the serverless Inference Endpoints API requests.\n            Defaults to `None`.\n        endpoint_name: the name of the Inference Endpoint to use for the LLM. Defaults to `None`.\n        endpoint_namespace: the namespace of the Inference Endpoint to use for the LLM. Defaults to `None`.\n        base_url: the base URL to use for the Inference Endpoints API requests.\n        api_key: the API key to authenticate the requests to the Inference Endpoints API.\n        tokenizer_id: the tokenizer ID to use for the LLM as available in the Hugging Face Hub.\n            Defaults to `None`, but defining one is recommended to properly format the prompt.\n        model_display_name: the model display name to use for the LLM. Defaults to `None`.\n        use_openai_client: whether to use the OpenAI client instead of the Hugging Face client.\n\n    Icon:\n        `:hugging:`\n\n    Examples:\n\n        Free serverless Inference API:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n\n        Dedicated Inference Endpoints:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n            api_key=\"&lt;HF_API_KEY&gt;\",\n            endpoint_namespace=\"&lt;USER|ORG&gt;\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n\n        Dedicated Inference Endpoints or TGI:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            api_key=\"&lt;HF_API_KEY&gt;\",\n            base_url=\"&lt;BASE_URL&gt;\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n    \"\"\"\n\n    model_id: Optional[str] = None\n\n    endpoint_name: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The name of the Inference Endpoint to use for the LLM.\",\n    )\n    endpoint_namespace: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The namespace of the Inference Endpoint to use for the LLM.\",\n    )\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The base URL to use for the Inference Endpoints API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default=os.getenv(_INFERENCE_ENDPOINTS_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Inference Endpoints API.\",\n    )\n\n    tokenizer_id: Optional[str] = None\n    model_display_name: Optional[str] = None\n    use_openai_client: bool = False\n\n    grammar: Optional[RuntimeParameter[Grammar]] = Field(\n        default=None,\n        description=\"The grammar to use across all the generations.\",\n    )\n\n    _model_name: Optional[str] = PrivateAttr(default=None)\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(default=None)\n    _api_key_env_var: str = PrivateAttr(_INFERENCE_ENDPOINTS_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[Union[\"AsyncInferenceClient\", \"AsyncOpenAI\"]] = PrivateAttr(...)\n\n    @model_validator(mode=\"after\")  # type: ignore\n    def only_one_of_model_id_endpoint_name_or_base_url_provided(\n        self,\n    ) -&gt; \"InferenceEndpointsLLM\":\n        \"\"\"Validates that only one of `model_id` or `endpoint_name` is provided; and if `base_url` is also\n        provided, a warning will be shown informing the user that the provided `base_url` will be ignored in\n        favour of the dynamically calculated one..\"\"\"\n\n        if self.base_url and (self.model_id or self.endpoint_name):\n            self._logger.warning(  # type: ignore\n                f\"Since the `base_url={self.base_url}` is available and either one of `model_id` or `endpoint_name`\"\n                \" is also provided, the `base_url` will either be ignored or overwritten with the one generated\"\n                \" from either of those args, for serverless or dedicated inference endpoints, respectively.\"\n            )\n\n        if self.base_url and not (self.model_id or self.endpoint_name):\n            return self\n\n        if self.model_id and not self.endpoint_name:\n            return self\n\n        if self.endpoint_name and not self.model_id:\n            return self\n\n        raise ValidationError(\n            \"Only one of `model_id` or `endpoint_name` must be provided. If `base_url` is provided too,\"\n            \" it will be overwritten instead. Found `model_id`={self.model_id}, `endpoint_name`={self.endpoint_name},\"\n            f\" and `base_url`={self.base_url}.\"\n        )\n\n    def load(self) -&gt; None:  # noqa: C901\n        \"\"\"Loads the either the `AsyncInferenceClient` or the `AsyncOpenAI` client to benefit\n        from async requests, running the Hugging Face Inference Endpoint underneath via the\n        `/v1/chat/completions` endpoint, exposed for the models running on TGI using the\n        `text-generation` task.\n\n        Raises:\n            ImportError: if the `openai` Python client is not installed.\n            ImportError: if the `huggingface-hub` Python client is not installed.\n            ValueError: if the model is not currently deployed or is not running the TGI framework.\n            ImportError: if the `transformers` Python client is not installed.\n        \"\"\"\n        super().load()\n\n        try:\n            from huggingface_hub import (\n                AsyncInferenceClient,\n                InferenceClient,\n                constants,\n                get_inference_endpoint,\n            )\n        except ImportError as ie:\n            raise ImportError(\n                \"Hugging Face Hub Python client is not installed. Please install it using\"\n                \" `pip install huggingface-hub`.\"\n            ) from ie\n\n        if self.api_key is None:\n            if not Path(constants.HF_TOKEN_PATH).exists():\n                raise ValueError(\n                    f\"To use `{self.__class__.__name__}` an API key must be provided via\"\n                    \" `api_key` attribute or runtime parameter, set the environment variable\"\n                    f\" `{self._api_key_env_var}` or use the `huggingface-hub` CLI to login\"\n                    \" with `huggingface-cli login`.\"\n                )\n            self.api_key = SecretStr(open(constants.HF_TOKEN_PATH).read().strip())\n\n        if self.model_id is not None:\n            client = InferenceClient()\n            status = client.get_model_status(self.model_id)\n\n            if (\n                status.state not in {\"Loadable\", \"Loaded\"}\n                and status.framework != \"text-generation-inference\"\n            ):\n                raise ValueError(\n                    f\"Model {self.model_id} is not currently deployed or is not running the TGI framework\"\n                )\n\n            self.base_url = client._resolve_url(\n                model=self.model_id, task=\"text-generation\"\n            )\n\n        if self.endpoint_name is not None:\n            client = get_inference_endpoint(\n                name=self.endpoint_name,\n                namespace=self.endpoint_namespace,\n                token=self.api_key.get_secret_value(),\n            )\n            if client.status in [\"paused\", \"scaledToZero\"]:\n                client.resume().wait(timeout=300)\n            elif client.status in [\"initializing\"]:\n                client.wait(timeout=300)\n\n            self.base_url = client.url\n            self._model_name = client.repository\n\n        if self.use_openai_client:\n            try:\n                from openai import AsyncOpenAI\n            except ImportError as ie:\n                raise ImportError(\n                    \"OpenAI Python client is not installed. Please install it using\"\n                    \" `pip install openai`.\"\n                ) from ie\n\n            self._aclient = AsyncOpenAI(\n                base_url=self.base_url,\n                api_key=self.api_key.get_secret_value(),\n                max_retries=6,\n            )\n        else:\n            self._aclient = AsyncInferenceClient(\n                model=self.base_url,\n                token=self.api_key.get_secret_value(),\n            )\n\n        if self.tokenizer_id:\n            try:\n                from transformers import AutoTokenizer\n            except ImportError as ie:\n                raise ImportError(\n                    \"Transformers Python client is not installed. Please install it using\"\n                    \" `pip install transformers`.\"\n                ) from ie\n\n            self._tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n\n    @property\n    @override\n    def model_name(self) -&gt; Union[str, None]:  # type: ignore\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return (\n            self.model_display_name\n            or self._model_name\n            or self.model_id\n            or self.endpoint_name\n            or self.base_url\n        )\n\n    async def _openai_agenerate(\n        self,\n        input: \"StandardInput\",\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: Optional[float] = None,\n        stop: Optional[Union[str, List[str]]] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates completions for the given input using the OpenAI async client.\"\"\"\n        completion = await self._aclient.chat.completions.create(  # type: ignore\n            messages=input,  # type: ignore\n            model=\"tgi\",\n            max_tokens=max_new_tokens,\n            n=1,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop,\n            timeout=50,\n        )\n        if completion.choices[0].message.content is None:\n            self._logger.warning(  # type: ignore\n                f\"\u26a0\ufe0f Received no response using OpenAI client (model: '{self.model_name}').\"\n                f\" Finish reason was: {completion.choices[0].finish_reason}\"\n            )\n        return [completion.choices[0].message.content]\n\n    # TODO: add `num_generations` parameter once either TGI or `AsyncInferenceClient` allows `n` parameter\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: \"FormattedInput\",\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        repetition_penalty: Optional[float] = None,\n        temperature: float = 1.0,\n        do_sample: bool = False,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        typical_p: Optional[float] = None,\n        stop_sequences: Optional[Union[str, List[str]]] = None,\n        return_full_text: bool = False,\n        seed: Optional[int] = None,\n        watermark: bool = False,\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Generates completions for the given input using the OpenAI async client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`. Only applies if `use_openai_client=True`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`. Only applies if `use_openai_client=True`.\n            repetition_penalty: the repetition penalty to use for the generation. Defaults\n                to `None`. Only applies if `use_openai_client=False`.\n            temperature: the temperature to use for the generation. Defaults to `1.0`.\n            do_sample: whether to use sampling for the generation. Defaults to `False`.\n                Only applies if `use_openai_client=False`.\n            top_k: the top-k value to use for the generation. Defaults to `0.8`, since neither\n                `0.0` nor `1.0` are valid values in TGI.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            typical_p: the typical-p value to use for the generation. Defaults to `0.5`.\n            stop_sequences: either a single string or a list of strings containing the sequences\n                to stop the generation at. Defaults to `None`, but will be set to the\n                `tokenizer.eos_token` if available.\n            return_full_text: whether to return the full text of the completion or just the\n                generated text. Defaults to `False`, meaning that only the generated text will be\n                returned.\n            seed: the seed to use for the generation. Defaults to `None`.\n            watermark: whether to add the watermark to the generated text. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        if stop_sequences is not None:\n            if isinstance(stop_sequences, str):\n                stop_sequences = [stop_sequences]\n            if len(stop_sequences) &gt; 4:\n                warnings.warn(\n                    \"Only up to 4 stop sequences are allowed, so keeping the first 4 items only.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                stop_sequences = stop_sequences[:4]\n\n        grammar = None\n        if isinstance(input, tuple):\n            input, grammar = input\n\n        if self.use_openai_client:\n            return await self._openai_agenerate(\n                input=input,\n                max_new_tokens=max_new_tokens,\n                frequency_penalty=frequency_penalty,\n                presence_penalty=presence_penalty,\n                temperature=temperature,\n                top_p=top_p,\n                stop=stop_sequences,\n            )\n\n        if self._tokenizer is not None:\n            prompt = self._tokenizer.apply_chat_template(  # type: ignore\n                conversation=input,  # type: ignore\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        else:\n            # TODO: should we apply a default chat template here instead? e.g. ChatML\n            prompt = \"\\n\".join([message[\"content\"] for message in input])\n\n        try:\n            completion = await self._aclient.text_generation(  # type: ignore\n                prompt=prompt,  # type: ignore\n                max_new_tokens=max_new_tokens,\n                do_sample=do_sample,\n                typical_p=typical_p,\n                repetition_penalty=repetition_penalty,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                stop_sequences=stop_sequences,\n                return_full_text=return_full_text,\n                watermark=watermark,\n                # NOTE: `self.grammar` applies to all the generations, while `grammar` is intended\n                # to be different per each input, and those are not intended to be used together\n                grammar=grammar or self.grammar,  # type: ignore\n                # NOTE: here to ensure that the cache is not used and a different response is\n                # generated every time\n                seed=seed or random.randint(0, 2147483647),\n            )\n            return [completion]\n        except Exception as e:\n            self._logger.warning(  # type: ignore\n                f\"\u26a0\ufe0f Received no response using Inference Client (model: '{self.model_name}').\"\n                f\" Finish reason was: {e}\"\n            )\n            return [None]\n\n    # TODO: remove this function once `AsyncInferenceClient` allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"FormattedInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.model_name","title":"<code>model_name: Union[str, None]</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.agenerate","title":"<code>agenerate(input, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, repetition_penalty=None, temperature=1.0, do_sample=False, top_k=None, top_p=None, typical_p=None, stop_sequences=None, return_full_text=False, seed=None, watermark=False)</code>  <code>async</code>","text":"<p>Generates completions for the given input using the OpenAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>FormattedInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>. Only applies if <code>use_openai_client=True</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>. Only applies if <code>use_openai_client=True</code>.</p> <code>0.0</code> <code>repetition_penalty</code> <code>Optional[float]</code> <p>the repetition penalty to use for the generation. Defaults to <code>None</code>. Only applies if <code>use_openai_client=False</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>do_sample</code> <code>bool</code> <p>whether to use sampling for the generation. Defaults to <code>False</code>. Only applies if <code>use_openai_client=False</code>.</p> <code>False</code> <code>top_k</code> <code>Optional[int]</code> <p>the top-k value to use for the generation. Defaults to <code>0.8</code>, since neither <code>0.0</code> nor <code>1.0</code> are valid values in TGI.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>None</code> <code>typical_p</code> <code>Optional[float]</code> <p>the typical-p value to use for the generation. Defaults to <code>0.5</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[Union[str, List[str]]]</code> <p>either a single string or a list of strings containing the sequences to stop the generation at. Defaults to <code>None</code>, but will be set to the <code>tokenizer.eos_token</code> if available.</p> <code>None</code> <code>return_full_text</code> <code>bool</code> <p>whether to return the full text of the completion or just the generated text. Defaults to <code>False</code>, meaning that only the generated text will be returned.</p> <code>False</code> <code>seed</code> <code>Optional[int]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>watermark</code> <code>bool</code> <p>whether to add the watermark to the generated text. Defaults to <code>None</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: \"FormattedInput\",\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    repetition_penalty: Optional[float] = None,\n    temperature: float = 1.0,\n    do_sample: bool = False,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    typical_p: Optional[float] = None,\n    stop_sequences: Optional[Union[str, List[str]]] = None,\n    return_full_text: bool = False,\n    seed: Optional[int] = None,\n    watermark: bool = False,\n) -&gt; \"GenerateOutput\":\n    \"\"\"Generates completions for the given input using the OpenAI async client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`. Only applies if `use_openai_client=True`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`. Only applies if `use_openai_client=True`.\n        repetition_penalty: the repetition penalty to use for the generation. Defaults\n            to `None`. Only applies if `use_openai_client=False`.\n        temperature: the temperature to use for the generation. Defaults to `1.0`.\n        do_sample: whether to use sampling for the generation. Defaults to `False`.\n            Only applies if `use_openai_client=False`.\n        top_k: the top-k value to use for the generation. Defaults to `0.8`, since neither\n            `0.0` nor `1.0` are valid values in TGI.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        typical_p: the typical-p value to use for the generation. Defaults to `0.5`.\n        stop_sequences: either a single string or a list of strings containing the sequences\n            to stop the generation at. Defaults to `None`, but will be set to the\n            `tokenizer.eos_token` if available.\n        return_full_text: whether to return the full text of the completion or just the\n            generated text. Defaults to `False`, meaning that only the generated text will be\n            returned.\n        seed: the seed to use for the generation. Defaults to `None`.\n        watermark: whether to add the watermark to the generated text. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    if stop_sequences is not None:\n        if isinstance(stop_sequences, str):\n            stop_sequences = [stop_sequences]\n        if len(stop_sequences) &gt; 4:\n            warnings.warn(\n                \"Only up to 4 stop sequences are allowed, so keeping the first 4 items only.\",\n                UserWarning,\n                stacklevel=2,\n            )\n            stop_sequences = stop_sequences[:4]\n\n    grammar = None\n    if isinstance(input, tuple):\n        input, grammar = input\n\n    if self.use_openai_client:\n        return await self._openai_agenerate(\n            input=input,\n            max_new_tokens=max_new_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop_sequences,\n        )\n\n    if self._tokenizer is not None:\n        prompt = self._tokenizer.apply_chat_template(  # type: ignore\n            conversation=input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n    else:\n        # TODO: should we apply a default chat template here instead? e.g. ChatML\n        prompt = \"\\n\".join([message[\"content\"] for message in input])\n\n    try:\n        completion = await self._aclient.text_generation(  # type: ignore\n            prompt=prompt,  # type: ignore\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            typical_p=typical_p,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop_sequences=stop_sequences,\n            return_full_text=return_full_text,\n            watermark=watermark,\n            # NOTE: `self.grammar` applies to all the generations, while `grammar` is intended\n            # to be different per each input, and those are not intended to be used together\n            grammar=grammar or self.grammar,  # type: ignore\n            # NOTE: here to ensure that the cache is not used and a different response is\n            # generated every time\n            seed=seed or random.randint(0, 2147483647),\n        )\n        return [completion]\n    except Exception as e:\n        self._logger.warning(  # type: ignore\n            f\"\u26a0\ufe0f Received no response using Inference Client (model: '{self.model_name}').\"\n            f\" Finish reason was: {e}\"\n        )\n        return [None]\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"FormattedInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.load","title":"<code>load()</code>","text":"<p>Loads the either the <code>AsyncInferenceClient</code> or the <code>AsyncOpenAI</code> client to benefit from async requests, running the Hugging Face Inference Endpoint underneath via the <code>/v1/chat/completions</code> endpoint, exposed for the models running on TGI using the <code>text-generation</code> task.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>if the <code>openai</code> Python client is not installed.</p> <code>ImportError</code> <p>if the <code>huggingface-hub</code> Python client is not installed.</p> <code>ValueError</code> <p>if the model is not currently deployed or is not running the TGI framework.</p> <code>ImportError</code> <p>if the <code>transformers</code> Python client is not installed.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>def load(self) -&gt; None:  # noqa: C901\n    \"\"\"Loads the either the `AsyncInferenceClient` or the `AsyncOpenAI` client to benefit\n    from async requests, running the Hugging Face Inference Endpoint underneath via the\n    `/v1/chat/completions` endpoint, exposed for the models running on TGI using the\n    `text-generation` task.\n\n    Raises:\n        ImportError: if the `openai` Python client is not installed.\n        ImportError: if the `huggingface-hub` Python client is not installed.\n        ValueError: if the model is not currently deployed or is not running the TGI framework.\n        ImportError: if the `transformers` Python client is not installed.\n    \"\"\"\n    super().load()\n\n    try:\n        from huggingface_hub import (\n            AsyncInferenceClient,\n            InferenceClient,\n            constants,\n            get_inference_endpoint,\n        )\n    except ImportError as ie:\n        raise ImportError(\n            \"Hugging Face Hub Python client is not installed. Please install it using\"\n            \" `pip install huggingface-hub`.\"\n        ) from ie\n\n    if self.api_key is None:\n        if not Path(constants.HF_TOKEN_PATH).exists():\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via\"\n                \" `api_key` attribute or runtime parameter, set the environment variable\"\n                f\" `{self._api_key_env_var}` or use the `huggingface-hub` CLI to login\"\n                \" with `huggingface-cli login`.\"\n            )\n        self.api_key = SecretStr(open(constants.HF_TOKEN_PATH).read().strip())\n\n    if self.model_id is not None:\n        client = InferenceClient()\n        status = client.get_model_status(self.model_id)\n\n        if (\n            status.state not in {\"Loadable\", \"Loaded\"}\n            and status.framework != \"text-generation-inference\"\n        ):\n            raise ValueError(\n                f\"Model {self.model_id} is not currently deployed or is not running the TGI framework\"\n            )\n\n        self.base_url = client._resolve_url(\n            model=self.model_id, task=\"text-generation\"\n        )\n\n    if self.endpoint_name is not None:\n        client = get_inference_endpoint(\n            name=self.endpoint_name,\n            namespace=self.endpoint_namespace,\n            token=self.api_key.get_secret_value(),\n        )\n        if client.status in [\"paused\", \"scaledToZero\"]:\n            client.resume().wait(timeout=300)\n        elif client.status in [\"initializing\"]:\n            client.wait(timeout=300)\n\n        self.base_url = client.url\n        self._model_name = client.repository\n\n    if self.use_openai_client:\n        try:\n            from openai import AsyncOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        self._aclient = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=6,\n        )\n    else:\n        self._aclient = AsyncInferenceClient(\n            model=self.base_url,\n            token=self.api_key.get_secret_value(),\n        )\n\n    if self.tokenizer_id:\n        try:\n            from transformers import AutoTokenizer\n        except ImportError as ie:\n            raise ImportError(\n                \"Transformers Python client is not installed. Please install it using\"\n                \" `pip install transformers`.\"\n            ) from ie\n\n        self._tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.only_one_of_model_id_endpoint_name_or_base_url_provided","title":"<code>only_one_of_model_id_endpoint_name_or_base_url_provided()</code>","text":"<p>Validates that only one of <code>model_id</code> or <code>endpoint_name</code> is provided; and if <code>base_url</code> is also provided, a warning will be shown informing the user that the provided <code>base_url</code> will be ignored in favour of the dynamically calculated one..</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@model_validator(mode=\"after\")  # type: ignore\ndef only_one_of_model_id_endpoint_name_or_base_url_provided(\n    self,\n) -&gt; \"InferenceEndpointsLLM\":\n    \"\"\"Validates that only one of `model_id` or `endpoint_name` is provided; and if `base_url` is also\n    provided, a warning will be shown informing the user that the provided `base_url` will be ignored in\n    favour of the dynamically calculated one..\"\"\"\n\n    if self.base_url and (self.model_id or self.endpoint_name):\n        self._logger.warning(  # type: ignore\n            f\"Since the `base_url={self.base_url}` is available and either one of `model_id` or `endpoint_name`\"\n            \" is also provided, the `base_url` will either be ignored or overwritten with the one generated\"\n            \" from either of those args, for serverless or dedicated inference endpoints, respectively.\"\n        )\n\n    if self.base_url and not (self.model_id or self.endpoint_name):\n        return self\n\n    if self.model_id and not self.endpoint_name:\n        return self\n\n    if self.endpoint_name and not self.model_id:\n        return self\n\n    raise ValidationError(\n        \"Only one of `model_id` or `endpoint_name` must be provided. If `base_url` is provided too,\"\n        \" it will be overwritten instead. Found `model_id`={self.model_id}, `endpoint_name`={self.endpoint_name},\"\n        f\" and `base_url`={self.base_url}.\"\n    )\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.transformers.TransformersLLM","title":"<code>TransformersLLM</code>","text":"<p>               Bases: <code>LLM</code>, <code>CudaDevicePlacementMixin</code></p> <p>Hugging Face <code>transformers</code> library LLM implementation using the text generation pipeline.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model Hugging Face Hub repo id or a path to a directory containing the model weights and configuration files.</p> <code>revision</code> <code>str</code> <p>if <code>model</code> refers to a Hugging Face Hub repository, then the revision (e.g. a branch name or a commit id) to use. Defaults to <code>\"main\"</code>.</p> <code>torch_dtype</code> <code>str</code> <p>the torch dtype to use for the model e.g. \"float16\", \"float32\", etc. Defaults to <code>\"auto\"</code>.</p> <code>trust_remote_code</code> <code>bool</code> <p>whether to trust or not remote (code in the Hugging Face Hub repository) code to load the model. Defaults to <code>False</code>.</p> <code>model_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>from_pretrained</code> method of the model.</p> <code>tokenizer</code> <code>Optional[str]</code> <p>the tokenizer Hugging Face Hub repo id or a path to a directory containing the tokenizer config files. If not provided, the one associated to the <code>model</code> will be used. Defaults to <code>None</code>.</p> <code>use_fast</code> <code>bool</code> <p>whether to use a fast tokenizer or not. Defaults to <code>True</code>.</p> <code>chat_template</code> <code>Optional[str]</code> <p>a chat template that will be used to build the prompts before sending them to the model. If not provided, the chat template defined in the tokenizer config will be used. If not provided and the tokenizer doesn't have a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> <code>device</code> <code>Optional[Union[str, int]]</code> <p>the name or index of the device where the model will be loaded. Defaults to <code>None</code>.</p> <code>device_map</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>a dictionary mapping each layer of the model to a device, or a mode like <code>\"sequential\"</code> or <code>\"auto\"</code>. Defaults to <code>None</code>.</p> <code>token</code> <code>Optional[str]</code> <p>the Hugging Face Hub token that will be used to authenticate to the Hugging Face Hub. If not provided, the <code>HF_TOKEN</code> environment or <code>huggingface_hub</code> package local configuration will be used. Defaults to <code>None</code>.</p> Icon <p><code>:hugging:</code></p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>class TransformersLLM(LLM, CudaDevicePlacementMixin):\n    \"\"\"Hugging Face `transformers` library LLM implementation using the text generation\n    pipeline.\n\n    Attributes:\n        model: the model Hugging Face Hub repo id or a path to a directory containing the\n            model weights and configuration files.\n        revision: if `model` refers to a Hugging Face Hub repository, then the revision\n            (e.g. a branch name or a commit id) to use. Defaults to `\"main\"`.\n        torch_dtype: the torch dtype to use for the model e.g. \"float16\", \"float32\", etc.\n            Defaults to `\"auto\"`.\n        trust_remote_code: whether to trust or not remote (code in the Hugging Face Hub\n            repository) code to load the model. Defaults to `False`.\n        model_kwargs: additional dictionary of keyword arguments that will be passed to\n            the `from_pretrained` method of the model.\n        tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing\n            the tokenizer config files. If not provided, the one associated to the `model`\n            will be used. Defaults to `None`.\n        use_fast: whether to use a fast tokenizer or not. Defaults to `True`.\n        chat_template: a chat template that will be used to build the prompts before\n            sending them to the model. If not provided, the chat template defined in the\n            tokenizer config will be used. If not provided and the tokenizer doesn't have\n            a chat template, then ChatML template will be used. Defaults to `None`.\n        device: the name or index of the device where the model will be loaded. Defaults\n            to `None`.\n        device_map: a dictionary mapping each layer of the model to a device, or a mode\n            like `\"sequential\"` or `\"auto\"`. Defaults to `None`.\n        token: the Hugging Face Hub token that will be used to authenticate to the Hugging\n            Face Hub. If not provided, the `HF_TOKEN` environment or `huggingface_hub` package\n            local configuration will be used. Defaults to `None`.\n\n    Icon:\n        `:hugging:`\n    \"\"\"\n\n    model: str\n    revision: str = \"main\"\n    torch_dtype: str = \"auto\"\n    trust_remote_code: bool = False\n    model_kwargs: Optional[Dict[str, Any]] = None\n    tokenizer: Optional[str] = None\n    use_fast: bool = True\n    chat_template: Optional[str] = None\n    device: Optional[Union[str, int]] = None\n    device_map: Optional[Union[str, Dict[str, Any]]] = None\n    token: Optional[str] = None\n\n    _pipeline: Optional[\"Pipeline\"] = PrivateAttr(...)\n    _prefix_allowed_tokens_fn: Union[Callable, None] = PrivateAttr(default=None)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the model and tokenizer and creates the text generation pipeline. In addition,\n        it will configure the tokenizer chat template.\"\"\"\n        if self.device == \"cuda\":\n            CudaDevicePlacementMixin.load(self)\n\n        try:\n            from transformers import pipeline\n        except ImportError as ie:\n            raise ImportError(\n                \"Transformers is not installed. Please install it using `pip install transformers`.\"\n            ) from ie\n\n        self._pipeline = pipeline(\n            \"text-generation\",\n            model=self.model,\n            revision=self.revision,\n            torch_dtype=self.torch_dtype,\n            trust_remote_code=self.trust_remote_code,\n            model_kwargs=self.model_kwargs or {},\n            tokenizer=self.tokenizer or self.model,\n            use_fast=self.use_fast,\n            device=self.device,\n            device_map=self.device_map,\n            token=self.token or os.getenv(\"HF_TOKEN\"),\n            return_full_text=False,\n        )\n\n        if self.chat_template is not None:\n            self._pipeline.tokenizer.chat_template = self.chat_template  # type: ignore\n        elif (\n            self._pipeline.tokenizer.chat_template is None  # type: ignore\n            and self._pipeline.tokenizer.default_chat_template is None  # type: ignore\n        ):\n            self._pipeline.tokenizer.chat_template = CHATML_TEMPLATE  # type: ignore\n\n        if self.structured_output:\n            self._prefix_allowed_tokens_fn = self._prepare_structured_output(\n                self.structured_output\n            )\n\n        super().load()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def prepare_input(self, input: \"StandardInput\") -&gt; str:\n        \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n        as an OpenAI conversation, and adding the generation prompt.\n        \"\"\"\n        return self._pipeline.tokenizer.apply_chat_template(  # type: ignore\n            input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        temperature: float = 0.1,\n        repetition_penalty: float = 1.1,\n        top_p: float = 1.0,\n        top_k: int = 0,\n        do_sample: bool = True,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for each input using the text generation\n        pipeline.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            repetition_penalty: the repetition penalty to use for the generation. Defaults\n                to `1.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            top_k: the top-k value to use for the generation. Defaults to `0`.\n            do_sample: whether to use sampling or not. Defaults to `True`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        prepared_inputs = [self.prepare_input(input=input) for input in inputs]\n\n        outputs: List[List[Dict[str, str]]] = self._pipeline(  # type: ignore\n            prepared_inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            repetition_penalty=repetition_penalty,\n            top_p=top_p,\n            top_k=top_k,\n            do_sample=do_sample,\n            num_return_sequences=num_generations,\n            prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n        )\n        return [\n            [generation[\"generated_text\"] for generation in output]\n            for output in outputs\n        ]\n\n    def get_last_hidden_states(\n        self, inputs: List[\"StandardInput\"]\n    ) -&gt; List[\"HiddenState\"]:\n        \"\"\"Gets the last `hidden_states` of the model for the given inputs. It doesn't\n        execute the task head.\n\n        Args:\n            inputs: a list of inputs in chat format to generate the embeddings for.\n\n        Returns:\n            A list containing the last hidden state for each sequence using a NumPy array\n            with shape [num_tokens, hidden_size].\n        \"\"\"\n        model: \"PreTrainedModel\" = (\n            self._pipeline.model.model  # type: ignore\n            if hasattr(self._pipeline.model, \"model\")  # type: ignore\n            else next(self._pipeline.model.children())  # type: ignore\n        )\n        tokenizer: \"PreTrainedTokenizer\" = self._pipeline.tokenizer  # type: ignore\n        input_ids = tokenizer(\n            [self.prepare_input(input) for input in inputs],  # type: ignore\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(model.device)\n        last_hidden_states = model(**input_ids)[\"last_hidden_state\"]\n\n        return [\n            seq_last_hidden_state[attention_mask.bool(), :].detach().cpu().numpy()\n            for seq_last_hidden_state, attention_mask in zip(\n                last_hidden_states,\n                input_ids[\"attention_mask\"],  # type: ignore\n            )\n        ]\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Callable, None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(\n            structured_output, \"transformers\", self._pipeline\n        )\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.transformers.TransformersLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.transformers.TransformersLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, temperature=0.1, repetition_penalty=1.1, top_p=1.0, top_k=0, do_sample=True)</code>","text":"<p>Generates <code>num_generations</code> responses for each input using the text generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>0.1</code> <code>repetition_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>1.1</code>.</p> <code>1.1</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to use for the generation. Defaults to <code>0</code>.</p> <code>0</code> <code>do_sample</code> <code>bool</code> <p>whether to use sampling or not. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    temperature: float = 0.1,\n    repetition_penalty: float = 1.1,\n    top_p: float = 1.0,\n    top_k: int = 0,\n    do_sample: bool = True,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for each input using the text generation\n    pipeline.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        repetition_penalty: the repetition penalty to use for the generation. Defaults\n            to `1.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        top_k: the top-k value to use for the generation. Defaults to `0`.\n        do_sample: whether to use sampling or not. Defaults to `True`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    prepared_inputs = [self.prepare_input(input=input) for input in inputs]\n\n    outputs: List[List[Dict[str, str]]] = self._pipeline(  # type: ignore\n        prepared_inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        top_p=top_p,\n        top_k=top_k,\n        do_sample=do_sample,\n        num_return_sequences=num_generations,\n        prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n    )\n    return [\n        [generation[\"generated_text\"] for generation in output]\n        for output in outputs\n    ]\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.transformers.TransformersLLM.get_last_hidden_states","title":"<code>get_last_hidden_states(inputs)</code>","text":"<p>Gets the last <code>hidden_states</code> of the model for the given inputs. It doesn't execute the task head.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate the embeddings for.</p> required <p>Returns:</p> Type Description <code>List[HiddenState]</code> <p>A list containing the last hidden state for each sequence using a NumPy array</p> <code>List[HiddenState]</code> <p>with shape [num_tokens, hidden_size].</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def get_last_hidden_states(\n    self, inputs: List[\"StandardInput\"]\n) -&gt; List[\"HiddenState\"]:\n    \"\"\"Gets the last `hidden_states` of the model for the given inputs. It doesn't\n    execute the task head.\n\n    Args:\n        inputs: a list of inputs in chat format to generate the embeddings for.\n\n    Returns:\n        A list containing the last hidden state for each sequence using a NumPy array\n        with shape [num_tokens, hidden_size].\n    \"\"\"\n    model: \"PreTrainedModel\" = (\n        self._pipeline.model.model  # type: ignore\n        if hasattr(self._pipeline.model, \"model\")  # type: ignore\n        else next(self._pipeline.model.children())  # type: ignore\n    )\n    tokenizer: \"PreTrainedTokenizer\" = self._pipeline.tokenizer  # type: ignore\n    input_ids = tokenizer(\n        [self.prepare_input(input) for input in inputs],  # type: ignore\n        return_tensors=\"pt\",\n        padding=True,\n    ).to(model.device)\n    last_hidden_states = model(**input_ids)[\"last_hidden_state\"]\n\n    return [\n        seq_last_hidden_state[attention_mask.bool(), :].detach().cpu().numpy()\n        for seq_last_hidden_state, attention_mask in zip(\n            last_hidden_states,\n            input_ids[\"attention_mask\"],  # type: ignore\n        )\n    ]\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.transformers.TransformersLLM.load","title":"<code>load()</code>","text":"<p>Loads the model and tokenizer and creates the text generation pipeline. In addition, it will configure the tokenizer chat template.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the model and tokenizer and creates the text generation pipeline. In addition,\n    it will configure the tokenizer chat template.\"\"\"\n    if self.device == \"cuda\":\n        CudaDevicePlacementMixin.load(self)\n\n    try:\n        from transformers import pipeline\n    except ImportError as ie:\n        raise ImportError(\n            \"Transformers is not installed. Please install it using `pip install transformers`.\"\n        ) from ie\n\n    self._pipeline = pipeline(\n        \"text-generation\",\n        model=self.model,\n        revision=self.revision,\n        torch_dtype=self.torch_dtype,\n        trust_remote_code=self.trust_remote_code,\n        model_kwargs=self.model_kwargs or {},\n        tokenizer=self.tokenizer or self.model,\n        use_fast=self.use_fast,\n        device=self.device,\n        device_map=self.device_map,\n        token=self.token or os.getenv(\"HF_TOKEN\"),\n        return_full_text=False,\n    )\n\n    if self.chat_template is not None:\n        self._pipeline.tokenizer.chat_template = self.chat_template  # type: ignore\n    elif (\n        self._pipeline.tokenizer.chat_template is None  # type: ignore\n        and self._pipeline.tokenizer.default_chat_template is None  # type: ignore\n    ):\n        self._pipeline.tokenizer.chat_template = CHATML_TEMPLATE  # type: ignore\n\n    if self.structured_output:\n        self._prefix_allowed_tokens_fn = self._prepare_structured_output(\n            self.structured_output\n        )\n\n    super().load()\n</code></pre>"},{"location":"api/llm/huggingface/#distilabel.llms.huggingface.transformers.TransformersLLM.prepare_input","title":"<code>prepare_input(input)</code>","text":"<p>Prepares the input by applying the chat template to the input, which is formatted as an OpenAI conversation, and adding the generation prompt.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def prepare_input(self, input: \"StandardInput\") -&gt; str:\n    \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n    as an OpenAI conversation, and adding the generation prompt.\n    \"\"\"\n    return self._pipeline.tokenizer.apply_chat_template(  # type: ignore\n        input,  # type: ignore\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n</code></pre>"},{"location":"api/llm/litellm/","title":"LiteLLM","text":""},{"location":"api/llm/litellm/#distilabel.llms.litellm.LiteLLM","title":"<code>LiteLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>LiteLLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gpt-3.5-turbo\" or \"mistral/mistral-large\", etc.</p> <code>verbose</code> <code>RuntimeParameter[bool]</code> <p>whether to log the LiteLLM client's logs. Defaults to <code>False</code>.</p> <code>structured_output</code> <code>RuntimeParameter[bool]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> Runtime parameters <ul> <li><code>verbose</code>: whether to log the LiteLLM client's logs. Defaults to <code>False</code>.</li> </ul> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>class LiteLLM(AsyncLLM):\n    \"\"\"LiteLLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gpt-3.5-turbo\" or \"mistral/mistral-large\",\n            etc.\n        verbose: whether to log the LiteLLM client's logs. Defaults to `False`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n\n    Runtime parameters:\n        - `verbose`: whether to log the LiteLLM client's logs. Defaults to `False`.\n    \"\"\"\n\n    model: str\n    verbose: RuntimeParameter[bool] = Field(\n        default=False, description=\"Whether to log the LiteLLM client's logs.\"\n    )\n\n    _aclient: Optional[Callable] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"\n        Loads the `acompletion` LiteLLM client to benefit from async requests.\n        \"\"\"\n        super().load()\n\n        try:\n            import litellm\n\n            litellm.telemetry = False\n        except ImportError as e:\n            raise ImportError(\n                \"LiteLLM Python client is not installed. Please install it using\"\n                \" `pip install litellm`.\"\n            ) from e\n        self._aclient = litellm.acompletion\n\n        if not self.verbose:\n            litellm.suppress_debug_info = True\n            for key in logging.Logger.manager.loggerDict.keys():\n                if \"litellm\" not in key.lower():\n                    continue\n                logging.getLogger(key).setLevel(logging.CRITICAL)\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"litellm\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        temperature: Optional[float] = 1.0,\n        top_p: Optional[float] = 1.0,\n        stop: Optional[Union[str, list]] = None,\n        max_tokens: Optional[int] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        metadata: Optional[dict] = None,\n        api_base: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,\n        mock_response: Optional[str] = None,\n        force_timeout: Optional[int] = 600,\n        custom_llm_provider: Optional[str] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the [LiteLLM async client](https://github.com/BerriAI/litellm).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            functions: a list of functions to apply to the conversation messages. Defaults to\n                `None`.\n            function_call: the name of the function to call within the conversation. Defaults\n                to `None`.\n            temperature: the temperature to use for the generation. Defaults to `1.0`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: Up to 4 sequences where the LLM API will stop generating further tokens.\n                Defaults to `None`.\n            max_tokens: The maximum number of tokens in the generated completion. Defaults to\n                `None`.\n            presence_penalty: It is used to penalize new tokens based on their existence in the\n                text so far. Defaults to `None`.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the\n                text so far. Defaults to `None`.\n            logit_bias: Used to modify the probability of specific tokens appearing in the\n                completion. Defaults to `None`.\n            user: A unique identifier representing your end-user. This can help the LLM provider\n                to monitor and detect abuse. Defaults to `None`.\n            metadata: Pass in additional metadata to tag your completion calls - eg. prompt\n                version, details, etc. Defaults to `None`.\n            api_base: Base URL for the API. Defaults to `None`.\n            api_version: API version. Defaults to `None`.\n            api_key: API key. Defaults to `None`.\n            model_list: List of api base, version, keys. Defaults to `None`.\n            mock_response: If provided, return a mock completion response for testing or debugging\n                purposes. Defaults to `None`.\n            force_timeout: The maximum execution time in seconds for the completion request.\n                Defaults to `600`.\n            custom_llm_provider: Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable)\n                model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to\n                `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        import litellm\n\n        kwargs = {\n            \"model\": self.model,\n            \"messages\": input,\n            \"n\": num_generations,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"stream\": False,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"metadata\": metadata,\n            \"api_base\": api_base,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"mock_response\": mock_response,\n            \"force_timeout\": force_timeout,\n            \"custom_llm_provider\": custom_llm_provider,\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        async def _call_aclient_until_n_choices() -&gt; List[\"Choices\"]:\n            choices = []\n            while len(choices) &lt; num_generations:\n                completion = await self._aclient(**kwargs)  # type: ignore\n                if not self.structured_output:\n                    completion = completion.choices\n                choices.extend(completion)\n            return choices\n\n        # litellm.drop_params is used to en/disable sending **kwargs parameters to the API if they cannot be used\n        try:\n            litellm.drop_params = False\n            choices = await _call_aclient_until_n_choices()\n        except litellm.exceptions.APIError as e:\n            if \"does not support parameters\" in str(e):\n                litellm.drop_params = True\n                choices = await _call_aclient_until_n_choices()\n            else:\n                raise e\n\n        generations = []\n\n        if self.structured_output:\n            generations.append([choice.model_dump_json() for choice in choices])\n            return generations\n\n        for choice in choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(\n                    f\"Received no response using LiteLLM client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n</code></pre>"},{"location":"api/llm/litellm/#distilabel.llms.litellm.LiteLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/litellm/#distilabel.llms.litellm.LiteLLM.agenerate","title":"<code>agenerate(input, num_generations=1, functions=None, function_call=None, temperature=1.0, top_p=1.0, stop=None, max_tokens=None, presence_penalty=None, frequency_penalty=None, logit_bias=None, user=None, metadata=None, api_base=None, api_version=None, api_key=None, model_list=None, mock_response=None, force_timeout=600, custom_llm_provider=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the LiteLLM async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>functions</code> <code>Optional[List]</code> <p>a list of functions to apply to the conversation messages. Defaults to <code>None</code>.</p> <code>None</code> <code>function_call</code> <code>Optional[str]</code> <p>the name of the function to call within the conversation. Defaults to <code>None</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[Union[str, list]]</code> <p>Up to 4 sequences where the LLM API will stop generating further tokens. Defaults to <code>None</code>.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens in the generated completion. Defaults to <code>None</code>.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>It is used to penalize new tokens based on their existence in the text so far. Defaults to <code>None</code>.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>It is used to penalize new tokens based on their frequency in the text so far. Defaults to <code>None</code>.</p> <code>None</code> <code>logit_bias</code> <code>Optional[dict]</code> <p>Used to modify the probability of specific tokens appearing in the completion. Defaults to <code>None</code>.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse. Defaults to <code>None</code>.</p> <code>None</code> <code>metadata</code> <code>Optional[dict]</code> <p>Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc. Defaults to <code>None</code>.</p> <code>None</code> <code>api_base</code> <code>Optional[str]</code> <p>Base URL for the API. Defaults to <code>None</code>.</p> <code>None</code> <code>api_version</code> <code>Optional[str]</code> <p>API version. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key. Defaults to <code>None</code>.</p> <code>None</code> <code>model_list</code> <code>Optional[list]</code> <p>List of api base, version, keys. Defaults to <code>None</code>.</p> <code>None</code> <code>mock_response</code> <code>Optional[str]</code> <p>If provided, return a mock completion response for testing or debugging purposes. Defaults to <code>None</code>.</p> <code>None</code> <code>force_timeout</code> <code>Optional[int]</code> <p>The maximum execution time in seconds for the completion request. Defaults to <code>600</code>.</p> <code>600</code> <code>custom_llm_provider</code> <code>Optional[str]</code> <p>Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable) model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    functions: Optional[List] = None,\n    function_call: Optional[str] = None,\n    temperature: Optional[float] = 1.0,\n    top_p: Optional[float] = 1.0,\n    stop: Optional[Union[str, list]] = None,\n    max_tokens: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[dict] = None,\n    user: Optional[str] = None,\n    metadata: Optional[dict] = None,\n    api_base: Optional[str] = None,\n    api_version: Optional[str] = None,\n    api_key: Optional[str] = None,\n    model_list: Optional[list] = None,\n    mock_response: Optional[str] = None,\n    force_timeout: Optional[int] = 600,\n    custom_llm_provider: Optional[str] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the [LiteLLM async client](https://github.com/BerriAI/litellm).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        functions: a list of functions to apply to the conversation messages. Defaults to\n            `None`.\n        function_call: the name of the function to call within the conversation. Defaults\n            to `None`.\n        temperature: the temperature to use for the generation. Defaults to `1.0`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: Up to 4 sequences where the LLM API will stop generating further tokens.\n            Defaults to `None`.\n        max_tokens: The maximum number of tokens in the generated completion. Defaults to\n            `None`.\n        presence_penalty: It is used to penalize new tokens based on their existence in the\n            text so far. Defaults to `None`.\n        frequency_penalty: It is used to penalize new tokens based on their frequency in the\n            text so far. Defaults to `None`.\n        logit_bias: Used to modify the probability of specific tokens appearing in the\n            completion. Defaults to `None`.\n        user: A unique identifier representing your end-user. This can help the LLM provider\n            to monitor and detect abuse. Defaults to `None`.\n        metadata: Pass in additional metadata to tag your completion calls - eg. prompt\n            version, details, etc. Defaults to `None`.\n        api_base: Base URL for the API. Defaults to `None`.\n        api_version: API version. Defaults to `None`.\n        api_key: API key. Defaults to `None`.\n        model_list: List of api base, version, keys. Defaults to `None`.\n        mock_response: If provided, return a mock completion response for testing or debugging\n            purposes. Defaults to `None`.\n        force_timeout: The maximum execution time in seconds for the completion request.\n            Defaults to `600`.\n        custom_llm_provider: Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable)\n            model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to\n            `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    import litellm\n\n    kwargs = {\n        \"model\": self.model,\n        \"messages\": input,\n        \"n\": num_generations,\n        \"functions\": functions,\n        \"function_call\": function_call,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"stream\": False,\n        \"stop\": stop,\n        \"max_tokens\": max_tokens,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"logit_bias\": logit_bias,\n        \"user\": user,\n        \"metadata\": metadata,\n        \"api_base\": api_base,\n        \"api_version\": api_version,\n        \"api_key\": api_key,\n        \"model_list\": model_list,\n        \"mock_response\": mock_response,\n        \"force_timeout\": force_timeout,\n        \"custom_llm_provider\": custom_llm_provider,\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    async def _call_aclient_until_n_choices() -&gt; List[\"Choices\"]:\n        choices = []\n        while len(choices) &lt; num_generations:\n            completion = await self._aclient(**kwargs)  # type: ignore\n            if not self.structured_output:\n                completion = completion.choices\n            choices.extend(completion)\n        return choices\n\n    # litellm.drop_params is used to en/disable sending **kwargs parameters to the API if they cannot be used\n    try:\n        litellm.drop_params = False\n        choices = await _call_aclient_until_n_choices()\n    except litellm.exceptions.APIError as e:\n        if \"does not support parameters\" in str(e):\n            litellm.drop_params = True\n            choices = await _call_aclient_until_n_choices()\n        else:\n            raise e\n\n    generations = []\n\n    if self.structured_output:\n        generations.append([choice.model_dump_json() for choice in choices])\n        return generations\n\n    for choice in choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(\n                f\"Received no response using LiteLLM client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"api/llm/litellm/#distilabel.llms.litellm.LiteLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>acompletion</code> LiteLLM client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"\n    Loads the `acompletion` LiteLLM client to benefit from async requests.\n    \"\"\"\n    super().load()\n\n    try:\n        import litellm\n\n        litellm.telemetry = False\n    except ImportError as e:\n        raise ImportError(\n            \"LiteLLM Python client is not installed. Please install it using\"\n            \" `pip install litellm`.\"\n        ) from e\n    self._aclient = litellm.acompletion\n\n    if not self.verbose:\n        litellm.suppress_debug_info = True\n        for key in logging.Logger.manager.loggerDict.keys():\n            if \"litellm\" not in key.lower():\n                continue\n            logging.getLogger(key).setLevel(logging.CRITICAL)\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"litellm\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"api/llm/llamacpp/","title":"LlamaCppLLM","text":""},{"location":"api/llm/llamacpp/#distilabel.llms.llamacpp.LlamaCppLLM","title":"<code>LlamaCppLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>llama.cpp LLM implementation running the Python bindings for the C++ code.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>RuntimeParameter[FilePath]</code> <p>contains the path to the GGUF quantized model, compatible with the installed version of the <code>llama.cpp</code> Python bindings.</p> <code>n_gpu_layers</code> <code>RuntimeParameter[int]</code> <p>the number of layers to use for the GPU. Defaults to <code>-1</code>, meaning that the available GPU device will be used.</p> <code>chat_format</code> <code>Optional[RuntimeParameter[str]]</code> <p>the chat format to use for the model. Defaults to <code>None</code>, which means the Llama format will be used.</p> <code>n_ctx</code> <code>int</code> <p>the context size to use for the model. Defaults to <code>512</code>.</p> <code>n_batch</code> <code>int</code> <p>the prompt processing maximum batch size to use for the model. Defaults to <code>512</code>.</p> <code>seed</code> <code>int</code> <p>random seed to use for the generation. Defaults to <code>4294967295</code>.</p> <code>verbose</code> <code>RuntimeParameter[bool]</code> <p>whether to print verbose output. Defaults to <code>False</code>.</p> <code>structured_output</code> <code>RuntimeParameter[bool]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>extra_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>Llama</code> class of <code>llama_cpp</code> library. Defaults to <code>{}</code>.</p> <code>_model</code> <code>Optional[Llama]</code> <p>the Llama model instance. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>model_path</code>: the path to the GGUF quantized model.</li> <li><code>n_gpu_layers</code>: the number of layers to use for the GPU. Defaults to <code>-1</code>.</li> <li><code>chat_format</code>: the chat format to use for the model. Defaults to <code>None</code>.</li> <li><code>verbose</code>: whether to print verbose output. Defaults to <code>False</code>.</li> <li><code>extra_kwargs</code>: additional dictionary of keyword arguments that will be passed to the     <code>Llama</code> class of <code>llama_cpp</code> library. Defaults to <code>{}</code>.</li> </ul> References <ul> <li><code>llama.cpp</code></li> <li><code>llama-cpp-python</code></li> </ul> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>class LlamaCppLLM(LLM):\n    \"\"\"llama.cpp LLM implementation running the Python bindings for the C++ code.\n\n    Attributes:\n        model_path: contains the path to the GGUF quantized model, compatible with the\n            installed version of the `llama.cpp` Python bindings.\n        n_gpu_layers: the number of layers to use for the GPU. Defaults to `-1`, meaning that\n            the available GPU device will be used.\n        chat_format: the chat format to use for the model. Defaults to `None`, which means the\n            Llama format will be used.\n        n_ctx: the context size to use for the model. Defaults to `512`.\n        n_batch: the prompt processing maximum batch size to use for the model. Defaults to `512`.\n        seed: random seed to use for the generation. Defaults to `4294967295`.\n        verbose: whether to print verbose output. Defaults to `False`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        extra_kwargs: additional dictionary of keyword arguments that will be passed to the\n            `Llama` class of `llama_cpp` library. Defaults to `{}`.\n        _model: the Llama model instance. This attribute is meant to be used internally and\n            should not be accessed directly. It will be set in the `load` method.\n\n    Runtime parameters:\n        - `model_path`: the path to the GGUF quantized model.\n        - `n_gpu_layers`: the number of layers to use for the GPU. Defaults to `-1`.\n        - `chat_format`: the chat format to use for the model. Defaults to `None`.\n        - `verbose`: whether to print verbose output. Defaults to `False`.\n        - `extra_kwargs`: additional dictionary of keyword arguments that will be passed to the\n            `Llama` class of `llama_cpp` library. Defaults to `{}`.\n\n    References:\n        - [`llama.cpp`](https://github.com/ggerganov/llama.cpp)\n        - [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python)\n    \"\"\"\n\n    model_path: RuntimeParameter[FilePath] = Field(\n        default=None, description=\"The path to the GGUF quantized model.\", exclude=True\n    )\n    n_gpu_layers: RuntimeParameter[int] = Field(\n        default=-1,\n        description=\"The number of layers that will be loaded in the GPU.\",\n    )\n    chat_format: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The chat format to use for the model. Defaults to `None`, which means the Llama format will be used.\",\n    )\n\n    n_ctx: int = 512\n    n_batch: int = 512\n    seed: int = 4294967295\n\n    verbose: RuntimeParameter[bool] = Field(\n        default=False,\n        description=\"Whether to print verbose output from llama.cpp library.\",\n    )\n    extra_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"Additional dictionary of keyword arguments that will be passed to the\"\n        \" `Llama` class of `llama_cpp` library. See all the supported arguments at: \"\n        \"https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__init__\",\n    )\n\n    _logits_processor: Optional[\"LogitsProcessorList\"] = PrivateAttr(default=None)\n    _model: Optional[\"Llama\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `Llama` model from the `model_path`.\"\"\"\n        try:\n            from llama_cpp import Llama\n        except ImportError as ie:\n            raise ImportError(\n                \"The `llama_cpp` package is required to use the `LlamaCppLLM` class.\"\n            ) from ie\n\n        self._model = Llama(\n            model_path=self.model_path.as_posix(),  # type: ignore\n            seed=self.seed,\n            n_ctx=self.n_ctx,\n            n_batch=self.n_batch,\n            chat_format=self.chat_format,\n            n_gpu_layers=self.n_gpu_layers,\n            verbose=self.verbose,\n            **self.extra_kwargs,\n        )\n\n        if self.structured_output:\n            self._logits_processor = self._prepare_structured_output(\n                self.structured_output\n            )\n\n        # NOTE: Here because of the custom `logging` interface used, since it will create the logging name\n        # out of the model name, which won't be available until the `Llama` instance is created.\n        super().load()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self._model.model_path  # type: ignore\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        extra_generation_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for the given input using the Llama model.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            extra_generation_kwargs: dictionary with additional arguments to be passed to\n                the `create_chat_completion` method. Reference at\n                https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n\n        batch_outputs = []\n        for input in inputs:\n            outputs = []\n            for _ in range(num_generations):\n                # NOTE(plaguss): There seems to be a bug in how the logits processor\n                # is used. Basically it consumes the FSM internally, and it isn't reinitialized\n                # after each generation, so subsequent calls yield nothing. This is a workaround\n                # until is fixed in the `llama_cpp` or `outlines` libraries.\n                if self.structured_output:\n                    self._logits_processor = self._prepare_structured_output(\n                        self.structured_output\n                    )\n                chat_completions: \"CreateChatCompletionResponse\" = (\n                    self._model.create_chat_completion(  # type: ignore\n                        messages=input,  # type: ignore\n                        max_tokens=max_new_tokens,\n                        frequency_penalty=frequency_penalty,\n                        presence_penalty=presence_penalty,\n                        temperature=temperature,\n                        top_p=top_p,\n                        logits_processor=self._logits_processor,\n                        **(extra_generation_kwargs or {}),\n                    )\n                )\n                outputs.append(chat_completions[\"choices\"][0][\"message\"][\"content\"])\n            batch_outputs.append(outputs)\n        return batch_outputs\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[\"LogitsProcessorList\", None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(structured_output, \"llamacpp\", self._model)\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"api/llm/llamacpp/#distilabel.llms.llamacpp.LlamaCppLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/llamacpp/#distilabel.llms.llamacpp.LlamaCppLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, extra_generation_kwargs=None)</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the Llama model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>extra_generation_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary with additional arguments to be passed to the <code>create_chat_completion</code> method. Reference at https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion</p> <code>None</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    extra_generation_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for the given input using the Llama model.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        extra_generation_kwargs: dictionary with additional arguments to be passed to\n            the `create_chat_completion` method. Reference at\n            https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n\n    batch_outputs = []\n    for input in inputs:\n        outputs = []\n        for _ in range(num_generations):\n            # NOTE(plaguss): There seems to be a bug in how the logits processor\n            # is used. Basically it consumes the FSM internally, and it isn't reinitialized\n            # after each generation, so subsequent calls yield nothing. This is a workaround\n            # until is fixed in the `llama_cpp` or `outlines` libraries.\n            if self.structured_output:\n                self._logits_processor = self._prepare_structured_output(\n                    self.structured_output\n                )\n            chat_completions: \"CreateChatCompletionResponse\" = (\n                self._model.create_chat_completion(  # type: ignore\n                    messages=input,  # type: ignore\n                    max_tokens=max_new_tokens,\n                    frequency_penalty=frequency_penalty,\n                    presence_penalty=presence_penalty,\n                    temperature=temperature,\n                    top_p=top_p,\n                    logits_processor=self._logits_processor,\n                    **(extra_generation_kwargs or {}),\n                )\n            )\n            outputs.append(chat_completions[\"choices\"][0][\"message\"][\"content\"])\n        batch_outputs.append(outputs)\n    return batch_outputs\n</code></pre>"},{"location":"api/llm/llamacpp/#distilabel.llms.llamacpp.LlamaCppLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>Llama</code> model from the <code>model_path</code>.</p> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `Llama` model from the `model_path`.\"\"\"\n    try:\n        from llama_cpp import Llama\n    except ImportError as ie:\n        raise ImportError(\n            \"The `llama_cpp` package is required to use the `LlamaCppLLM` class.\"\n        ) from ie\n\n    self._model = Llama(\n        model_path=self.model_path.as_posix(),  # type: ignore\n        seed=self.seed,\n        n_ctx=self.n_ctx,\n        n_batch=self.n_batch,\n        chat_format=self.chat_format,\n        n_gpu_layers=self.n_gpu_layers,\n        verbose=self.verbose,\n        **self.extra_kwargs,\n    )\n\n    if self.structured_output:\n        self._logits_processor = self._prepare_structured_output(\n            self.structured_output\n        )\n\n    # NOTE: Here because of the custom `logging` interface used, since it will create the logging name\n    # out of the model name, which won't be available until the `Llama` instance is created.\n    super().load()\n</code></pre>"},{"location":"api/llm/mistral/","title":"MistralLLM","text":""},{"location":"api/llm/mistral/#distilabel.llms.mistral.MistralLLM","title":"<code>MistralLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Mistral LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"mistral-tiny\", \"mistral-large\", etc.</p> <code>endpoint</code> <code>str</code> <p>the endpoint to use for the Mistral API. Defaults to \"https://api.mistral.ai\".</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Mistral API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of retries to attempt when a request fails. Defaults to <code>5</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response. Defaults to <code>120</code>.</p> <code>max_concurrent_requests</code> <code>RuntimeParameter[int]</code> <p>the maximum number of concurrent requests to send. Defaults to <code>64</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> <code>_aclient</code> <code>Optional[MistralAsyncClient]</code> <p>the <code>MistralAsyncClient</code> to use for the Mistral API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>api_key</code>: the API key to authenticate the requests to the Mistral API.</li> <li><code>max_retries</code>: the maximum number of retries to attempt when a request fails.     Defaults to <code>5</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response. Defaults to <code>120</code>.</li> <li><code>max_concurrent_requests</code>: the maximum number of concurrent requests to send.     Defaults to <code>64</code>.</li> </ul> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>class MistralLLM(AsyncLLM):\n    \"\"\"Mistral LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"mistral-tiny\", \"mistral-large\", etc.\n        endpoint: the endpoint to use for the Mistral API. Defaults to \"https://api.mistral.ai\".\n        api_key: the API key to authenticate the requests to the Mistral API. Defaults to `None` which\n            means that the value set for the environment variable `OPENAI_API_KEY` will be used, or\n            `None` if not set.\n        max_retries: the maximum number of retries to attempt when a request fails. Defaults to `5`.\n        timeout: the maximum time in seconds to wait for a response. Defaults to `120`.\n        max_concurrent_requests: the maximum number of concurrent requests to send. Defaults\n            to `64`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n        _api_key_env_var: the name of the environment variable to use for the API key. It is meant to\n            be used internally.\n        _aclient: the `MistralAsyncClient` to use for the Mistral API. It is meant to be used internally.\n            Set in the `load` method.\n\n    Runtime parameters:\n        - `api_key`: the API key to authenticate the requests to the Mistral API.\n        - `max_retries`: the maximum number of retries to attempt when a request fails.\n            Defaults to `5`.\n        - `timeout`: the maximum time in seconds to wait for a response. Defaults to `120`.\n        - `max_concurrent_requests`: the maximum number of concurrent requests to send.\n            Defaults to `64`.\n    \"\"\"\n\n    model: str\n    endpoint: str = \"https://api.mistral.ai\"\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_MISTRALAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Mistral API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n    max_concurrent_requests: RuntimeParameter[int] = Field(\n        default=64, description=\"The maximum number of concurrent requests to send.\"\n    )\n\n    _api_key_env_var: str = PrivateAttr(_MISTRALAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"MistralAsyncClient\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `MistralAsyncClient` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from mistralai.async_client import MistralAsyncClient\n        except ImportError as ie:\n            raise ImportError(\n                \"MistralAI Python client is not installed. Please install it using\"\n                \" `pip install mistralai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = MistralAsyncClient(\n            api_key=self.api_key.get_secret_value(),\n            endpoint=self.endpoint,\n            max_retries=self.max_retries,\n            timeout=self.timeout,\n            max_concurrent_requests=self.max_concurrent_requests,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"mistral\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    # TODO: add `num_generations` parameter once Mistral client allows `n` parameter\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        max_new_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the MistralAI async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"max_tokens\": max_new_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n        }\n        generations = []\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n            # TODO:\u00a0This should work just with the _aclient.chat method, but it's not working.\n            # We need to check instructor and see if we can create a PR.\n            completion = await self._aclient.chat.completions.create(**kwargs)\n        else:\n            completion = await self._aclient.chat(**kwargs)\n\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(\n                    f\"Received no response using MistralAI client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n\n    # TODO: remove this function once Mistral client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"api/llm/mistral/#distilabel.llms.mistral.MistralLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/mistral/#distilabel.llms.mistral.MistralLLM.agenerate","title":"<code>agenerate(input, max_new_tokens=None, temperature=None, top_p=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the MistralAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_new_tokens</code> <code>Optional[int]</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    max_new_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the MistralAI async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"max_tokens\": max_new_tokens,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n    }\n    generations = []\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n        # TODO:\u00a0This should work just with the _aclient.chat method, but it's not working.\n        # We need to check instructor and see if we can create a PR.\n        completion = await self._aclient.chat.completions.create(**kwargs)\n    else:\n        completion = await self._aclient.chat(**kwargs)\n\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(\n                f\"Received no response using MistralAI client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"api/llm/mistral/#distilabel.llms.mistral.MistralLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"api/llm/mistral/#distilabel.llms.mistral.MistralLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>MistralAsyncClient</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `MistralAsyncClient` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from mistralai.async_client import MistralAsyncClient\n    except ImportError as ie:\n        raise ImportError(\n            \"MistralAI Python client is not installed. Please install it using\"\n            \" `pip install mistralai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = MistralAsyncClient(\n        api_key=self.api_key.get_secret_value(),\n        endpoint=self.endpoint,\n        max_retries=self.max_retries,\n        timeout=self.timeout,\n        max_concurrent_requests=self.max_concurrent_requests,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"mistral\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"api/llm/ollama/","title":"OllamaLLM","text":""},{"location":"api/llm/ollama/#distilabel.llms.ollama.OllamaLLM","title":"<code>OllamaLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Ollama LLM implementation running the Async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"notus\".</p> <code>host</code> <code>Optional[RuntimeParameter[str]]</code> <p>the Ollama server host.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the timeout for the LLM. Defaults to <code>120</code>.</p> <code>_aclient</code> <code>Optional[AsyncClient]</code> <p>the <code>AsyncClient</code> to use for the Ollama API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>host</code>: the Ollama server host.</li> <li><code>timeout</code>: the client timeout for the Ollama API. Defaults to <code>120</code>.</li> </ul> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>class OllamaLLM(AsyncLLM):\n    \"\"\"Ollama LLM implementation running the Async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"notus\".\n        host: the Ollama server host.\n        timeout: the timeout for the LLM. Defaults to `120`.\n        _aclient: the `AsyncClient` to use for the Ollama API. It is meant to be used internally.\n            Set in the `load` method.\n\n    Runtime parameters:\n        - `host`: the Ollama server host.\n        - `timeout`: the client timeout for the Ollama API. Defaults to `120`.\n    \"\"\"\n\n    model: str\n    host: Optional[RuntimeParameter[str]] = Field(\n        default=None, description=\"The host of the Ollama API.\"\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120, description=\"The timeout for the Ollama API.\"\n    )\n    follow_redirects: bool = True\n\n    _aclient: Optional[\"AsyncClient\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncClient` to use Ollama async API.\"\"\"\n        super().load()\n\n        try:\n            from ollama import AsyncClient\n\n            self._aclient = AsyncClient(\n                host=self.host,\n                timeout=self.timeout,\n                follow_redirects=self.follow_redirects,\n            )\n        except ImportError as e:\n            raise ImportError(\n                \"Ollama Python client is not installed. Please install it using\"\n                \" `pip install ollama`.\"\n            ) from e\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        format: Literal[\"\", \"json\"] = \"\",\n        # TODO: include relevant options from `Options` in `agenerate` method.\n        options: Union[Options, None] = None,\n        keep_alive: Union[bool, None] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Generates a response asynchronously, using the [Ollama Async API definition](https://github.com/ollama/ollama-python).\n\n        Args:\n            input: the input to use for the generation.\n            num_generations: the number of generations to produce. Defaults to `1`.\n            format: the format to use for the generation. Defaults to `\"\"`.\n            options: the options to use for the generation. Defaults to `None`.\n            keep_alive: whether to keep the connection alive. Defaults to `None`.\n\n        Returns:\n            A list of strings as completion for the given input.\n        \"\"\"\n        generations = []\n        # TODO: remove this for-loop and override the `generate` method\n        for _ in range(num_generations):\n            completion = await self._aclient.chat(  # type: ignore\n                model=self.model,\n                messages=input,  # type: ignore\n                stream=False,\n                format=format,\n                options=options,\n                keep_alive=keep_alive,\n            )\n            # TODO: improve error handling\n            generations.append(completion[\"message\"][\"content\"])\n\n        return generations\n</code></pre>"},{"location":"api/llm/ollama/#distilabel.llms.ollama.OllamaLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/ollama/#distilabel.llms.ollama.OllamaLLM.agenerate","title":"<code>agenerate(input, num_generations=1, format='', options=None, keep_alive=None)</code>  <code>async</code>","text":"<p>Generates a response asynchronously, using the Ollama Async API definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>the input to use for the generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to produce. Defaults to <code>1</code>.</p> <code>1</code> <code>format</code> <code>Literal['', 'json']</code> <p>the format to use for the generation. Defaults to <code>\"\"</code>.</p> <code>''</code> <code>options</code> <code>Union[Options, None]</code> <p>the options to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>keep_alive</code> <code>Union[bool, None]</code> <p>whether to keep the connection alive. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings as completion for the given input.</p> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    format: Literal[\"\", \"json\"] = \"\",\n    # TODO: include relevant options from `Options` in `agenerate` method.\n    options: Union[Options, None] = None,\n    keep_alive: Union[bool, None] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Generates a response asynchronously, using the [Ollama Async API definition](https://github.com/ollama/ollama-python).\n\n    Args:\n        input: the input to use for the generation.\n        num_generations: the number of generations to produce. Defaults to `1`.\n        format: the format to use for the generation. Defaults to `\"\"`.\n        options: the options to use for the generation. Defaults to `None`.\n        keep_alive: whether to keep the connection alive. Defaults to `None`.\n\n    Returns:\n        A list of strings as completion for the given input.\n    \"\"\"\n    generations = []\n    # TODO: remove this for-loop and override the `generate` method\n    for _ in range(num_generations):\n        completion = await self._aclient.chat(  # type: ignore\n            model=self.model,\n            messages=input,  # type: ignore\n            stream=False,\n            format=format,\n            options=options,\n            keep_alive=keep_alive,\n        )\n        # TODO: improve error handling\n        generations.append(completion[\"message\"][\"content\"])\n\n    return generations\n</code></pre>"},{"location":"api/llm/ollama/#distilabel.llms.ollama.OllamaLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncClient</code> to use Ollama async API.</p> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncClient` to use Ollama async API.\"\"\"\n    super().load()\n\n    try:\n        from ollama import AsyncClient\n\n        self._aclient = AsyncClient(\n            host=self.host,\n            timeout=self.timeout,\n            follow_redirects=self.follow_redirects,\n        )\n    except ImportError as e:\n        raise ImportError(\n            \"Ollama Python client is not installed. Please install it using\"\n            \" `pip install ollama`.\"\n        ) from e\n</code></pre>"},{"location":"api/llm/openai/","title":"OpenAILLM","text":""},{"location":"api/llm/openai/#distilabel.llms.openai.OpenAILLM","title":"<code>OpenAILLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>OpenAI LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gpt-3.5-turbo\", \"gpt-4\", etc. Supported models can be found here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the OpenAI API requests. Defaults to <code>None</code>, which means that the value set for the environment variable <code>OPENAI_BASE_URL</code> will be used, or \"https://api.openai.com/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of times to retry the request to the API before failing. Defaults to <code>6</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response from the API. Defaults to <code>120</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> Runtime parameters <ul> <li><code>base_url</code>: the base URL to use for the OpenAI API requests. Defaults to <code>None</code>.</li> <li><code>api_key</code>: the API key to authenticate the requests to the OpenAI API. Defaults     to <code>None</code>.</li> <li><code>max_retries</code>: the maximum number of times to retry the request to the API before     failing. Defaults to <code>6</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response from the API. Defaults     to <code>120</code>.</li> </ul> Icon <p><code>:simple-openai:</code></p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>class OpenAILLM(AsyncLLM):\n    \"\"\"OpenAI LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gpt-3.5-turbo\", \"gpt-4\", etc.\n            Supported models can be found [here](https://platform.openai.com/docs/guides/text-generation).\n        base_url: the base URL to use for the OpenAI API requests. Defaults to `None`, which\n            means that the value set for the environment variable `OPENAI_BASE_URL` will\n            be used, or \"https://api.openai.com/v1\" if not set.\n        api_key: the API key to authenticate the requests to the OpenAI API. Defaults to\n            `None` which means that the value set for the environment variable `OPENAI_API_KEY`\n            will be used, or `None` if not set.\n        max_retries: the maximum number of times to retry the request to the API before\n            failing. Defaults to `6`.\n        timeout: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n\n    Runtime parameters:\n        - `base_url`: the base URL to use for the OpenAI API requests. Defaults to `None`.\n        - `api_key`: the API key to authenticate the requests to the OpenAI API. Defaults\n            to `None`.\n        - `max_retries`: the maximum number of times to retry the request to the API before\n            failing. Defaults to `6`.\n        - `timeout`: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n\n    Icon:\n        `:simple-openai:`\n    \"\"\"\n\n    model: str\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"OPENAI_BASE_URL\", \"https://api.openai.com/v1\"\n        ),\n        description=\"The base URL to use for the OpenAI API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_OPENAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the OpenAI API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_OPENAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncOpenAI\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncOpenAI` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from openai import AsyncOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"openai\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        stop: Optional[Union[str, List[str]]] = None,\n        response_format: str = \"text\",\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the OpenAI async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: a string or a list of strings to use as a stop sequence for the generation.\n                Defaults to `None`.\n            response_format: the format of the response to return. Must be one of\n                \"text\" or \"json\". Read the documentation [here](https://platform.openai.com/docs/guides/text-generation/json-mode)\n                for more information on how to use the JSON model from OpenAI. Defaults to `text`.\n\n        Note:\n            If response_format\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        if response_format == \"json\":\n            response_format = \"json_object\"\n        elif response_format != \"text\":\n            raise ValueError(\n                f\"Invalid response format '{response_format}'. Must be either 'text' or 'json'.\"\n            )\n\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"max_tokens\": max_new_tokens,\n            \"n\": num_generations,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"stop\": stop,\n            \"timeout\": 50,\n            \"response_format\": {\"type\": response_format},\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n        completion = await self._aclient.chat.completions.create(**kwargs)\n\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(  # type: ignore\n                    f\"Received no response using OpenAI client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n</code></pre>"},{"location":"api/llm/openai/#distilabel.llms.openai.OpenAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/openai/#distilabel.llms.openai.OpenAILLM.agenerate","title":"<code>agenerate(input, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, stop=None, response_format='text')</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the OpenAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>a string or a list of strings to use as a stop sequence for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>response_format</code> <code>str</code> <p>the format of the response to return. Must be one of \"text\" or \"json\". Read the documentation here for more information on how to use the JSON model from OpenAI. Defaults to <code>text</code>.</p> <code>'text'</code> Note <p>If response_format</p> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stop: Optional[Union[str, List[str]]] = None,\n    response_format: str = \"text\",\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the OpenAI async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: a string or a list of strings to use as a stop sequence for the generation.\n            Defaults to `None`.\n        response_format: the format of the response to return. Must be one of\n            \"text\" or \"json\". Read the documentation [here](https://platform.openai.com/docs/guides/text-generation/json-mode)\n            for more information on how to use the JSON model from OpenAI. Defaults to `text`.\n\n    Note:\n        If response_format\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    if response_format == \"json\":\n        response_format = \"json_object\"\n    elif response_format != \"text\":\n        raise ValueError(\n            f\"Invalid response format '{response_format}'. Must be either 'text' or 'json'.\"\n        )\n\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"max_tokens\": max_new_tokens,\n        \"n\": num_generations,\n        \"frequency_penalty\": frequency_penalty,\n        \"presence_penalty\": presence_penalty,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"stop\": stop,\n        \"timeout\": 50,\n        \"response_format\": {\"type\": response_format},\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n    completion = await self._aclient.chat.completions.create(**kwargs)\n\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(  # type: ignore\n                f\"Received no response using OpenAI client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"api/llm/openai/#distilabel.llms.openai.OpenAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncOpenAI</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncOpenAI` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from openai import AsyncOpenAI\n    except ImportError as ie:\n        raise ImportError(\n            \"OpenAI Python client is not installed. Please install it using\"\n            \" `pip install openai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = AsyncOpenAI(\n        base_url=self.base_url,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"openai\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"api/llm/together/","title":"TogetherLLM","text":""},{"location":"api/llm/together/#distilabel.llms.together.TogetherLLM","title":"<code>TogetherLLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>TogetherLLM LLM implementation running the async API client of OpenAI.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM e.g. \"mistralai/Mixtral-8x7B-Instruct-v0.1\". Supported models can be found here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Together API can be set with <code>TOGETHER_BASE_URL</code>. Defaults to <code>None</code> which means that the value set for the environment variable <code>TOGETHER_BASE_URL</code> will be used, or \"https://api.together.xyz/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Together API. Defaults to <code>None</code> which means that the value set for the environment variable <code>TOGETHER_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> Source code in <code>src/distilabel/llms/together.py</code> <pre><code>class TogetherLLM(OpenAILLM):\n    \"\"\"TogetherLLM LLM implementation running the async API client of OpenAI.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"mistralai/Mixtral-8x7B-Instruct-v0.1\".\n            Supported models can be found [here](https://api.together.xyz/models).\n        base_url: the base URL to use for the Together API can be set with `TOGETHER_BASE_URL`.\n            Defaults to `None` which means that the value set for the environment variable\n            `TOGETHER_BASE_URL` will be used, or \"https://api.together.xyz/v1\" if not set.\n        api_key: the API key to authenticate the requests to the Together API. Defaults to `None`\n            which means that the value set for the environment variable `TOGETHER_API_KEY` will be\n            used, or `None` if not set.\n        _api_key_env_var: the name of the environment variable to use for the API key. It\n            is meant to be used internally.\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"TOGETHER_BASE_URL\", \"https://api.together.xyz/v1\"\n        ),\n        description=\"The base URL to use for the Together API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_TOGETHER_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Together API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_TOGETHER_API_KEY_ENV_VAR_NAME)\n</code></pre>"},{"location":"api/llm/vertexai/","title":"VertexAILLM","text":""},{"location":"api/llm/vertexai/#distilabel.llms.vertexai.VertexAILLM","title":"<code>VertexAILLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>VertexAI LLM implementation running the async API clients for Gemini.</p> <ul> <li>Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini</li> </ul> <p>To use the <code>VertexAILLM</code> is necessary to have configured the Google Cloud authentication using one of these methods:</p> <ul> <li>Setting <code>GOOGLE_CLOUD_CREDENTIALS</code> environment variable</li> <li>Using <code>gcloud auth application-default login</code> command</li> <li>Using <code>vertexai.init</code> function from the <code>google-cloud-aiplatform</code> library</li> </ul> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gemini-1.0-pro\". Supported models.</p> <code>_aclient</code> <code>Optional[GenerativeModel]</code> <p>the <code>GenerativeModel</code> to use for the Vertex AI Gemini API. It is meant to be used internally. Set in the <code>load</code> method.</p> Icon <p><code>:simple-googlecloud:</code></p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>class VertexAILLM(AsyncLLM):\n    \"\"\"VertexAI LLM implementation running the async API clients for Gemini.\n\n    - Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n\n    To use the `VertexAILLM` is necessary to have configured the Google Cloud authentication\n    using one of these methods:\n\n    - Setting `GOOGLE_CLOUD_CREDENTIALS` environment variable\n    - Using `gcloud auth application-default login` command\n    - Using `vertexai.init` function from the `google-cloud-aiplatform` library\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gemini-1.0-pro\". [Supported models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models).\n        _aclient: the `GenerativeModel` to use for the Vertex AI Gemini API. It is meant\n            to be used internally. Set in the `load` method.\n\n    Icon:\n        `:simple-googlecloud:`\n    \"\"\"\n\n    model: str\n\n    _aclient: Optional[\"GenerativeModel\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `GenerativeModel` class which has access to `generate_content_async` to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from vertexai.generative_models import GenerationConfig, GenerativeModel\n\n            self._generation_config_class = GenerationConfig\n        except ImportError as e:\n            raise ImportError(\n                \"vertexai is not installed. Please install it using\"\n                \" `pip install google-cloud-aiplatform`.\"\n            ) from e\n\n        if _is_gemini_model(self.model):\n            self._aclient = GenerativeModel(model_name=self.model)\n        else:\n            raise NotImplementedError(\n                \"`VertexAILLM` is only implemented for `gemini` models that allow for `ChatType` data.\"\n            )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def _chattype_to_content(self, input: \"StandardInput\") -&gt; List[\"Content\"]:\n        \"\"\"Converts a chat type to a list of content items expected by the API.\n\n        Args:\n            input: the chat type to be converted.\n\n        Returns:\n            List[str]: a list of content items expected by the API.\n        \"\"\"\n        from vertexai.generative_models import Content, Part\n\n        contents = []\n        for message in input:\n            if message[\"role\"] not in [\"user\", \"model\"]:\n                raise ValueError(\n                    \"`VertexAILLM only supports the roles 'user' or 'model'.\"\n                )\n            contents.append(\n                Content(\n                    role=message[\"role\"], parts=[Part.from_text(message[\"content\"])]\n                )\n            )\n        return contents\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        max_output_tokens: Optional[int] = None,\n        stop_sequences: Optional[List[str]] = None,\n        safety_settings: Optional[Dict[str, Any]] = None,\n        tools: Optional[List[Dict[str, Any]]] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the [VertexAI async client definition](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            temperature: Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to `None`.\n            top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to `None`.\n            top_k: If specified, top-k sampling will be used. Defaults to `None`.\n            max_output_tokens: The maximum number of output tokens to generate per message. Defaults to `None`.\n            stop_sequences: A list of stop sequences. Defaults to `None`.\n            safety_settings: Safety configuration for returned content from the API. Defaults to `None`.\n            tools: A potential list of tools that can be used by the API. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        from vertexai.generative_models import GenerationConfig\n\n        contents = self._chattype_to_content(input)\n        generations = []\n        # TODO: remove this for-loop and override `generate`\n        for _ in range(num_generations):\n            content = await self._aclient.generate_content_async(  # type: ignore\n                contents=contents,\n                generation_config=GenerationConfig(\n                    candidate_count=1,  # only one candidate allowed per call\n                    temperature=temperature,\n                    top_k=top_k,\n                    top_p=top_p,\n                    max_output_tokens=max_output_tokens,\n                    stop_sequences=stop_sequences,\n                ),\n                safety_settings=safety_settings,\n                tools=tools,\n                stream=False,\n            )\n\n            text = None\n            try:\n                text = content.candidates[0].text\n            except ValueError:\n                self._logger.warning(\n                    f\"Received no response using VertexAI client (model: '{self.model}').\"\n                    f\" Finish reason was: '{content.candidates[0].finish_reason}'.\"\n                )\n            generations.append(text)\n\n        return generations\n</code></pre>"},{"location":"api/llm/vertexai/#distilabel.llms.vertexai.VertexAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/vertexai/#distilabel.llms.vertexai.VertexAILLM.agenerate","title":"<code>agenerate(input, num_generations=1, temperature=None, top_p=None, top_k=None, max_output_tokens=None, stop_sequences=None, safety_settings=None, tools=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the VertexAI async client definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>temperature</code> <code>Optional[float]</code> <p>Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to <code>None</code>.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to <code>None</code>.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>If specified, top-k sampling will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_output_tokens</code> <code>Optional[int]</code> <p>The maximum number of output tokens to generate per message. Defaults to <code>None</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>A list of stop sequences. Defaults to <code>None</code>.</p> <code>None</code> <code>safety_settings</code> <code>Optional[Dict[str, Any]]</code> <p>Safety configuration for returned content from the API. Defaults to <code>None</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Dict[str, Any]]]</code> <p>A potential list of tools that can be used by the API. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    safety_settings: Optional[Dict[str, Any]] = None,\n    tools: Optional[List[Dict[str, Any]]] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the [VertexAI async client definition](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to `None`.\n        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to `None`.\n        top_k: If specified, top-k sampling will be used. Defaults to `None`.\n        max_output_tokens: The maximum number of output tokens to generate per message. Defaults to `None`.\n        stop_sequences: A list of stop sequences. Defaults to `None`.\n        safety_settings: Safety configuration for returned content from the API. Defaults to `None`.\n        tools: A potential list of tools that can be used by the API. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    from vertexai.generative_models import GenerationConfig\n\n    contents = self._chattype_to_content(input)\n    generations = []\n    # TODO: remove this for-loop and override `generate`\n    for _ in range(num_generations):\n        content = await self._aclient.generate_content_async(  # type: ignore\n            contents=contents,\n            generation_config=GenerationConfig(\n                candidate_count=1,  # only one candidate allowed per call\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                max_output_tokens=max_output_tokens,\n                stop_sequences=stop_sequences,\n            ),\n            safety_settings=safety_settings,\n            tools=tools,\n            stream=False,\n        )\n\n        text = None\n        try:\n            text = content.candidates[0].text\n        except ValueError:\n            self._logger.warning(\n                f\"Received no response using VertexAI client (model: '{self.model}').\"\n                f\" Finish reason was: '{content.candidates[0].finish_reason}'.\"\n            )\n        generations.append(text)\n\n    return generations\n</code></pre>"},{"location":"api/llm/vertexai/#distilabel.llms.vertexai.VertexAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>GenerativeModel</code> class which has access to <code>generate_content_async</code> to benefit from async requests.</p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `GenerativeModel` class which has access to `generate_content_async` to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from vertexai.generative_models import GenerationConfig, GenerativeModel\n\n        self._generation_config_class = GenerationConfig\n    except ImportError as e:\n        raise ImportError(\n            \"vertexai is not installed. Please install it using\"\n            \" `pip install google-cloud-aiplatform`.\"\n        ) from e\n\n    if _is_gemini_model(self.model):\n        self._aclient = GenerativeModel(model_name=self.model)\n    else:\n        raise NotImplementedError(\n            \"`VertexAILLM` is only implemented for `gemini` models that allow for `ChatType` data.\"\n        )\n</code></pre>"},{"location":"api/llm/vllm/","title":"vLLM","text":""},{"location":"api/llm/vllm/#distilabel.llms.vllm.vLLM","title":"<code>vLLM</code>","text":"<p>               Bases: <code>LLM</code>, <code>CudaDevicePlacementMixin</code></p> <p><code>vLLM</code> library LLM implementation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model Hugging Face Hub repo id or a path to a directory containing the model weights and configuration files.</p> <code>dtype</code> <code>str</code> <p>the data type to use for the model. Defaults to <code>auto</code>.</p> <code>trust_remote_code</code> <code>bool</code> <p>whether to trust the remote code when loading the model. Defaults to <code>False</code>.</p> <code>quantization</code> <code>Optional[str]</code> <p>the quantization mode to use for the model. Defaults to <code>None</code>.</p> <code>revision</code> <code>Optional[str]</code> <p>the revision of the model to load. Defaults to <code>None</code>.</p> <code>tokenizer</code> <code>Optional[str]</code> <p>the tokenizer Hugging Face Hub repo id or a path to a directory containing the tokenizer files. If not provided, the tokenizer will be loaded from the model directory. Defaults to <code>None</code>.</p> <code>tokenizer_mode</code> <code>Literal['auto', 'slow']</code> <p>the mode to use for the tokenizer. Defaults to <code>auto</code>.</p> <code>tokenizer_revision</code> <code>Optional[str]</code> <p>the revision of the tokenizer to load. Defaults to <code>None</code>.</p> <code>skip_tokenizer_init</code> <code>bool</code> <p>whether to skip the initialization of the tokenizer. Defaults to <code>False</code>.</p> <code>chat_template</code> <code>Optional[str]</code> <p>a chat template that will be used to build the prompts before sending them to the model. If not provided, the chat template defined in the tokenizer config will be used. If not provided and the tokenizer doesn't have a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> <code>structured_output</code> <code>Optional[str]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>seed</code> <code>int</code> <p>the seed to use for the random number generator. Defaults to <code>0</code>.</p> <code>extra_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>LLM</code> class of <code>vllm</code> library. Defaults to <code>{}</code>.</p> <code>_model</code> <code>Optional[LLM]</code> <p>the <code>vLLM</code> model instance. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> <code>_tokenizer</code> <code>Optional[PreTrainedTokenizer]</code> <p>the tokenizer instance used to format the prompt before passing it to the <code>LLM</code>. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> References <ul> <li>https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py</li> </ul> Runtime parameters <ul> <li><code>extra_kwargs</code>: additional dictionary of keyword arguments that will be passed to     the <code>LLM</code> class of <code>vllm</code> library.</li> </ul> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>class vLLM(LLM, CudaDevicePlacementMixin):\n    \"\"\"`vLLM` library LLM implementation.\n\n    Attributes:\n        model: the model Hugging Face Hub repo id or a path to a directory containing the\n            model weights and configuration files.\n        dtype: the data type to use for the model. Defaults to `auto`.\n        trust_remote_code: whether to trust the remote code when loading the model. Defaults\n            to `False`.\n        quantization: the quantization mode to use for the model. Defaults to `None`.\n        revision: the revision of the model to load. Defaults to `None`.\n        tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing\n            the tokenizer files. If not provided, the tokenizer will be loaded from the\n            model directory. Defaults to `None`.\n        tokenizer_mode: the mode to use for the tokenizer. Defaults to `auto`.\n        tokenizer_revision: the revision of the tokenizer to load. Defaults to `None`.\n        skip_tokenizer_init: whether to skip the initialization of the tokenizer. Defaults\n            to `False`.\n        chat_template: a chat template that will be used to build the prompts before\n            sending them to the model. If not provided, the chat template defined in the\n            tokenizer config will be used. If not provided and the tokenizer doesn't have\n            a chat template, then ChatML template will be used. Defaults to `None`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        seed: the seed to use for the random number generator. Defaults to `0`.\n        extra_kwargs: additional dictionary of keyword arguments that will be passed to the\n            `LLM` class of `vllm` library. Defaults to `{}`.\n        _model: the `vLLM` model instance. This attribute is meant to be used internally\n            and should not be accessed directly. It will be set in the `load` method.\n        _tokenizer: the tokenizer instance used to format the prompt before passing it to\n            the `LLM`. This attribute is meant to be used internally and should not be\n            accessed directly. It will be set in the `load` method.\n\n    References:\n        - https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\n\n    Runtime parameters:\n        - `extra_kwargs`: additional dictionary of keyword arguments that will be passed to\n            the `LLM` class of `vllm` library.\n    \"\"\"\n\n    model: str\n    dtype: str = \"auto\"\n    trust_remote_code: bool = False\n    quantization: Optional[str] = None\n    revision: Optional[str] = None\n\n    tokenizer: Optional[str] = None\n    tokenizer_mode: Literal[\"auto\", \"slow\"] = \"auto\"\n    tokenizer_revision: Optional[str] = None\n    skip_tokenizer_init: bool = False\n    chat_template: Optional[str] = None\n\n    seed: int = 0\n\n    extra_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"Additional dictionary of keyword arguments that will be passed to the\"\n        \" `vLLM` class of `vllm` library. See all the supported arguments at: \"\n        \"https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\",\n    )\n\n    _model: Optional[\"_vLLM\"] = PrivateAttr(...)\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(...)\n    _logits_processor: Optional[Callable] = PrivateAttr(default=None)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `vLLM` model using either the path or the Hugging Face Hub repository id.\n        Additionally, this method also sets the `chat_template` for the tokenizer, so as to properly\n        parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the\n        default value is ChatML format, unless explicitly provided.\n        \"\"\"\n        super().load()\n\n        CudaDevicePlacementMixin.load(self)\n\n        try:\n            from vllm import LLM as _vLLM\n            from vllm import SamplingParams as _SamplingParams\n\n            global SamplingParams\n            SamplingParams = _SamplingParams\n        except ImportError as ie:\n            raise ImportError(\n                \"vLLM is not installed. Please install it using `pip install vllm`.\"\n            ) from ie\n\n        self._model = _vLLM(\n            self.model,\n            dtype=self.dtype,\n            trust_remote_code=self.trust_remote_code,\n            quantization=self.quantization,\n            revision=self.revision,\n            tokenizer=self.tokenizer,\n            tokenizer_mode=self.tokenizer_mode,\n            tokenizer_revision=self.tokenizer_revision,\n            skip_tokenizer_init=self.skip_tokenizer_init,\n            seed=self.seed,\n            **self.extra_kwargs,\n        )\n\n        self._tokenizer = self._model.get_tokenizer()  # type: ignore\n        if self.chat_template is not None:\n            self._tokenizer.chat_template = self.chat_template  # type: ignore\n        elif (\n            self._tokenizer.chat_template is None  # type: ignore\n            and self._tokenizer.default_chat_template is None  # type: ignore\n        ):\n            self._tokenizer.chat_template = CHATML_TEMPLATE\n\n        if self.structured_output:\n            self._logits_processor = self._prepare_structured_output(\n                self.structured_output\n            )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def prepare_input(self, input: \"StandardInput\") -&gt; str:\n        \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n        as an OpenAI conversation, and adding the generation prompt.\n        \"\"\"\n        return self._tokenizer.apply_chat_template(  # type: ignore\n            input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,  # type: ignore\n        )\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        extra_sampling_params: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for each input using the text generation\n        pipeline.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            top_k: the top-k value to use for the generation. Defaults to `0`.\n            extra_sampling_params: dictionary with additional arguments to be passed to\n                the `SamplingParams` class from `vllm`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        prepared_inputs = [self.prepare_input(input) for input in inputs]\n\n        if extra_sampling_params is None:\n            extra_sampling_params = {}\n\n        sampling_params = SamplingParams(  # type: ignore\n            n=num_generations,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            max_tokens=max_new_tokens,\n            logits_processors=(\n                [self._logits_processor] if self._logits_processor else None\n            ),\n            **extra_sampling_params,\n        )\n\n        batch_outputs = self._model.generate(  # type: ignore\n            prepared_inputs,\n            sampling_params,\n            use_tqdm=False,  # type: ignore\n        )\n        return [\n            [output.text for output in outputs.outputs] for outputs in batch_outputs\n        ]\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Callable, None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(structured_output, \"vllm\", self._model)\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"api/llm/vllm/#distilabel.llms.vllm.vLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"api/llm/vllm/#distilabel.llms.vllm.vLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, extra_sampling_params=None)</code>","text":"<p>Generates <code>num_generations</code> responses for each input using the text generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to use for the generation. Defaults to <code>0</code>.</p> <code>-1</code> <code>extra_sampling_params</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary with additional arguments to be passed to the <code>SamplingParams</code> class from <code>vllm</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = -1,\n    extra_sampling_params: Optional[Dict[str, Any]] = None,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for each input using the text generation\n    pipeline.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        top_k: the top-k value to use for the generation. Defaults to `0`.\n        extra_sampling_params: dictionary with additional arguments to be passed to\n            the `SamplingParams` class from `vllm`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    prepared_inputs = [self.prepare_input(input) for input in inputs]\n\n    if extra_sampling_params is None:\n        extra_sampling_params = {}\n\n    sampling_params = SamplingParams(  # type: ignore\n        n=num_generations,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        max_tokens=max_new_tokens,\n        logits_processors=(\n            [self._logits_processor] if self._logits_processor else None\n        ),\n        **extra_sampling_params,\n    )\n\n    batch_outputs = self._model.generate(  # type: ignore\n        prepared_inputs,\n        sampling_params,\n        use_tqdm=False,  # type: ignore\n    )\n    return [\n        [output.text for output in outputs.outputs] for outputs in batch_outputs\n    ]\n</code></pre>"},{"location":"api/llm/vllm/#distilabel.llms.vllm.vLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>vLLM</code> model using either the path or the Hugging Face Hub repository id. Additionally, this method also sets the <code>chat_template</code> for the tokenizer, so as to properly parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the default value is ChatML format, unless explicitly provided.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `vLLM` model using either the path or the Hugging Face Hub repository id.\n    Additionally, this method also sets the `chat_template` for the tokenizer, so as to properly\n    parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the\n    default value is ChatML format, unless explicitly provided.\n    \"\"\"\n    super().load()\n\n    CudaDevicePlacementMixin.load(self)\n\n    try:\n        from vllm import LLM as _vLLM\n        from vllm import SamplingParams as _SamplingParams\n\n        global SamplingParams\n        SamplingParams = _SamplingParams\n    except ImportError as ie:\n        raise ImportError(\n            \"vLLM is not installed. Please install it using `pip install vllm`.\"\n        ) from ie\n\n    self._model = _vLLM(\n        self.model,\n        dtype=self.dtype,\n        trust_remote_code=self.trust_remote_code,\n        quantization=self.quantization,\n        revision=self.revision,\n        tokenizer=self.tokenizer,\n        tokenizer_mode=self.tokenizer_mode,\n        tokenizer_revision=self.tokenizer_revision,\n        skip_tokenizer_init=self.skip_tokenizer_init,\n        seed=self.seed,\n        **self.extra_kwargs,\n    )\n\n    self._tokenizer = self._model.get_tokenizer()  # type: ignore\n    if self.chat_template is not None:\n        self._tokenizer.chat_template = self.chat_template  # type: ignore\n    elif (\n        self._tokenizer.chat_template is None  # type: ignore\n        and self._tokenizer.default_chat_template is None  # type: ignore\n    ):\n        self._tokenizer.chat_template = CHATML_TEMPLATE\n\n    if self.structured_output:\n        self._logits_processor = self._prepare_structured_output(\n            self.structured_output\n        )\n</code></pre>"},{"location":"api/llm/vllm/#distilabel.llms.vllm.vLLM.prepare_input","title":"<code>prepare_input(input)</code>","text":"<p>Prepares the input by applying the chat template to the input, which is formatted as an OpenAI conversation, and adding the generation prompt.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>def prepare_input(self, input: \"StandardInput\") -&gt; str:\n    \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n    as an OpenAI conversation, and adding the generation prompt.\n    \"\"\"\n    return self._tokenizer.apply_chat_template(  # type: ignore\n        input,  # type: ignore\n        tokenize=False,\n        add_generation_prompt=True,  # type: ignore\n    )\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline","text":"<p>This section contains the API reference for the <code>distilabel</code> pipelines. For an example on how to use the pipelines, see the Tutorial - Pipeline.</p>"},{"location":"api/pipeline/#distilabel.pipeline.base.BasePipeline","title":"<code>BasePipeline</code>","text":"<p>               Bases: <code>_Serializable</code></p> <p>Base class for a <code>distilabel</code> pipeline.</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>The name of the pipeline.</p> <code>description</code> <p>A description of the pipeline.</p> <code>dag</code> <p>The <code>DAG</code> instance that represents the pipeline.</p> <code>_cache_dir</code> <p>The directory where the pipeline will be cached.</p> <code>_logger</code> <p>The logger instance that will be used by the pipeline.</p> <code>_batch_manager</code> <code>Optional[_BatchManager]</code> <p>The batch manager that will manage the batches received from the steps while running the pipeline. It will be created when the pipeline is run, from scratch or from cache. Defaults to <code>None</code>.</p> <code>_write_buffer</code> <code>Optional[_WriteBuffer]</code> <p>The buffer that will store the data of the leaf steps of the pipeline while running, so the <code>Distiset</code> can be created at the end. It will be created when the pipeline is run. Defaults to <code>None</code>.</p> <code>_logging_parameters</code> <code>Dict[str, Any]</code> <p>A dictionary containing the parameters that will passed to <code>setup_logging</code> function to initialize the logging. Defaults to <code>{}</code>.</p> <code>_fs</code> <code>Optional[AbstractFileSystem]</code> <p>The <code>fsspec</code> filesystem to be used to store the data of the <code>_Batch</code>es passed between the steps. It will be set when the pipeline is run. Defaults to <code>None</code>.</p> <code>_storage_base_path</code> <code>Optional[str]</code> <p>The base path where the data of the <code>_Batch</code>es passed between the steps will be stored. It will be set then the pipeline is run. Defaults to <code>None</code>.</p> <code>_use_fs_to_pass_data</code> <code>bool</code> <p>Whether to use the file system to pass the data of the <code>_Batch</code>es between the steps. Even if this parameter is <code>False</code>, the <code>Batch</code>es received by <code>GlobalStep</code>s will always use the file system to pass the data. Defaults to <code>False</code>.</p> <code>_dry_run</code> <code>bool</code> <p>A flag to indicate if the pipeline is running in dry run mode. Defaults to <code>False</code>.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>class BasePipeline(_Serializable):\n    \"\"\"Base class for a `distilabel` pipeline.\n\n    Attributes:\n        name: The name of the pipeline.\n        description: A description of the pipeline.\n        dag: The `DAG` instance that represents the pipeline.\n        _cache_dir: The directory where the pipeline will be cached.\n        _logger: The logger instance that will be used by the pipeline.\n        _batch_manager: The batch manager that will manage the batches received from the\n            steps while running the pipeline. It will be created when the pipeline is run,\n            from scratch or from cache. Defaults to `None`.\n        _write_buffer: The buffer that will store the data of the leaf steps of the pipeline\n            while running, so the `Distiset` can be created at the end. It will be created\n            when the pipeline is run. Defaults to `None`.\n        _logging_parameters: A dictionary containing the parameters that will passed to\n            `setup_logging` function to initialize the logging. Defaults to `{}`.\n        _fs: The `fsspec` filesystem to be used to store the data of the `_Batch`es passed\n            between the steps. It will be set when the pipeline is run. Defaults to `None`.\n        _storage_base_path: The base path where the data of the `_Batch`es passed between\n            the steps will be stored. It will be set then the pipeline is run. Defaults\n            to `None`.\n        _use_fs_to_pass_data: Whether to use the file system to pass the data of the\n            `_Batch`es between the steps. Even if this parameter is `False`, the `Batch`es\n            received by `GlobalStep`s will always use the file system to pass the data.\n            Defaults to `False`.\n        _dry_run: A flag to indicate if the pipeline is running in dry run mode. Defaults\n            to `False`.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        description: Optional[str] = None,\n        cache_dir: Optional[Union[str, \"PathLike\"]] = None,\n        enable_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the `BasePipeline` instance.\n\n        Args:\n            name: The name of the pipeline.\n            description: A description of the pipeline. Defaults to `None`.\n            cache_dir: A directory where the pipeline will be cached. Defaults to `None`.\n            enable_metadata: Whether to include the distilabel metadata column for the pipeline\n                in the final `Distiset`. It contains metadata used by distilabel, for example\n                the raw outputs of the `LLM` without processing would be here, inside `raw_output_...`\n                field. Defaults to `False`.\n        \"\"\"\n        self.name = name\n        self.description = description\n        self._enable_metadata = enable_metadata\n        self.dag = DAG()\n\n        if cache_dir:\n            self._cache_dir = Path(cache_dir)\n        elif env_cache_dir := os.getenv(\"DISTILABEL_CACHE_DIR\"):\n            self._cache_dir = Path(env_cache_dir)\n        else:\n            self._cache_dir = BASE_CACHE_DIR\n\n        self._logger = logging.getLogger(\"distilabel.pipeline\")\n\n        self._batch_manager: Optional[\"_BatchManager\"] = None\n        self._write_buffer: Optional[\"_WriteBuffer\"] = None\n        self._logging_parameters: Dict[str, Any] = {\n            \"filename\": self._cache_location[\"log_file\"]\n        }\n\n        self._fs: Optional[fsspec.AbstractFileSystem] = None\n        self._storage_base_path: Optional[str] = None\n        self._use_fs_to_pass_data: bool = False\n        self._dry_run: bool = False\n\n    def __enter__(self) -&gt; Self:\n        \"\"\"Set the global pipeline instance when entering a pipeline context.\"\"\"\n        _GlobalPipelineManager.set_pipeline(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback) -&gt; None:\n        \"\"\"Unset the global pipeline instance when exiting a pipeline context.\"\"\"\n        _GlobalPipelineManager.set_pipeline(None)\n\n    def _create_signature(self) -&gt; str:\n        \"\"\"Makes a signature (hash) of a pipeline, using the step ids and the adjacency between them.\n\n        The main use is to find the pipeline in the cache folder.\n\n        Returns:\n            int: Signature of the pipeline.\n        \"\"\"\n        hasher = hashlib.sha1()\n\n        steps_info = []\n        pipeline_dump = self.dump()[\"pipeline\"]\n\n        for step in pipeline_dump[\"steps\"]:\n            step_info = step[\"name\"]\n            for argument, value in sorted(step[STEP_ATTR_NAME].items()):\n                if (argument == TYPE_INFO_KEY) or (value is None):\n                    continue\n\n                if isinstance(value, dict):\n                    # input_mappings/output_mappings\n                    step_info += \"-\".join(\n                        [f\"{str(k)}-{str(v)}\" for k, v in value.items()]\n                    )\n                elif isinstance(value, (list, tuple)):\n                    # runtime_parameters_info\n                    step_info += \"-\".join([str(v) for v in value])\n                elif isinstance(value, (int, str, float)):\n                    # batch_size/name\n                    step_info += str(value)\n                else:\n                    raise ValueError(\n                        f\"Field '{argument}' in step '{step['name']}' has type {type(value)}, explicitly cast the type to 'str'.\"\n                    )\n\n            steps_info.append(step_info)\n\n        connections_info = [\n            f\"{c['from']}-{'-'.join(c['to'])}\" for c in pipeline_dump[\"connections\"]\n        ]\n\n        routing_batch_functions_info = []\n        for function in pipeline_dump[\"routing_batch_functions\"]:\n            step = function[\"step\"]\n            routing_batch_function: \"RoutingBatchFunction\" = self.dag.get_step(step)[\n                ROUTING_BATCH_FUNCTION_ATTR_NAME\n            ]\n            if type_info := routing_batch_function._get_type_info():\n                step += f\"-{type_info}\"\n\n        hasher.update(\n            \",\".join(\n                steps_info + connections_info + routing_batch_functions_info\n            ).encode()\n        )\n\n        return hasher.hexdigest()\n\n    def _set_logging_parameters(self, parameters: Dict[str, Any]) -&gt; None:\n        \"\"\"Set the parameters that will be passed to the `setup_logging` function to\n        initialize the logging.\n\n        Args:\n            parameters: A dictionary with the parameters that will be passed to the\n                `setup_logging` function.\n        \"\"\"\n        self._logging_parameters = parameters\n\n    def run(\n        self,\n        parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        use_cache: bool = True,\n        storage_parameters: Optional[Dict[str, Any]] = None,\n        use_fs_to_pass_data: bool = False,\n    ) -&gt; \"Distiset\":  # type: ignore\n        \"\"\"Run the pipeline. It will set the runtime parameters for the steps and validate\n        the pipeline.\n\n        This method should be extended by the specific pipeline implementation,\n        adding the logic to run the pipeline.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n                the runtime parameters for the step as the value. Defaults to `None`.\n            use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n                `True`.\n            storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n                that will be used to store the data of the `_Batch`es passed between the\n                steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n                `GlobalStep` it will be always used). It must have at least the \"path\" key,\n                and it can contain additional keys depending on the protocol. By default,\n                it will use the local file system and a directory in the cache directory.\n                Defaults to `None`.\n            use_fs_to_pass_data: Whether to use the file system to pass the data of\n                the `_Batch`es between the steps. Even if this parameter is `False`, the\n                `Batch`es received by `GlobalStep`s will always use the file system to\n                pass the data. Defaults to `False`.\n\n        Returns:\n            The `Distiset` created by the pipeline.\n        \"\"\"\n\n        # Set the runtime parameters that will be used during the pipeline execution.\n        # They are used to generate the signature of the pipeline that is used to hit the\n        # cache when the pipeline is run, so it's important to do it first.\n        self._set_runtime_parameters(parameters or {})\n\n        setup_logging(\n            **{\n                **self._logging_parameters,\n                \"filename\": str(self._cache_location[\"log_file\"]),\n            }\n        )\n\n        # Validate the pipeline DAG to check that all the steps are chainable, there are\n        # no missing runtime parameters, batch sizes are correct, etc.\n        self.dag.validate()\n\n        # Load the `_BatchManager` from cache or create one from scratch\n        self._load_batch_manager(use_cache)\n\n        # Setup the filesystem that will be used to pass the data of the `_Batch`es\n        self._setup_fsspec(storage_parameters)\n        self._use_fs_to_pass_data = use_fs_to_pass_data\n\n        if self._dry_run:\n            self._logger.info(\"\ud83c\udf35 Dry run mode\")\n\n        # If the batch manager is not able to generate batches, that means that the loaded\n        # `_BatchManager` from cache didn't have any remaining batches to process i.e.\n        # the previous pipeline execution was completed successfully.\n        if not self._batch_manager.can_generate():  # type: ignore\n            self._logger.info(\n                \"\ud83d\udcbe Loaded batch manager from cache doesn't contain any remaining data.\"\n                \" Returning `Distiset` from cache data...\"\n            )\n            stop_logging()\n            return create_distiset(\n                self._cache_location[\"data\"],\n                pipeline_path=self._cache_location[\"pipeline\"],\n                log_filename_path=self._cache_location[\"log_file\"],\n                enable_metadata=self._enable_metadata,\n            )\n\n        self._setup_write_buffer()\n\n    def dry_run(\n        self,\n        parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        batch_size: int = 1,\n    ) -&gt; \"Distiset\":\n        \"\"\"Do a dry run to test the pipeline runs as expected.\n\n        Running a `Pipeline` in dry run mode will set all the `batch_size` of generator steps\n        to the specified `batch_size`, and run just with a single batch, effectively\n        running the whole pipeline with a single example. The cache will be set to `False`.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n                the runtime parameters for the step as the value. Defaults to `None`.\n            batch_size: The batch size of the unique batch generated by the generators\n                steps of the pipeline. Defaults to `1`.\n\n        Returns:\n            Will return the `Distiset` as the main run method would do.\n        \"\"\"\n        self._dry_run = True\n\n        for step_name in self.dag:\n            step = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n\n            if step.is_generator:\n                if not parameters:\n                    parameters = {}\n                parameters[step_name] = {\"batch_size\": batch_size}\n\n        distiset = self.run(parameters=parameters, use_cache=False)\n\n        self._dry_run = False\n        return distiset\n\n    def get_runtime_parameters_info(self) -&gt; Dict[str, List[Dict[str, Any]]]:\n        \"\"\"Get the runtime parameters for the steps in the pipeline.\n\n        Returns:\n            A dictionary with the step name as the key and a list of dictionaries with\n            the parameter name and the parameter info as the value.\n        \"\"\"\n        runtime_parameters = {}\n        for step_name in self.dag:\n            step: \"_Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n            runtime_parameters[step_name] = step.get_runtime_parameters_info()\n        return runtime_parameters\n\n    def _setup_fsspec(\n        self, storage_parameters: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Setups the `fsspec` filesystem to be used to store the data of the `_Batch`es\n        passed between the steps.\n\n        Args:\n            storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n                that will be used to store the data of the `_Batch`es passed between the\n                steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n                `GlobalStep` it will be always used). It must have at least the \"path\" key,\n                and it can contain additional keys depending on the protocol. By default,\n                it will use the local file system and a directory in the cache directory.\n                Defaults to `None`.\n        \"\"\"\n        if not storage_parameters:\n            self._fs = fsspec.filesystem(\"file\")\n            self._storage_base_path = (\n                f\"file://{self._cache_location['batch_input_data']}\"\n            )\n            return\n\n        if \"path\" not in storage_parameters:\n            raise ValueError(\n                \"The 'path' key must be present in the `storage_parameters` dictionary\"\n                \" if it's not `None`.\"\n            )\n\n        path = storage_parameters.pop(\"path\")\n        protocol = UPath(path).protocol\n\n        self._fs = fsspec.filesystem(protocol, **storage_parameters)\n        self._storage_base_path = path\n\n    def _add_step(self, step: \"_Step\") -&gt; None:\n        \"\"\"Add a step to the pipeline.\n\n        Args:\n            step: The step to be added to the pipeline.\n        \"\"\"\n        self.dag.add_step(step)\n\n    def _add_edge(self, from_step: str, to_step: str) -&gt; None:\n        \"\"\"Add an edge between two steps in the pipeline.\n\n        Args:\n            from_step: The name of the step that will generate the input for `to_step`.\n            to_step: The name of the step that will receive the input from `from_step`.\n        \"\"\"\n        self.dag.add_edge(from_step, to_step)\n\n        # Check if `from_step` has a `routing_batch_function`. If it does, then mark\n        # `to_step` as a step that will receive a routed batch.\n        node = self.dag.get_step(from_step)  # type: ignore\n        routing_batch_function = node.get(ROUTING_BATCH_FUNCTION_ATTR_NAME, None)\n        self.dag.set_step_attr(\n            name=to_step,\n            attr=RECEIVES_ROUTED_BATCHES_ATTR_NAME,\n            value=routing_batch_function is not None,\n        )\n\n    def _add_routing_batch_function(\n        self, step_name: str, routing_batch_function: \"RoutingBatchFunction\"\n    ) -&gt; None:\n        \"\"\"Add a routing batch function to a step.\n\n        Args:\n            step_name: The name of the step that will receive the routed batch.\n            routing_batch_function: The function that will route the batch to the step.\n        \"\"\"\n        self.dag.set_step_attr(\n            name=step_name,\n            attr=ROUTING_BATCH_FUNCTION_ATTR_NAME,\n            value=routing_batch_function,\n        )\n\n    def _set_runtime_parameters(self, parameters: Dict[str, Dict[str, Any]]) -&gt; None:\n        \"\"\"Set the runtime parameters for the steps in the pipeline.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n            the parameter name as the key and the parameter value as the value.\n        \"\"\"\n        step_names = set(self.dag.G)\n        for step_name, step_parameters in parameters.items():\n            if step_name not in step_names:\n                self._logger.warning(\n                    f\"\u2753 Step '{step_name}' provided in `Pipeline.run(parameters={{...}})` not found in the pipeline.\"\n                    f\" Available steps are: {step_names}.\"\n                )\n            else:\n                step: \"_Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n                step.set_runtime_parameters(step_parameters)\n\n    def _model_dump(self, obj: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"Dumps the DAG content to a dict.\n\n        Args:\n            obj (Any): Unused, just kept to match the signature of the parent method.\n            kwargs (Any): Unused, just kept to match the signature of the parent method.\n\n        Returns:\n            Dict[str, Any]: Internal representation of the DAG from networkx in a serializable format.\n        \"\"\"\n        return self.dag.dump()\n\n    def dump(self, **kwargs: Any) -&gt; Dict[str, Any]:\n        return {\n            \"distilabel\": {\"version\": __version__},\n            \"pipeline\": {\n                \"name\": self.name,\n                \"description\": self.description,\n                **super().dump(),\n            },\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; Self:\n        \"\"\"Create a Pipeline from a dict containing the serialized data.\n\n        Note:\n            It's intended for internal use.\n\n        Args:\n            data (Dict[str, Any]): Dictionary containing the serialized data from a Pipeline.\n\n        Returns:\n            BasePipeline: Pipeline recreated from the dictionary info.\n        \"\"\"\n        name = data[\"pipeline\"][\"name\"]\n        description = data[\"pipeline\"].get(\"description\")\n        with cls(name=name, description=description) as pipe:\n            pipe.dag = DAG.from_dict(data[\"pipeline\"])\n        return pipe\n\n    @property\n    def _cache_location(self) -&gt; _CacheLocation:\n        \"\"\"Dictionary containing the the object that will stored and the location,\n        whether it is a filename or a folder.\n\n        Returns:\n            Path: Filenames where the pipeline content will be serialized.\n        \"\"\"\n        folder = self._cache_dir / self.name / self._create_signature()\n        return {\n            \"pipeline\": folder / \"pipeline.yaml\",\n            \"batch_manager\": folder / \"batch_manager.json\",\n            \"data\": folder / \"data\",\n            \"batch_input_data\": folder / \"batch_input_data\",\n            \"log_file\": folder / \"pipeline.log\",\n        }\n\n    def _cache(self) -&gt; None:\n        \"\"\"Saves the `BasePipeline` using the `_cache_filename`.\"\"\"\n        if self._dry_run:\n            return\n\n        self.save(\n            path=self._cache_location[\"pipeline\"],\n            format=self._cache_location[\"pipeline\"].suffix.replace(\".\", \"\"),  # type: ignore\n        )\n        if self._batch_manager is not None:\n            self._batch_manager.cache(self._cache_location[\"batch_manager\"])\n        self._logger.debug(\"Pipeline and batch manager saved to cache.\")\n\n    def _load_batch_manager(self, use_cache: bool = True) -&gt; None:\n        \"\"\"Will try to load the `_BatchManager` from the cache dir if found. Otherwise,\n        it will create one from scratch.\n        \"\"\"\n        batch_manager_cache_loc = self._cache_location[\"batch_manager\"]\n        if use_cache and batch_manager_cache_loc.exists():\n            self._logger.info(\n                f\"\ud83d\udcbe Loading `_BatchManager` from cache: '{batch_manager_cache_loc}'\"\n            )\n            self._batch_manager = _BatchManager.load_from_cache(batch_manager_cache_loc)\n        else:\n            self._batch_manager = _BatchManager.from_dag(self.dag)\n\n    def _setup_write_buffer(self) -&gt; None:\n        \"\"\"Setups the `_WriteBuffer` that will store the data of the leaf steps of the\n        pipeline while running, so the `Distiset` can be created at the end.\n        \"\"\"\n        buffer_data_path = self._cache_location[\"data\"]\n        self._logger.info(f\"\ud83d\udcdd Pipeline data will be written to '{buffer_data_path}'\")\n        self._write_buffer = _WriteBuffer(buffer_data_path, self.dag.leaf_steps)\n\n    def _send_batch_to_step(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Sends a batch to the input queue of a step, writing the data of the batch\n        to the filesystem and setting `batch.data_path` with the path where the data\n        was written (if requiered i.e. the step is a global step or `use_fs_to_pass_data`)\n\n        This method should be extended by the specific pipeline implementation, adding\n        the logic to send the batch to the step.\n\n        Args:\n            batch: The batch to send.\n        \"\"\"\n        self._logger.debug(\n            f\"Setting batch {batch.seq_no} as last batch sent to '{batch.step_name}': {batch}\"\n        )\n        self._batch_manager.set_last_batch_sent(batch)  # type: ignore\n\n        step: \"_Step\" = self.dag.get_step(batch.step_name)[STEP_ATTR_NAME]\n        if not step.is_generator and (step.is_global or self._use_fs_to_pass_data):\n            base_path = UPath(self._storage_base_path) / step.name  # type: ignore\n            self._logger.debug(\n                f\"Writing {batch.seq_no} batch for '{batch.step_name}' step to filesystem: {base_path}\"\n            )\n            batch.write_batch_data_to_fs(self._fs, base_path)  # type: ignore\n\n        self._logger.debug(\n            f\"Sending batch {batch.seq_no} to step '{batch.step_name}': {batch}\"\n        )\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.base.BasePipeline.__enter__","title":"<code>__enter__()</code>","text":"<p>Set the global pipeline instance when entering a pipeline context.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Set the global pipeline instance when entering a pipeline context.\"\"\"\n    _GlobalPipelineManager.set_pipeline(self)\n    return self\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.base.BasePipeline.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Unset the global pipeline instance when exiting a pipeline context.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback) -&gt; None:\n    \"\"\"Unset the global pipeline instance when exiting a pipeline context.\"\"\"\n    _GlobalPipelineManager.set_pipeline(None)\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.base.BasePipeline.__init__","title":"<code>__init__(name, description=None, cache_dir=None, enable_metadata=False)</code>","text":"<p>Initialize the <code>BasePipeline</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pipeline.</p> required <code>description</code> <code>Optional[str]</code> <p>A description of the pipeline. Defaults to <code>None</code>.</p> <code>None</code> <code>cache_dir</code> <code>Optional[Union[str, PathLike]]</code> <p>A directory where the pipeline will be cached. Defaults to <code>None</code>.</p> <code>None</code> <code>enable_metadata</code> <code>bool</code> <p>Whether to include the distilabel metadata column for the pipeline in the final <code>Distiset</code>. It contains metadata used by distilabel, for example the raw outputs of the <code>LLM</code> without processing would be here, inside <code>raw_output_...</code> field. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: Optional[str] = None,\n    cache_dir: Optional[Union[str, \"PathLike\"]] = None,\n    enable_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the `BasePipeline` instance.\n\n    Args:\n        name: The name of the pipeline.\n        description: A description of the pipeline. Defaults to `None`.\n        cache_dir: A directory where the pipeline will be cached. Defaults to `None`.\n        enable_metadata: Whether to include the distilabel metadata column for the pipeline\n            in the final `Distiset`. It contains metadata used by distilabel, for example\n            the raw outputs of the `LLM` without processing would be here, inside `raw_output_...`\n            field. Defaults to `False`.\n    \"\"\"\n    self.name = name\n    self.description = description\n    self._enable_metadata = enable_metadata\n    self.dag = DAG()\n\n    if cache_dir:\n        self._cache_dir = Path(cache_dir)\n    elif env_cache_dir := os.getenv(\"DISTILABEL_CACHE_DIR\"):\n        self._cache_dir = Path(env_cache_dir)\n    else:\n        self._cache_dir = BASE_CACHE_DIR\n\n    self._logger = logging.getLogger(\"distilabel.pipeline\")\n\n    self._batch_manager: Optional[\"_BatchManager\"] = None\n    self._write_buffer: Optional[\"_WriteBuffer\"] = None\n    self._logging_parameters: Dict[str, Any] = {\n        \"filename\": self._cache_location[\"log_file\"]\n    }\n\n    self._fs: Optional[fsspec.AbstractFileSystem] = None\n    self._storage_base_path: Optional[str] = None\n    self._use_fs_to_pass_data: bool = False\n    self._dry_run: bool = False\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.base.BasePipeline.dry_run","title":"<code>dry_run(parameters=None, batch_size=1)</code>","text":"<p>Do a dry run to test the pipeline runs as expected.</p> <p>Running a <code>Pipeline</code> in dry run mode will set all the <code>batch_size</code> of generator steps to the specified <code>batch_size</code>, and run just with a single batch, effectively running the whole pipeline with a single example. The cache will be set to <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a dictionary with the runtime parameters for the step as the value. Defaults to <code>None</code>.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size of the unique batch generated by the generators steps of the pipeline. Defaults to <code>1</code>.</p> <code>1</code> <p>Returns:</p> Type Description <code>Distiset</code> <p>Will return the <code>Distiset</code> as the main run method would do.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def dry_run(\n    self,\n    parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    batch_size: int = 1,\n) -&gt; \"Distiset\":\n    \"\"\"Do a dry run to test the pipeline runs as expected.\n\n    Running a `Pipeline` in dry run mode will set all the `batch_size` of generator steps\n    to the specified `batch_size`, and run just with a single batch, effectively\n    running the whole pipeline with a single example. The cache will be set to `False`.\n\n    Args:\n        parameters: A dictionary with the step name as the key and a dictionary with\n            the runtime parameters for the step as the value. Defaults to `None`.\n        batch_size: The batch size of the unique batch generated by the generators\n            steps of the pipeline. Defaults to `1`.\n\n    Returns:\n        Will return the `Distiset` as the main run method would do.\n    \"\"\"\n    self._dry_run = True\n\n    for step_name in self.dag:\n        step = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n\n        if step.is_generator:\n            if not parameters:\n                parameters = {}\n            parameters[step_name] = {\"batch_size\": batch_size}\n\n    distiset = self.run(parameters=parameters, use_cache=False)\n\n    self._dry_run = False\n    return distiset\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.base.BasePipeline.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a Pipeline from a dict containing the serialized data.</p> Note <p>It's intended for internal use.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing the serialized data from a Pipeline.</p> required <p>Returns:</p> Name Type Description <code>BasePipeline</code> <code>Self</code> <p>Pipeline recreated from the dictionary info.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; Self:\n    \"\"\"Create a Pipeline from a dict containing the serialized data.\n\n    Note:\n        It's intended for internal use.\n\n    Args:\n        data (Dict[str, Any]): Dictionary containing the serialized data from a Pipeline.\n\n    Returns:\n        BasePipeline: Pipeline recreated from the dictionary info.\n    \"\"\"\n    name = data[\"pipeline\"][\"name\"]\n    description = data[\"pipeline\"].get(\"description\")\n    with cls(name=name, description=description) as pipe:\n        pipe.dag = DAG.from_dict(data[\"pipeline\"])\n    return pipe\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.base.BasePipeline.get_runtime_parameters_info","title":"<code>get_runtime_parameters_info()</code>","text":"<p>Get the runtime parameters for the steps in the pipeline.</p> <p>Returns:</p> Type Description <code>Dict[str, List[Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a list of dictionaries with</p> <code>Dict[str, List[Dict[str, Any]]]</code> <p>the parameter name and the parameter info as the value.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def get_runtime_parameters_info(self) -&gt; Dict[str, List[Dict[str, Any]]]:\n    \"\"\"Get the runtime parameters for the steps in the pipeline.\n\n    Returns:\n        A dictionary with the step name as the key and a list of dictionaries with\n        the parameter name and the parameter info as the value.\n    \"\"\"\n    runtime_parameters = {}\n    for step_name in self.dag:\n        step: \"_Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n        runtime_parameters[step_name] = step.get_runtime_parameters_info()\n    return runtime_parameters\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.base.BasePipeline.run","title":"<code>run(parameters=None, use_cache=True, storage_parameters=None, use_fs_to_pass_data=False)</code>","text":"<p>Run the pipeline. It will set the runtime parameters for the steps and validate the pipeline.</p> <p>This method should be extended by the specific pipeline implementation, adding the logic to run the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a dictionary with the runtime parameters for the step as the value. Defaults to <code>None</code>.</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use the cache from previous pipeline runs. Defaults to <code>True</code>.</p> <code>True</code> <code>storage_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary with the storage parameters (<code>fsspec</code> and path) that will be used to store the data of the <code>_Batch</code>es passed between the steps if <code>use_fs_to_pass_data</code> is <code>True</code> (for the batches received by a <code>GlobalStep</code> it will be always used). It must have at least the \"path\" key, and it can contain additional keys depending on the protocol. By default, it will use the local file system and a directory in the cache directory. Defaults to <code>None</code>.</p> <code>None</code> <code>use_fs_to_pass_data</code> <code>bool</code> <p>Whether to use the file system to pass the data of the <code>_Batch</code>es between the steps. Even if this parameter is <code>False</code>, the <code>Batch</code>es received by <code>GlobalStep</code>s will always use the file system to pass the data. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distiset</code> <p>The <code>Distiset</code> created by the pipeline.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def run(\n    self,\n    parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    use_cache: bool = True,\n    storage_parameters: Optional[Dict[str, Any]] = None,\n    use_fs_to_pass_data: bool = False,\n) -&gt; \"Distiset\":  # type: ignore\n    \"\"\"Run the pipeline. It will set the runtime parameters for the steps and validate\n    the pipeline.\n\n    This method should be extended by the specific pipeline implementation,\n    adding the logic to run the pipeline.\n\n    Args:\n        parameters: A dictionary with the step name as the key and a dictionary with\n            the runtime parameters for the step as the value. Defaults to `None`.\n        use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n            `True`.\n        storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n            that will be used to store the data of the `_Batch`es passed between the\n            steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n            `GlobalStep` it will be always used). It must have at least the \"path\" key,\n            and it can contain additional keys depending on the protocol. By default,\n            it will use the local file system and a directory in the cache directory.\n            Defaults to `None`.\n        use_fs_to_pass_data: Whether to use the file system to pass the data of\n            the `_Batch`es between the steps. Even if this parameter is `False`, the\n            `Batch`es received by `GlobalStep`s will always use the file system to\n            pass the data. Defaults to `False`.\n\n    Returns:\n        The `Distiset` created by the pipeline.\n    \"\"\"\n\n    # Set the runtime parameters that will be used during the pipeline execution.\n    # They are used to generate the signature of the pipeline that is used to hit the\n    # cache when the pipeline is run, so it's important to do it first.\n    self._set_runtime_parameters(parameters or {})\n\n    setup_logging(\n        **{\n            **self._logging_parameters,\n            \"filename\": str(self._cache_location[\"log_file\"]),\n        }\n    )\n\n    # Validate the pipeline DAG to check that all the steps are chainable, there are\n    # no missing runtime parameters, batch sizes are correct, etc.\n    self.dag.validate()\n\n    # Load the `_BatchManager` from cache or create one from scratch\n    self._load_batch_manager(use_cache)\n\n    # Setup the filesystem that will be used to pass the data of the `_Batch`es\n    self._setup_fsspec(storage_parameters)\n    self._use_fs_to_pass_data = use_fs_to_pass_data\n\n    if self._dry_run:\n        self._logger.info(\"\ud83c\udf35 Dry run mode\")\n\n    # If the batch manager is not able to generate batches, that means that the loaded\n    # `_BatchManager` from cache didn't have any remaining batches to process i.e.\n    # the previous pipeline execution was completed successfully.\n    if not self._batch_manager.can_generate():  # type: ignore\n        self._logger.info(\n            \"\ud83d\udcbe Loaded batch manager from cache doesn't contain any remaining data.\"\n            \" Returning `Distiset` from cache data...\"\n        )\n        stop_logging()\n        return create_distiset(\n            self._cache_location[\"data\"],\n            pipeline_path=self._cache_location[\"pipeline\"],\n            log_filename_path=self._cache_location[\"log_file\"],\n            enable_metadata=self._enable_metadata,\n        )\n\n    self._setup_write_buffer()\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.local.Pipeline","title":"<code>Pipeline</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Local pipeline implementation using <code>multiprocessing</code>.</p> Source code in <code>src/distilabel/pipeline/local.py</code> <pre><code>class Pipeline(BasePipeline):\n    \"\"\"Local pipeline implementation using `multiprocessing`.\"\"\"\n\n    def run(\n        self,\n        parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        use_cache: bool = True,\n        storage_parameters: Optional[Dict[str, Any]] = None,\n        use_fs_to_pass_data: bool = False,\n    ) -&gt; \"Distiset\":\n        \"\"\"Runs the pipeline.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n                the runtime parameters for the step as the value. Defaults to `None`.\n            use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n                `True`.\n            storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n                that will be used to store the data of the `_Batch`es passed between the\n                steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n                `GlobalStep` it will be always used). It must have at least the \"path\" key,\n                and it can contain additional keys depending on the protocol. By default,\n                it will use the local file system and a directory in the cache directory.\n                Defaults to `None`.\n            use_fs_to_pass_data: Whether to use the file system to pass the data of\n                the `_Batch`es between the steps. Even if this parameter is `False`, the\n                `Batch`es received by `GlobalStep`s will always use the file system to\n                pass the data. Defaults to `False`.\n\n        Returns:\n            The `Distiset` created by the pipeline.\n\n        Raises:\n            RuntimeError: If the pipeline fails to load all the steps.\n        \"\"\"\n        log_queue = mp.Queue()\n\n        self._set_logging_parameters(\n            {\"log_queue\": log_queue, \"filename\": self._cache_location[\"log_file\"]}\n        )\n\n        if distiset := super().run(\n            parameters, use_cache, storage_parameters, use_fs_to_pass_data\n        ):\n            return distiset\n\n        num_processes = len(self.dag)\n        ctx = mp.get_context()  # type: ignore\n        with ctx.Manager() as manager, ctx.Pool(\n            num_processes,\n            initializer=_init_worker,\n            initargs=(log_queue,),\n        ) as pool:\n            self.output_queue: \"Queue[Any]\" = manager.Queue()\n            self.shared_info = self._create_shared_info_dict(manager)\n            self._handle_keyboard_interrupt(manager=manager, pool=pool)\n\n            # Run the steps using the pool of processes\n            self._run_steps_in_loop(pool, manager, self.output_queue, self.shared_info)\n\n            # Wait for all the steps to be loaded correctly\n            if not self._all_steps_loaded():\n                self._write_buffer.close()  # type: ignore\n                self._batch_manager = None\n                stop_logging()\n                raise RuntimeError(\n                    \"Failed to load all the steps. Could not run pipeline.\"\n                ) from _SUBPROCESS_EXCEPTION\n\n            # Send the \"first\" batches to the steps so the batches starts flowing through\n            # the input queues and output queue\n            self._request_initial_batches()\n\n            # Start a loop to receive the output batches from the steps\n            self._run_output_queue_loop_in_thread()\n\n            # Send `None` to steps `input_queue`s just in case some step is still waiting\n            self._notify_steps_to_stop()\n\n        # `Pool.__exit__` has already called `terminate`, `join` the pool to make sure\n        # all the processes have finished\n        pool.join()\n        manager.join()\n\n        self._write_buffer.close()  # type: ignore\n        distiset = create_distiset(\n            self._cache_location[\"data\"],\n            pipeline_path=self._cache_location[\"pipeline\"],\n            log_filename_path=self._cache_location[\"log_file\"],\n            enable_metadata=self._enable_metadata,\n        )\n        stop_logging()\n        return distiset\n\n    def _run_output_queue_loop_in_thread(self) -&gt; None:\n        \"\"\"Runs the output queue loop in a separate thread to receive the output batches\n        from the steps. This is done to avoid the signal handler to block the loop, which\n        would prevent the pipeline from stopping correctly.\"\"\"\n        thread = threading.Thread(target=self._output_queue_loop)\n        thread.start()\n        thread.join()\n\n    def _notify_steps_to_stop(self) -&gt; None:\n        \"\"\"Notifies the steps to stop their infinite running loop by sending `None` to\n        their input queues.\"\"\"\n        for step_name in self.dag:\n            if input_queue := self.dag.get_step(step_name).get(INPUT_QUEUE_ATTR_NAME):\n                input_queue.put(None)\n\n    def _output_queue_loop(self) -&gt; None:\n        \"\"\"Loop to receive the output batches from the steps and manage the flow of the\n        batches through the pipeline.\"\"\"\n        while self._batch_manager.can_generate() and not _STOP_CALLED:  # type: ignore\n            self._logger.debug(\"Waiting for output batch from step...\")\n            if (batch := self.output_queue.get()) is None:\n                self._logger.debug(\"Received `None` from output queue. Breaking loop.\")\n                break\n\n            self._logger.debug(\n                f\"Received batch with seq_no {batch.seq_no} from step '{batch.step_name}'\"\n                f\" from output queue: {batch}\"\n            )\n\n            if batch.data_path:\n                self._logger.debug(\n                    f\"Reading {batch.seq_no} batch data from '{batch.step_name}': '{batch.data_path}'\"\n                )\n                batch.read_batch_data_from_fs()\n\n            if batch.step_name in self.dag.leaf_steps:\n                self._write_buffer.add_batch(batch)  # type: ignore\n\n            # If `_STOP_CALLED` was set to `True` while waiting for the output queue, then\n            # we need to handle the stop of the pipeline and break the loop to avoid\n            # propagating the batches through the pipeline and making the stop process\n            # slower.\n            if _STOP_CALLED:\n                self._handle_batch_on_stop(batch)\n                break\n\n            self._manage_batch_flow(batch)\n\n        if _STOP_CALLED:\n            self._handle_stop()\n\n    def _manage_batch_flow(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Checks if the step that generated the batch has more data in its buffer to\n        generate a new batch. If there's data, then a new batch is sent to the step. If\n        the step has no data in its buffer, then the predecessors generator steps are\n        requested to send a new batch.\n\n        Args:\n            batch: The batch that was processed.\n        \"\"\"\n        assert self._batch_manager, \"Batch manager is not set\"\n\n        # Make sure to send the `LAST_BATCH_SENT_FLAG` to the predecessors of the convergence\n        # step if the batch is the last one, so they stop their processing loop even if\n        # they haven't received the last batch because of the routing function.\n        if self._is_convergence_step(batch.step_name) and batch.last_batch:\n            for step_name in self.dag.get_step_predecessors(batch.step_name):\n                self._send_last_batch_flag_to_step(step_name)\n\n        route_to, routed = self._get_successors(batch)\n\n        # Keep track of the steps that the batch was routed to\n        if routed:\n            batch.batch_routed_to = route_to\n\n        self._register_batch(batch)\n\n        step = self._get_step_from_batch(batch)\n\n        # Add the batch to the successors input buffers\n        for successor in route_to:\n            # Copy batch to avoid modifying the same reference in the batch manager\n            batch_to_add = batch.copy() if len(route_to) &gt; 1 else batch\n\n            self._batch_manager.add_batch(successor, batch_to_add)\n\n            # Check if the step is a generator and if there are successors that need data\n            # from this step. This usually happens when the generator `batch_size` is smaller\n            # than the `input_batch_size` of the successor steps.\n            if (\n                step.is_generator\n                and step.name in self._batch_manager.step_empty_buffers(successor)\n            ):\n                last_batch_sent = self._batch_manager.get_last_batch_sent(step.name)\n                self._send_batch_to_step(last_batch_sent.next_batch())  # type: ignore\n\n            # If successor step has enough data in its buffer to create a new batch, then\n            # send the batch to the step.\n            if new_batch := self._batch_manager.get_batch(successor):\n                self._send_batch_to_step(new_batch)\n\n        if not step.is_generator:\n            # Step (\"this\", the one from which the batch was received) has enough data on its\n            # buffers to create a new batch\n            if new_batch := self._batch_manager.get_batch(step.name):  # type: ignore\n                self._send_batch_to_step(new_batch)\n            else:\n                self._request_more_batches_if_needed(step)\n\n        self._cache()\n\n    def _register_batch(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Registers a batch in the batch manager.\n\n        Args:\n            batch: The batch to register.\n        \"\"\"\n        self._batch_manager.register_batch(batch)  # type: ignore\n        self._logger.debug(\n            f\"Batch {batch.seq_no} from step '{batch.step_name}' registered in batch\"\n            \" manager\"\n        )\n\n    def _get_successors(self, batch: \"_Batch\") -&gt; Tuple[List[str], bool]:\n        \"\"\"Gets the successors and the successors to which the batch has to be routed.\n\n        Args:\n            batch: The batch to which the successors will be determined.\n\n        Returns:\n            The successors to route the batch to and whether the batch was routed using\n            a routing function.\n        \"\"\"\n        node = self.dag.get_step(batch.step_name)\n        step: \"Step\" = node[STEP_ATTR_NAME]\n        successors = list(self.dag.get_step_successors(step.name))  # type: ignore\n        route_to = successors\n\n        # Check if the step has a routing function to send the batch to specific steps\n        if routing_batch_function := node.get(ROUTING_BATCH_FUNCTION_ATTR_NAME):\n            route_to = routing_batch_function(batch, successors)\n            successors_str = \", \".join(f\"'{successor}'\" for successor in route_to)\n            self._logger.info(\n                f\"\ud83d\ude8f Using '{step.name}' routing function to send batch {batch.seq_no} to steps: {successors_str}\"\n            )\n\n        return route_to, route_to != successors\n\n    def _get_step_from_batch(self, batch: \"_Batch\") -&gt; \"Step\":\n        \"\"\"Gets the `Step` instance from a batch.\n\n        Args:\n            batch: The batch to get the step from.\n\n        Returns:\n            The `Step` instance.\n        \"\"\"\n        return self.dag.get_step(batch.step_name)[STEP_ATTR_NAME]\n\n    def _request_more_batches_if_needed(self, step: \"Step\") -&gt; None:\n        \"\"\"Request more batches to the predecessors steps of `step` if needed.\n\n        Args:\n            step: The step of which it has to be checked if more batches are needed from\n                its predecessors.\n        \"\"\"\n        empty_buffers = self._batch_manager.step_empty_buffers(step.name)  # type: ignore\n        for previous_step_name in empty_buffers:\n            if previous_step_name not in self.dag.root_steps:\n                continue\n\n            last_batch = self._batch_manager.get_last_batch_sent(previous_step_name)  # type: ignore\n            if last_batch is None:\n                continue\n\n            self._logger.debug(\n                f\"Step '{step.name}' input buffer for step '{previous_step_name}' is\"\n                \" empty. Requesting new batch...\"\n            )\n            self._send_batch_to_step(last_batch.next_batch())\n\n    def _handle_stop(self) -&gt; None:\n        \"\"\"Handles the stop of the pipeline execution, which will stop the steps from\n        processing more batches and wait for the output queue to be empty, to not lose\n        any data that was already processed by the steps before the stop was called.\"\"\"\n        self._logger.debug(\"Handling stop of the pipeline execution...\")\n\n        # Add the remaining batches in the input queues back to the batch manager\n        for step_name in self.dag:\n            node = self.dag.get_step(step_name)\n            step: \"_Step\" = node[STEP_ATTR_NAME]\n            if step.is_generator:\n                continue\n            if input_queue := node.get(INPUT_QUEUE_ATTR_NAME):\n                while not input_queue.empty():\n                    batch = input_queue.get()\n                    if batch is None:\n                        continue\n                    self._batch_manager.add_batch(  # type: ignore\n                        to_step=step_name, batch=batch, prepend=True\n                    )\n                    self._logger.debug(\n                        f\"Adding batch back to the batch manager: {batch}\"\n                    )\n                input_queue.put(None)\n\n        # Wait for the input queue to be empty, which means that all the steps finished\n        # processing the batches that were sent before the stop flag.\n        for step_name in self.dag:\n            self._wait_step_input_queue_empty(step_name)\n\n        # Consume the output queue until it's empty to not lose any data that was already\n        # processed by the steps before stop was called.\n        while not self.output_queue.empty():\n            batch = self.output_queue.get()\n            if batch is None:\n                continue\n\n            if batch.step_name in self.dag.leaf_steps:\n                self._write_buffer.add_batch(batch)  # type: ignore\n\n            self._handle_batch_on_stop(batch)\n\n        self._cache()\n\n    def _handle_batch_on_stop(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Handles a batch that was received from the output queue when the pipeline was\n        stopped. It will add and register the batch in the batch manager.\n\n        Args:\n            batch: The batch to handle.\n        \"\"\"\n        self._batch_manager.register_batch(batch)  # type: ignore\n        step: \"Step\" = self.dag.get_step(batch.step_name)[STEP_ATTR_NAME]\n        for successor in self.dag.get_step_successors(step.name):  # type: ignore\n            self._batch_manager.add_batch(successor, batch)  # type: ignore\n\n    def _wait_step_input_queue_empty(self, step_name: str) -&gt; Union[\"Queue[Any]\", None]:\n        \"\"\"Waits for the input queue of a step to be empty.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            The input queue of the step if it's not loaded or finished, `None` otherwise.\n        \"\"\"\n        if self._check_step_not_loaded_or_finished(step_name):\n            return None\n\n        if input_queue := self.dag.get_step(step_name).get(INPUT_QUEUE_ATTR_NAME):\n            while input_queue.qsize() != 0:\n                pass\n            return input_queue\n\n    def _create_shared_info_dict(self, manager: \"SyncManager\") -&gt; \"DictProxy[str, Any]\":\n        \"\"\"Creates the shared information dictionary to be used by the processes.\n\n        Args:\n            manager: The manager to create the shared information.\n\n        Returns:\n            The shared information dictionary.\n        \"\"\"\n        # TODO: not very important, but we could use a different lock for each matter\n        return manager.dict(\n            **{\n                _STEPS_LOADED_KEY: manager.list(),\n                _STEPS_LOADED_LOCK_KEY: manager.Lock(),\n                _CUDA_LLM_DEVICE_PLACEMENT_KEY: manager.dict(**{}),\n                _CUDA_LLM_DEVICE_PLACEMENT_LOCK_KEY: manager.Lock(),\n            }\n        )\n\n    def _all_steps_loaded(self) -&gt; bool:\n        \"\"\"Waits for all the steps to load.\n\n        Returns:\n            `True` if all the steps have been loaded correctly, `False` otherwise.\n        \"\"\"\n\n        def _update_all_steps_loaded(steps_loaded: List[str]) -&gt; None:\n            with _STEPS_LOADED_LOCK:\n                _STEPS_LOADED.update(steps_loaded)\n\n        self._logger.info(\"\u23f3 Waiting for all the steps to load...\")\n        previous_message = None\n        while not _STOP_CALLED:\n            with self.shared_info[_STEPS_LOADED_LOCK_KEY]:\n                steps_loaded = self.shared_info[_STEPS_LOADED_KEY]\n                num_steps_loaded = (\n                    len(steps_loaded)\n                    if steps_loaded != [_STEPS_LOADED_ERROR_CODE]\n                    else 0\n                )\n                self._logger.debug(f\"Steps loaded: {steps_loaded}\")\n\n                message = f\"\u23f3 Steps loaded: {num_steps_loaded}/{len(self.dag)}\"\n                if num_steps_loaded &gt; 0 and message != previous_message:\n                    self._logger.info(message)\n                    previous_message = message\n\n                if num_steps_loaded == len(self.dag):\n                    self._logger.info(\"\u2705 All the steps have been loaded!\")\n                    _update_all_steps_loaded(steps_loaded)\n                    return True\n\n                if steps_loaded == [_STEPS_LOADED_ERROR_CODE]:\n                    self._logger.error(\"\u274c Failed to load all the steps\")\n                    _update_all_steps_loaded(steps_loaded)\n                    return False\n\n            time.sleep(2.5)\n\n        return not _STOP_CALLED\n\n    def _request_initial_batches(self) -&gt; None:\n        \"\"\"Requests the initial batches to the generator steps.\"\"\"\n        assert self._batch_manager, \"Batch manager is not set\"\n\n        for step in self._batch_manager._steps.values():\n            if batch := step.get_batch():\n                self._logger.debug(\n                    f\"Sending initial batch to '{step.step_name}' step: {batch}\"\n                )\n                self._send_batch_to_step(batch)\n\n        for step_name in self.dag.root_steps:\n            seq_no = 0\n            if last_batch := self._batch_manager.get_last_batch(step_name):\n                seq_no = last_batch.seq_no + 1\n            batch = _Batch(seq_no=seq_no, step_name=step_name, last_batch=self._dry_run)\n            self._logger.debug(\n                f\"Requesting initial batch to '{step_name}' generator step: {batch}\"\n            )\n            self._send_batch_to_step(batch)\n\n    def _send_batch_to_step(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Sends a batch to the input queue of a step.\n\n        Args:\n            batch: The batch to send.\n        \"\"\"\n        super()._send_batch_to_step(batch)\n        input_queue = self.dag.get_step(batch.step_name)[INPUT_QUEUE_ATTR_NAME]\n        input_queue.put(batch)\n\n    def _is_convergence_step(self, step_name: str) -&gt; None:\n        \"\"\"Checks if a step is a convergence step.\n\n        Args:\n            step_name: The name of the step.\n        \"\"\"\n        return self.dag.get_step(step_name).get(CONVERGENCE_STEP_ATTR_NAME)\n\n    def _send_last_batch_flag_to_step(self, step_name: str) -&gt; None:\n        \"\"\"Sends the `LAST_BATCH_SENT_FLAG` to a step to stop processing batches.\n\n        Args:\n            step_name: The name of the step.\n        \"\"\"\n        batch = self._batch_manager.get_last_batch_sent(step_name)  # type: ignore\n        if batch and batch.last_batch:\n            return\n\n        self._logger.debug(\n            f\"Sending `LAST_BATCH_SENT_FLAG` to '{step_name}' step to stop processing\"\n            \" batches...\"\n        )\n        input_queue = self.dag.get_step(step_name)[INPUT_QUEUE_ATTR_NAME]\n        input_queue.put(LAST_BATCH_SENT_FLAG)\n        self._batch_manager.set_last_batch_flag_sent_to(step_name)  # type: ignore\n\n    def _run_steps_in_loop(\n        self,\n        pool: \"Pool\",\n        manager: \"SyncManager\",\n        output_queue: \"Queue[_Batch]\",\n        shared_info: \"DictProxy[str, Any]\",\n    ) -&gt; None:\n        \"\"\"Using the `pool`, runs the steps in the DAG in an infinite loop waiting for\n        input batches and sending the output batches to the `output_queue`.\n\n        Each `Step` is wrapped in a `_ProcessWrapper`, which will handle the lifecycle of\n        the `Step` and the communication with the `input_queue` and `output_queue`. The\n        `_ProcessWrapper.run` method is the target function of the process.\n\n        Args:\n            pool: The pool of processes.\n            manager: The manager to create the queues.\n            output_queue: The queue to send the output batches.\n            shared_info: The shared information between the processes.\n        \"\"\"\n        for step_name in self.dag:\n            step: \"Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n            input_queue = manager.Queue()\n            self.dag.set_step_attr(step.name, INPUT_QUEUE_ATTR_NAME, input_queue)  # type: ignore\n\n            # Set `pipeline` to `None` as in some Python environments the pipeline is not\n            # picklable and it will raise an error when trying to send the step to the process.\n            # `TypeError: cannot pickle 'code' object`\n            step.pipeline = None\n\n            process_wrapper = _ProcessWrapper(\n                step=step,\n                input_queue=input_queue,\n                output_queue=output_queue,\n                shared_info=shared_info,\n                dry_run=self._dry_run,\n            )\n\n            pool.apply_async(\n                process_wrapper.run,\n                callback=self._finished_callback,\n                error_callback=self._error_callback,\n            )  # type: ignore\n\n    def _error_callback(self, e: BaseException) -&gt; None:\n        \"\"\"Error callback that will be called when an error occurs in a `Step` process.\n\n        Args:\n            e: The exception raised by the process.\n        \"\"\"\n        global _SUBPROCESS_EXCEPTION\n\n        # First we check that the exception is a `_ProcessWrapperException`, otherwise, we\n        # print it out and stop the pipeline, since some errors may be unhandled\n        if not isinstance(e, _ProcessWrapperException):\n            self._logger.error(f\"\u274c Failed with an unhandled exception: {e}\")\n            self._stop()\n            return\n\n        if e.is_load_error:\n            self._logger.error(f\"\u274c Failed to load step '{e.step.name}': {e.message}\")\n            with self.shared_info[_STEPS_LOADED_LOCK_KEY]:\n                self.shared_info[_STEPS_LOADED_KEY] = [_STEPS_LOADED_ERROR_CODE]\n            _SUBPROCESS_EXCEPTION = e.subprocess_exception\n            _SUBPROCESS_EXCEPTION.__traceback__ = tblib.Traceback.from_string(  # type: ignore\n                e.formatted_traceback\n            ).as_traceback()\n            return\n\n        # If the step is global, is not in the last trophic level and has no successors,\n        # then we can ignore the error and continue executing the pipeline\n        step_name: str = e.step.name  # type: ignore\n        if (\n            e.step.is_global\n            and not self.dag.step_in_last_trophic_level(step_name)\n            and list(self.dag.get_step_successors(step_name)) == []\n        ):\n            self._logger.error(\n                f\"\u270b An error occurred when running global step '{step_name}' with no\"\n                \" successors and not in the last trophic level. Pipeline execution can\"\n                f\" continue. Error will be ignored.\"\n            )\n            self._logger.error(f\"Subprocess traceback:\\n\\n{e.formatted_traceback}\")\n            return\n\n        # Global step with successors failed\n        self._logger.error(f\"An error occurred in global step '{step_name}'\")\n        self._logger.error(f\"Subprocess traceback:\\n\\n{e.formatted_traceback}\")\n        self._cache()\n        self._stop()\n\n    def _finished_callback(self, step_name: str) -&gt; None:\n        \"\"\"Callback that will be called when a `Step` process finishes.\n\n        Args:\n            step_name: The name of the step that finished.\n        \"\"\"\n        with _STEPS_FINISHED_LOCK:\n            _STEPS_FINISHED.add(step_name)\n\n    def _check_step_not_loaded_or_finished(self, step_name: str) -&gt; bool:\n        \"\"\"Checks if a step is not loaded or already finished.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            `True` if the step is not loaded or already finished, `False` otherwise.\n        \"\"\"\n        with _STEPS_LOADED_LOCK:\n            if step_name not in _STEPS_LOADED:\n                return True\n\n        with _STEPS_FINISHED_LOCK:\n            if step_name in _STEPS_FINISHED:\n                return True\n\n        return False\n\n    def _stop(\n        self, manager: Optional[\"SyncManager\"] = None, pool: Optional[\"Pool\"] = None\n    ) -&gt; None:\n        \"\"\"Stops the pipeline execution. It will first send `None` to the input queues\n        of all the steps and then wait until the output queue is empty i.e. all the steps\n        finished processing the batches that were sent before the stop flag. Then it will\n        send `None` to the output queue to notify the pipeline to stop.\"\"\"\n\n        global _STOP_CALLED\n\n        with _STOP_CALLED_LOCK:\n            if _STOP_CALLED:\n                global _STOP_CALLS\n                _STOP_CALLS += 1\n                if _STOP_CALLS == 1:\n                    self._logger.warning(\n                        \"\ud83d\uded1 Press again to force the pipeline to stop.\"\n                    )\n                elif _STOP_CALLS &gt; 1:\n                    self._logger.warning(\"\ud83d\uded1 Forcing pipeline interruption.\")\n\n                    if pool:\n                        pool.terminate()\n                        pool.join()\n\n                    if manager:\n                        manager.shutdown()\n                        manager.join()\n\n                    stop_logging()\n\n                    sys.exit(1)\n\n                return\n            _STOP_CALLED = True\n\n        self._logger.debug(f\"Steps loaded before calling `stop`: {_STEPS_LOADED}\")\n        self._logger.info(\n            \"\ud83d\uded1 Stopping pipeline. Waiting for steps to finish processing batches...\"\n        )\n        self._logger.debug(\"Sending `None` to the output queue to notify stop...\")\n        self.output_queue.put(None)\n\n    def _handle_keyboard_interrupt(\n        self, manager: Optional[\"SyncManager\"] = None, pool: Optional[\"Pool\"] = None\n    ) -&gt; None:\n        \"\"\"Handles KeyboardInterrupt signal sent during the Pipeline.run method.\n\n        It will try to call self._stop (if the pipeline didn't started yet, it won't\n        have any effect), and if the pool is already started, will close it before exiting\n        the program.\n        \"\"\"\n\n        def signal_handler(signumber: int, frame: Any) -&gt; None:\n            self._stop(manager=manager, pool=pool)\n\n        signal.signal(signal.SIGINT, signal_handler)\n</code></pre>"},{"location":"api/pipeline/#distilabel.pipeline.local.Pipeline.run","title":"<code>run(parameters=None, use_cache=True, storage_parameters=None, use_fs_to_pass_data=False)</code>","text":"<p>Runs the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a dictionary with the runtime parameters for the step as the value. Defaults to <code>None</code>.</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use the cache from previous pipeline runs. Defaults to <code>True</code>.</p> <code>True</code> <code>storage_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary with the storage parameters (<code>fsspec</code> and path) that will be used to store the data of the <code>_Batch</code>es passed between the steps if <code>use_fs_to_pass_data</code> is <code>True</code> (for the batches received by a <code>GlobalStep</code> it will be always used). It must have at least the \"path\" key, and it can contain additional keys depending on the protocol. By default, it will use the local file system and a directory in the cache directory. Defaults to <code>None</code>.</p> <code>None</code> <code>use_fs_to_pass_data</code> <code>bool</code> <p>Whether to use the file system to pass the data of the <code>_Batch</code>es between the steps. Even if this parameter is <code>False</code>, the <code>Batch</code>es received by <code>GlobalStep</code>s will always use the file system to pass the data. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distiset</code> <p>The <code>Distiset</code> created by the pipeline.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the pipeline fails to load all the steps.</p> Source code in <code>src/distilabel/pipeline/local.py</code> <pre><code>def run(\n    self,\n    parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    use_cache: bool = True,\n    storage_parameters: Optional[Dict[str, Any]] = None,\n    use_fs_to_pass_data: bool = False,\n) -&gt; \"Distiset\":\n    \"\"\"Runs the pipeline.\n\n    Args:\n        parameters: A dictionary with the step name as the key and a dictionary with\n            the runtime parameters for the step as the value. Defaults to `None`.\n        use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n            `True`.\n        storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n            that will be used to store the data of the `_Batch`es passed between the\n            steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n            `GlobalStep` it will be always used). It must have at least the \"path\" key,\n            and it can contain additional keys depending on the protocol. By default,\n            it will use the local file system and a directory in the cache directory.\n            Defaults to `None`.\n        use_fs_to_pass_data: Whether to use the file system to pass the data of\n            the `_Batch`es between the steps. Even if this parameter is `False`, the\n            `Batch`es received by `GlobalStep`s will always use the file system to\n            pass the data. Defaults to `False`.\n\n    Returns:\n        The `Distiset` created by the pipeline.\n\n    Raises:\n        RuntimeError: If the pipeline fails to load all the steps.\n    \"\"\"\n    log_queue = mp.Queue()\n\n    self._set_logging_parameters(\n        {\"log_queue\": log_queue, \"filename\": self._cache_location[\"log_file\"]}\n    )\n\n    if distiset := super().run(\n        parameters, use_cache, storage_parameters, use_fs_to_pass_data\n    ):\n        return distiset\n\n    num_processes = len(self.dag)\n    ctx = mp.get_context()  # type: ignore\n    with ctx.Manager() as manager, ctx.Pool(\n        num_processes,\n        initializer=_init_worker,\n        initargs=(log_queue,),\n    ) as pool:\n        self.output_queue: \"Queue[Any]\" = manager.Queue()\n        self.shared_info = self._create_shared_info_dict(manager)\n        self._handle_keyboard_interrupt(manager=manager, pool=pool)\n\n        # Run the steps using the pool of processes\n        self._run_steps_in_loop(pool, manager, self.output_queue, self.shared_info)\n\n        # Wait for all the steps to be loaded correctly\n        if not self._all_steps_loaded():\n            self._write_buffer.close()  # type: ignore\n            self._batch_manager = None\n            stop_logging()\n            raise RuntimeError(\n                \"Failed to load all the steps. Could not run pipeline.\"\n            ) from _SUBPROCESS_EXCEPTION\n\n        # Send the \"first\" batches to the steps so the batches starts flowing through\n        # the input queues and output queue\n        self._request_initial_batches()\n\n        # Start a loop to receive the output batches from the steps\n        self._run_output_queue_loop_in_thread()\n\n        # Send `None` to steps `input_queue`s just in case some step is still waiting\n        self._notify_steps_to_stop()\n\n    # `Pool.__exit__` has already called `terminate`, `join` the pool to make sure\n    # all the processes have finished\n    pool.join()\n    manager.join()\n\n    self._write_buffer.close()  # type: ignore\n    distiset = create_distiset(\n        self._cache_location[\"data\"],\n        pipeline_path=self._cache_location[\"pipeline\"],\n        log_filename_path=self._cache_location[\"log_file\"],\n        enable_metadata=self._enable_metadata,\n    )\n    stop_logging()\n    return distiset\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/","title":"Routing batch function","text":""},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunc","title":"<code>RoutingBatchFunc = Callable[[List[str]], List[str]]</code>  <code>module-attribute</code>","text":"<p>Type alias for a routing batch function. It takes a list of all the downstream steps and returns a list with the names of the steps that should receive the batch.</p>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction","title":"<code>RoutingBatchFunction</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>_Serializable</code></p> <p>A thin wrapper around a routing batch function that can be used to route batches from one upstream step to specific downstream steps.</p> <p>Attributes:</p> Name Type Description <code>routing_function</code> <code>RoutingBatchFunc</code> <p>The routing function that takes a list of all the downstream steps and returns a list with the names of the steps that should receive the batch.</p> <code>_step</code> <code>Union[_Step, None]</code> <p>The upstream step that is connected to the routing batch function.</p> <code>_routed_batch_registry</code> <code>Dict[str, Dict[int, List[str]]]</code> <p>A dictionary that keeps track of the batches that have been routed to specific downstream steps.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>class RoutingBatchFunction(BaseModel, _Serializable):\n    \"\"\"A thin wrapper around a routing batch function that can be used to route batches\n    from one upstream step to specific downstream steps.\n\n    Attributes:\n        routing_function: The routing function that takes a list of all the downstream steps\n            and returns a list with the names of the steps that should receive the batch.\n        _step: The upstream step that is connected to the routing batch function.\n        _routed_batch_registry: A dictionary that keeps track of the batches that have been\n            routed to specific downstream steps.\n    \"\"\"\n\n    routing_function: RoutingBatchFunc\n    description: Optional[str] = None\n\n    _step: Union[\"_Step\", None] = PrivateAttr(default=None)\n    _routed_batch_registry: Dict[str, Dict[int, List[str]]] = PrivateAttr(\n        default_factory=dict\n    )\n    _factory_function_module: Union[str, None] = PrivateAttr(default=None)\n    _factory_function_name: Union[str, None] = PrivateAttr(default=None)\n    _factory_function_kwargs: Union[Dict[str, Any], None] = PrivateAttr(default=None)\n\n    def route_batch(self, batch: \"_Batch\", steps: List[str]) -&gt; List[str]:\n        \"\"\"Returns a list of selected downstream steps from `steps` to which the `batch`\n        should be routed.\n\n        Args:\n            batch: The batch that should be routed.\n            steps: A list of all the downstream steps that can receive the batch.\n\n        Returns:\n            A list with the names of the steps that should receive the batch.\n        \"\"\"\n        routed_steps = self.routing_function(steps)\n        self._register_routed_batch(batch, routed_steps)\n        return routed_steps\n\n    def set_factory_function(\n        self,\n        factory_function_module: str,\n        factory_function_name: str,\n        factory_function_kwargs: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Sets the factory function that was used to create the `routing_batch_function`.\n\n        Args:\n            factory_function_module: The module name where the factory function is defined.\n            factory_function_name: The name of the factory function that was used to create\n                the `routing_batch_function`.\n            factory_function_kwargs: The keyword arguments that were used when calling the\n                factory function.\n        \"\"\"\n        self._factory_function_module = factory_function_module\n        self._factory_function_name = factory_function_name\n        self._factory_function_kwargs = factory_function_kwargs\n\n    def __call__(self, batch: \"_Batch\", steps: List[str]) -&gt; List[str]:\n        \"\"\"Returns a list of selected downstream steps from `steps` to which the `batch`\n        should be routed.\n\n        Args:\n            batch: The batch that should be routed.\n            steps: A list of all the downstream steps that can receive the batch.\n\n        Returns:\n            A list with the names of the steps that should receive the batch.\n        \"\"\"\n        return self.route_batch(batch, steps)\n\n    def _register_routed_batch(self, batch: \"_Batch\", routed_steps: List[str]) -&gt; None:\n        \"\"\"Registers a batch that has been routed to specific downstream steps.\n\n        Args:\n            batch: The batch that has been routed.\n            routed_steps: The list of downstream steps that have been selected to receive\n                the batch.\n        \"\"\"\n        upstream_step = batch.step_name\n        batch_seq_no = batch.seq_no\n        self._routed_batch_registry.setdefault(upstream_step, {}).setdefault(\n            batch_seq_no, routed_steps\n        )\n\n    def __rshift__(\n        self, other: List[\"DownstreamConnectableSteps\"]\n    ) -&gt; List[\"DownstreamConnectableSteps\"]:\n        \"\"\"Connects a list of dowstream steps to the upstream step of the routing batch\n        function.\n\n        Args:\n            other: A list of downstream steps that should be connected to the upstream step\n                of the routing batch function.\n\n        Returns:\n            The list of downstream steps that have been connected to the upstream step of the\n            routing batch function.\n        \"\"\"\n        if not isinstance(other, list):\n            raise ValueError(\n                f\"Can only set a `routing_batch_function` for a list of steps. Got: {other}.\"\n                \" Please, review the right-hand side of the `routing_batch_function &gt;&gt; other`\"\n                \" expression. It should be\"\n                \" `upstream_step &gt;&gt; routing_batch_function &gt;&gt; [downstream_step_1, dowstream_step_2, ...]`.\"\n            )\n\n        if not self._step:\n            raise ValueError(\n                \"Routing batch function doesn't have an upstream step. Cannot connect downstream\"\n                \" steps before connecting the upstream step. Connect this routing batch\"\n                \" function to an upstream step using the `&gt;&gt;` operator. For example:\"\n                \" `upstream_step &gt;&gt; routing_batch_function &gt;&gt; [downstream_step_1, downstream_step_2, ...]`.\"\n            )\n\n        for step in other:\n            self._step.connect(step)\n        return other\n\n    def dump(self, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"Dumps the routing batch function to a dictionary, and the information of the\n        factory function used to create this routing batch function.\n\n        Args:\n            **kwargs: Additional keyword arguments that should be included in the dump.\n\n        Returns:\n            A dictionary with the routing batch function information and the factory function\n            information.\n        \"\"\"\n        dump_info: Dict[str, Any] = {\"step\": self._step.name}  # type: ignore\n\n        if self.description:\n            dump_info[\"description\"] = self.description\n\n        if type_info := self._get_type_info():\n            dump_info[TYPE_INFO_KEY] = type_info\n\n        return dump_info\n\n    def _get_type_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the information of the factory function used to create the routing batch\n        function.\n\n        Returns:\n            A dictionary with the factory function information.\n        \"\"\"\n\n        type_info = {}\n\n        if self._factory_function_module:\n            type_info[\"module\"] = self._factory_function_module\n\n        if self._factory_function_name:\n            type_info[\"name\"] = self._factory_function_name\n\n        if self._factory_function_kwargs:\n            type_info[\"kwargs\"] = self._factory_function_kwargs\n\n        return type_info\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; Self:\n        \"\"\"Loads a routing batch function from a dictionary. It must contain the information\n        of the factory function used to create the routing batch function.\n\n        Args:\n            data: A dictionary with the routing batch function information and the factory\n                function information.\n        \"\"\"\n        type_info = data.get(TYPE_INFO_KEY)\n        if not type_info:\n            step = data.get(\"step\")\n            raise ValueError(\n                f\"The routing batch function for step '{step}' was created without a factory\"\n                \" function, and it cannot be reconstructed.\"\n            )\n\n        module = type_info.get(\"module\")\n        name = type_info.get(\"name\")\n        kwargs = type_info.get(\"kwargs\")\n\n        if not module or not name or not kwargs:\n            raise ValueError(\n                \"The routing batch function was created with a factory function, but the\"\n                \" information is incomplete. Cannot reconstruct the routing batch function.\"\n            )\n\n        routing_batch_function = _get_module_attr(module=module, name=name)(**kwargs)\n        routing_batch_function.description = data.get(\"description\")\n        routing_batch_function.set_factory_function(\n            factory_function_module=module,\n            factory_function_name=name,\n            factory_function_kwargs=kwargs,\n        )\n\n        return routing_batch_function\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.__call__","title":"<code>__call__(batch, steps)</code>","text":"<p>Returns a list of selected downstream steps from <code>steps</code> to which the <code>batch</code> should be routed.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>_Batch</code> <p>The batch that should be routed.</p> required <code>steps</code> <code>List[str]</code> <p>A list of all the downstream steps that can receive the batch.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list with the names of the steps that should receive the batch.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def __call__(self, batch: \"_Batch\", steps: List[str]) -&gt; List[str]:\n    \"\"\"Returns a list of selected downstream steps from `steps` to which the `batch`\n    should be routed.\n\n    Args:\n        batch: The batch that should be routed.\n        steps: A list of all the downstream steps that can receive the batch.\n\n    Returns:\n        A list with the names of the steps that should receive the batch.\n    \"\"\"\n    return self.route_batch(batch, steps)\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.__rshift__","title":"<code>__rshift__(other)</code>","text":"<p>Connects a list of dowstream steps to the upstream step of the routing batch function.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>List[DownstreamConnectableSteps]</code> <p>A list of downstream steps that should be connected to the upstream step of the routing batch function.</p> required <p>Returns:</p> Type Description <code>List[DownstreamConnectableSteps]</code> <p>The list of downstream steps that have been connected to the upstream step of the</p> <code>List[DownstreamConnectableSteps]</code> <p>routing batch function.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def __rshift__(\n    self, other: List[\"DownstreamConnectableSteps\"]\n) -&gt; List[\"DownstreamConnectableSteps\"]:\n    \"\"\"Connects a list of dowstream steps to the upstream step of the routing batch\n    function.\n\n    Args:\n        other: A list of downstream steps that should be connected to the upstream step\n            of the routing batch function.\n\n    Returns:\n        The list of downstream steps that have been connected to the upstream step of the\n        routing batch function.\n    \"\"\"\n    if not isinstance(other, list):\n        raise ValueError(\n            f\"Can only set a `routing_batch_function` for a list of steps. Got: {other}.\"\n            \" Please, review the right-hand side of the `routing_batch_function &gt;&gt; other`\"\n            \" expression. It should be\"\n            \" `upstream_step &gt;&gt; routing_batch_function &gt;&gt; [downstream_step_1, dowstream_step_2, ...]`.\"\n        )\n\n    if not self._step:\n        raise ValueError(\n            \"Routing batch function doesn't have an upstream step. Cannot connect downstream\"\n            \" steps before connecting the upstream step. Connect this routing batch\"\n            \" function to an upstream step using the `&gt;&gt;` operator. For example:\"\n            \" `upstream_step &gt;&gt; routing_batch_function &gt;&gt; [downstream_step_1, downstream_step_2, ...]`.\"\n        )\n\n    for step in other:\n        self._step.connect(step)\n    return other\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.dump","title":"<code>dump(**kwargs)</code>","text":"<p>Dumps the routing batch function to a dictionary, and the information of the factory function used to create this routing batch function.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that should be included in the dump.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary with the routing batch function information and the factory function</p> <code>Dict[str, Any]</code> <p>information.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def dump(self, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"Dumps the routing batch function to a dictionary, and the information of the\n    factory function used to create this routing batch function.\n\n    Args:\n        **kwargs: Additional keyword arguments that should be included in the dump.\n\n    Returns:\n        A dictionary with the routing batch function information and the factory function\n        information.\n    \"\"\"\n    dump_info: Dict[str, Any] = {\"step\": self._step.name}  # type: ignore\n\n    if self.description:\n        dump_info[\"description\"] = self.description\n\n    if type_info := self._get_type_info():\n        dump_info[TYPE_INFO_KEY] = type_info\n\n    return dump_info\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Loads a routing batch function from a dictionary. It must contain the information of the factory function used to create the routing batch function.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>A dictionary with the routing batch function information and the factory function information.</p> required Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; Self:\n    \"\"\"Loads a routing batch function from a dictionary. It must contain the information\n    of the factory function used to create the routing batch function.\n\n    Args:\n        data: A dictionary with the routing batch function information and the factory\n            function information.\n    \"\"\"\n    type_info = data.get(TYPE_INFO_KEY)\n    if not type_info:\n        step = data.get(\"step\")\n        raise ValueError(\n            f\"The routing batch function for step '{step}' was created without a factory\"\n            \" function, and it cannot be reconstructed.\"\n        )\n\n    module = type_info.get(\"module\")\n    name = type_info.get(\"name\")\n    kwargs = type_info.get(\"kwargs\")\n\n    if not module or not name or not kwargs:\n        raise ValueError(\n            \"The routing batch function was created with a factory function, but the\"\n            \" information is incomplete. Cannot reconstruct the routing batch function.\"\n        )\n\n    routing_batch_function = _get_module_attr(module=module, name=name)(**kwargs)\n    routing_batch_function.description = data.get(\"description\")\n    routing_batch_function.set_factory_function(\n        factory_function_module=module,\n        factory_function_name=name,\n        factory_function_kwargs=kwargs,\n    )\n\n    return routing_batch_function\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.route_batch","title":"<code>route_batch(batch, steps)</code>","text":"<p>Returns a list of selected downstream steps from <code>steps</code> to which the <code>batch</code> should be routed.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>_Batch</code> <p>The batch that should be routed.</p> required <code>steps</code> <code>List[str]</code> <p>A list of all the downstream steps that can receive the batch.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list with the names of the steps that should receive the batch.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def route_batch(self, batch: \"_Batch\", steps: List[str]) -&gt; List[str]:\n    \"\"\"Returns a list of selected downstream steps from `steps` to which the `batch`\n    should be routed.\n\n    Args:\n        batch: The batch that should be routed.\n        steps: A list of all the downstream steps that can receive the batch.\n\n    Returns:\n        A list with the names of the steps that should receive the batch.\n    \"\"\"\n    routed_steps = self.routing_function(steps)\n    self._register_routed_batch(batch, routed_steps)\n    return routed_steps\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.set_factory_function","title":"<code>set_factory_function(factory_function_module, factory_function_name, factory_function_kwargs)</code>","text":"<p>Sets the factory function that was used to create the <code>routing_batch_function</code>.</p> <p>Parameters:</p> Name Type Description Default <code>factory_function_module</code> <code>str</code> <p>The module name where the factory function is defined.</p> required <code>factory_function_name</code> <code>str</code> <p>The name of the factory function that was used to create the <code>routing_batch_function</code>.</p> required <code>factory_function_kwargs</code> <code>Dict[str, Any]</code> <p>The keyword arguments that were used when calling the factory function.</p> required Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def set_factory_function(\n    self,\n    factory_function_module: str,\n    factory_function_name: str,\n    factory_function_kwargs: Dict[str, Any],\n) -&gt; None:\n    \"\"\"Sets the factory function that was used to create the `routing_batch_function`.\n\n    Args:\n        factory_function_module: The module name where the factory function is defined.\n        factory_function_name: The name of the factory function that was used to create\n            the `routing_batch_function`.\n        factory_function_kwargs: The keyword arguments that were used when calling the\n            factory function.\n    \"\"\"\n    self._factory_function_module = factory_function_module\n    self._factory_function_name = factory_function_name\n    self._factory_function_kwargs = factory_function_kwargs\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.routing_batch_function","title":"<code>routing_batch_function(description=None)</code>","text":"<p>Creates a routing batch function that can be used to route batches from one upstream step to specific downstream steps.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>Optional[str]</code> <p>An optional description for the routing batch function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[RoutingBatchFunc], RoutingBatchFunction]</code> <p>A <code>RoutingBatchFunction</code> instance that can be used with the <code>&gt;&gt;</code> operators and with</p> <code>Callable[[RoutingBatchFunc], RoutingBatchFunction]</code> <p>the <code>Pipeline.connect</code> method when defining the pipeline.</p> <p>Example:</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline, routing_batch_function\nfrom distilabel.steps import LoadHubDataset, CombineColumns\n\n\n@routing_batch_function\ndef random_routing_batch(steps: List[str]) -&gt; List[str]:\n    return random.sample(steps, 2)\n\n\nwith Pipeline(name=\"routing-batch-function\") as pipeline:\n    load_data = LoadHubDataset()\n\n    generations = []\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        generations.append(task)\n\n    combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n    load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n</code></pre> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def routing_batch_function(\n    description: Optional[str] = None,\n) -&gt; Callable[[RoutingBatchFunc], RoutingBatchFunction]:\n    \"\"\"Creates a routing batch function that can be used to route batches from one upstream\n    step to specific downstream steps.\n\n    Args:\n        description: An optional description for the routing batch function.\n\n    Returns:\n        A `RoutingBatchFunction` instance that can be used with the `&gt;&gt;` operators and with\n        the `Pipeline.connect` method when defining the pipeline.\n\n    Example:\n\n    ```python\n    from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\n    from distilabel.pipeline import Pipeline, routing_batch_function\n    from distilabel.steps import LoadHubDataset, CombineColumns\n\n\n    @routing_batch_function\n    def random_routing_batch(steps: List[str]) -&gt; List[str]:\n        return random.sample(steps, 2)\n\n\n    with Pipeline(name=\"routing-batch-function\") as pipeline:\n        load_data = LoadHubDataset()\n\n        generations = []\n        for llm in (\n            OpenAILLM(model=\"gpt-4-0125-preview\"),\n            MistralLLM(model=\"mistral-large-2402\"),\n            VertexAILLM(model=\"gemini-1.5-pro\"),\n        ):\n            task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n            generations.append(task)\n\n        combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n        load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n    ```\n    \"\"\"\n\n    def decorator(func: RoutingBatchFunc) -&gt; RoutingBatchFunction:\n        factory_function_name, factory_function_module, factory_function_kwargs = (\n            None,\n            None,\n            None,\n        )\n\n        # Check if `routing_batch_function` was created using a factory function from an installed package\n        stack = inspect.stack()\n        if len(stack) &gt; 2:\n            factory_function_frame_info = stack[1]\n\n            # Function factory path\n            if factory_function_frame_info.function != \"&lt;module&gt;\":\n                factory_function_name = factory_function_frame_info.function\n                factory_function_module = inspect.getmodule(\n                    factory_function_frame_info.frame\n                ).__name__  # type: ignore\n\n                # Function factory kwargs\n                factory_function_kwargs = factory_function_frame_info.frame.f_locals\n\n        routing_batch_function = RoutingBatchFunction(\n            routing_function=func,\n            description=description,\n        )\n\n        if (\n            factory_function_module\n            and factory_function_name\n            and factory_function_kwargs\n        ):\n            routing_batch_function.set_factory_function(\n                factory_function_module=factory_function_module,\n                factory_function_name=factory_function_name,\n                factory_function_kwargs=factory_function_kwargs,\n            )\n\n        return routing_batch_function\n\n    return decorator\n</code></pre>"},{"location":"api/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.sample_n_steps","title":"<code>sample_n_steps(n)</code>","text":"<p>A simple function that creates a routing batch function that samples <code>n</code> steps from the list of all the downstream steps.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of steps to sample from the list of all the downstream steps.</p> required <p>Returns:</p> Type Description <code>RoutingBatchFunction</code> <p>A <code>RoutingBatchFunction</code> instance that can be used with the <code>&gt;&gt;</code> operators and with</p> <code>RoutingBatchFunction</code> <p>the <code>Pipeline.connect</code> method when defining the pipeline.</p> <p>Example:</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline, sample_n_steps\nfrom distilabel.steps import LoadHubDataset, CombineColumns\n\n\nrandom_routing_batch = sample_n_steps(2)\n\n\nwith Pipeline(name=\"routing-batch-function\") as pipeline:\n    load_data = LoadHubDataset()\n\n    generations = []\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        generations.append(task)\n\n    combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n    load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n</code></pre> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def sample_n_steps(n: int) -&gt; RoutingBatchFunction:\n    \"\"\"A simple function that creates a routing batch function that samples `n` steps from\n    the list of all the downstream steps.\n\n    Args:\n        n: The number of steps to sample from the list of all the downstream steps.\n\n    Returns:\n        A `RoutingBatchFunction` instance that can be used with the `&gt;&gt;` operators and with\n        the `Pipeline.connect` method when defining the pipeline.\n\n    Example:\n\n    ```python\n    from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\n    from distilabel.pipeline import Pipeline, sample_n_steps\n    from distilabel.steps import LoadHubDataset, CombineColumns\n\n\n    random_routing_batch = sample_n_steps(2)\n\n\n    with Pipeline(name=\"routing-batch-function\") as pipeline:\n        load_data = LoadHubDataset()\n\n        generations = []\n        for llm in (\n            OpenAILLM(model=\"gpt-4-0125-preview\"),\n            MistralLLM(model=\"mistral-large-2402\"),\n            VertexAILLM(model=\"gemini-1.5-pro\"),\n        ):\n            task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n            generations.append(task)\n\n        combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n        load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n    ```\n    \"\"\"\n\n    @routing_batch_function(\n        description=f\"Sample {n} steps from the list of downstream steps.\"\n    )\n    def sample_n(steps: List[str]) -&gt; List[str]:\n        return random.sample(steps, n)\n\n    return sample_n\n</code></pre>"},{"location":"api/pipeline/typing/","title":"Pipeline Typing","text":""},{"location":"api/pipeline/typing/#distilabel.pipeline.typing.DownstreamConnectable","title":"<code>DownstreamConnectable = Union['Step', 'GlobalStep']</code>  <code>module-attribute</code>","text":"<p>Alias for the <code>Step</code> types that can be connected as downstream steps.</p>"},{"location":"api/pipeline/typing/#distilabel.pipeline.typing.DownstreamConnectableSteps","title":"<code>DownstreamConnectableSteps = TypeVar('DownstreamConnectableSteps', bound=DownstreamConnectable, covariant=True)</code>  <code>module-attribute</code>","text":"<p>Type for the <code>Step</code> types that can be connected as downstream steps.</p>"},{"location":"api/pipeline/typing/#distilabel.pipeline.typing.UpstreamConnectableSteps","title":"<code>UpstreamConnectableSteps = TypeVar('UpstreamConnectableSteps', bound=Union['Step', 'GlobalStep', 'GeneratorStep'])</code>  <code>module-attribute</code>","text":"<p>Type for the <code>Step</code> types that can be connected as upstream steps.</p>"},{"location":"api/pipeline/utils/","title":"Pipeline Utils","text":""},{"location":"api/pipeline/utils/#distilabel.pipeline.utils.combine_dicts","title":"<code>combine_dicts(*inputs, merge_keys, output_merge_keys=None)</code>","text":"<p>Combines multiple list of dictionaries into a single list of dictionaries on the specified <code>merge_keys</code>. If <code>output_merge_keys</code> are provided, then it will also rename <code>merge_keys</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>list of dictionaries to combine.</p> <code>()</code> <code>merge_keys</code> <code>List[str]</code> <p>list of keys to merge on.</p> required <code>output_merge_keys</code> <code>Optional[List[str]]</code> <p>list of keys to rename the merge keys to. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>StepInput</code> <p>A list of dictionaries where the values of the <code>merge_keys</code> are combined into a</p> <code>StepInput</code> <p>list and renamed to <code>output_merge_keys</code>.</p> Source code in <code>src/distilabel/pipeline/utils.py</code> <pre><code>def combine_dicts(\n    *inputs: StepInput,\n    merge_keys: List[str],\n    output_merge_keys: Optional[List[str]] = None,\n) -&gt; StepInput:\n    \"\"\"Combines multiple list of dictionaries into a single list of dictionaries on the\n    specified `merge_keys`. If `output_merge_keys` are provided, then it will also rename\n    `merge_keys`.\n\n    Args:\n        inputs: list of dictionaries to combine.\n        merge_keys: list of keys to merge on.\n        output_merge_keys: list of keys to rename the merge keys to. Defaults to `None`.\n\n    Returns:\n        A list of dictionaries where the values of the `merge_keys` are combined into a\n        list and renamed to `output_merge_keys`.\n    \"\"\"\n    if output_merge_keys is not None and len(output_merge_keys) != len(merge_keys):\n        raise ValueError(\n            \"The length of output_merge_keys must be the same as the length of merge_keys\"\n        )\n    if output_merge_keys is None:\n        output_merge_keys = [f\"merged_{key}\" for key in merge_keys]\n    merge_keys_dict = dict(zip(merge_keys, output_merge_keys))\n\n    result = []\n    # Use zip to iterate over lists based on their index\n    for dicts_at_index in zip(*inputs):\n        combined_dict = {}\n        # Iterate over dicts at the same index\n        for d in dicts_at_index:\n            # Iterate over key-value pairs in each dict\n            for key, value in d.items():\n                # If the key is in the merge_keys, append the value to the existing list\n                if key in merge_keys_dict.keys():\n                    combined_dict.setdefault(merge_keys_dict[key], []).append(value)\n                # If the key is not in the merge_keys, create a new key-value pair\n                else:\n                    combined_dict[key] = value\n        result.append(combined_dict)\n    return result\n</code></pre>"},{"location":"api/step/","title":"Step","text":"<p>This section contains the API reference for the <code>distilabel</code> step, both for the <code>_Step</code> base class and the <code>Step</code> class.</p> <p>For more information and examples on how to use existing steps or create custom ones, please refer to Tutorial - Step.</p>"},{"location":"api/step/#distilabel.steps.base.StepInput","title":"<code>StepInput = Annotated[List[Dict[str, Any]], _STEP_INPUT_ANNOTATION]</code>  <code>module-attribute</code>","text":"<p>StepInput is just an <code>Annotated</code> alias of the typing <code>List[Dict[str, Any]]</code> with extra metadata that allows <code>distilabel</code> to perform validations over the <code>process</code> step method defined in each <code>Step</code></p>"},{"location":"api/step/#distilabel.steps.base._Step","title":"<code>_Step</code>","text":"<p>               Bases: <code>RuntimeParametersMixin</code>, <code>BaseModel</code>, <code>_Serializable</code>, <code>ABC</code></p> <p>Base class for the steps that can be included in a <code>Pipeline</code>.</p> <p>A <code>Step</code> is a class defining some processing logic. The input and outputs for this processing logic are lists of dictionaries with the same keys:</p> <pre><code>```python\n[\n    {\"column1\": \"value1\", \"column2\": \"value2\", ...},\n    {\"column1\": \"value1\", \"column2\": \"value2\", ...},\n    {\"column1\": \"value1\", \"column2\": \"value2\", ...},\n]\n```\n</code></pre> <p>The processing logic is defined in the <code>process</code> method, which depending on the number of previous steps, can receive more than one list of dictionaries, each with the output of the previous steps. In order to make <code>distilabel</code> know where the outputs from the previous steps are, the <code>process</code> function from each <code>Step</code> must have an argument or positional argument annotated with <code>StepInput</code>.</p> <pre><code>```python\nclass StepWithOnePreviousStep(Step):\n    def process(self, inputs: StepInput) -&gt; StepOutput:\n        yield [...]\n\nclass StepWithSeveralPreviousStep(Step):\n    # mind the * to indicate that the argument is a list of StepInput\n    def process(self, inputs: *StepInput) -&gt; StepOutput:\n        yield [...]\n```\n</code></pre> <p>In order to perform static validations and to check that the chaining of the steps in the pipeline is valid, a <code>Step</code> must also define the <code>inputs</code> and <code>outputs</code> properties:</p> <ul> <li><code>inputs</code>: a list of strings with the names of the columns that the step needs as     input. It can be an empty list if the step is a generator step.</li> <li><code>outputs</code>: a list of strings with the names of the columns that the step will     produce as output.</li> </ul> <p>Optionally, a <code>Step</code> can override the <code>load</code> method to perform any initialization logic before the <code>process</code> method is called. For example, to load an LLM, stablish a connection to a database, etc.</p> <p>Finally, the <code>Step</code> class inherits from <code>pydantic.BaseModel</code>, so attributes can be easily defined, validated, serialized and included in the <code>__init__</code> method of the step.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class _Step(RuntimeParametersMixin, BaseModel, _Serializable, ABC):\n    \"\"\"Base class for the steps that can be included in a `Pipeline`.\n\n    A `Step` is a class defining some processing logic. The input and outputs for this\n    processing logic are lists of dictionaries with the same keys:\n\n        ```python\n        [\n            {\"column1\": \"value1\", \"column2\": \"value2\", ...},\n            {\"column1\": \"value1\", \"column2\": \"value2\", ...},\n            {\"column1\": \"value1\", \"column2\": \"value2\", ...},\n        ]\n        ```\n\n    The processing logic is defined in the `process` method, which depending on the\n    number of previous steps, can receive more than one list of dictionaries, each with\n    the output of the previous steps. In order to make `distilabel` know where the outputs\n    from the previous steps are, the `process` function from each `Step` must have an argument\n    or positional argument annotated with `StepInput`.\n\n        ```python\n        class StepWithOnePreviousStep(Step):\n            def process(self, inputs: StepInput) -&gt; StepOutput:\n                yield [...]\n\n        class StepWithSeveralPreviousStep(Step):\n            # mind the * to indicate that the argument is a list of StepInput\n            def process(self, inputs: *StepInput) -&gt; StepOutput:\n                yield [...]\n        ```\n\n    In order to perform static validations and to check that the chaining of the steps\n    in the pipeline is valid, a `Step` must also define the `inputs` and `outputs`\n    properties:\n\n    - `inputs`: a list of strings with the names of the columns that the step needs as\n        input. It can be an empty list if the step is a generator step.\n    - `outputs`: a list of strings with the names of the columns that the step will\n        produce as output.\n\n    Optionally, a `Step` can override the `load` method to perform any initialization\n    logic before the `process` method is called. For example, to load an LLM, stablish a\n    connection to a database, etc.\n\n    Finally, the `Step` class inherits from `pydantic.BaseModel`, so attributes can be easily\n    defined, validated, serialized and included in the `__init__` method of the step.\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        validate_default=True,\n        validate_assignment=True,\n        extra=\"forbid\",\n    )\n\n    name: Optional[str] = Field(default=None, pattern=r\"^[a-zA-Z0-9_-]+$\")\n    pipeline: Any = Field(default=None, exclude=True, repr=False)\n    input_mappings: Dict[str, str] = {}\n    output_mappings: Dict[str, str] = {}\n\n    _built_from_decorator: bool = PrivateAttr(default=False)\n    _logger: \"Logger\" = PrivateAttr(None)\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        from distilabel.pipeline.base import _GlobalPipelineManager\n\n        super().model_post_init(__context)\n\n        if self.pipeline is None:\n            self.pipeline = _GlobalPipelineManager.get_pipeline()\n\n        if self.pipeline is None:\n            _logger = logging.getLogger(f\"distilabel.step.{self.name}\")\n            _logger.warning(\n                f\"Step '{self.name}' hasn't received a pipeline, and it hasn't been\"\n                \" created within a `Pipeline` context. Please, use\"\n                \" `with Pipeline() as pipeline:` and create the step within the context.\"\n            )\n\n        if not self.name:\n            # This must be done before the check for repeated names, but assuming\n            # we are passing the pipeline from the _GlobalPipelineManager, should\n            # be done after that.\n            self.name = _infer_step_name(type(self).__name__, self.pipeline)\n\n        if self.pipeline is not None:\n            # If not set an error will be raised in `Pipeline.run` parent\n            self.pipeline._add_step(self)\n\n    def connect(\n        self,\n        *steps: \"_Step\",\n        routing_batch_function: Optional[\"RoutingBatchFunction\"] = None,\n    ) -&gt; None:\n        \"\"\"Connects the current step to another step in the pipeline, which means that\n        the output of this step will be the input of the other step.\n\n        Args:\n            steps: The steps to connect to the current step.\n            routing_batch_function: A function that receives a list of steps and returns\n                a list of steps to which the output batch generated by this step should be\n                routed. It should be used to define the routing logic of the pipeline. If\n                not provided, the output batch will be routed to all the connected steps.\n                Defaults to `None`.\n        \"\"\"\n        assert self.pipeline is not None\n\n        if routing_batch_function:\n            self._set_routing_batch_function(routing_batch_function)\n\n        for step in steps:\n            self.pipeline._add_edge(from_step=self.name, to_step=step.name)  # type: ignore\n\n    def _set_routing_batch_function(\n        self, routing_batch_function: \"RoutingBatchFunction\"\n    ) -&gt; None:\n        \"\"\"Sets a routing batch function for the batches generated by this step, so they\n        get routed to specific downstream steps.\n\n        Args:\n            routing_batch_function: The routing batch function that will be used to route\n                the batches generated by this step.\n        \"\"\"\n        self.pipeline._add_routing_batch_function(\n            step_name=self.name,  # type: ignore\n            routing_batch_function=routing_batch_function,\n        )\n        routing_batch_function._step = self\n\n    @overload\n    def __rshift__(self, other: \"RoutingBatchFunction\") -&gt; \"RoutingBatchFunction\": ...\n\n    @overload\n    def __rshift__(\n        self, other: List[\"DownstreamConnectableSteps\"]\n    ) -&gt; List[\"DownstreamConnectableSteps\"]: ...\n\n    @overload\n    def __rshift__(self, other: \"DownstreamConnectable\") -&gt; \"DownstreamConnectable\": ...\n\n    def __rshift__(\n        self,\n        other: Union[\n            \"DownstreamConnectable\",\n            \"RoutingBatchFunction\",\n            List[\"DownstreamConnectableSteps\"],\n        ],\n    ) -&gt; Union[\n        \"DownstreamConnectable\",\n        \"RoutingBatchFunction\",\n        List[\"DownstreamConnectableSteps\"],\n    ]:\n        \"\"\"Allows using the `&gt;&gt;` operator to connect steps in the pipeline.\n\n        Args:\n            other: The step to connect, a list of steps to connect to or a routing batch\n                function to be set for the step.\n\n        Returns:\n            The connected step, the list of connected steps or the routing batch function.\n\n        Example:\n            ```python\n            step1 &gt;&gt; step2\n            # Would be equivalent to:\n            step1.connect(step2)\n\n            # It also allows to connect a list of steps\n            step1 &gt;&gt; [step2, step3]\n            ```\n        \"\"\"\n        # Here to avoid circular imports\n        from distilabel.pipeline.routing_batch_function import RoutingBatchFunction\n\n        if isinstance(other, list):\n            self.connect(*other)\n            return other\n\n        if isinstance(other, RoutingBatchFunction):\n            self._set_routing_batch_function(other)\n            return other\n\n        self.connect(other)\n        return other\n\n    def __rrshift__(self, other: List[\"UpstreamConnectableSteps\"]) -&gt; Self:\n        \"\"\"Allows using the [step1, step2] &gt;&gt; step3 operator to connect a list of steps in the pipeline\n        to a single step, as the list doesn't have the __rshift__ operator.\n\n        Args:\n            other: The step to connect to.\n\n        Returns:\n            The connected step\n\n        Example:\n            ```python\n            [step2, step3] &gt;&gt; step1\n            # Would be equivalent to:\n            step2.connect(step1)\n            step3.connect(step1)\n            ```\n        \"\"\"\n        for o in other:\n            o.connect(self)\n        return self\n\n    def load(self) -&gt; None:\n        \"\"\"Method to perform any initialization logic before the `process` method is\n        called. For example, to load an LLM, stablish a connection to a database, etc.\n        \"\"\"\n        self._logger = logging.getLogger(f\"distilabel.step.{self.name}\")\n\n    @property\n    def is_generator(self) -&gt; bool:\n        \"\"\"Whether the step is a generator step or not.\n\n        Returns:\n            `True` if the step is a generator step, `False` otherwise.\n        \"\"\"\n        return isinstance(self, GeneratorStep)\n\n    @property\n    def is_global(self) -&gt; bool:\n        \"\"\"Whether the step is a global step or not.\n\n        Returns:\n            `True` if the step is a global step, `False` otherwise.\n        \"\"\"\n        return isinstance(self, GlobalStep)\n\n    @property\n    def is_normal(self) -&gt; bool:\n        \"\"\"Whether the step is a normal step or not.\n\n        Returns:\n            `True` if the step is a normal step, `False` otherwise.\n        \"\"\"\n        return not self.is_generator and not self.is_global\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of strings with the names of the columns that the step needs as input.\n\n        Returns:\n            List of strings with the names of the columns that the step needs as input.\n        \"\"\"\n        return []\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of strings with the names of the columns that the step will produce as\n        output.\n\n        Returns:\n            List of strings with the names of the columns that the step will produce as\n            output.\n        \"\"\"\n        return []\n\n    @cached_property\n    def process_parameters(self) -&gt; List[inspect.Parameter]:\n        \"\"\"Returns the parameters of the `process` method of the step.\n\n        Returns:\n            The parameters of the `process` method of the step.\n        \"\"\"\n        return list(inspect.signature(self.process).parameters.values())  # type: ignore\n\n    def has_multiple_inputs(self) -&gt; bool:\n        \"\"\"Whether the `process` method of the step receives more than one input or not\n        i.e. has a `*` argument annotated with `StepInput`.\n\n        Returns:\n            `True` if the `process` method of the step receives more than one input,\n            `False` otherwise.\n        \"\"\"\n        return any(\n            param.kind == param.VAR_POSITIONAL for param in self.process_parameters\n        )\n\n    def get_process_step_input(self) -&gt; Union[inspect.Parameter, None]:\n        \"\"\"Returns the parameter of the `process` method of the step annotated with\n        `StepInput`.\n\n        Returns:\n            The parameter of the `process` method of the step annotated with `StepInput`,\n            or `None` if there is no parameter annotated with `StepInput`.\n\n        Raises:\n            TypeError: If the step has more than one parameter annotated with `StepInput`.\n        \"\"\"\n        step_input_parameter = None\n        for parameter in self.process_parameters:\n            if is_parameter_annotated_with(parameter, _STEP_INPUT_ANNOTATION):\n                if step_input_parameter is not None:\n                    raise TypeError(\n                        f\"Step '{self.name}' should have only one parameter with type\"\n                        \" hint `StepInput`.\"\n                    )\n                step_input_parameter = parameter\n        return step_input_parameter\n\n    def verify_inputs_mappings(self) -&gt; None:\n        \"\"\"Verifies that the `inputs_mappings` of the step are valid i.e. the input\n        columns exist in the inputs of the step.\n\n        Raises:\n            ValueError: If the `inputs_mappings` of the step are not valid.\n        \"\"\"\n        if not self.input_mappings:\n            return\n\n        for input in self.input_mappings:\n            if input not in self.inputs:\n                raise ValueError(\n                    f\"The input column '{input}' doesn't exist in the inputs of the\"\n                    f\" step '{self.name}'. Inputs of the step are: {self.inputs}.\"\n                    \" Please, review the `inputs_mappings` argument of the step.\"\n                )\n\n    def verify_outputs_mappings(self) -&gt; None:\n        \"\"\"Verifies that the `outputs_mappings` of the step are valid i.e. the output\n        columns exist in the outputs of the step.\n\n        Raises:\n            ValueError: If the `outputs_mappings` of the step are not valid.\n        \"\"\"\n        if not self.output_mappings:\n            return\n\n        for output in self.output_mappings:\n            if output not in self.outputs:\n                raise ValueError(\n                    f\"The output column '{output}' doesn't exist in the outputs of the\"\n                    f\" step '{self.name}'. Outputs of the step are: {self.outputs}.\"\n                    \" Please, review the `outputs_mappings` argument of the step.\"\n                )\n\n    def get_inputs(self) -&gt; List[str]:\n        \"\"\"Gets the inputs of the step after the `input_mappings`. This method is meant\n        to be used to run validations on the inputs of the step.\n\n        Returns:\n            The inputs of the step after the `input_mappings`.\n        \"\"\"\n        return [self.input_mappings.get(input, input) for input in self.inputs]\n\n    def get_outputs(self) -&gt; List[str]:\n        \"\"\"Gets the outputs of the step after the `outputs_mappings`. This method is\n        meant to be used to run validations on the outputs of the step.\n\n        Returns:\n            The outputs of the step after the `outputs_mappings`.\n        \"\"\"\n        return [self.output_mappings.get(output, output) for output in self.outputs]\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"_Step\":\n        \"\"\"Create a Step from a dict containing the serialized data.\n\n        Needs the information from the step and the Pipeline it belongs to.\n\n        Note:\n            It's intended for internal use.\n\n        Args:\n            data: dictionary containing the serialized data from a `Step` and the\n                `Pipeline` it belongs to.\n\n        Returns:\n            A `Step` instance.\n        \"\"\"\n        # Remove the \"type_info\" to avoid errors on instantiation\n        _data = data.copy()\n        if TYPE_INFO_KEY in _data.keys():\n            _data.pop(TYPE_INFO_KEY)\n\n        # Before passing the data to instantiate the general step, we have to instantiate\n        # some of the internal objects. For the moment we only take into account the LLM,\n        # we should take care if we update any of the objects.\n        if llm := _data.get(\"llm\"):\n            from distilabel.utils.serialization import _get_module_attr\n\n            nested_cls = _get_module_attr(**llm.pop(TYPE_INFO_KEY))\n            # Load the LLM and update the _data inplace\n            nested_cls = nested_cls(**llm)\n            _data.update({\"llm\": nested_cls})\n\n        # Enums need a specific restoring process\n        for k, v in _data.items():\n            if isinstance(v, dict) and \"_type\" in v and v[\"_type\"] == \"enum\":\n                _data[k] = Enum(v[\"_name\"], v[\"_values\"], type=eval(v[\"_enum_type\"]))\n\n        # Skip `runtime_parameters_info` since extras are not allowed\n        _data.pop(\"runtime_parameters_info\", None)\n\n        # Every step needs the pipeline, and the remaining arguments are general\n        step = cls(**_data)\n\n        return step\n\n    def _model_dump(self, obj: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n        dump = super()._model_dump(obj, **kwargs)\n        dump[\"runtime_parameters_info\"] = self.get_runtime_parameters_info()\n        return dump\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of strings with the names of the columns that the step needs as input.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of strings with the names of the columns that the step needs as input.</p>"},{"location":"api/step/#distilabel.steps.base._Step.is_generator","title":"<code>is_generator: bool</code>  <code>property</code>","text":"<p>Whether the step is a generator step or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the step is a generator step, <code>False</code> otherwise.</p>"},{"location":"api/step/#distilabel.steps.base._Step.is_global","title":"<code>is_global: bool</code>  <code>property</code>","text":"<p>Whether the step is a global step or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the step is a global step, <code>False</code> otherwise.</p>"},{"location":"api/step/#distilabel.steps.base._Step.is_normal","title":"<code>is_normal: bool</code>  <code>property</code>","text":"<p>Whether the step is a normal step or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the step is a normal step, <code>False</code> otherwise.</p>"},{"location":"api/step/#distilabel.steps.base._Step.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of strings with the names of the columns that the step will produce as output.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of strings with the names of the columns that the step will produce as</p> <code>List[str]</code> <p>output.</p>"},{"location":"api/step/#distilabel.steps.base._Step.process_parameters","title":"<code>process_parameters: List[inspect.Parameter]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the parameters of the <code>process</code> method of the step.</p> <p>Returns:</p> Type Description <code>List[Parameter]</code> <p>The parameters of the <code>process</code> method of the step.</p>"},{"location":"api/step/#distilabel.steps.base._Step.__rrshift__","title":"<code>__rrshift__(other)</code>","text":"<p>Allows using the [step1, step2] &gt;&gt; step3 operator to connect a list of steps in the pipeline to a single step, as the list doesn't have the rshift operator.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>List[UpstreamConnectableSteps]</code> <p>The step to connect to.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The connected step</p> Example <pre><code>[step2, step3] &gt;&gt; step1\n# Would be equivalent to:\nstep2.connect(step1)\nstep3.connect(step1)\n</code></pre> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def __rrshift__(self, other: List[\"UpstreamConnectableSteps\"]) -&gt; Self:\n    \"\"\"Allows using the [step1, step2] &gt;&gt; step3 operator to connect a list of steps in the pipeline\n    to a single step, as the list doesn't have the __rshift__ operator.\n\n    Args:\n        other: The step to connect to.\n\n    Returns:\n        The connected step\n\n    Example:\n        ```python\n        [step2, step3] &gt;&gt; step1\n        # Would be equivalent to:\n        step2.connect(step1)\n        step3.connect(step1)\n        ```\n    \"\"\"\n    for o in other:\n        o.connect(self)\n    return self\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.__rshift__","title":"<code>__rshift__(other)</code>","text":"<p>Allows using the <code>&gt;&gt;</code> operator to connect steps in the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[DownstreamConnectable, RoutingBatchFunction, List[DownstreamConnectableSteps]]</code> <p>The step to connect, a list of steps to connect to or a routing batch function to be set for the step.</p> required <p>Returns:</p> Type Description <code>Union[DownstreamConnectable, RoutingBatchFunction, List[DownstreamConnectableSteps]]</code> <p>The connected step, the list of connected steps or the routing batch function.</p> Example <pre><code>step1 &gt;&gt; step2\n# Would be equivalent to:\nstep1.connect(step2)\n\n# It also allows to connect a list of steps\nstep1 &gt;&gt; [step2, step3]\n</code></pre> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def __rshift__(\n    self,\n    other: Union[\n        \"DownstreamConnectable\",\n        \"RoutingBatchFunction\",\n        List[\"DownstreamConnectableSteps\"],\n    ],\n) -&gt; Union[\n    \"DownstreamConnectable\",\n    \"RoutingBatchFunction\",\n    List[\"DownstreamConnectableSteps\"],\n]:\n    \"\"\"Allows using the `&gt;&gt;` operator to connect steps in the pipeline.\n\n    Args:\n        other: The step to connect, a list of steps to connect to or a routing batch\n            function to be set for the step.\n\n    Returns:\n        The connected step, the list of connected steps or the routing batch function.\n\n    Example:\n        ```python\n        step1 &gt;&gt; step2\n        # Would be equivalent to:\n        step1.connect(step2)\n\n        # It also allows to connect a list of steps\n        step1 &gt;&gt; [step2, step3]\n        ```\n    \"\"\"\n    # Here to avoid circular imports\n    from distilabel.pipeline.routing_batch_function import RoutingBatchFunction\n\n    if isinstance(other, list):\n        self.connect(*other)\n        return other\n\n    if isinstance(other, RoutingBatchFunction):\n        self._set_routing_batch_function(other)\n        return other\n\n    self.connect(other)\n    return other\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.connect","title":"<code>connect(*steps, routing_batch_function=None)</code>","text":"<p>Connects the current step to another step in the pipeline, which means that the output of this step will be the input of the other step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>_Step</code> <p>The steps to connect to the current step.</p> <code>()</code> <code>routing_batch_function</code> <code>Optional[RoutingBatchFunction]</code> <p>A function that receives a list of steps and returns a list of steps to which the output batch generated by this step should be routed. It should be used to define the routing logic of the pipeline. If not provided, the output batch will be routed to all the connected steps. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def connect(\n    self,\n    *steps: \"_Step\",\n    routing_batch_function: Optional[\"RoutingBatchFunction\"] = None,\n) -&gt; None:\n    \"\"\"Connects the current step to another step in the pipeline, which means that\n    the output of this step will be the input of the other step.\n\n    Args:\n        steps: The steps to connect to the current step.\n        routing_batch_function: A function that receives a list of steps and returns\n            a list of steps to which the output batch generated by this step should be\n            routed. It should be used to define the routing logic of the pipeline. If\n            not provided, the output batch will be routed to all the connected steps.\n            Defaults to `None`.\n    \"\"\"\n    assert self.pipeline is not None\n\n    if routing_batch_function:\n        self._set_routing_batch_function(routing_batch_function)\n\n    for step in steps:\n        self.pipeline._add_edge(from_step=self.name, to_step=step.name)  # type: ignore\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a Step from a dict containing the serialized data.</p> <p>Needs the information from the step and the Pipeline it belongs to.</p> Note <p>It's intended for internal use.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>dictionary containing the serialized data from a <code>Step</code> and the <code>Pipeline</code> it belongs to.</p> required <p>Returns:</p> Type Description <code>_Step</code> <p>A <code>Step</code> instance.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"_Step\":\n    \"\"\"Create a Step from a dict containing the serialized data.\n\n    Needs the information from the step and the Pipeline it belongs to.\n\n    Note:\n        It's intended for internal use.\n\n    Args:\n        data: dictionary containing the serialized data from a `Step` and the\n            `Pipeline` it belongs to.\n\n    Returns:\n        A `Step` instance.\n    \"\"\"\n    # Remove the \"type_info\" to avoid errors on instantiation\n    _data = data.copy()\n    if TYPE_INFO_KEY in _data.keys():\n        _data.pop(TYPE_INFO_KEY)\n\n    # Before passing the data to instantiate the general step, we have to instantiate\n    # some of the internal objects. For the moment we only take into account the LLM,\n    # we should take care if we update any of the objects.\n    if llm := _data.get(\"llm\"):\n        from distilabel.utils.serialization import _get_module_attr\n\n        nested_cls = _get_module_attr(**llm.pop(TYPE_INFO_KEY))\n        # Load the LLM and update the _data inplace\n        nested_cls = nested_cls(**llm)\n        _data.update({\"llm\": nested_cls})\n\n    # Enums need a specific restoring process\n    for k, v in _data.items():\n        if isinstance(v, dict) and \"_type\" in v and v[\"_type\"] == \"enum\":\n            _data[k] = Enum(v[\"_name\"], v[\"_values\"], type=eval(v[\"_enum_type\"]))\n\n    # Skip `runtime_parameters_info` since extras are not allowed\n    _data.pop(\"runtime_parameters_info\", None)\n\n    # Every step needs the pipeline, and the remaining arguments are general\n    step = cls(**_data)\n\n    return step\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.get_inputs","title":"<code>get_inputs()</code>","text":"<p>Gets the inputs of the step after the <code>input_mappings</code>. This method is meant to be used to run validations on the inputs of the step.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The inputs of the step after the <code>input_mappings</code>.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def get_inputs(self) -&gt; List[str]:\n    \"\"\"Gets the inputs of the step after the `input_mappings`. This method is meant\n    to be used to run validations on the inputs of the step.\n\n    Returns:\n        The inputs of the step after the `input_mappings`.\n    \"\"\"\n    return [self.input_mappings.get(input, input) for input in self.inputs]\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.get_outputs","title":"<code>get_outputs()</code>","text":"<p>Gets the outputs of the step after the <code>outputs_mappings</code>. This method is meant to be used to run validations on the outputs of the step.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The outputs of the step after the <code>outputs_mappings</code>.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def get_outputs(self) -&gt; List[str]:\n    \"\"\"Gets the outputs of the step after the `outputs_mappings`. This method is\n    meant to be used to run validations on the outputs of the step.\n\n    Returns:\n        The outputs of the step after the `outputs_mappings`.\n    \"\"\"\n    return [self.output_mappings.get(output, output) for output in self.outputs]\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.get_process_step_input","title":"<code>get_process_step_input()</code>","text":"<p>Returns the parameter of the <code>process</code> method of the step annotated with <code>StepInput</code>.</p> <p>Returns:</p> Type Description <code>Union[Parameter, None]</code> <p>The parameter of the <code>process</code> method of the step annotated with <code>StepInput</code>,</p> <code>Union[Parameter, None]</code> <p>or <code>None</code> if there is no parameter annotated with <code>StepInput</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the step has more than one parameter annotated with <code>StepInput</code>.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def get_process_step_input(self) -&gt; Union[inspect.Parameter, None]:\n    \"\"\"Returns the parameter of the `process` method of the step annotated with\n    `StepInput`.\n\n    Returns:\n        The parameter of the `process` method of the step annotated with `StepInput`,\n        or `None` if there is no parameter annotated with `StepInput`.\n\n    Raises:\n        TypeError: If the step has more than one parameter annotated with `StepInput`.\n    \"\"\"\n    step_input_parameter = None\n    for parameter in self.process_parameters:\n        if is_parameter_annotated_with(parameter, _STEP_INPUT_ANNOTATION):\n            if step_input_parameter is not None:\n                raise TypeError(\n                    f\"Step '{self.name}' should have only one parameter with type\"\n                    \" hint `StepInput`.\"\n                )\n            step_input_parameter = parameter\n    return step_input_parameter\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.has_multiple_inputs","title":"<code>has_multiple_inputs()</code>","text":"<p>Whether the <code>process</code> method of the step receives more than one input or not i.e. has a <code>*</code> argument annotated with <code>StepInput</code>.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the <code>process</code> method of the step receives more than one input,</p> <code>bool</code> <p><code>False</code> otherwise.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def has_multiple_inputs(self) -&gt; bool:\n    \"\"\"Whether the `process` method of the step receives more than one input or not\n    i.e. has a `*` argument annotated with `StepInput`.\n\n    Returns:\n        `True` if the `process` method of the step receives more than one input,\n        `False` otherwise.\n    \"\"\"\n    return any(\n        param.kind == param.VAR_POSITIONAL for param in self.process_parameters\n    )\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.load","title":"<code>load()</code>","text":"<p>Method to perform any initialization logic before the <code>process</code> method is called. For example, to load an LLM, stablish a connection to a database, etc.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Method to perform any initialization logic before the `process` method is\n    called. For example, to load an LLM, stablish a connection to a database, etc.\n    \"\"\"\n    self._logger = logging.getLogger(f\"distilabel.step.{self.name}\")\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.verify_inputs_mappings","title":"<code>verify_inputs_mappings()</code>","text":"<p>Verifies that the <code>inputs_mappings</code> of the step are valid i.e. the input columns exist in the inputs of the step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>inputs_mappings</code> of the step are not valid.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def verify_inputs_mappings(self) -&gt; None:\n    \"\"\"Verifies that the `inputs_mappings` of the step are valid i.e. the input\n    columns exist in the inputs of the step.\n\n    Raises:\n        ValueError: If the `inputs_mappings` of the step are not valid.\n    \"\"\"\n    if not self.input_mappings:\n        return\n\n    for input in self.input_mappings:\n        if input not in self.inputs:\n            raise ValueError(\n                f\"The input column '{input}' doesn't exist in the inputs of the\"\n                f\" step '{self.name}'. Inputs of the step are: {self.inputs}.\"\n                \" Please, review the `inputs_mappings` argument of the step.\"\n            )\n</code></pre>"},{"location":"api/step/#distilabel.steps.base._Step.verify_outputs_mappings","title":"<code>verify_outputs_mappings()</code>","text":"<p>Verifies that the <code>outputs_mappings</code> of the step are valid i.e. the output columns exist in the outputs of the step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>outputs_mappings</code> of the step are not valid.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def verify_outputs_mappings(self) -&gt; None:\n    \"\"\"Verifies that the `outputs_mappings` of the step are valid i.e. the output\n    columns exist in the outputs of the step.\n\n    Raises:\n        ValueError: If the `outputs_mappings` of the step are not valid.\n    \"\"\"\n    if not self.output_mappings:\n        return\n\n    for output in self.output_mappings:\n        if output not in self.outputs:\n            raise ValueError(\n                f\"The output column '{output}' doesn't exist in the outputs of the\"\n                f\" step '{self.name}'. Outputs of the step are: {self.outputs}.\"\n                \" Please, review the `outputs_mappings` argument of the step.\"\n            )\n</code></pre>"},{"location":"api/step/#distilabel.steps.base.Step","title":"<code>Step</code>","text":"<p>               Bases: <code>_Step</code>, <code>ABC</code></p> <p>Base class for the steps that can be included in a <code>Pipeline</code>.</p> <p>Attributes:</p> Name Type Description <code>input_batch_size</code> <code>RuntimeParameter[PositiveInt]</code> <p>The number of rows that will contain the batches processed by the step. Defaults to <code>50</code>.</p> Runtime parameters <ul> <li><code>input_batch_size</code>: The number of rows that will contain the batches processed     by the step. Defaults to <code>50</code>.</li> </ul> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class Step(_Step, ABC):\n    \"\"\"Base class for the steps that can be included in a `Pipeline`.\n\n    Attributes:\n        input_batch_size: The number of rows that will contain the batches processed by\n            the step. Defaults to `50`.\n\n    Runtime parameters:\n        - `input_batch_size`: The number of rows that will contain the batches processed\n            by the step. Defaults to `50`.\n    \"\"\"\n\n    input_batch_size: RuntimeParameter[PositiveInt] = Field(\n        default=DEFAULT_INPUT_BATCH_SIZE,\n        description=\"The number of rows that will contain the batches processed by the\"\n        \" step.\",\n    )\n\n    @abstractmethod\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"Method that defines the processing logic of the step. It should yield the\n        output rows.\n\n        Args:\n            *inputs: An argument used to receive the outputs of the previous steps. The\n                number of arguments depends on the number of previous steps. It doesn't\n                need to be an `*args` argument, it can be a regular argument annotated\n                with `StepInput` if the step has only one previous step.\n        \"\"\"\n        pass\n\n    def process_applying_mappings(self, *args: List[Dict[str, Any]]) -&gt; \"StepOutput\":\n        \"\"\"Runs the `process` method of the step applying the `input_mappings` to the input\n        rows and the `outputs_mappings` to the output rows. This is the function that\n        should be used to run the processing logic of the step.\n\n        Yields:\n            The output rows.\n        \"\"\"\n\n        inputs = self._apply_input_mappings(args) if self.input_mappings else args\n\n        # If the `Step` was built using the `@step` decorator, then we need to pass\n        # the runtime parameters as kwargs, so they can be used within the processing\n        # function\n        generator = (\n            self.process(*inputs)\n            if not self._built_from_decorator\n            else self.process(*inputs, **self._runtime_parameters)\n        )\n\n        for output_rows in generator:\n            yield [\n                {\n                    # Apply output mapping and revert input mapping\n                    self.output_mappings.get(k, None)\n                    or self.input_mappings.get(k, None)\n                    or k: v\n                    for k, v in row.items()\n                }\n                for row in output_rows\n            ]\n\n    def _revert_input_mappings(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Reverts the `input_mappings` of the step to the input row.\n\n        Args:\n            input: The input row.\n\n        Returns:\n            The input row with the `input_mappings` reverted.\n        \"\"\"\n        return {self.input_mappings.get(k, k): v for k, v in input.items()}\n\n    def _apply_input_mappings(\n        self, inputs: Tuple[List[Dict[str, Any]], ...]\n    ) -&gt; List[List[Dict[str, Any]]]:\n        \"\"\"Applies the `input_mappings` to the input rows.\n\n        Args:\n            inputs: The input rows.\n\n        Returns:\n            The input rows with the `input_mappings` applied.\n        \"\"\"\n        reverted_input_mappings = {v: k for k, v in self.input_mappings.items()}\n\n        return [\n            [\n                {reverted_input_mappings.get(k, k): v for k, v in row.items()}\n                for row in row_inputs\n            ]\n            for row_inputs in inputs\n        ]\n</code></pre>"},{"location":"api/step/#distilabel.steps.base.Step.process","title":"<code>process(*inputs)</code>  <code>abstractmethod</code>","text":"<p>Method that defines the processing logic of the step. It should yield the output rows.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>An argument used to receive the outputs of the previous steps. The number of arguments depends on the number of previous steps. It doesn't need to be an <code>*args</code> argument, it can be a regular argument annotated with <code>StepInput</code> if the step has only one previous step.</p> <code>()</code> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>@abstractmethod\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"Method that defines the processing logic of the step. It should yield the\n    output rows.\n\n    Args:\n        *inputs: An argument used to receive the outputs of the previous steps. The\n            number of arguments depends on the number of previous steps. It doesn't\n            need to be an `*args` argument, it can be a regular argument annotated\n            with `StepInput` if the step has only one previous step.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/step/#distilabel.steps.base.Step.process_applying_mappings","title":"<code>process_applying_mappings(*args)</code>","text":"<p>Runs the <code>process</code> method of the step applying the <code>input_mappings</code> to the input rows and the <code>outputs_mappings</code> to the output rows. This is the function that should be used to run the processing logic of the step.</p> <p>Yields:</p> Type Description <code>StepOutput</code> <p>The output rows.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def process_applying_mappings(self, *args: List[Dict[str, Any]]) -&gt; \"StepOutput\":\n    \"\"\"Runs the `process` method of the step applying the `input_mappings` to the input\n    rows and the `outputs_mappings` to the output rows. This is the function that\n    should be used to run the processing logic of the step.\n\n    Yields:\n        The output rows.\n    \"\"\"\n\n    inputs = self._apply_input_mappings(args) if self.input_mappings else args\n\n    # If the `Step` was built using the `@step` decorator, then we need to pass\n    # the runtime parameters as kwargs, so they can be used within the processing\n    # function\n    generator = (\n        self.process(*inputs)\n        if not self._built_from_decorator\n        else self.process(*inputs, **self._runtime_parameters)\n    )\n\n    for output_rows in generator:\n        yield [\n            {\n                # Apply output mapping and revert input mapping\n                self.output_mappings.get(k, None)\n                or self.input_mappings.get(k, None)\n                or k: v\n                for k, v in row.items()\n            }\n            for row in output_rows\n        ]\n</code></pre>"},{"location":"api/step/decorator/","title":"@step","text":"<p>This section contains the reference for the <code>@step</code> decorator, used to create new <code>Step</code> subclasses without having to manually define the class.</p> <p>For more information check the Tutorial - Step page.</p>"},{"location":"api/step/decorator/#distilabel.steps.decorator.step","title":"<code>step(inputs=None, outputs=None, step_type='normal')</code>","text":"<p>Creates an <code>Step</code> from a processing function.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[List[str], None]</code> <p>a list containing the name of the inputs columns/keys expected by this step. If not provided the default will be an empty list <code>[]</code> and it will be assumed that the step doesn't need any specific columns. Defaults to <code>None</code>.</p> <code>None</code> <code>outputs</code> <code>Union[List[str], None]</code> <p>a list containing the name of the outputs columns/keys that the step will generate. If not provided the default will be an empty list <code>[]</code> and it will be assumed that the step doesn't need any specific columns. Defaults to <code>None</code>.</p> <code>None</code> <code>step_type</code> <code>Literal['normal', 'global', 'generator']</code> <p>the kind of step to create. Valid choices are: \"normal\" (<code>Step</code>), \"global\" (<code>GlobalStep</code>) or \"generator\" (<code>GeneratorStep</code>). Defaults to <code>\"normal\"</code>.</p> <code>'normal'</code> <p>Returns:</p> Type Description <code>Callable[..., Type[_Step]]</code> <p>A callable that will generate the type given the processing function.</p> <p>Example:</p> <pre><code># Normal step\n@step(inputs=[\"instruction\"], outputs=[\"generation\"])\ndef GenerationStep(inputs: StepInput, dummy_generation: RuntimeParameter[str]) -&gt; StepOutput:\n    for input in inputs:\n        input[\"generation\"] = dummy_generation\n    yield inputs\n\n# Global step\n@step(inputs=[\"instruction\"], step_type=\"global\")\ndef FilteringStep(inputs: StepInput, max_length: RuntimeParameter[int] = 256) -&gt; StepOutput:\n    yield [\n        input\n        for input in inputs\n        if len(input[\"instruction\"]) &lt;= max_length\n    ]\n\n# Generator step\n@step(outputs=[\"num\"], step_type=\"generator\")\ndef RowGenerator(num_rows: RuntimeParameter[int] = 500) -&gt; GeneratorStepOutput:\n    data = list(range(num_rows))\n    for i in range(0, len(data), 100):\n        last_batch = i + 100 &gt;= len(data)\n        yield [{\"num\": num} for num in data[i : i + 100]], last_batch\n</code></pre> Source code in <code>src/distilabel/steps/decorator.py</code> <pre><code>def step(\n    inputs: Union[List[str], None] = None,\n    outputs: Union[List[str], None] = None,\n    step_type: Literal[\"normal\", \"global\", \"generator\"] = \"normal\",\n) -&gt; Callable[..., Type[\"_Step\"]]:\n    \"\"\"Creates an `Step` from a processing function.\n\n    Args:\n        inputs: a list containing the name of the inputs columns/keys expected by this step.\n            If not provided the default will be an empty list `[]` and it will be assumed\n            that the step doesn't need any specific columns. Defaults to `None`.\n        outputs: a list containing the name of the outputs columns/keys that the step\n            will generate. If not provided the default will be an empty list `[]` and it\n            will be assumed that the step doesn't need any specific columns. Defaults to\n            `None`.\n        step_type: the kind of step to create. Valid choices are: \"normal\" (`Step`),\n            \"global\" (`GlobalStep`) or \"generator\" (`GeneratorStep`). Defaults to\n            `\"normal\"`.\n\n    Returns:\n        A callable that will generate the type given the processing function.\n\n    Example:\n\n    ```python\n    # Normal step\n    @step(inputs=[\"instruction\"], outputs=[\"generation\"])\n    def GenerationStep(inputs: StepInput, dummy_generation: RuntimeParameter[str]) -&gt; StepOutput:\n        for input in inputs:\n            input[\"generation\"] = dummy_generation\n        yield inputs\n\n    # Global step\n    @step(inputs=[\"instruction\"], step_type=\"global\")\n    def FilteringStep(inputs: StepInput, max_length: RuntimeParameter[int] = 256) -&gt; StepOutput:\n        yield [\n            input\n            for input in inputs\n            if len(input[\"instruction\"]) &lt;= max_length\n        ]\n\n    # Generator step\n    @step(outputs=[\"num\"], step_type=\"generator\")\n    def RowGenerator(num_rows: RuntimeParameter[int] = 500) -&gt; GeneratorStepOutput:\n        data = list(range(num_rows))\n        for i in range(0, len(data), 100):\n            last_batch = i + 100 &gt;= len(data)\n            yield [{\"num\": num} for num in data[i : i + 100]], last_batch\n    ```\n    \"\"\"\n\n    inputs = inputs or []\n    outputs = outputs or []\n\n    def decorator(func: ProcessingFunc) -&gt; Type[\"_Step\"]:\n        if step_type not in _STEP_MAPPING:\n            raise ValueError(\n                f\"Invalid step type '{step_type}'. Please, review the '{func.__name__}'\"\n                \" function decorated with the `@step` decorator and provide a valid\"\n                \" `step_type`. Valid choices are: 'normal', 'global' or 'generator'.\"\n            )\n\n        BaseClass = _STEP_MAPPING[step_type]\n\n        signature = inspect.signature(func)\n\n        runtime_parameters = {\n            name: (\n                param.annotation,\n                param.default if param.default != param.empty else None,\n            )\n            for name, param in signature.parameters.items()\n        }\n\n        runtime_parameters = {}\n        step_input_parameter = None\n        for name, param in signature.parameters.items():\n            if is_parameter_annotated_with(param, _RUNTIME_PARAMETER_ANNOTATION):\n                runtime_parameters[name] = (\n                    param.annotation,\n                    param.default if param.default != param.empty else None,\n                )\n\n            if not step_type == \"generator\" and is_parameter_annotated_with(\n                param, _STEP_INPUT_ANNOTATION\n            ):\n                if step_input_parameter is not None:\n                    raise ValueError(\n                        f\"Function '{func.__name__}' has more than one parameter annotated\"\n                        f\" with `StepInput`. Please, review the '{func.__name__}' function\"\n                        \" decorated with the `@step` decorator and provide only one\"\n                        \" argument annotated with `StepInput`.\"\n                    )\n                step_input_parameter = param\n\n        RuntimeParametersModel = create_model(  # type: ignore\n            \"RuntimeParametersModel\",\n            **runtime_parameters,  # type: ignore\n        )\n\n        def inputs_property(self) -&gt; List[str]:\n            return inputs\n\n        def outputs_property(self) -&gt; List[str]:\n            return outputs\n\n        def process(\n            self, *args: Any, **kwargs: Any\n        ) -&gt; Union[\"StepOutput\", \"GeneratorStepOutput\"]:\n            return func(*args, **kwargs)\n\n        return type(  # type: ignore\n            func.__name__,\n            (\n                BaseClass,\n                RuntimeParametersModel,\n            ),\n            {\n                \"process\": process,\n                \"inputs\": property(inputs_property),\n                \"outputs\": property(outputs_property),\n                \"__module__\": func.__module__,\n                \"__doc__\": func.__doc__,\n                \"_built_from_decorator\": True,\n                # Override the `get_process_step_input` method to return the parameter\n                # of the original function annotated with `StepInput`.\n                \"get_process_step_input\": lambda self: step_input_parameter,\n            },\n        )\n\n    return decorator\n</code></pre>"},{"location":"api/step/generator_step/","title":"GeneratorStep","text":"<p>This section contains the API reference for the <code>GeneratorStep</code> class.</p> <p>For more information and examples on how to use existing generator steps or create custom ones, please refer to Tutorial - Step - GeneratorStep.</p> <p>               Bases: <code>_Step</code>, <code>ABC</code></p> <p>A special kind of <code>Step</code> that is able to generate data i.e. it doesn't receive any input from the previous steps.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>RuntimeParameter[int]</code> <p>The number of rows that will contain the batches generated by the step. Defaults to <code>50</code>.</p> Runtime parameters <ul> <li><code>batch_size</code>: The number of rows that will contain the batches generated by     the step. Defaults to <code>50</code>.</li> </ul> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class GeneratorStep(_Step, ABC):\n    \"\"\"A special kind of `Step` that is able to generate data i.e. it doesn't receive\n    any input from the previous steps.\n\n    Attributes:\n        batch_size: The number of rows that will contain the batches generated by the\n            step. Defaults to `50`.\n\n    Runtime parameters:\n        - `batch_size`: The number of rows that will contain the batches generated by\n            the step. Defaults to `50`.\n    \"\"\"\n\n    batch_size: RuntimeParameter[int] = Field(\n        default=50,\n        description=\"The number of rows that will contain the batches generated by the\"\n        \" step.\",\n    )\n\n    @abstractmethod\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n        \"\"\"Method that defines the generation logic of the step. It should yield the\n        output rows and a boolean indicating if it's the last batch or not.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            The output rows and a boolean indicating if it's the last batch or not.\n        \"\"\"\n        pass\n\n    def process_applying_mappings(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n        \"\"\"Runs the `process` method of the step applying the `outputs_mappings` to the\n        output rows. This is the function that should be used to run the generation logic\n        of the step.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            The output rows and a boolean indicating if it's the last batch or not.\n        \"\"\"\n\n        # If the `Step` was built using the `@step` decorator, then we need to pass\n        # the runtime parameters as `kwargs`, so they can be used within the processing\n        # function\n        generator = (\n            self.process(offset=offset)\n            if not self._built_from_decorator\n            else self.process(offset=offset, **self._runtime_parameters)\n        )\n\n        for output_rows, last_batch in generator:\n            yield (\n                [\n                    {self.output_mappings.get(k, k): v for k, v in row.items()}\n                    for row in output_rows\n                ],\n                last_batch,\n            )\n</code></pre>"},{"location":"api/step/generator_step/#distilabel.steps.base.GeneratorStep.process","title":"<code>process(offset=0)</code>  <code>abstractmethod</code>","text":"<p>Method that defines the generation logic of the step. It should yield the output rows and a boolean indicating if it's the last batch or not.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>The output rows and a boolean indicating if it's the last batch or not.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>@abstractmethod\ndef process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n    \"\"\"Method that defines the generation logic of the step. It should yield the\n    output rows and a boolean indicating if it's the last batch or not.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        The output rows and a boolean indicating if it's the last batch or not.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/step/generator_step/#distilabel.steps.base.GeneratorStep.process_applying_mappings","title":"<code>process_applying_mappings(offset=0)</code>","text":"<p>Runs the <code>process</code> method of the step applying the <code>outputs_mappings</code> to the output rows. This is the function that should be used to run the generation logic of the step.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>The output rows and a boolean indicating if it's the last batch or not.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def process_applying_mappings(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n    \"\"\"Runs the `process` method of the step applying the `outputs_mappings` to the\n    output rows. This is the function that should be used to run the generation logic\n    of the step.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        The output rows and a boolean indicating if it's the last batch or not.\n    \"\"\"\n\n    # If the `Step` was built using the `@step` decorator, then we need to pass\n    # the runtime parameters as `kwargs`, so they can be used within the processing\n    # function\n    generator = (\n        self.process(offset=offset)\n        if not self._built_from_decorator\n        else self.process(offset=offset, **self._runtime_parameters)\n    )\n\n    for output_rows, last_batch in generator:\n        yield (\n            [\n                {self.output_mappings.get(k, k): v for k, v in row.items()}\n                for row in output_rows\n            ],\n            last_batch,\n        )\n</code></pre>"},{"location":"api/step/global_step/","title":"GlobalStep","text":"<p>This section contains the API reference for the <code>GlobalStep</code> class.</p> <p>For more information and examples on how to use existing global steps or create custom ones, please refer to Tutorial - Step - GlobalStep.</p> <p>               Bases: <code>Step</code>, <code>ABC</code></p> <p>A special kind of <code>Step</code> which it's <code>process</code> method receives all the data processed by their previous steps at once, instead of receiving it in batches. This kind of steps are useful when the processing logic requires to have all the data at once, for example to train a model, to perform a global aggregation, etc.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class GlobalStep(Step, ABC):\n    \"\"\"A special kind of `Step` which it's `process` method receives all the data processed\n    by their previous steps at once, instead of receiving it in batches. This kind of steps\n    are useful when the processing logic requires to have all the data at once, for example\n    to train a model, to perform a global aggregation, etc.\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        return []\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return []\n</code></pre>"},{"location":"api/step_gallery/argilla/","title":"Argilla","text":"<p>This section contains the existing steps integrated with <code>Argilla</code> so as to easily push the generated datasets to Argilla.</p>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.base.Argilla","title":"<code>Argilla</code>","text":"<p>               Bases: <code>Step</code>, <code>ABC</code></p> <p>Abstract step that provides a class to subclass from, that contains the boilerplate code required to interact with Argilla, as well as some extra validations on top of it. It also defines the abstract methods that need to be implemented in order to add a new dataset type as a step.</p> Note <p>This class is not intended to be instanced directly, but via subclass.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <code>RuntimeParameter[str]</code> <p>The name of the dataset in Argilla where the records will be added.</p> <code>dataset_workspace</code> <code>Optional[RuntimeParameter[str]]</code> <p>The workspace where the dataset will be created in Argilla. Defaults to <code>None</code>, which means it will be created in the default workspace.</p> <code>api_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_URL</code> environment variable.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> Runtime parameters <ul> <li><code>dataset_name</code>: The name of the dataset in Argilla where the records will be     added.</li> <li><code>dataset_workspace</code>: The workspace where the dataset will be created in Argilla.     Defaults to <code>None</code>, which means it will be created in the default workspace.</li> <li><code>api_url</code>: The base URL to use for the Argilla API requests.</li> <li><code>api_key</code>: The API key to authenticate the requests to the Argilla API.</li> </ul> Input columns <ul> <li>dynamic, based on the <code>inputs</code> value provided</li> </ul> Source code in <code>src/distilabel/steps/argilla/base.py</code> <pre><code>class Argilla(Step, ABC):\n    \"\"\"Abstract step that provides a class to subclass from, that contains the boilerplate code\n    required to interact with Argilla, as well as some extra validations on top of it. It also defines\n    the abstract methods that need to be implemented in order to add a new dataset type as a step.\n\n    Note:\n        This class is not intended to be instanced directly, but via subclass.\n\n    Attributes:\n        dataset_name: The name of the dataset in Argilla where the records will be added.\n        dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to\n            `None`, which means it will be created in the default workspace.\n        api_url: The URL of the Argilla API. Defaults to `None`, which means it will be read from\n            the `ARGILLA_API_URL` environment variable.\n        api_key: The API key to authenticate with Argilla. Defaults to `None`, which means it will\n            be read from the `ARGILLA_API_KEY` environment variable.\n\n    Runtime parameters:\n        - `dataset_name`: The name of the dataset in Argilla where the records will be\n            added.\n        - `dataset_workspace`: The workspace where the dataset will be created in Argilla.\n            Defaults to `None`, which means it will be created in the default workspace.\n        - `api_url`: The base URL to use for the Argilla API requests.\n        - `api_key`: The API key to authenticate the requests to the Argilla API.\n\n    Input columns:\n        - dynamic, based on the `inputs` value provided\n    \"\"\"\n\n    dataset_name: RuntimeParameter[str] = Field(\n        default=None, description=\"The name of the dataset in Argilla.\"\n    )\n    dataset_workspace: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The workspace where the dataset will be created in Argilla. Defaults\"\n        \"to `None` which means it will be created in the default workspace.\",\n    )\n\n    api_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\"ARGILLA_API_URL\"),\n        description=\"The base URL to use for the Argilla API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_ARGILLA_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Argilla API.\",\n    )\n\n    _rg_dataset: Optional[\"RemoteFeedbackDataset\"] = PrivateAttr(...)\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Checks that the Argilla Python SDK is installed, and then filters the Argilla warnings.\"\"\"\n        super().model_post_init(__context)\n\n        try:\n            import argilla as rg  # noqa\n        except ImportError as ie:\n            raise ImportError(\n                \"Argilla is not installed. Please install it using `pip install argilla`.\"\n            ) from ie\n\n        warnings.filterwarnings(\"ignore\")\n\n    def _rg_init(self) -&gt; None:\n        \"\"\"Initializes the Argilla API client with the provided `api_url` and `api_key`.\"\"\"\n        try:\n            if \"hf.space\" in self.api_url and \"HF_TOKEN\" in os.environ:\n                headers = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n            else:\n                headers = None\n            rg.init(\n                api_url=self.api_url,\n                api_key=self.api_key.get_secret_value(),\n                extra_headers=headers,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(f\"Failed to initialize the Argilla API: {e}\") from e\n\n    def _rg_dataset_exists(self) -&gt; bool:\n        \"\"\"Checks if the dataset already exists in Argilla.\"\"\"\n        return self.dataset_name in [\n            dataset.name\n            for dataset in rg.FeedbackDataset.list(workspace=self.dataset_workspace)  # type: ignore\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs of the step is an empty list, since the steps subclassing from this one, will\n        always be leaf nodes and won't propagate the inputs neither generate any outputs.\n        \"\"\"\n        return []\n\n    def load(self) -&gt; None:\n        \"\"\"Method to perform any initialization logic before the `process` method is\n        called. For example, to load an LLM, stablish a connection to a database, etc.\n        \"\"\"\n        super().load()\n\n        self._rg_init()\n\n    @property\n    @abstractmethod\n    def inputs(self) -&gt; List[str]: ...\n\n    @abstractmethod\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\": ...\n</code></pre>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.base.Argilla.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs of the step is an empty list, since the steps subclassing from this one, will always be leaf nodes and won't propagate the inputs neither generate any outputs.</p>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.base.Argilla.load","title":"<code>load()</code>","text":"<p>Method to perform any initialization logic before the <code>process</code> method is called. For example, to load an LLM, stablish a connection to a database, etc.</p> Source code in <code>src/distilabel/steps/argilla/base.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Method to perform any initialization logic before the `process` method is\n    called. For example, to load an LLM, stablish a connection to a database, etc.\n    \"\"\"\n    super().load()\n\n    self._rg_init()\n</code></pre>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.base.Argilla.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Checks that the Argilla Python SDK is installed, and then filters the Argilla warnings.</p> Source code in <code>src/distilabel/steps/argilla/base.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Checks that the Argilla Python SDK is installed, and then filters the Argilla warnings.\"\"\"\n    super().model_post_init(__context)\n\n    try:\n        import argilla as rg  # noqa\n    except ImportError as ie:\n        raise ImportError(\n            \"Argilla is not installed. Please install it using `pip install argilla`.\"\n        ) from ie\n\n    warnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.preference.PreferenceToArgilla","title":"<code>PreferenceToArgilla</code>","text":"<p>               Bases: <code>Argilla</code></p> <p>Creates a preference dataset in Argilla.</p> <p>Step that creates a dataset in Argilla during the load phase, and then pushes the input batches into it as records. This dataset is a preference dataset, where there's one field for the instruction and one extra field per each generation within the same record, and then a rating question per each of the generation fields. The rating question asks the annotator to set a rating from 1 to 5 for each of the provided generations.</p> Note <p>This step is meant to be used in conjunction with the <code>UltraFeedback</code> step, or any other step generating both ratings and responses for a given set of instruction and generations for the given instruction. But alternatively, it can also be used with any other task or step generating only the <code>instruction</code> and <code>generations</code>, as the <code>ratings</code> and <code>rationales</code> are optional.</p> <p>Attributes:</p> Name Type Description <code>num_generations</code> <code>int</code> <p>The number of generations to include in the dataset.</p> <code>dataset_name</code> <code>int</code> <p>The name of the dataset in Argilla.</p> <code>dataset_workspace</code> <code>int</code> <p>The workspace where the dataset will be created in Argilla. Defaults to <code>None</code>, which means it will be created in the default workspace.</p> <code>api_url</code> <code>int</code> <p>The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_URL</code> environment variable.</p> <code>api_key</code> <code>int</code> <p>The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> Runtime parameters <ul> <li><code>api_url</code>: The base URL to use for the Argilla API requests.</li> <li><code>api_key</code>: The API key to authenticate the requests to the Argilla API.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the completion.</li> <li>generations (<code>List[str]</code>): The completion that was generated based on the input instruction.</li> <li>ratings (<code>List[str]</code>, optional): The ratings for the generations. If not provided, the     generated ratings won't be pushed to Argilla.</li> <li>rationales (<code>List[str]</code>, optional): The rationales for the ratings. If not provided, the     generated rationales won't be pushed to Argilla.</li> </ul> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>class PreferenceToArgilla(Argilla):\n    \"\"\"Creates a preference dataset in Argilla.\n\n    Step that creates a dataset in Argilla during the load phase, and then pushes the input\n    batches into it as records. This dataset is a preference dataset, where there's one field\n    for the instruction and one extra field per each generation within the same record, and then\n    a rating question per each of the generation fields. The rating question asks the annotator to\n    set a rating from 1 to 5 for each of the provided generations.\n\n    Note:\n        This step is meant to be used in conjunction with the `UltraFeedback` step, or any other step\n        generating both ratings and responses for a given set of instruction and generations for the\n        given instruction. But alternatively, it can also be used with any other task or step generating\n        only the `instruction` and `generations`, as the `ratings` and `rationales` are optional.\n\n    Attributes:\n        num_generations: The number of generations to include in the dataset.\n        dataset_name: The name of the dataset in Argilla.\n        dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to\n            `None`, which means it will be created in the default workspace.\n        api_url: The URL of the Argilla API. Defaults to `None`, which means it will be read from\n            the `ARGILLA_API_URL` environment variable.\n        api_key: The API key to authenticate with Argilla. Defaults to `None`, which means it will\n            be read from the `ARGILLA_API_KEY` environment variable.\n\n    Runtime parameters:\n        - `api_url`: The base URL to use for the Argilla API requests.\n        - `api_key`: The API key to authenticate the requests to the Argilla API.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the completion.\n        - generations (`List[str]`): The completion that was generated based on the input instruction.\n        - ratings (`List[str]`, optional): The ratings for the generations. If not provided, the\n            generated ratings won't be pushed to Argilla.\n        - rationales (`List[str]`, optional): The rationales for the ratings. If not provided, the\n            generated rationales won't be pushed to Argilla.\n    \"\"\"\n\n    num_generations: int\n\n    _id: str = PrivateAttr(default=\"id\")\n    _instruction: str = PrivateAttr(...)\n    _generations: str = PrivateAttr(...)\n    _ratings: str = PrivateAttr(...)\n    _rationales: str = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Sets the `_instruction` and `_generations` attributes based on the `inputs_mapping`, otherwise\n        uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n        the text-generation scenario. And then it pushes it to Argilla.\n        \"\"\"\n        super().load()\n\n        # Both `instruction` and `generations` will be used as the fields of the dataset\n        self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n        self._generations = self.input_mappings.get(\"generations\", \"generations\")\n        # Both `ratings` and `rationales` will be used as suggestions to the default questions of the dataset\n        self._ratings = self.input_mappings.get(\"ratings\", \"ratings\")\n        self._rationales = self.input_mappings.get(\"rationales\", \"rationales\")\n\n        if self._rg_dataset_exists():\n            _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n                name=self.dataset_name,\n                workspace=self.dataset_workspace,\n            )\n\n            for field in _rg_dataset.fields:\n                if (\n                    field.name\n                    not in [self._id, self._instruction]\n                    + [\n                        f\"{self._generations}-{idx}\"\n                        for idx in range(self.num_generations)\n                    ]\n                    and field.required\n                ):\n                    raise ValueError(\n                        f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                        f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`,\"\n                        f\" nor `{self._generations}`.\"\n                    )\n\n            self._rg_dataset = _rg_dataset\n        else:\n            _rg_dataset = rg.FeedbackDataset(  # type: ignore\n                fields=[\n                    rg.TextField(name=self._id, title=self._id),  # type: ignore\n                    rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                    *self._generation_fields(),  # type: ignore\n                ],\n                questions=self._rating_rationale_pairs(),  # type: ignore\n            )\n            self._rg_dataset = _rg_dataset.push_to_argilla(\n                name=self.dataset_name,  # type: ignore\n                workspace=self.dataset_workspace,\n            )\n\n    def _generation_fields(self) -&gt; List[\"TextField\"]:\n        \"\"\"Method to generate the fields for each of the generations.\"\"\"\n        return [\n            rg.TextField(  # type: ignore\n                name=f\"{self._generations}-{idx}\",\n                title=f\"{self._generations}-{idx}\",\n                required=True if idx == 0 else False,\n            )\n            for idx in range(self.num_generations)\n        ]\n\n    def _rating_rationale_pairs(\n        self,\n    ) -&gt; List[Union[\"RatingQuestion\", \"TextQuestion\"]]:\n        \"\"\"Method to generate the rating and rationale questions for each of the generations.\"\"\"\n        questions = []\n        for idx in range(self.num_generations):\n            questions.extend(\n                [\n                    rg.RatingQuestion(  # type: ignore\n                        name=f\"{self._generations}-{idx}-rating\",\n                        title=f\"Rate {self._generations}-{idx} given {self._instruction}.\",\n                        description=f\"Ignore this question if the corresponding `{self._generations}-{idx}` field is not available.\"\n                        if idx != 0\n                        else None,\n                        values=[1, 2, 3, 4, 5],\n                        required=True if idx == 0 else False,\n                    ),\n                    rg.TextQuestion(  # type: ignore\n                        name=f\"{self._generations}-{idx}-rationale\",\n                        title=f\"Specify the rationale for {self._generations}-{idx}'s rating.\",\n                        description=f\"Ignore this question if the corresponding `{self._generations}-{idx}` field is not available.\"\n                        if idx != 0\n                        else None,\n                        required=False,\n                    ),\n                ]\n            )\n        return questions\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the step are the `instruction` and the `generations`. Optionally, one could also\n        provide the `ratings` and the `rationales` for the generations.\"\"\"\n        return [\"instruction\", \"generations\"]\n\n    def _add_suggestions_if_any(\n        self, input: Dict[str, Any]\n    ) -&gt; List[\"SuggestionSchema\"]:\n        \"\"\"Method to generate the suggestions for the `FeedbackRecord` based on the input.\"\"\"\n        # Since the `suggestions` i.e. answers to the `questions` are optional, will default to {}\n        suggestions = []\n        # If `ratings` is in `input`, then add those as suggestions\n        if self._ratings in input:\n            suggestions.extend(\n                [\n                    {\n                        \"question_name\": f\"{self._generations}-{idx}-rating\",\n                        \"value\": rating,\n                    }\n                    for idx, rating in enumerate(input[self._ratings])\n                    if rating is not None\n                    and isinstance(rating, int)\n                    and rating in [1, 2, 3, 4, 5]\n                ],\n            )\n        # If `rationales` is in `input`, then add those as suggestions\n        if self._rationales in input:\n            suggestions.extend(\n                [\n                    {\n                        \"question_name\": f\"{self._generations}-{idx}-rationale\",\n                        \"value\": rationale,\n                    }\n                    for idx, rationale in enumerate(input[self._rationales])\n                    if rationale is not None and isinstance(rationale, str)\n                ],\n            )\n        return suggestions\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        records = []\n        for input in inputs:\n            # Generate the SHA-256 hash of the instruction to use it as the metadata\n            instruction_id = hashlib.sha256(\n                input[\"instruction\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            generations = {\n                f\"{self._generations}-{idx}\": generation\n                for idx, generation in enumerate(input[\"generations\"])  # type: ignore\n            }\n\n            records.append(  # type: ignore\n                rg.FeedbackRecord(  # type: ignore\n                    fields={\n                        \"id\": instruction_id,\n                        \"instruction\": input[\"instruction\"],  # type: ignore\n                        **generations,\n                    },\n                    suggestions=self._add_suggestions_if_any(input),  # type: ignore\n                )\n            )\n        self._rg_dataset.add_records(records)  # type: ignore\n        yield inputs\n</code></pre>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.preference.PreferenceToArgilla.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the step are the <code>instruction</code> and the <code>generations</code>. Optionally, one could also provide the <code>ratings</code> and the <code>rationales</code> for the generations.</p>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.preference.PreferenceToArgilla.load","title":"<code>load()</code>","text":"<p>Sets the <code>_instruction</code> and <code>_generations</code> attributes based on the <code>inputs_mapping</code>, otherwise uses the default values; and then uses those values to create a <code>FeedbackDataset</code> suited for the text-generation scenario. And then it pushes it to Argilla.</p> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Sets the `_instruction` and `_generations` attributes based on the `inputs_mapping`, otherwise\n    uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n    the text-generation scenario. And then it pushes it to Argilla.\n    \"\"\"\n    super().load()\n\n    # Both `instruction` and `generations` will be used as the fields of the dataset\n    self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n    self._generations = self.input_mappings.get(\"generations\", \"generations\")\n    # Both `ratings` and `rationales` will be used as suggestions to the default questions of the dataset\n    self._ratings = self.input_mappings.get(\"ratings\", \"ratings\")\n    self._rationales = self.input_mappings.get(\"rationales\", \"rationales\")\n\n    if self._rg_dataset_exists():\n        _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n            name=self.dataset_name,\n            workspace=self.dataset_workspace,\n        )\n\n        for field in _rg_dataset.fields:\n            if (\n                field.name\n                not in [self._id, self._instruction]\n                + [\n                    f\"{self._generations}-{idx}\"\n                    for idx in range(self.num_generations)\n                ]\n                and field.required\n            ):\n                raise ValueError(\n                    f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                    f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`,\"\n                    f\" nor `{self._generations}`.\"\n                )\n\n        self._rg_dataset = _rg_dataset\n    else:\n        _rg_dataset = rg.FeedbackDataset(  # type: ignore\n            fields=[\n                rg.TextField(name=self._id, title=self._id),  # type: ignore\n                rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                *self._generation_fields(),  # type: ignore\n            ],\n            questions=self._rating_rationale_pairs(),  # type: ignore\n        )\n        self._rg_dataset = _rg_dataset.push_to_argilla(\n            name=self.dataset_name,  # type: ignore\n            workspace=self.dataset_workspace,\n        )\n</code></pre>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.preference.PreferenceToArgilla.process","title":"<code>process(inputs)</code>","text":"<p>Creates and pushes the records as FeedbackRecords to the Argilla dataset.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    records = []\n    for input in inputs:\n        # Generate the SHA-256 hash of the instruction to use it as the metadata\n        instruction_id = hashlib.sha256(\n            input[\"instruction\"].encode(\"utf-8\")  # type: ignore\n        ).hexdigest()\n\n        generations = {\n            f\"{self._generations}-{idx}\": generation\n            for idx, generation in enumerate(input[\"generations\"])  # type: ignore\n        }\n\n        records.append(  # type: ignore\n            rg.FeedbackRecord(  # type: ignore\n                fields={\n                    \"id\": instruction_id,\n                    \"instruction\": input[\"instruction\"],  # type: ignore\n                    **generations,\n                },\n                suggestions=self._add_suggestions_if_any(input),  # type: ignore\n            )\n        )\n    self._rg_dataset.add_records(records)  # type: ignore\n    yield inputs\n</code></pre>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.text_generation.TextGenerationToArgilla","title":"<code>TextGenerationToArgilla</code>","text":"<p>               Bases: <code>Argilla</code></p> <p>Creates a text generation dataset in Argilla.</p> <p><code>Step</code> that creates a dataset in Argilla during the load phase, and then pushes the input batches into it as records. This dataset is a text-generation dataset, where there's one field per each input, and then a label question to rate the quality of the completion in either bad (represented with \ud83d\udc4e) or good (represented with \ud83d\udc4d).</p> Note <p>This step is meant to be used in conjunction with a <code>TextGeneration</code> step and no column mapping is needed, as it will use the default values for the <code>instruction</code> and <code>generation</code> columns.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <p>The name of the dataset in Argilla.</p> <code>dataset_workspace</code> <p>The workspace where the dataset will be created in Argilla. Defaults to <code>None</code>, which means it will be created in the default workspace.</p> <code>api_url</code> <p>The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_URL</code> environment variable.</p> <code>api_key</code> <p>The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> Runtime parameters <ul> <li><code>api_url</code>: The base URL to use for the Argilla API requests.</li> <li><code>api_key</code>: The API key to authenticate the requests to the Argilla API.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the completion.</li> <li>generation (<code>str</code> or <code>List[str]</code>): The completions that were generated based on the input instruction.</li> </ul> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>class TextGenerationToArgilla(Argilla):\n    \"\"\"Creates a text generation dataset in Argilla.\n\n    `Step` that creates a dataset in Argilla during the load phase, and then pushes the input\n    batches into it as records. This dataset is a text-generation dataset, where there's one field\n    per each input, and then a label question to rate the quality of the completion in either bad\n    (represented with \ud83d\udc4e) or good (represented with \ud83d\udc4d).\n\n    Note:\n        This step is meant to be used in conjunction with a `TextGeneration` step and no column mapping\n        is needed, as it will use the default values for the `instruction` and `generation` columns.\n\n    Attributes:\n        dataset_name: The name of the dataset in Argilla.\n        dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to\n            `None`, which means it will be created in the default workspace.\n        api_url: The URL of the Argilla API. Defaults to `None`, which means it will be read from\n            the `ARGILLA_API_URL` environment variable.\n        api_key: The API key to authenticate with Argilla. Defaults to `None`, which means it will\n            be read from the `ARGILLA_API_KEY` environment variable.\n\n    Runtime parameters:\n        - `api_url`: The base URL to use for the Argilla API requests.\n        - `api_key`: The API key to authenticate the requests to the Argilla API.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the completion.\n        - generation (`str` or `List[str]`): The completions that were generated based on the input instruction.\n    \"\"\"\n\n    _id: str = PrivateAttr(default=\"id\")\n    _instruction: str = PrivateAttr(...)\n    _generation: str = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Sets the `_instruction` and `_generation` attributes based on the `inputs_mapping`, otherwise\n        uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n        the text-generation scenario. And then it pushes it to Argilla.\n        \"\"\"\n        super().load()\n\n        self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n        self._generation = self.input_mappings.get(\"generation\", \"generation\")\n\n        if self._rg_dataset_exists():\n            _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n                name=self.dataset_name,\n                workspace=self.dataset_workspace,\n            )\n\n            for field in _rg_dataset.fields:\n                if (\n                    field.name not in [self._id, self._instruction, self._generation]\n                    and field.required\n                ):\n                    raise ValueError(\n                        f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                        f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`\"\n                        f\", nor `{self._generation}`.\"\n                    )\n\n            self._rg_dataset = _rg_dataset\n        else:\n            _rg_dataset = rg.FeedbackDataset(  # type: ignore\n                fields=[\n                    rg.TextField(name=self._id, title=self._id),  # type: ignore\n                    rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                    rg.TextField(name=self._generation, title=self._generation),  # type: ignore\n                ],\n                questions=[\n                    rg.LabelQuestion(  # type: ignore\n                        name=\"quality\",\n                        title=f\"What's the quality of the {self._generation} for the given {self._instruction}?\",\n                        labels={\"bad\": \"\ud83d\udc4e\", \"good\": \"\ud83d\udc4d\"},\n                    )\n                ],\n            )\n            self._rg_dataset = _rg_dataset.push_to_argilla(\n                name=self.dataset_name,  # type: ignore\n                workspace=self.dataset_workspace,\n            )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the step are the `instruction` and the `generation`.\"\"\"\n        return [\"instruction\", \"generation\"]\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        records = []\n        for input in inputs:\n            # Generate the SHA-256 hash of the instruction to use it as the metadata\n            instruction_id = hashlib.sha256(\n                input[\"instruction\"].encode(\"utf-8\")\n            ).hexdigest()\n\n            generations = input[\"generation\"]\n\n            # If the `generation` is not a list, then convert it into a list\n            if not isinstance(generations, list):\n                generations = [generations]\n\n            # Create a `generations_set` to avoid adding duplicates\n            generations_set = set()\n\n            for generation in generations:\n                # If the generation is already in the set, then skip it\n                if generation in generations_set:\n                    continue\n                # Otherwise, add it to the set\n                generations_set.add(generation)\n\n                records.append(\n                    rg.FeedbackRecord(  # type: ignore\n                        fields={\n                            self._id: instruction_id,\n                            self._instruction: input[\"instruction\"],\n                            self._generation: generation,\n                        },\n                    )\n                )\n        self._rg_dataset.add_records(records)  # type: ignore\n        yield inputs\n</code></pre>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.text_generation.TextGenerationToArgilla.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the step are the <code>instruction</code> and the <code>generation</code>.</p>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.text_generation.TextGenerationToArgilla.load","title":"<code>load()</code>","text":"<p>Sets the <code>_instruction</code> and <code>_generation</code> attributes based on the <code>inputs_mapping</code>, otherwise uses the default values; and then uses those values to create a <code>FeedbackDataset</code> suited for the text-generation scenario. And then it pushes it to Argilla.</p> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Sets the `_instruction` and `_generation` attributes based on the `inputs_mapping`, otherwise\n    uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n    the text-generation scenario. And then it pushes it to Argilla.\n    \"\"\"\n    super().load()\n\n    self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n    self._generation = self.input_mappings.get(\"generation\", \"generation\")\n\n    if self._rg_dataset_exists():\n        _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n            name=self.dataset_name,\n            workspace=self.dataset_workspace,\n        )\n\n        for field in _rg_dataset.fields:\n            if (\n                field.name not in [self._id, self._instruction, self._generation]\n                and field.required\n            ):\n                raise ValueError(\n                    f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                    f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`\"\n                    f\", nor `{self._generation}`.\"\n                )\n\n        self._rg_dataset = _rg_dataset\n    else:\n        _rg_dataset = rg.FeedbackDataset(  # type: ignore\n            fields=[\n                rg.TextField(name=self._id, title=self._id),  # type: ignore\n                rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                rg.TextField(name=self._generation, title=self._generation),  # type: ignore\n            ],\n            questions=[\n                rg.LabelQuestion(  # type: ignore\n                    name=\"quality\",\n                    title=f\"What's the quality of the {self._generation} for the given {self._instruction}?\",\n                    labels={\"bad\": \"\ud83d\udc4e\", \"good\": \"\ud83d\udc4d\"},\n                )\n            ],\n        )\n        self._rg_dataset = _rg_dataset.push_to_argilla(\n            name=self.dataset_name,  # type: ignore\n            workspace=self.dataset_workspace,\n        )\n</code></pre>"},{"location":"api/step_gallery/argilla/#distilabel.steps.argilla.text_generation.TextGenerationToArgilla.process","title":"<code>process(inputs)</code>","text":"<p>Creates and pushes the records as FeedbackRecords to the Argilla dataset.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    records = []\n    for input in inputs:\n        # Generate the SHA-256 hash of the instruction to use it as the metadata\n        instruction_id = hashlib.sha256(\n            input[\"instruction\"].encode(\"utf-8\")\n        ).hexdigest()\n\n        generations = input[\"generation\"]\n\n        # If the `generation` is not a list, then convert it into a list\n        if not isinstance(generations, list):\n            generations = [generations]\n\n        # Create a `generations_set` to avoid adding duplicates\n        generations_set = set()\n\n        for generation in generations:\n            # If the generation is already in the set, then skip it\n            if generation in generations_set:\n                continue\n            # Otherwise, add it to the set\n            generations_set.add(generation)\n\n            records.append(\n                rg.FeedbackRecord(  # type: ignore\n                    fields={\n                        self._id: instruction_id,\n                        self._instruction: input[\"instruction\"],\n                        self._generation: generation,\n                    },\n                )\n            )\n    self._rg_dataset.add_records(records)  # type: ignore\n    yield inputs\n</code></pre>"},{"location":"api/step_gallery/columns/","title":"Columns","text":"<p>This section contains the existing steps intended to be used for commong column operations to apply to the batches.</p>"},{"location":"api/step_gallery/columns/#distilabel.steps.combine.CombineColumns","title":"<code>CombineColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Combines columns from a list of <code>StepInput</code>.</p> <p><code>CombineColumns</code> is a <code>Step</code> that implements the <code>process</code> method that calls the <code>combine_dicts</code> function to handle and combine a list of <code>StepInput</code>. Also <code>CombineColumns</code> provides two attributes <code>columns</code> and <code>output_columns</code> to specify the columns to merge and the output columns which will override the default value for the properties <code>inputs</code> and <code>outputs</code>, respectively.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[str]</code> <p>List of strings with the names of the columns to merge.</p> <code>output_columns</code> <code>Optional[List[str]]</code> <p>Optional list of strings with the names of the output columns.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to merge.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> and <code>output_columns</code> attributes): The columns     that were merged.</li> </ul> Source code in <code>src/distilabel/steps/combine.py</code> <pre><code>class CombineColumns(Step):\n    \"\"\"Combines columns from a list of `StepInput`.\n\n    `CombineColumns` is a `Step` that implements the `process` method that calls the `combine_dicts`\n    function to handle and combine a list of `StepInput`. Also `CombineColumns` provides two attributes\n    `columns` and `output_columns` to specify the columns to merge and the output columns\n    which will override the default value for the properties `inputs` and `outputs`, respectively.\n\n    Attributes:\n        columns: List of strings with the names of the columns to merge.\n        output_columns: Optional list of strings with the names of the output columns.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to merge.\n\n    Output columns:\n        - dynamic (determined by `columns` and `output_columns` attributes): The columns\n            that were merged.\n    \"\"\"\n\n    columns: List[str]\n    output_columns: Optional[List[str]] = None\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the column names in `output_columns` or\n        `merged_{column}` for each column in `columns`.\"\"\"\n        return (\n            self.output_columns\n            if self.output_columns is not None\n            else [f\"merged_{column}\" for column in self.columns]\n        )\n\n    @override\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"The `process` method calls the `combine_dicts` function to handle and combine a list of `StepInput`.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with the combined `StepInput` using the `combine_dicts` function.\n        \"\"\"\n        yield combine_dicts(\n            *inputs,\n            merge_keys=self.inputs,\n            output_merge_keys=self.outputs,\n        )\n</code></pre>"},{"location":"api/step_gallery/columns/#distilabel.steps.combine.CombineColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the column names in <code>columns</code>.</p>"},{"location":"api/step_gallery/columns/#distilabel.steps.combine.CombineColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the column names in <code>output_columns</code> or <code>merged_{column}</code> for each column in <code>columns</code>.</p>"},{"location":"api/step_gallery/columns/#distilabel.steps.combine.CombineColumns.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method calls the <code>combine_dicts</code> function to handle and combine a list of <code>StepInput</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with the combined <code>StepInput</code> using the <code>combine_dicts</code> function.</p> Source code in <code>src/distilabel/steps/combine.py</code> <pre><code>@override\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"The `process` method calls the `combine_dicts` function to handle and combine a list of `StepInput`.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with the combined `StepInput` using the `combine_dicts` function.\n    \"\"\"\n    yield combine_dicts(\n        *inputs,\n        merge_keys=self.inputs,\n        output_merge_keys=self.outputs,\n    )\n</code></pre>"},{"location":"api/step_gallery/columns/#distilabel.steps.expand.ExpandColumns","title":"<code>ExpandColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Expand columns that contain lists into multiple rows.</p> <p><code>ExpandColumns</code> is a <code>Step</code> that takes a list of columns and expands them into multiple rows. The new rows will have the same data as the original row, except for the expanded column, which will contain a single item from the original list.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>Union[Dict[str, str], List[str]]</code> <p>A dictionary that maps the column to be expanded to the new column name or a list of columns to be expanded. If a list is provided, the new column name will be the same as the column name.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to be expanded into     multiple rows.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> attribute):  The expanded columns.</li> </ul> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>class ExpandColumns(Step):\n    \"\"\"Expand columns that contain lists into multiple rows.\n\n    `ExpandColumns` is a `Step` that takes a list of columns and expands them into multiple\n    rows. The new rows will have the same data as the original row, except for the expanded\n    column, which will contain a single item from the original list.\n\n    Attributes:\n        columns: A dictionary that maps the column to be expanded to the new column name\n            or a list of columns to be expanded. If a list is provided, the new column name\n            will be the same as the column name.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to be expanded into\n            multiple rows.\n\n    Output columns:\n        - dynamic (determined by `columns` attribute):  The expanded columns.\n    \"\"\"\n\n    columns: Union[Dict[str, str], List[str]]\n\n    @field_validator(\"columns\")\n    @classmethod\n    def always_dict(cls, value: Union[Dict[str, str], List[str]]) -&gt; Dict[str, str]:\n        \"\"\"Ensure that the columns are always a dictionary.\n\n        Args:\n            value: The columns to be expanded.\n\n        Returns:\n            The columns to be expanded as a dictionary.\n        \"\"\"\n        if isinstance(value, list):\n            return {col: col for col in value}\n\n        return value\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The columns to be expanded.\"\"\"\n        return list(self.columns.keys())\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The expanded columns.\"\"\"\n        return [\n            new_column if new_column else expand_column\n            for expand_column, new_column in self.columns.items()\n        ]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Expand the columns in the input data.\n\n        Args:\n            inputs: The input data.\n\n        Yields:\n            The expanded rows.\n        \"\"\"\n        yield [row for input in inputs for row in self._expand_columns(input)]\n\n    def _expand_columns(self, input: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Expand the columns in the input data.\n\n        Args:\n            input: The input data.\n\n        Returns:\n            The expanded rows.\n        \"\"\"\n        expanded_rows = []\n        for expand_column, new_column in self.columns.items():  # type: ignore\n            data = input.get(expand_column)\n            rows = []\n            for item, expanded in zip_longest(*[data, expanded_rows], fillvalue=input):\n                rows.append({**expanded, new_column: item})\n            expanded_rows = rows\n        return expanded_rows\n</code></pre>"},{"location":"api/step_gallery/columns/#distilabel.steps.expand.ExpandColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The columns to be expanded.</p>"},{"location":"api/step_gallery/columns/#distilabel.steps.expand.ExpandColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The expanded columns.</p>"},{"location":"api/step_gallery/columns/#distilabel.steps.expand.ExpandColumns.always_dict","title":"<code>always_dict(value)</code>  <code>classmethod</code>","text":"<p>Ensure that the columns are always a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[Dict[str, str], List[str]]</code> <p>The columns to be expanded.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>The columns to be expanded as a dictionary.</p> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>@field_validator(\"columns\")\n@classmethod\ndef always_dict(cls, value: Union[Dict[str, str], List[str]]) -&gt; Dict[str, str]:\n    \"\"\"Ensure that the columns are always a dictionary.\n\n    Args:\n        value: The columns to be expanded.\n\n    Returns:\n        The columns to be expanded as a dictionary.\n    \"\"\"\n    if isinstance(value, list):\n        return {col: col for col in value}\n\n    return value\n</code></pre>"},{"location":"api/step_gallery/columns/#distilabel.steps.expand.ExpandColumns.process","title":"<code>process(inputs)</code>","text":"<p>Expand the columns in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>The input data.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>The expanded rows.</p> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Expand the columns in the input data.\n\n    Args:\n        inputs: The input data.\n\n    Yields:\n        The expanded rows.\n    \"\"\"\n    yield [row for input in inputs for row in self._expand_columns(input)]\n</code></pre>"},{"location":"api/step_gallery/columns/#distilabel.steps.keep.KeepColumns","title":"<code>KeepColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Keeps selected columns in the dataset.</p> <p><code>KeepColumns</code> is a <code>Step</code> that implements the <code>process</code> method that keeps only the columns specified in the <code>columns</code> attribute. Also <code>KeepColumns</code> provides an attribute <code>columns</code> to specify the columns to keep which will override the default value for the properties <code>inputs</code> and <code>outputs</code>.</p> Note <p>The order in which the columns are provided is important, as the output will be sorted using the provided order, which is useful before pushing either a <code>dataset.Dataset</code> via the <code>PushToHub</code> step or a <code>distilabel.Distiset</code> via the <code>Pipeline.run</code> output variable.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[str]</code> <p>List of strings with the names of the columns to keep.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to keep.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns that were kept.</li> </ul> Source code in <code>src/distilabel/steps/keep.py</code> <pre><code>class KeepColumns(Step):\n    \"\"\"Keeps selected columns in the dataset.\n\n    `KeepColumns` is a `Step` that implements the `process` method that keeps only the columns\n    specified in the `columns` attribute. Also `KeepColumns` provides an attribute `columns` to\n    specify the columns to keep which will override the default value for the properties `inputs`\n    and `outputs`.\n\n    Note:\n        The order in which the columns are provided is important, as the output will be sorted\n        using the provided order, which is useful before pushing either a `dataset.Dataset` via\n        the `PushToHub` step or a `distilabel.Distiset` via the `Pipeline.run` output variable.\n\n    Attributes:\n        columns: List of strings with the names of the columns to keep.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to keep.\n\n    Output columns:\n        - dynamic (determined by `columns` attribute): The columns that were kept.\n    \"\"\"\n\n    columns: List[str]\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @override\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"The `process` method keeps only the columns specified in the `columns` attribute.\n\n        Args:\n            *inputs: A list of dictionaries with the input data.\n\n        Yields:\n            A list of dictionaries with the output data.\n        \"\"\"\n        for input in inputs:\n            outputs = []\n            for item in input:\n                outputs.append({col: item[col] for col in self.columns})\n            yield outputs\n</code></pre>"},{"location":"api/step_gallery/columns/#distilabel.steps.keep.KeepColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the column names in <code>columns</code>.</p>"},{"location":"api/step_gallery/columns/#distilabel.steps.keep.KeepColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the column names in <code>columns</code>.</p>"},{"location":"api/step_gallery/columns/#distilabel.steps.keep.KeepColumns.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method keeps only the columns specified in the <code>columns</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of dictionaries with the input data.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of dictionaries with the output data.</p> Source code in <code>src/distilabel/steps/keep.py</code> <pre><code>@override\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"The `process` method keeps only the columns specified in the `columns` attribute.\n\n    Args:\n        *inputs: A list of dictionaries with the input data.\n\n    Yields:\n        A list of dictionaries with the output data.\n    \"\"\"\n    for input in inputs:\n        outputs = []\n        for item in input:\n            outputs.append({col: item[col] for col in self.columns})\n        yield outputs\n</code></pre>"},{"location":"api/step_gallery/extra/","title":"Extra","text":""},{"location":"api/step_gallery/extra/#distilabel.steps.deita.DeitaFiltering","title":"<code>DeitaFiltering</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Filter dataset rows using DEITA filtering strategy.</p> <p>Filter the dataset based on the DEITA score and the cosine distance between the embeddings. It's an implementation of the filtering step from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>data_budget</code> <code>RuntimeParameter[int]</code> <p>The desired size of the dataset after filtering.</p> <code>diversity_threshold</code> <code>RuntimeParameter[float]</code> <p>If a row has a cosine distance with respect to it's nearest neighbor greater than this value, it will be included in the filtered dataset. Defaults to <code>0.9</code>.</p> <code>normalize_embeddings</code> <code>RuntimeParameter[bool]</code> <p>Whether to normalize the embeddings before computing the cosine distance. Defaults to <code>True</code>.</p> Runtime parameters <ul> <li><code>data_budget</code>: The desired size of the dataset after filtering.</li> <li><code>diversity_threshold</code>: If a row has a cosine distance with respect to it's nearest     neighbor greater than this value, it will be included in the filtered dataset.</li> </ul> Input columns <ul> <li>evol_instruction_score (<code>float</code>): The score of the instruction generated by     <code>ComplexityScorer</code> step.</li> <li>evol_response_score (<code>float</code>): The score of the response generated by     <code>QualityScorer</code> step.</li> <li>embedding (<code>List[float]</code>): The embedding generated for the conversation of the     instruction-response pair using <code>GenerateEmbeddings</code> step.</li> </ul> Output columns <ul> <li>deita_score (<code>float</code>): The DEITA score for the instruction-response pair.</li> <li>deita_score_computed_with (<code>List[str]</code>): The scores used to compute the DEITA     score.</li> <li>nearest_neighbor_distance (<code>float</code>): The cosine distance between the embeddings     of the instruction-response pair.</li> </ul> Categories <ul> <li>filtering</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/deita.py</code> <pre><code>class DeitaFiltering(GlobalStep):\n    \"\"\"Filter dataset rows using DEITA filtering strategy.\n\n    Filter the dataset based on the DEITA score and the cosine distance between the embeddings.\n    It's an implementation of the filtering step from the paper 'What Makes Good Data\n    for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        data_budget: The desired size of the dataset after filtering.\n        diversity_threshold: If a row has a cosine distance with respect to it's nearest\n            neighbor greater than this value, it will be included in the filtered dataset.\n            Defaults to `0.9`.\n        normalize_embeddings: Whether to normalize the embeddings before computing the cosine\n            distance. Defaults to `True`.\n\n    Runtime parameters:\n        - `data_budget`: The desired size of the dataset after filtering.\n        - `diversity_threshold`: If a row has a cosine distance with respect to it's nearest\n            neighbor greater than this value, it will be included in the filtered dataset.\n\n    Input columns:\n        - evol_instruction_score (`float`): The score of the instruction generated by\n            `ComplexityScorer` step.\n        - evol_response_score (`float`): The score of the response generated by\n            `QualityScorer` step.\n        - embedding (`List[float]`): The embedding generated for the conversation of the\n            instruction-response pair using `GenerateEmbeddings` step.\n\n    Output columns:\n        - deita_score (`float`): The DEITA score for the instruction-response pair.\n        - deita_score_computed_with (`List[str]`): The scores used to compute the DEITA\n            score.\n        - nearest_neighbor_distance (`float`): The cosine distance between the embeddings\n            of the instruction-response pair.\n\n    Categories:\n        - filtering\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    data_budget: RuntimeParameter[int] = Field(\n        default=None, description=\"The desired size of the dataset after filtering.\"\n    )\n    diversity_threshold: RuntimeParameter[float] = Field(\n        default=0.9,\n        description=\"If a row has a cosine distance with respect to it's nearest neighbor\"\n        \" greater than this value, it will be included in the filtered dataset.\",\n    )\n    normalize_embeddings: RuntimeParameter[bool] = Field(\n        default=True,\n        description=\"Whether to normalize the embeddings before computing the cosine distance.\",\n    )\n    distance_metric: RuntimeParameter[Literal[\"cosine\", \"manhattan\"]] = Field(\n        default=\"cosine\",\n        description=\"The distance metric to use. Currently only 'cosine' is supported.\",\n    )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        return [\"evol_instruction_score\", \"evol_response_score\", \"embedding\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return [\"deita_score\", \"nearest_neighbor_distance\", \"deita_score_computed_with\"]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Filter the dataset based on the DEITA score and the cosine distance between the\n        embeddings.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The filtered dataset.\n        \"\"\"\n        inputs = self._compute_deita_score(inputs)\n        inputs = self._compute_nearest_neighbor(inputs)\n        inputs.sort(key=lambda x: x[\"deita_score\"], reverse=True)\n\n        selected_rows = []\n        for input in inputs:\n            if len(selected_rows) &gt;= self.data_budget:  # type: ignore\n                break\n            if input[\"nearest_neighbor_distance\"] &gt;= self.diversity_threshold:\n                selected_rows.append(input)\n        yield selected_rows\n\n    def _compute_deita_score(self, inputs: StepInput) -&gt; StepInput:\n        \"\"\"Computes the DEITA score for each instruction-response pair. The DEITA score is\n        the product of the instruction score and the response score.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The input data with the DEITA score computed.\n        \"\"\"\n        for input_ in inputs:\n            evol_instruction_score = input_.get(\"evol_instruction_score\")\n            evol_response_score = input_.get(\"evol_response_score\")\n\n            if evol_instruction_score and evol_response_score:\n                deita_score = evol_instruction_score * evol_response_score\n                score_computed_with = [\"evol_instruction_score\", \"evol_response_score\"]\n            elif evol_instruction_score:\n                self._logger.warning(\n                    \"Response score is missing for the instruction-response pair. Using\"\n                    \" instruction score as DEITA score.\"\n                )\n                deita_score = evol_instruction_score\n                score_computed_with = [\"evol_instruction_score\"]\n            elif evol_response_score:\n                self._logger.warning(\n                    \"Instruction score is missing for the instruction-response pair. Using\"\n                    \" response score as DEITA score.\"\n                )\n                deita_score = evol_response_score\n                score_computed_with = [\"evol_response_score\"]\n            else:\n                self._logger.warning(\n                    \"Instruction and response scores are missing for the instruction-response\"\n                    \" pair. Setting DEITA score to 0.\"\n                )\n                deita_score = 0\n                score_computed_with = []\n\n            input_.update(\n                {\n                    \"deita_score\": deita_score,\n                    \"deita_score_computed_with\": score_computed_with,\n                }\n            )\n        return inputs\n\n    def _compute_nearest_neighbor(self, inputs: StepInput) -&gt; StepInput:\n        \"\"\"Computes the cosine distance between the embeddings of the instruction-response\n        pairs and the nearest neighbor.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The input data with the cosine distance computed.\n        \"\"\"\n        embeddings = np.array([input[\"embedding\"] for input in inputs])\n        if self.normalize_embeddings:\n            embeddings = self._normalize_embeddings(embeddings)\n        self._logger.info(\"\ud83d\udccf Computing nearest neighbor distance...\")\n\n        if self.distance_metric == \"cosine\":\n            self._logger.info(\"\ud83d\udccf Using cosine distance.\")\n            distances = self._cosine_distance(embeddings)\n        else:\n            self._logger.info(\"\ud83d\udccf Using manhattan distance.\")\n            distances = self._manhattan_distance(embeddings)\n\n        for distance, input in zip(distances, inputs):\n            input[\"nearest_neighbor_distance\"] = distance\n        return inputs\n\n    def _normalize_embeddings(self, embeddings: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize the embeddings.\n\n        Args:\n            embeddings: The embeddings to normalize.\n\n        Returns:\n            The normalized embeddings.\n        \"\"\"\n        self._logger.info(\"\u2696\ufe0f Normalizing embeddings...\")\n        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n        return embeddings / norms\n\n    def _cosine_distance(self, embeddings: np.array) -&gt; np.array:  # type: ignore\n        \"\"\"Computes the cosine distance between the embeddings.\n\n        Args:\n            embeddings: The embeddings.\n\n        Returns:\n            The cosine distance between the embeddings.\n        \"\"\"\n        cosine_similarity = np.dot(embeddings, embeddings.T)\n        cosine_distance = 1 - cosine_similarity\n        # Ignore self-distance\n        np.fill_diagonal(cosine_distance, np.inf)\n        return np.min(cosine_distance, axis=1)\n\n    def _manhattan_distance(self, embeddings: np.array) -&gt; np.array:  # type: ignore\n        \"\"\"Computes the manhattan distance between the embeddings.\n\n        Args:\n            embeddings: The embeddings.\n\n        Returns:\n            The manhattan distance between the embeddings.\n        \"\"\"\n        manhattan_distance = np.abs(embeddings[:, None] - embeddings).sum(-1)\n        # Ignore self-distance\n        np.fill_diagonal(manhattan_distance, np.inf)\n        return np.min(manhattan_distance, axis=1)\n</code></pre>"},{"location":"api/step_gallery/extra/#distilabel.steps.deita.DeitaFiltering.process","title":"<code>process(inputs)</code>","text":"<p>Filter the dataset based on the DEITA score and the cosine distance between the embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>The input data.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>The filtered dataset.</p> Source code in <code>src/distilabel/steps/deita.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Filter the dataset based on the DEITA score and the cosine distance between the\n    embeddings.\n\n    Args:\n        inputs: The input data.\n\n    Returns:\n        The filtered dataset.\n    \"\"\"\n    inputs = self._compute_deita_score(inputs)\n    inputs = self._compute_nearest_neighbor(inputs)\n    inputs.sort(key=lambda x: x[\"deita_score\"], reverse=True)\n\n    selected_rows = []\n    for input in inputs:\n        if len(selected_rows) &gt;= self.data_budget:  # type: ignore\n            break\n        if input[\"nearest_neighbor_distance\"] &gt;= self.diversity_threshold:\n            selected_rows.append(input)\n    yield selected_rows\n</code></pre>"},{"location":"api/step_gallery/extra/#distilabel.steps.typing.GeneratorStepOutput","title":"<code>GeneratorStepOutput = Iterator[Tuple[List[Dict[str, Any]], bool]]</code>  <code>module-attribute</code>","text":"<p>GeneratorStepOutput is an alias of the typing <code>Iterator[Tuple[List[Dict[str, Any]], bool]]</code></p>"},{"location":"api/step_gallery/extra/#distilabel.steps.typing.StepOutput","title":"<code>StepOutput = Iterator[List[Dict[str, Any]]]</code>  <code>module-attribute</code>","text":"<p>StepOutput is an alias of the typing <code>Iterator[List[Dict[str, Any]]]</code></p>"},{"location":"api/task/","title":"Task","text":"<p>This section contains the API reference for the <code>distilabel</code> tasks.</p> <p>For more information on how the <code>Task</code> works and see some examples, check the Tutorial - Task page.</p>"},{"location":"api/task/#distilabel.steps.tasks.base._Task","title":"<code>_Task</code>","text":"<p>               Bases: <code>_Step</code>, <code>ABC</code></p> <p>_Task is an abstract class that implements the <code>_Step</code> interface and adds the <code>format_input</code> and <code>format_output</code> methods to format the inputs and outputs of the task. It also adds a <code>llm</code> attribute to be used as the LLM to generate the outputs.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>the <code>LLM</code> to be used to generate the outputs of the task.</p> <code>group_generations</code> <code>bool</code> <p>whether to group the <code>num_generations</code> generated per input in a list or create a row per generation. Defaults to <code>False</code>.</p> <code>add_raw_output</code> <code>RuntimeParameter[bool]</code> <p>whether to include a field with the raw output of the LLM in the <code>distilabel_metadata</code> field of the output. Can be helpful to not loose data with <code>Tasks</code> that need to format the output of the <code>LLM</code>. Defaults to <code>False</code>.</p> <code>num_generations</code> <code>RuntimeParameter[int]</code> <p>The number of generations to be produced per input.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>class _Task(_Step, ABC):\n    \"\"\"_Task is an abstract class that implements the `_Step` interface and adds the\n    `format_input` and `format_output` methods to format the inputs and outputs of the\n    task. It also adds a `llm` attribute to be used as the LLM to generate the outputs.\n\n    Attributes:\n        llm: the `LLM` to be used to generate the outputs of the task.\n        group_generations: whether to group the `num_generations` generated per input in\n            a list or create a row per generation. Defaults to `False`.\n        add_raw_output: whether to include a field with the raw output of the LLM in the\n            `distilabel_metadata` field of the output. Can be helpful to not loose data\n            with `Tasks` that need to format the output of the `LLM`. Defaults to `False`.\n        num_generations: The number of generations to be produced per input.\n    \"\"\"\n\n    llm: LLM\n\n    group_generations: bool = False\n    add_raw_output: RuntimeParameter[bool] = Field(\n        default=True,\n        description=(\n            \"Whether to include the raw output of the LLM in the key `raw_output_&lt;TASK_NAME&gt;`\"\n            \" of the `distilabel_metadata` dictionary output column\"\n        ),\n    )\n    num_generations: RuntimeParameter[int] = Field(\n        default=1, description=\"The number of generations to be produced per input.\"\n    )\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the LLM via the `LLM.load()` method (done for safer serialization).\"\"\"\n        super().load()\n        self.llm.load()\n\n    @abstractmethod\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Abstract method to format the outputs of the task. It needs to receive an output\n        as a string, and generates a Python dictionary with the outputs of the task. In\n        addition the `input` used to generate the output is also received just in case it's\n        needed to be able to parse the output correctly.\n        \"\"\"\n        pass\n\n    def _format_outputs(\n        self, outputs: \"GenerateOutput\", inputs: List[Dict[str, Any]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Formats the outputs of the task using the `format_output` method. If the output\n        is `None` (i.e. the LLM failed to generate a response), then the outputs will be\n        set to `None` as well.\n\n        Args:\n            outputs: The outputs of the LLM.\n            inputs: The inputs used to generate the outputs.\n\n        Returns:\n            A list containing a dictionary with the outputs of the task for each input.\n        \"\"\"\n        formatted_outputs = []\n        for output, input in zip(outputs, inputs * len(outputs)):\n            try:\n                formatted_output = self.format_output(output, input)\n                formatted_output = self._maybe_add_raw_output(\n                    formatted_output, output, add_raw_output=self.add_raw_output\n                )\n                formatted_outputs.append(formatted_output)\n            except Exception as e:\n                self._logger.warning(  # type: ignore\n                    f\"Task '{self.name}' failed to format output: {e}. Saving raw response.\"  # type: ignore\n                )\n                formatted_outputs.append(self._output_on_failure(output, input))\n        return formatted_outputs\n\n    def _output_on_failure(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"In case of failure to format the output, this method will return a dictionary including\n        a new field `distilabel_meta` with the raw output of the LLM.\n        \"\"\"\n        # Create a dictionary with the outputs of the task (every output set to None)\n        outputs = {output: None for output in self.outputs}\n        outputs[\"model_name\"] = self.llm.model_name  # type: ignore\n        outputs = self._maybe_add_raw_output(\n            outputs, output, add_raw_output=self.add_raw_output\n        )\n        return outputs\n\n    def _maybe_add_raw_output(\n        self,\n        output: Dict[str, Any],\n        raw_output: Union[str, None],\n        add_raw_output: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Adds the raw output of the LLM to the output dictionary if `add_raw_output` is True.\"\"\"\n        if add_raw_output:\n            meta = output.get(DISTILABEL_METADATA_KEY, {})\n            meta[f\"raw_output_{self.name}\"] = raw_output\n            output[DISTILABEL_METADATA_KEY] = meta\n        return output\n</code></pre>"},{"location":"api/task/#distilabel.steps.tasks.base._Task.format_output","title":"<code>format_output(output, input)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to format the outputs of the task. It needs to receive an output as a string, and generates a Python dictionary with the outputs of the task. In addition the <code>input</code> used to generate the output is also received just in case it's needed to be able to parse the output correctly.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>@abstractmethod\ndef format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"Abstract method to format the outputs of the task. It needs to receive an output\n    as a string, and generates a Python dictionary with the outputs of the task. In\n    addition the `input` used to generate the output is also received just in case it's\n    needed to be able to parse the output correctly.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/task/#distilabel.steps.tasks.base._Task.load","title":"<code>load()</code>","text":"<p>Loads the LLM via the <code>LLM.load()</code> method (done for safer serialization).</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the LLM via the `LLM.load()` method (done for safer serialization).\"\"\"\n    super().load()\n    self.llm.load()\n</code></pre>"},{"location":"api/task/#distilabel.steps.tasks.base.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>_Task</code>, <code>Step</code></p> <p>Task is a class that implements the <code>_Task</code> abstract class and adds the <code>Step</code> interface to be used as a step in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>the <code>LLM</code> to be used to generate the outputs of the task.</p> <code>group_generations</code> <p>whether to group the <code>num_generations</code> generated per input in a list or create a row per generation. Defaults to <code>False</code>.</p> <code>num_generations</code> <p>The number of generations to be produced per input.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>class Task(_Task, Step):\n    \"\"\"Task is a class that implements the `_Task` abstract class and adds the `Step`\n    interface to be used as a step in the pipeline.\n\n    Attributes:\n        llm: the `LLM` to be used to generate the outputs of the task.\n        group_generations: whether to group the `num_generations` generated per input in\n            a list or create a row per generation. Defaults to `False`.\n        num_generations: The number of generations to be produced per input.\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, input: Dict[str, Any]) -&gt; \"FormattedInput\":\n        \"\"\"Abstract method to format the inputs of the task. It needs to receive an input\n        as a Python dictionary, and generates an OpenAI chat-like list of dicts.\"\"\"\n        pass\n\n    def _format_inputs(self, inputs: List[Dict[str, Any]]) -&gt; List[\"FormattedInput\"]:\n        \"\"\"Formats the inputs of the task using the `format_input` method.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list containing the formatted inputs, which are `ChatType`-like following\n            the OpenAI formatting.\n        \"\"\"\n        return [self.format_input(input) for input in inputs]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        formatted_inputs = self._format_inputs(inputs)\n        outputs = self.llm.generate(\n            inputs=formatted_inputs,\n            num_generations=self.num_generations,  # type: ignore\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n\n        task_outputs = []\n        for input, input_outputs in zip(inputs, outputs):\n            formatted_outputs = self._format_outputs(input_outputs, inputs)\n\n            if self.group_generations:\n                combined = combine_dicts(*formatted_outputs)\n                task_outputs.append(\n                    {**input, \"model_name\": self.llm.model_name, **combined}\n                )\n                continue\n\n            # Create a row per generation\n            for formatted_output in formatted_outputs:\n                task_outputs.append(\n                    {**input, \"model_name\": self.llm.model_name, **formatted_output}\n                )\n\n        yield task_outputs\n</code></pre>"},{"location":"api/task/#distilabel.steps.tasks.base.Task.format_input","title":"<code>format_input(input)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to format the inputs of the task. It needs to receive an input as a Python dictionary, and generates an OpenAI chat-like list of dicts.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, input: Dict[str, Any]) -&gt; \"FormattedInput\":\n    \"\"\"Abstract method to format the inputs of the task. It needs to receive an input\n    as a Python dictionary, and generates an OpenAI chat-like list of dicts.\"\"\"\n    pass\n</code></pre>"},{"location":"api/task/#distilabel.steps.tasks.base.Task.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    formatted_inputs = self._format_inputs(inputs)\n    outputs = self.llm.generate(\n        inputs=formatted_inputs,\n        num_generations=self.num_generations,  # type: ignore\n        **self.llm.generation_kwargs,  # type: ignore\n    )\n\n    task_outputs = []\n    for input, input_outputs in zip(inputs, outputs):\n        formatted_outputs = self._format_outputs(input_outputs, inputs)\n\n        if self.group_generations:\n            combined = combine_dicts(*formatted_outputs)\n            task_outputs.append(\n                {**input, \"model_name\": self.llm.model_name, **combined}\n            )\n            continue\n\n        # Create a row per generation\n        for formatted_output in formatted_outputs:\n            task_outputs.append(\n                {**input, \"model_name\": self.llm.model_name, **formatted_output}\n            )\n\n    yield task_outputs\n</code></pre>"},{"location":"api/task/generator_task/","title":"GeneratorTask","text":"<p>This section contains the API reference for the <code>distilabel</code> generator tasks.</p> <p>For more information on how the <code>GeneratorTask</code> works and see some examples, check the Tutorial - Task - GeneratorTask page.</p> <p>               Bases: <code>_Task</code>, <code>GeneratorStep</code></p> <p>GeneratorTask is a class that implements the <code>_Task</code> abstract class and adds the <code>GeneratorStep</code> interface to be used as a step in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>the <code>LLM</code> to be used to generate the outputs of the task.</p> <code>group_generations</code> <p>whether to group the <code>num_generations</code> generated per input in a list or create a row per generation. Defaults to <code>False</code>.</p> <code>num_generations</code> <p>The number of generations to be produced per input.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>class GeneratorTask(_Task, GeneratorStep):\n    \"\"\"GeneratorTask is a class that implements the `_Task` abstract class and adds the\n    `GeneratorStep` interface to be used as a step in the pipeline.\n\n    Attributes:\n        llm: the `LLM` to be used to generate the outputs of the task.\n        group_generations: whether to group the `num_generations` generated per input in\n            a list or create a row per generation. Defaults to `False`.\n        num_generations: The number of generations to be produced per input.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/task_gallery/","title":"Task Gallery","text":"<p>This section contains the existing <code>Task</code> subclasses implemented in <code>distilabel</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ChatGeneration","title":"<code>ChatGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generates text based on a conversation.</p> <p><code>ChatGeneration</code> is a pre-defined task that defines the <code>messages</code> as the input and <code>generation</code> as the output. This task is used to generate text based on a conversation. The <code>model_name</code> is also returned as part of the output in order to enhance it.</p> Input columns <ul> <li>messages (<code>List[Dict[Literal[\"role\", \"content\"], str]]</code>): The messages to generate the     follow up completion from.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text from the assistant.</li> <li>model_name (<code>str</code>): The model name used to generate the text.</li> </ul> Categories <ul> <li>chat-generation</li> </ul> Icon <p><code>:material-chat:</code></p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>class ChatGeneration(Task):\n    \"\"\"Generates text based on a conversation.\n\n    `ChatGeneration` is a pre-defined task that defines the `messages` as the input\n    and `generation` as the output. This task is used to generate text based on a conversation.\n    The `model_name` is also returned as part of the output in order to enhance it.\n\n    Input columns:\n        - messages (`List[Dict[Literal[\"role\", \"content\"], str]]`): The messages to generate the\n            follow up completion from.\n\n    Output columns:\n        - generation (`str`): The generated text from the assistant.\n        - model_name (`str`): The model name used to generate the text.\n\n    Categories:\n        - chat-generation\n\n    Icon:\n        `:material-chat:`\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `messages`.\"\"\"\n        return [\"messages\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the messages provided\n        are already formatted that way i.e. following the OpenAI chat format.\"\"\"\n\n        if not is_openai_format(input[\"messages\"]):\n            raise ValueError(\n                \"Input `instruction` must be a string or an OpenAI chat-like format. \"\n                f\"Got: {input['messages']}. Please check: 'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n            )\n\n        if input[\"messages\"][-1][\"role\"] != \"user\":\n            raise ValueError(\n                \"The last message must be from the user. Please check: \"\n                \"'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n            )\n\n        return input[\"messages\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ChatGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>messages</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ChatGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ChatGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the messages provided are already formatted that way i.e. following the OpenAI chat format.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the messages provided\n    are already formatted that way i.e. following the OpenAI chat format.\"\"\"\n\n    if not is_openai_format(input[\"messages\"]):\n        raise ValueError(\n            \"Input `instruction` must be a string or an OpenAI chat-like format. \"\n            f\"Got: {input['messages']}. Please check: 'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n        )\n\n    if input[\"messages\"][-1][\"role\"] != \"user\":\n        raise ValueError(\n            \"The last message must be from the user. Please check: \"\n            \"'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n        )\n\n    return input[\"messages\"]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ChatGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ComplexityScorer","title":"<code>ComplexityScorer</code>","text":"<p>               Bases: <code>Task</code></p> <p>Score instructions based on their complexity using an <code>LLM</code>.</p> <p><code>ComplexityScorer</code> is a pre-defined task used to rank a list of instructions based in their complexity. It's an implementation of the complexity score task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instructions (<code>List[str]</code>): The list of instructions to be scored.</li> </ul> Output columns <ul> <li>scores (<code>List[float]</code>): The score for each instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the scores.</li> </ul> Categories <ul> <li>scorer</li> <li>complexity</li> <li>instruction</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>class ComplexityScorer(Task):\n    \"\"\"Score instructions based on their complexity using an `LLM`.\n\n    `ComplexityScorer` is a pre-defined task used to rank a list of instructions based in\n    their complexity. It's an implementation of the complexity score task from the paper\n    'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection\n    in Instruction Tuning'.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instructions (`List[str]`): The list of instructions to be scored.\n\n    Output columns:\n        - scores (`List[float]`): The score for each instruction.\n        - model_name (`str`): The model name used to generate the scores.\n\n    Categories:\n        - scorer\n        - complexity\n        - instruction\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"complexity-scorer.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the `instructions`.\"\"\"\n        return [\"instructions\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(instructions=input[\"instructions\"]),  # type: ignore\n            }\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are: a list of `scores` containing the complexity score for each\n        instruction in `instructions`, and the `model_name`.\"\"\"\n        return [\"scores\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the score of each instruction.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the key `scores` containing the scores for each instruction.\n        \"\"\"\n        if output is None:\n            return {\"scores\": [None] * len(input[\"instructions\"])}\n\n        scores = []\n        score_lines = output.split(\"\\n\")\n        for i, line in enumerate(score_lines):\n            match = _PARSE_SCORE_LINE_REGEX.match(line)\n            score = float(match.group(1)) if match else None\n            scores.append(score)\n            if i == len(input[\"instructions\"]) - 1:\n                break\n        return {\"scores\": scores}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ComplexityScorer.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the <code>instructions</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ComplexityScorer.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are: a list of <code>scores</code> containing the complexity score for each instruction in <code>instructions</code>, and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ComplexityScorer.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(instructions=input[\"instructions\"]),  # type: ignore\n        }\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ComplexityScorer.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a list with the score of each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the key <code>scores</code> containing the scores for each instruction.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the score of each instruction.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the key `scores` containing the scores for each instruction.\n    \"\"\"\n    if output is None:\n        return {\"scores\": [None] * len(input[\"instructions\"])}\n\n    scores = []\n    score_lines = output.split(\"\\n\")\n    for i, line in enumerate(score_lines):\n        match = _PARSE_SCORE_LINE_REGEX.match(line)\n        score = float(match.group(1)) if match else None\n        scores.append(score)\n        if i == len(input[\"instructions\"]) - 1:\n            break\n    return {\"scores\": scores}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.ComplexityScorer.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"complexity-scorer.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolComplexity","title":"<code>EvolComplexity</code>","text":"<p>               Bases: <code>EvolInstruct</code></p> <p>Evolve instructions to make them more complex using an <code>LLM</code>.</p> <p><code>EvolComplexity</code> is a task that evolves instructions to make them more complex, and it is based in the EvolInstruct task, but using slight different prompts, but the exact same evolutionary approach.</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>Dict[str, str]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.</li> <li><code>seed</code>: The number of evolutions to be run.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction to evolve.</li> </ul> Output columns <ul> <li>evolved_instruction (<code>str</code>): The evolved instruction.</li> <li>answer (<code>str</code>, optional): The answer to the instruction if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>deita</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/evol_complexity/base.py</code> <pre><code>class EvolComplexity(EvolInstruct):\n    \"\"\"Evolve instructions to make them more complex using an `LLM`.\n\n    `EvolComplexity` is a task that evolves instructions to make them more complex,\n    and it is based in the EvolInstruct task, but using slight different prompts, but the\n    exact same evolutionary approach.\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\n        - `seed`: The number of evolutions to be run.\n\n    Input columns:\n        - instruction (`str`): The instruction to evolve.\n\n    Output columns:\n        - evolved_instruction (`str`): The evolved instruction.\n        - answer (`str`, optional): The answer to the instruction if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - deita\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolComplexityGenerator","title":"<code>EvolComplexityGenerator</code>","text":"<p>               Bases: <code>EvolInstructGenerator</code></p> <p>Generate evolved instructions with increased complexity using an <code>LLM</code>.</p> <p><code>EvolComplexityGenerator</code> is a generation task that evolves instructions to make them more complex, and it is based in the EvolInstruct task, but using slight different prompts, but the exact same evolutionary approach.</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>Dict[str, str]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.</li> <li><code>seed</code>: The number of evolutions to be run.</li> </ul> Output columns <ul> <li>instruction (<code>str</code>): The evolved instruction.</li> <li>answer (<code>str</code>, optional): The answer to the instruction if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>generation</li> <li>deita</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/evol_complexity/generator.py</code> <pre><code>class EvolComplexityGenerator(EvolInstructGenerator):\n    \"\"\"Generate evolved instructions with increased complexity using an `LLM`.\n\n    `EvolComplexityGenerator` is a generation task that evolves instructions to make\n    them more complex, and it is based in the EvolInstruct task, but using slight different\n    prompts, but the exact same evolutionary approach.\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\n        - `seed`: The number of evolutions to be run.\n\n    Output columns:\n        - instruction (`str`): The evolved instruction.\n        - answer (`str`, optional): The answer to the instruction if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - generation\n        - deita\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    mutation_templates: Dict[str, str] = GENERATION_MUTATION_TEMPLATES\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct","title":"<code>EvolInstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Evolve instructions using an <code>LLM</code>.</p> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> <p>Attributes:</p> Name Type Description <code>num_evolutions</code> <code>int</code> <p>The number of evolutions to be performed.</p> <code>store_evolutions</code> <code>bool</code> <p>Whether to store all the evolutions or just the last one. Defaults to <code>False</code>.</p> <code>generate_answers</code> <code>bool</code> <p>Whether to generate answers for the evolved instructions. Defaults to <code>False</code>.</p> <code>include_original_instruction</code> <code>bool</code> <p>Whether to include the original instruction in the <code>evolved_instructions</code> output column. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for evolving the instructions. Defaults to the ones provided in the <code>utils.py</code> file.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction to evolve.</li> </ul> Output columns <ul> <li>evolved_instruction (<code>str</code>): The evolved instruction if <code>store_evolutions=False</code>.</li> <li>evolved_instructions (<code>List[str]</code>): The evolved instructions if <code>store_evolutions=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> <li>answer (<code>str</code>): The answer to the evolved instruction if <code>generate_answers=True</code>     and <code>store_evolutions=False</code>.</li> <li>answers (<code>List[str]</code>): The answers to the evolved instructions if <code>generate_answers=True</code>     and <code>store_evolutions=True</code>.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> </ul> References <ul> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> <li>GitHub: h2oai/h2o-wizardlm</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>class EvolInstruct(Task):\n    \"\"\"Evolve instructions using an `LLM`.\n\n    WizardLM: Empowering Large Language Models to Follow Complex Instructions\n\n    Attributes:\n        num_evolutions: The number of evolutions to be performed.\n        store_evolutions: Whether to store all the evolutions or just the last one. Defaults\n            to `False`.\n        generate_answers: Whether to generate answers for the evolved instructions. Defaults\n            to `False`.\n        include_original_instruction: Whether to include the original instruction in the\n            `evolved_instructions` output column. Defaults to `False`.\n        mutation_templates: The mutation templates to be used for evolving the instructions.\n            Defaults to the ones provided in the `utils.py` file.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Input columns:\n        - instruction (`str`): The instruction to evolve.\n\n    Output columns:\n        - evolved_instruction (`str`): The evolved instruction if `store_evolutions=False`.\n        - evolved_instructions (`List[str]`): The evolved instructions if `store_evolutions=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n        - answer (`str`): The answer to the evolved instruction if `generate_answers=True`\n            and `store_evolutions=False`.\n        - answers (`List[str]`): The answers to the evolved instructions if `generate_answers=True`\n            and `store_evolutions=True`.\n\n    Categories:\n        - evol\n        - instruction\n\n    References:\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n        - [GitHub: h2oai/h2o-wizardlm](https://github.com/h2oai/h2o-wizardlm)\n    \"\"\"\n\n    num_evolutions: int\n    store_evolutions: bool = False\n    generate_answers: bool = False\n    include_original_instruction: bool = False\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to seed a random seed.\",\n    )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`.\"\"\"\n        return [\"instruction\"]\n\n    def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation. And the\n        `system_prompt` is added as the first message if it exists.\"\"\"\n        return [{\"role\": \"user\", \"content\": input}]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `evolved_instruction/s`, the `answer` if `generate_answers=True`\n        and the `model_name`.\"\"\"\n        # TODO: having to define a `model_name` column every time as the `Task.outputs` is not ideal,\n        # this could be handled always and the value could be included within the DAG validation when\n        # a `Task` is used, since all the `Task` subclasses will have an `llm` with a `model_name` attr.\n        _outputs = [\n            (\n                \"evolved_instruction\"\n                if not self.store_evolutions\n                else \"evolved_instructions\"\n            ),\n            \"model_name\",\n        ]\n        if self.generate_answers:\n            _outputs.append(\"answer\" if not self.store_evolutions else \"answers\")\n        return _outputs\n\n    @override\n    def format_output(  # type: ignore\n        self, instructions: Union[str, List[str]], answers: Optional[List[str]] = None\n    ) -&gt; Dict[str, Any]:  # type: ignore\n        \"\"\"The output for the task is a dict with: `evolved_instruction` or `evolved_instructions`,\n        depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n        `answer` if `generate_answers=True`; and, finally, the `model_name`.\n\n        Args:\n            instructions: The instructions to be included within the output.\n            answers: The answers to be included within the output if `generate_answers=True`.\n\n        Returns:\n            If `store_evolutions=False` and `generate_answers=True` return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};\n            if `store_evolutions=True` and `generate_answers=True` return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};\n            if `store_evolutions=False` and `generate_answers=False` return {\"evolved_instruction\": ..., \"model_name\": ...};\n            if `store_evolutions=True` and `generate_answers=False` return {\"evolved_instructions\": ..., \"model_name\": ...}.\n        \"\"\"\n        _output = {}\n        if not self.store_evolutions:\n            _output[\"evolved_instruction\"] = instructions[-1]\n        else:\n            _output[\"evolved_instructions\"] = instructions\n\n        if self.generate_answers and answers:\n            if not self.store_evolutions:\n                _output[\"answer\"] = answers[-1]\n            else:\n                _output[\"answers\"] = answers\n\n        _output[\"model_name\"] = self.llm.model_name\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates`.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, instruction: str) -&gt; str:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            instruction: The instruction to be included within the mutation prompt.\n\n        Returns:\n            A random mutation prompt with the provided instruction.\n        \"\"\"\n        mutation = np.random.choice(self.mutation_templates_names)\n        return self.mutation_templates[mutation].replace(\"&lt;PROMPT&gt;\", instruction)  # type: ignore\n\n    def _evolve_instructions(self, inputs: \"StepInput\") -&gt; List[List[str]]:\n        \"\"\"Evolves the instructions provided as part of the inputs of the task.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list where each item is a list with either the last evolved instruction if\n            `store_evolutions=False` or all the evolved instructions if `store_evolutions=True`.\n        \"\"\"\n\n        instructions: List[List[str]] = [[input[\"instruction\"]] for input in inputs]\n\n        for iter_no in range(self.num_evolutions):\n            formatted_prompts = []\n            for instruction in instructions:\n                formatted_prompts.append(self._apply_random_mutation(instruction[-1]))\n\n            formatted_prompts = [\n                self.format_input(prompt) for prompt in formatted_prompts\n            ]\n            generated_prompts = flatten_responses(\n                self.llm.generate(\n                    formatted_prompts,\n                    **self.llm.generation_kwargs,  # type: ignore\n                )\n            )\n\n            evolved_instructions = []\n            for generated_prompt in generated_prompts:\n                generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n                evolved_instructions.append(generated_prompt)\n\n            if self.store_evolutions:\n                instructions = [\n                    instruction + [evolved_instruction]\n                    for instruction, evolved_instruction in zip(\n                        instructions, evolved_instructions\n                    )\n                ]\n            else:\n                instructions = [\n                    [evolved_instruction]\n                    for evolved_instruction in evolved_instructions\n                ]\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} evolving {len(instructions)} instructions!\"\n            )\n\n        return instructions\n\n    def _generate_answers(\n        self, evolved_instructions: List[List[str]]\n    ) -&gt; List[List[str]]:\n        \"\"\"Generates the answer for the instructions in `instructions`.\n\n        Args:\n            evolved_instructions: A list of lists where each item is a list with either the last\n                evolved instruction if `store_evolutions=False` or all the evolved instructions\n                if `store_evolutions=True`.\n\n        Returns:\n            A list of answers for each instruction.\n        \"\"\"\n        formatted_instructions = [\n            self.format_input(instruction)\n            for instructions in evolved_instructions\n            for instruction in instructions\n        ]\n\n        responses = self.llm.generate(\n            formatted_instructions,\n            num_generations=1,\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n\n        step = (\n            self.num_evolutions\n            if not self.include_original_instruction\n            else self.num_evolutions + 1\n        )\n        return [\n            flatten_responses(responses[i : i + step])\n            for i in range(0, len(responses), step)\n        ]\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        evolved_instructions = self._evolve_instructions(inputs)\n\n        if self.store_evolutions:\n            # Remove the input instruction from the `evolved_instructions` list\n            from_ = 1 if not self.include_original_instruction else 0\n            evolved_instructions = [\n                instruction[from_:] for instruction in evolved_instructions\n            ]\n\n        if not self.generate_answers:\n            for input, instruction in zip(inputs, evolved_instructions):\n                input.update(self.format_output(instruction))\n            yield inputs\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished evolving {len(evolved_instructions)} instructions!\"\n        )\n\n        if self.generate_answers:\n            self._logger.info(\n                f\"\ud83e\udde0 Generating answers for the {len(evolved_instructions)} evolved instructions!\"\n            )\n\n            answers = self._generate_answers(evolved_instructions)\n\n            self._logger.info(\n                f\"\ud83c\udf89 Finished generating answers for the {len(evolved_instructions)} evolved\"\n                \" instructions!\"\n            )\n\n            for idx, (input, instruction) in enumerate(\n                zip(inputs, evolved_instructions)\n            ):\n                input.update(self.format_output(instruction, answers[idx]))\n            yield inputs\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>evolved_instruction/s</code>, the <code>answer</code> if <code>generate_answers=True</code> and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct._apply_random_mutation","title":"<code>_apply_random_mutation(instruction)</code>","text":"<p>Applies a random mutation from the ones provided as part of the <code>mutation_templates</code> enum, and returns the provided instruction within the mutation prompt.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction to be included within the mutation prompt.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A random mutation prompt with the provided instruction.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>def _apply_random_mutation(self, instruction: str) -&gt; str:\n    \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n    enum, and returns the provided instruction within the mutation prompt.\n\n    Args:\n        instruction: The instruction to be included within the mutation prompt.\n\n    Returns:\n        A random mutation prompt with the provided instruction.\n    \"\"\"\n    mutation = np.random.choice(self.mutation_templates_names)\n    return self.mutation_templates[mutation].replace(\"&lt;PROMPT&gt;\", instruction)  # type: ignore\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct._evolve_instructions","title":"<code>_evolve_instructions(inputs)</code>","text":"<p>Evolves the instructions provided as part of the inputs of the task.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>A list where each item is a list with either the last evolved instruction if</p> <code>List[List[str]]</code> <p><code>store_evolutions=False</code> or all the evolved instructions if <code>store_evolutions=True</code>.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>def _evolve_instructions(self, inputs: \"StepInput\") -&gt; List[List[str]]:\n    \"\"\"Evolves the instructions provided as part of the inputs of the task.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list where each item is a list with either the last evolved instruction if\n        `store_evolutions=False` or all the evolved instructions if `store_evolutions=True`.\n    \"\"\"\n\n    instructions: List[List[str]] = [[input[\"instruction\"]] for input in inputs]\n\n    for iter_no in range(self.num_evolutions):\n        formatted_prompts = []\n        for instruction in instructions:\n            formatted_prompts.append(self._apply_random_mutation(instruction[-1]))\n\n        formatted_prompts = [\n            self.format_input(prompt) for prompt in formatted_prompts\n        ]\n        generated_prompts = flatten_responses(\n            self.llm.generate(\n                formatted_prompts,\n                **self.llm.generation_kwargs,  # type: ignore\n            )\n        )\n\n        evolved_instructions = []\n        for generated_prompt in generated_prompts:\n            generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n            evolved_instructions.append(generated_prompt)\n\n        if self.store_evolutions:\n            instructions = [\n                instruction + [evolved_instruction]\n                for instruction, evolved_instruction in zip(\n                    instructions, evolved_instructions\n                )\n            ]\n        else:\n            instructions = [\n                [evolved_instruction]\n                for evolved_instruction in evolved_instructions\n            ]\n\n        self._logger.info(\n            f\"\ud83d\udd04 Ran iteration {iter_no} evolving {len(instructions)} instructions!\"\n        )\n\n    return instructions\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct._generate_answers","title":"<code>_generate_answers(evolved_instructions)</code>","text":"<p>Generates the answer for the instructions in <code>instructions</code>.</p> <p>Parameters:</p> Name Type Description Default <code>evolved_instructions</code> <code>List[List[str]]</code> <p>A list of lists where each item is a list with either the last evolved instruction if <code>store_evolutions=False</code> or all the evolved instructions if <code>store_evolutions=True</code>.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>A list of answers for each instruction.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>def _generate_answers(\n    self, evolved_instructions: List[List[str]]\n) -&gt; List[List[str]]:\n    \"\"\"Generates the answer for the instructions in `instructions`.\n\n    Args:\n        evolved_instructions: A list of lists where each item is a list with either the last\n            evolved instruction if `store_evolutions=False` or all the evolved instructions\n            if `store_evolutions=True`.\n\n    Returns:\n        A list of answers for each instruction.\n    \"\"\"\n    formatted_instructions = [\n        self.format_input(instruction)\n        for instructions in evolved_instructions\n        for instruction in instructions\n    ]\n\n    responses = self.llm.generate(\n        formatted_instructions,\n        num_generations=1,\n        **self.llm.generation_kwargs,  # type: ignore\n    )\n\n    step = (\n        self.num_evolutions\n        if not self.include_original_instruction\n        else self.num_evolutions + 1\n    )\n    return [\n        flatten_responses(responses[i : i + step])\n        for i in range(0, len(responses), step)\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation. And the <code>system_prompt</code> is added as the first message if it exists.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation. And the\n    `system_prompt` is added as the first message if it exists.\"\"\"\n    return [{\"role\": \"user\", \"content\": input}]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct.format_output","title":"<code>format_output(instructions, answers=None)</code>","text":"<p>The output for the task is a dict with: <code>evolved_instruction</code> or <code>evolved_instructions</code>, depending whether the value is either <code>False</code> or <code>True</code> for <code>store_evolutions</code>, respectively; <code>answer</code> if <code>generate_answers=True</code>; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>Union[str, List[str]]</code> <p>The instructions to be included within the output.</p> required <code>answers</code> <code>Optional[List[str]]</code> <p>The answers to be included within the output if <code>generate_answers=True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>If <code>store_evolutions=False</code> and <code>generate_answers=True</code> return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> and <code>generate_answers=True</code> return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=False</code> and <code>generate_answers=False</code> return {\"evolved_instruction\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> and <code>generate_answers=False</code> return {\"evolved_instructions\": ..., \"model_name\": ...}.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>@override\ndef format_output(  # type: ignore\n    self, instructions: Union[str, List[str]], answers: Optional[List[str]] = None\n) -&gt; Dict[str, Any]:  # type: ignore\n    \"\"\"The output for the task is a dict with: `evolved_instruction` or `evolved_instructions`,\n    depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n    `answer` if `generate_answers=True`; and, finally, the `model_name`.\n\n    Args:\n        instructions: The instructions to be included within the output.\n        answers: The answers to be included within the output if `generate_answers=True`.\n\n    Returns:\n        If `store_evolutions=False` and `generate_answers=True` return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};\n        if `store_evolutions=True` and `generate_answers=True` return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};\n        if `store_evolutions=False` and `generate_answers=False` return {\"evolved_instruction\": ..., \"model_name\": ...};\n        if `store_evolutions=True` and `generate_answers=False` return {\"evolved_instructions\": ..., \"model_name\": ...}.\n    \"\"\"\n    _output = {}\n    if not self.store_evolutions:\n        _output[\"evolved_instruction\"] = instructions[-1]\n    else:\n        _output[\"evolved_instructions\"] = instructions\n\n    if self.generate_answers and answers:\n        if not self.store_evolutions:\n            _output[\"answer\"] = answers[-1]\n        else:\n            _output[\"answers\"] = answers\n\n    _output[\"model_name\"] = self.llm.model_name\n    return _output\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstruct.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    evolved_instructions = self._evolve_instructions(inputs)\n\n    if self.store_evolutions:\n        # Remove the input instruction from the `evolved_instructions` list\n        from_ = 1 if not self.include_original_instruction else 0\n        evolved_instructions = [\n            instruction[from_:] for instruction in evolved_instructions\n        ]\n\n    if not self.generate_answers:\n        for input, instruction in zip(inputs, evolved_instructions):\n            input.update(self.format_output(instruction))\n        yield inputs\n\n    self._logger.info(\n        f\"\ud83c\udf89 Finished evolving {len(evolved_instructions)} instructions!\"\n    )\n\n    if self.generate_answers:\n        self._logger.info(\n            f\"\ud83e\udde0 Generating answers for the {len(evolved_instructions)} evolved instructions!\"\n        )\n\n        answers = self._generate_answers(evolved_instructions)\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished generating answers for the {len(evolved_instructions)} evolved\"\n            \" instructions!\"\n        )\n\n        for idx, (input, instruction) in enumerate(\n            zip(inputs, evolved_instructions)\n        ):\n            input.update(self.format_output(instruction, answers[idx]))\n        yield inputs\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator","title":"<code>EvolInstructGenerator</code>","text":"<p>               Bases: <code>GeneratorTask</code></p> <p>Generate evolved instructions using an <code>LLM</code>.</p> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <code>int</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <code>bool</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>RuntimeParameter[int]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>RuntimeParameter[int]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs     to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs     to be lower than, to be considered valid.</li> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Output columns <ul> <li>instruction (<code>str</code>): The generated instruction if <code>generate_answers=False</code>.</li> <li>answer (<code>str</code>): The generated answer if <code>generate_answers=True</code>.</li> <li>instructions (<code>List[str]</code>): The generated instructions if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to generate and evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>generation</li> </ul> References <ul> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> <li>GitHub: h2oai/h2o-wizardlm</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>class EvolInstructGenerator(GeneratorTask):\n    \"\"\"Generate evolved instructions using an `LLM`.\n\n    WizardLM: Empowering Large Language Models to Follow Complex Instructions\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs\n            to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs\n            to be lower than, to be considered valid.\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Output columns:\n        - instruction (`str`): The generated instruction if `generate_answers=False`.\n        - answer (`str`): The generated answer if `generate_answers=True`.\n        - instructions (`List[str]`): The generated instructions if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to generate and evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - generation\n\n    References:\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n        - [GitHub: h2oai/h2o-wizardlm](https://github.com/h2oai/h2o-wizardlm)\n    \"\"\"\n\n    num_instructions: int\n    generate_answers: bool = False\n    mutation_templates: Dict[str, str] = GENERATION_MUTATION_TEMPLATES\n\n    min_length: RuntimeParameter[int] = Field(\n        default=512,\n        description=\"Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\",\n    )\n    max_length: RuntimeParameter[int] = Field(\n        default=1024,\n        description=\"Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\",\n    )\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to seed a random seed.\",\n    )\n    _seed_texts: Optional[List[str]] = PrivateAttr(default_factory=list)\n    _prompts: Optional[List[str]] = PrivateAttr(default_factory=list)\n\n    def _generate_seed_texts(self) -&gt; List[str]:\n        \"\"\"Generates a list of seed texts to be used as part of the starting prompts for the task.\n\n        It will use the `FRESH_START` mutation template, as it needs to generate text from scratch; and\n        a list of English words will be used to generate the seed texts that will be provided to the\n        mutation method and included within the prompt.\n\n        Returns:\n            A list of seed texts to be used as part of the starting prompts for the task.\n        \"\"\"\n        seed_texts = []\n        for _ in range(self.num_instructions * 10):\n            num_words = np.random.choice([1, 2, 3, 4])\n            seed_texts.append(\n                self.mutation_templates[\"FRESH_START\"].replace(  # type: ignore\n                    \"&lt;PROMPT&gt;\",\n                    \", \".join(\n                        [\n                            np.random.choice(self._english_nouns).strip()\n                            for _ in range(num_words)\n                        ]\n                    ),\n                )\n            )\n        return seed_texts\n\n    @override\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n        This is useful if you want to do some validation that requires the entire model to be initialized.\n        \"\"\"\n        super().model_post_init(__context)\n\n        np.random.seed(self.seed)\n\n        self._seed_texts = self._generate_seed_texts()\n        self._prompts = [\n            np.random.choice(self._seed_texts) for _ in range(self.num_instructions)\n        ]\n\n    @cached_property\n    def _english_nouns(self) -&gt; List[str]:\n        \"\"\"A list of English nouns to be used as part of the starting prompts for the task.\n\n        References:\n            - https://github.com/h2oai/h2o-wizardlm\n        \"\"\"\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps/tasks/evol_instruct/english_nouns.txt\"\n        )\n        with open(_path, mode=\"r\") as f:\n            return [line.strip() for line in f.readlines()]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `instruction`, the `answer` if `generate_answers=True`\n        and the `model_name`.\"\"\"\n        _outputs = [\"instruction\", \"model_name\"]\n        if self.generate_answers:\n            _outputs.append(\"answer\")\n        return _outputs\n\n    def format_output(  # type: ignore\n        self, instruction: str, answer: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output for the task is a dict with: `instruction`; `answer` if `generate_answers=True`;\n        and, finally, the `model_name`.\n\n        Args:\n            instruction: The instruction to be included within the output.\n            answer: The answer to be included within the output if `generate_answers=True`.\n\n        Returns:\n            If `generate_answers=True` return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};\n            if `generate_answers=False` return {\"instruction\": ..., \"model_name\": ...};\n        \"\"\"\n        _output = {\n            \"instruction\": instruction,\n            \"model_name\": self.llm.model_name,\n        }\n        if self.generate_answers and answer is not None:\n            _output[\"answer\"] = answer\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates`.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, iter_no: int) -&gt; List[\"ChatType\"]:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            iter_no: The iteration number to be used to check whether the iteration is the\n                first one i.e. FRESH_START, or not.\n\n        Returns:\n            A random mutation prompt with the provided instruction formatted as an OpenAI conversation.\n        \"\"\"\n        prompts = []\n        for idx in range(self.num_instructions):\n            if (\n                iter_no == 0\n                or \"Write one question or request containing\" in self._prompts[idx]  # type: ignore\n            ):\n                mutation = \"FRESH_START\"\n            else:\n                mutation = np.random.choice(self.mutation_templates_names)\n                if mutation == \"FRESH_START\":\n                    self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n\n            prompt_with_template = (\n                self.mutation_templates[mutation].replace(  # type: ignore\n                    \"&lt;PROMPT&gt;\",\n                    self._prompts[idx],  # type: ignore\n                )  # type: ignore\n                if iter_no != 0\n                else self._prompts[idx]  # type: ignore\n            )\n            prompts.append([{\"role\": \"user\", \"content\": prompt_with_template}])\n        return prompts\n\n    def _generate_answers(self, instructions: List[List[str]]) -&gt; List[str]:\n        \"\"\"Generates the answer for the last instruction in `instructions`.\n\n        Args:\n            instructions: A list of lists where each item is a list with either the last\n                evolved instruction if `store_evolutions=False` or all the evolved instructions\n                if `store_evolutions=True`.\n\n        Returns:\n            A list of answers for the last instruction in `instructions`.\n        \"\"\"\n        # TODO: update to generate answers for all the instructions\n        _formatted_instructions = [\n            [{\"role\": \"user\", \"content\": instruction[-1]}]\n            for instruction in instructions\n        ]\n        responses = self.llm.generate(\n            _formatted_instructions,\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n        return flatten_responses(responses)\n\n    @override\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task, and a boolean\n            flag indicating whether the task has finished or not i.e. is the last batch.\n        \"\"\"\n        instructions = []\n        mutation_no = 0\n\n        iter_no = 0\n        while len(instructions) &lt; self.num_instructions:\n            prompts = self._apply_random_mutation(iter_no=iter_no)\n\n            generated_prompts = flatten_responses(\n                self.llm.generate(prompts, **self.llm.generation_kwargs)  # type: ignore\n            )\n            for idx, generated_prompt in enumerate(generated_prompts):\n                generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n                if self.max_length &gt;= len(generated_prompt) &gt;= self.min_length:  # type: ignore\n                    instructions.append(generated_prompt)\n                    self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n                else:\n                    self._prompts[idx] = generated_prompt  # type: ignore\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} with {len(instructions)} instructions already evolved!\"\n            )\n            iter_no += 1\n\n            if len(instructions) &gt; self.num_instructions:\n                instructions = instructions[: self.num_instructions]\n            if len(instructions) &gt; mutation_no:\n                mutation_no = len(instructions) - mutation_no\n\n            if not self.generate_answers and len(instructions[-mutation_no:]) &gt; 0:\n                yield (\n                    [\n                        self.format_output(mutated_instruction)\n                        for mutated_instruction in instructions[-mutation_no:]\n                    ],\n                    len(instructions) &gt;= self.num_instructions,\n                )\n\n        self._logger.info(f\"\ud83c\udf89 Finished evolving {len(instructions)} instructions!\")\n\n        if self.generate_answers:\n            self._logger.info(\n                f\"\ud83e\udde0 Generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            answers = self._generate_answers(instructions)\n\n            self._logger.info(\n                f\"\ud83c\udf89 Finished generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            yield (\n                [\n                    self.format_output(instruction, answer)\n                    for instruction, answer in zip(instructions, answers)\n                ],\n                True,\n            )\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator._english_nouns","title":"<code>_english_nouns: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>A list of English nouns to be used as part of the starting prompts for the task.</p> References <ul> <li>https://github.com/h2oai/h2o-wizardlm</li> </ul>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>instruction</code>, the <code>answer</code> if <code>generate_answers=True</code> and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator._apply_random_mutation","title":"<code>_apply_random_mutation(iter_no)</code>","text":"<p>Applies a random mutation from the ones provided as part of the <code>mutation_templates</code> enum, and returns the provided instruction within the mutation prompt.</p> <p>Parameters:</p> Name Type Description Default <code>iter_no</code> <code>int</code> <p>The iteration number to be used to check whether the iteration is the first one i.e. FRESH_START, or not.</p> required <p>Returns:</p> Type Description <code>List[ChatType]</code> <p>A random mutation prompt with the provided instruction formatted as an OpenAI conversation.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>def _apply_random_mutation(self, iter_no: int) -&gt; List[\"ChatType\"]:\n    \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n    enum, and returns the provided instruction within the mutation prompt.\n\n    Args:\n        iter_no: The iteration number to be used to check whether the iteration is the\n            first one i.e. FRESH_START, or not.\n\n    Returns:\n        A random mutation prompt with the provided instruction formatted as an OpenAI conversation.\n    \"\"\"\n    prompts = []\n    for idx in range(self.num_instructions):\n        if (\n            iter_no == 0\n            or \"Write one question or request containing\" in self._prompts[idx]  # type: ignore\n        ):\n            mutation = \"FRESH_START\"\n        else:\n            mutation = np.random.choice(self.mutation_templates_names)\n            if mutation == \"FRESH_START\":\n                self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n\n        prompt_with_template = (\n            self.mutation_templates[mutation].replace(  # type: ignore\n                \"&lt;PROMPT&gt;\",\n                self._prompts[idx],  # type: ignore\n            )  # type: ignore\n            if iter_no != 0\n            else self._prompts[idx]  # type: ignore\n        )\n        prompts.append([{\"role\": \"user\", \"content\": prompt_with_template}])\n    return prompts\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator._generate_answers","title":"<code>_generate_answers(instructions)</code>","text":"<p>Generates the answer for the last instruction in <code>instructions</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>List[List[str]]</code> <p>A list of lists where each item is a list with either the last evolved instruction if <code>store_evolutions=False</code> or all the evolved instructions if <code>store_evolutions=True</code>.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of answers for the last instruction in <code>instructions</code>.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>def _generate_answers(self, instructions: List[List[str]]) -&gt; List[str]:\n    \"\"\"Generates the answer for the last instruction in `instructions`.\n\n    Args:\n        instructions: A list of lists where each item is a list with either the last\n            evolved instruction if `store_evolutions=False` or all the evolved instructions\n            if `store_evolutions=True`.\n\n    Returns:\n        A list of answers for the last instruction in `instructions`.\n    \"\"\"\n    # TODO: update to generate answers for all the instructions\n    _formatted_instructions = [\n        [{\"role\": \"user\", \"content\": instruction[-1]}]\n        for instruction in instructions\n    ]\n    responses = self.llm.generate(\n        _formatted_instructions,\n        **self.llm.generation_kwargs,  # type: ignore\n    )\n    return flatten_responses(responses)\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator._generate_seed_texts","title":"<code>_generate_seed_texts()</code>","text":"<p>Generates a list of seed texts to be used as part of the starting prompts for the task.</p> <p>It will use the <code>FRESH_START</code> mutation template, as it needs to generate text from scratch; and a list of English words will be used to generate the seed texts that will be provided to the mutation method and included within the prompt.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of seed texts to be used as part of the starting prompts for the task.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>def _generate_seed_texts(self) -&gt; List[str]:\n    \"\"\"Generates a list of seed texts to be used as part of the starting prompts for the task.\n\n    It will use the `FRESH_START` mutation template, as it needs to generate text from scratch; and\n    a list of English words will be used to generate the seed texts that will be provided to the\n    mutation method and included within the prompt.\n\n    Returns:\n        A list of seed texts to be used as part of the starting prompts for the task.\n    \"\"\"\n    seed_texts = []\n    for _ in range(self.num_instructions * 10):\n        num_words = np.random.choice([1, 2, 3, 4])\n        seed_texts.append(\n            self.mutation_templates[\"FRESH_START\"].replace(  # type: ignore\n                \"&lt;PROMPT&gt;\",\n                \", \".join(\n                    [\n                        np.random.choice(self._english_nouns).strip()\n                        for _ in range(num_words)\n                    ]\n                ),\n            )\n        )\n    return seed_texts\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator.format_output","title":"<code>format_output(instruction, answer=None)</code>","text":"<p>The output for the task is a dict with: <code>instruction</code>; <code>answer</code> if <code>generate_answers=True</code>; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction to be included within the output.</p> required <code>answer</code> <code>Optional[str]</code> <p>The answer to be included within the output if <code>generate_answers=True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>If <code>generate_answers=True</code> return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>generate_answers=False</code> return {\"instruction\": ..., \"model_name\": ...};</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>def format_output(  # type: ignore\n    self, instruction: str, answer: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"The output for the task is a dict with: `instruction`; `answer` if `generate_answers=True`;\n    and, finally, the `model_name`.\n\n    Args:\n        instruction: The instruction to be included within the output.\n        answer: The answer to be included within the output if `generate_answers=True`.\n\n    Returns:\n        If `generate_answers=True` return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};\n        if `generate_answers=False` return {\"instruction\": ..., \"model_name\": ...};\n    \"\"\"\n    _output = {\n        \"instruction\": instruction,\n        \"model_name\": self.llm.model_name,\n    }\n    if self.generate_answers and answer is not None:\n        _output[\"answer\"] = answer\n    return _output\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Override this method to perform additional initialization after <code>__init__</code> and <code>model_construct</code>. This is useful if you want to do some validation that requires the entire model to be initialized.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>@override\ndef model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n    This is useful if you want to do some validation that requires the entire model to be initialized.\n    \"\"\"\n    super().model_post_init(__context)\n\n    np.random.seed(self.seed)\n\n    self._seed_texts = self._generate_seed_texts()\n    self._prompts = [\n        np.random.choice(self._seed_texts) for _ in range(self.num_instructions)\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolInstructGenerator.process","title":"<code>process(offset=0)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>A list of Python dictionaries with the outputs of the task, and a boolean</p> <code>GeneratorStepOutput</code> <p>flag indicating whether the task has finished or not i.e. is the last batch.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>@override\ndef process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task, and a boolean\n        flag indicating whether the task has finished or not i.e. is the last batch.\n    \"\"\"\n    instructions = []\n    mutation_no = 0\n\n    iter_no = 0\n    while len(instructions) &lt; self.num_instructions:\n        prompts = self._apply_random_mutation(iter_no=iter_no)\n\n        generated_prompts = flatten_responses(\n            self.llm.generate(prompts, **self.llm.generation_kwargs)  # type: ignore\n        )\n        for idx, generated_prompt in enumerate(generated_prompts):\n            generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n            if self.max_length &gt;= len(generated_prompt) &gt;= self.min_length:  # type: ignore\n                instructions.append(generated_prompt)\n                self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n            else:\n                self._prompts[idx] = generated_prompt  # type: ignore\n\n        self._logger.info(\n            f\"\ud83d\udd04 Ran iteration {iter_no} with {len(instructions)} instructions already evolved!\"\n        )\n        iter_no += 1\n\n        if len(instructions) &gt; self.num_instructions:\n            instructions = instructions[: self.num_instructions]\n        if len(instructions) &gt; mutation_no:\n            mutation_no = len(instructions) - mutation_no\n\n        if not self.generate_answers and len(instructions[-mutation_no:]) &gt; 0:\n            yield (\n                [\n                    self.format_output(mutated_instruction)\n                    for mutated_instruction in instructions[-mutation_no:]\n                ],\n                len(instructions) &gt;= self.num_instructions,\n            )\n\n    self._logger.info(f\"\ud83c\udf89 Finished evolving {len(instructions)} instructions!\")\n\n    if self.generate_answers:\n        self._logger.info(\n            f\"\ud83e\udde0 Generating answers for the {len(instructions)} evolved instructions!\"\n        )\n\n        answers = self._generate_answers(instructions)\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished generating answers for the {len(instructions)} evolved instructions!\"\n        )\n\n        yield (\n            [\n                self.format_output(instruction, answer)\n                for instruction, answer in zip(instructions, answers)\n            ],\n            True,\n        )\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality","title":"<code>EvolQuality</code>","text":"<p>               Bases: <code>Task</code></p> <p>Evolve the quality of the responses using an <code>LLM</code>.</p> <p><code>EvolQuality</code> task is used to evolve the quality of the responses given a prompt, by generating a new response with a language model. This step implements the evolution quality task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>num_evolutions</code> <code>int</code> <p>The number of evolutions to be performed on the responses.</p> <code>store_evolutions</code> <code>bool</code> <p>Whether to store all the evolved responses or just the last one. Defaults to <code>False</code>.</p> <code>include_original_response</code> <code>bool</code> <p>Whether to include the original response within the evolved responses. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used to evolve the responses.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the <code>responses</code>.</li> <li>response (<code>str</code>): The responses to be rewritten.</li> </ul> Output columns <ul> <li>evolved_response (<code>str</code>): The evolved response if <code>store_evolutions=False</code>.</li> <li>evolved_responses (<code>List[str]</code>): The evolved responses if <code>store_evolutions=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the responses.</li> </ul> Categories <ul> <li>evol</li> <li>response</li> <li>deita</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>class EvolQuality(Task):\n    \"\"\"Evolve the quality of the responses using an `LLM`.\n\n    `EvolQuality` task is used to evolve the quality of the responses given a prompt,\n    by generating a new response with a language model. This step implements the evolution\n    quality task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of\n    Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        num_evolutions: The number of evolutions to be performed on the responses.\n        store_evolutions: Whether to store all the evolved responses or just the last one.\n            Defaults to `False`.\n        include_original_response: Whether to include the original response within the evolved\n            responses. Defaults to `False`.\n        mutation_templates: The mutation templates to be used to evolve the responses.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the `responses`.\n        - response (`str`): The responses to be rewritten.\n\n    Output columns:\n        - evolved_response (`str`): The evolved response if `store_evolutions=False`.\n        - evolved_responses (`List[str]`): The evolved responses if `store_evolutions=True`.\n        - model_name (`str`): The name of the LLM used to evolve the responses.\n\n    Categories:\n        - evol\n        - response\n        - deita\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    num_evolutions: int\n    store_evolutions: bool = False\n    include_original_response: bool = False\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to set a random seed.\",\n    )\n\n    @override\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n        This is useful if you want to do some validation that requires the entire model to be initialized.\n        \"\"\"\n        super().model_post_init(__context)\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `instruction` and `response`.\"\"\"\n        return [\"instruction\", \"response\"]\n\n    def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation. And the\n        `system_prompt` is added as the first message if it exists.\"\"\"\n        return [{\"role\": \"user\", \"content\": input}]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `evolved_response/s` and the `model_name`.\"\"\"\n        # TODO: having to define a `model_name` column every time as the `Task.outputs` is not ideal,\n        # this could be handled always and the value could be included within the DAG validation when\n        # a `Task` is used, since all the `Task` subclasses will have an `llm` with a `model_name` attr.\n        _outputs = [\n            (\"evolved_response\" if not self.store_evolutions else \"evolved_responses\"),\n            \"model_name\",\n        ]\n\n        return _outputs\n\n    def format_output(self, responses: Union[str, List[str]]) -&gt; Dict[str, Any]:  # type: ignore\n        \"\"\"The output for the task is a dict with: `evolved_response` or `evolved_responses`,\n        depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n        and, finally, the `model_name`.\n\n        Args:\n            responses: The responses to be included within the output.\n\n        Returns:\n            if `store_evolutions=False` return {\"evolved_response\": ..., \"model_name\": ...};\n            if `store_evolutions=True` return {\"evolved_responses\": ..., \"model_name\": ...}.\n        \"\"\"\n        _output = {}\n\n        if not self.store_evolutions:\n            _output[\"evolved_response\"] = responses[-1]\n        else:\n            _output[\"evolved_responses\"] = responses\n\n        _output[\"model_name\"] = self.llm.model_name\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates` enum.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, instruction: str, response: str) -&gt; str:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            instruction: The instruction to be included within the mutation prompt.\n\n        Returns:\n            A random mutation prompt with the provided instruction.\n        \"\"\"\n        mutation = np.random.choice(self.mutation_templates_names)\n        return (\n            self.mutation_templates[mutation]\n            .replace(\"&lt;PROMPT&gt;\", instruction)\n            .replace(\"&lt;RESPONSE&gt;\", response)\n        )\n\n    def _evolve_reponses(self, inputs: \"StepInput\") -&gt; List[List[str]]:\n        \"\"\"Evolves the instructions provided as part of the inputs of the task.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list where each item is a list with either the last evolved instruction if\n            `store_evolutions=False` or all the evolved instructions if `store_evolutions=True`.\n        \"\"\"\n        np.random.seed(self.seed)\n        instructions: List[List[str]] = [[input[\"instruction\"]] for input in inputs]\n        responses: List[List[str]] = [[input[\"response\"]] for input in inputs]\n\n        for iter_no in range(self.num_evolutions):\n            formatted_prompts = []\n            for instruction, response in zip(instructions, responses):\n                formatted_prompts.append(\n                    self._apply_random_mutation(instruction[-1], response[-1])\n                )\n\n            formatted_prompts = [\n                self.format_input(prompt) for prompt in formatted_prompts\n            ]\n\n            generated_responses = self.llm.generate(\n                formatted_prompts,\n                **self.llm.generation_kwargs,  # type: ignore\n            )\n\n            if self.store_evolutions:\n                responses = [\n                    response + [evolved_response[0]]\n                    for response, evolved_response in zip(\n                        responses, generated_responses\n                    )\n                ]\n            else:\n                responses = [\n                    [evolved_response[0]] for evolved_response in generated_responses\n                ]\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} evolving {len(responses)} responses!\"\n            )\n\n        return responses\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        responses = self._evolve_reponses(inputs)\n\n        if self.store_evolutions:\n            # Remove the input instruction from the `evolved_responses` list\n            from_ = 1 if not self.include_original_response else 0\n            responses = [response[from_:] for response in responses]\n\n        for input, response in zip(inputs, responses):\n            input.update(self.format_output(response))\n        yield inputs\n\n        self._logger.info(f\"\ud83c\udf89 Finished evolving {len(responses)} instructions!\")\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>instruction</code> and <code>response</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code> enum.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>evolved_response/s</code> and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality._apply_random_mutation","title":"<code>_apply_random_mutation(instruction, response)</code>","text":"<p>Applies a random mutation from the ones provided as part of the <code>mutation_templates</code> enum, and returns the provided instruction within the mutation prompt.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction to be included within the mutation prompt.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A random mutation prompt with the provided instruction.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>def _apply_random_mutation(self, instruction: str, response: str) -&gt; str:\n    \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n    enum, and returns the provided instruction within the mutation prompt.\n\n    Args:\n        instruction: The instruction to be included within the mutation prompt.\n\n    Returns:\n        A random mutation prompt with the provided instruction.\n    \"\"\"\n    mutation = np.random.choice(self.mutation_templates_names)\n    return (\n        self.mutation_templates[mutation]\n        .replace(\"&lt;PROMPT&gt;\", instruction)\n        .replace(\"&lt;RESPONSE&gt;\", response)\n    )\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality._evolve_reponses","title":"<code>_evolve_reponses(inputs)</code>","text":"<p>Evolves the instructions provided as part of the inputs of the task.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>A list where each item is a list with either the last evolved instruction if</p> <code>List[List[str]]</code> <p><code>store_evolutions=False</code> or all the evolved instructions if <code>store_evolutions=True</code>.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>def _evolve_reponses(self, inputs: \"StepInput\") -&gt; List[List[str]]:\n    \"\"\"Evolves the instructions provided as part of the inputs of the task.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list where each item is a list with either the last evolved instruction if\n        `store_evolutions=False` or all the evolved instructions if `store_evolutions=True`.\n    \"\"\"\n    np.random.seed(self.seed)\n    instructions: List[List[str]] = [[input[\"instruction\"]] for input in inputs]\n    responses: List[List[str]] = [[input[\"response\"]] for input in inputs]\n\n    for iter_no in range(self.num_evolutions):\n        formatted_prompts = []\n        for instruction, response in zip(instructions, responses):\n            formatted_prompts.append(\n                self._apply_random_mutation(instruction[-1], response[-1])\n            )\n\n        formatted_prompts = [\n            self.format_input(prompt) for prompt in formatted_prompts\n        ]\n\n        generated_responses = self.llm.generate(\n            formatted_prompts,\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n\n        if self.store_evolutions:\n            responses = [\n                response + [evolved_response[0]]\n                for response, evolved_response in zip(\n                    responses, generated_responses\n                )\n            ]\n        else:\n            responses = [\n                [evolved_response[0]] for evolved_response in generated_responses\n            ]\n\n        self._logger.info(\n            f\"\ud83d\udd04 Ran iteration {iter_no} evolving {len(responses)} responses!\"\n        )\n\n    return responses\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation. And the <code>system_prompt</code> is added as the first message if it exists.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation. And the\n    `system_prompt` is added as the first message if it exists.\"\"\"\n    return [{\"role\": \"user\", \"content\": input}]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality.format_output","title":"<code>format_output(responses)</code>","text":"<p>The output for the task is a dict with: <code>evolved_response</code> or <code>evolved_responses</code>, depending whether the value is either <code>False</code> or <code>True</code> for <code>store_evolutions</code>, respectively; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>Union[str, List[str]]</code> <p>The responses to be included within the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>if <code>store_evolutions=False</code> return {\"evolved_response\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> return {\"evolved_responses\": ..., \"model_name\": ...}.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>def format_output(self, responses: Union[str, List[str]]) -&gt; Dict[str, Any]:  # type: ignore\n    \"\"\"The output for the task is a dict with: `evolved_response` or `evolved_responses`,\n    depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n    and, finally, the `model_name`.\n\n    Args:\n        responses: The responses to be included within the output.\n\n    Returns:\n        if `store_evolutions=False` return {\"evolved_response\": ..., \"model_name\": ...};\n        if `store_evolutions=True` return {\"evolved_responses\": ..., \"model_name\": ...}.\n    \"\"\"\n    _output = {}\n\n    if not self.store_evolutions:\n        _output[\"evolved_response\"] = responses[-1]\n    else:\n        _output[\"evolved_responses\"] = responses\n\n    _output[\"model_name\"] = self.llm.model_name\n    return _output\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Override this method to perform additional initialization after <code>__init__</code> and <code>model_construct</code>. This is useful if you want to do some validation that requires the entire model to be initialized.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>@override\ndef model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n    This is useful if you want to do some validation that requires the entire model to be initialized.\n    \"\"\"\n    super().model_post_init(__context)\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.EvolQuality.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    responses = self._evolve_reponses(inputs)\n\n    if self.store_evolutions:\n        # Remove the input instruction from the `evolved_responses` list\n        from_ = 1 if not self.include_original_response else 0\n        responses = [response[from_:] for response in responses]\n\n    for input, response in zip(inputs, responses):\n        input.update(self.format_output(response))\n    yield inputs\n\n    self._logger.info(f\"\ud83c\udf89 Finished evolving {len(responses)} instructions!\")\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateEmbeddings","title":"<code>GenerateEmbeddings</code>","text":"<p>               Bases: <code>Step</code></p> <p>Generate embeddings using the last hidden state of an <code>LLM</code>.</p> <p>Generate embeddings for a text input using the last hidden state of an <code>LLM</code>, as described in the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>The <code>LLM</code> to use to generate the embeddings.</p> Input columns <ul> <li>text (<code>str</code>, <code>List[Dict[str, str]]</code>): The input text or conversation to generate     embeddings for.</li> </ul> Output columns <ul> <li>embedding (<code>List[float]</code>): The embedding of the input text or conversation.</li> <li>model_name (<code>str</code>): The model name used to generate the embeddings.</li> </ul> Categories <ul> <li>embedding</li> <li>llm</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> </ul> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>class GenerateEmbeddings(Step):\n    \"\"\"Generate embeddings using the last hidden state of an `LLM`.\n\n    Generate embeddings for a text input using the last hidden state of an `LLM`, as\n    described in the paper 'What Makes Good Data for Alignment? A Comprehensive Study of\n    Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        llm: The `LLM` to use to generate the embeddings.\n\n    Input columns:\n        - text (`str`, `List[Dict[str, str]]`): The input text or conversation to generate\n            embeddings for.\n\n    Output columns:\n        - embedding (`List[float]`): The embedding of the input text or conversation.\n        - model_name (`str`): The model name used to generate the embeddings.\n\n    Categories:\n        - embedding\n        - llm\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    llm: LLM\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `LLM` used to generate the embeddings.\"\"\"\n        super().load()\n\n        self.llm.load()\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task is a `text` column containing either a string or a\n        list of dictionaries in OpenAI chat-like format.\"\"\"\n        return [\"text\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task is an `embedding` column containing the embedding of\n        the `text` input.\"\"\"\n        return [\"embedding\", \"model_name\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"Formats the input to be used by the LLM to generate the embeddings. The input\n        can be in `ChatType` format or a string. If a string, it will be converted to a\n        list of dictionaries in OpenAI chat-like format.\n\n        Args:\n            input: The input to format.\n\n        Returns:\n            The OpenAI chat-like format of the input.\n        \"\"\"\n        text = input[\"text\"] = input[\"text\"]\n\n        # input is in `ChatType` format\n        if isinstance(text, str):\n            return [{\"role\": \"user\", \"content\": text}]\n\n        if is_openai_format(text):\n            return text\n\n        raise ValueError(\n            f\"Couldn't format input for step {self.name}. The `text` input column has to\"\n            \" be a string or a list of dictionaries in OpenAI chat-like format.\"\n        )\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Generates an embedding for each input using the last hidden state of the `LLM`.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        formatted_inputs = [self.format_input(input) for input in inputs]\n        last_hidden_states = self.llm.get_last_hidden_states(formatted_inputs)\n        for input, hidden_state in zip(inputs, last_hidden_states):\n            input[\"embedding\"] = hidden_state[-1].tolist()\n            input[\"model_name\"] = self.llm.model_name\n        yield inputs\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateEmbeddings.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task is a <code>text</code> column containing either a string or a list of dictionaries in OpenAI chat-like format.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateEmbeddings.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task is an <code>embedding</code> column containing the embedding of the <code>text</code> input.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateEmbeddings.format_input","title":"<code>format_input(input)</code>","text":"<p>Formats the input to be used by the LLM to generate the embeddings. The input can be in <code>ChatType</code> format or a string. If a string, it will be converted to a list of dictionaries in OpenAI chat-like format.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, Any]</code> <p>The input to format.</p> required <p>Returns:</p> Type Description <code>ChatType</code> <p>The OpenAI chat-like format of the input.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"Formats the input to be used by the LLM to generate the embeddings. The input\n    can be in `ChatType` format or a string. If a string, it will be converted to a\n    list of dictionaries in OpenAI chat-like format.\n\n    Args:\n        input: The input to format.\n\n    Returns:\n        The OpenAI chat-like format of the input.\n    \"\"\"\n    text = input[\"text\"] = input[\"text\"]\n\n    # input is in `ChatType` format\n    if isinstance(text, str):\n        return [{\"role\": \"user\", \"content\": text}]\n\n    if is_openai_format(text):\n        return text\n\n    raise ValueError(\n        f\"Couldn't format input for step {self.name}. The `text` input column has to\"\n        \" be a string or a list of dictionaries in OpenAI chat-like format.\"\n    )\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateEmbeddings.load","title":"<code>load()</code>","text":"<p>Loads the <code>LLM</code> used to generate the embeddings.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `LLM` used to generate the embeddings.\"\"\"\n    super().load()\n\n    self.llm.load()\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateEmbeddings.process","title":"<code>process(inputs)</code>","text":"<p>Generates an embedding for each input using the last hidden state of the <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Generates an embedding for each input using the last hidden state of the `LLM`.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    formatted_inputs = [self.format_input(input) for input in inputs]\n    last_hidden_states = self.llm.get_last_hidden_states(formatted_inputs)\n    for input, hidden_state in zip(inputs, last_hidden_states):\n        input[\"embedding\"] = hidden_state[-1].tolist()\n        input[\"model_name\"] = self.llm.model_name\n    yield inputs\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateSentencePair","title":"<code>GenerateSentencePair</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate a positive and negative (optionally) sentences given an anchor sentence.</p> <p><code>GenerateSentencePair</code> is a pre-defined task that given an anchor sentence generates a positive sentence related to the anchor and optionally a negative sentence unrelated to the anchor. This task is useful to generate training datasets for training embeddings models.</p> <p>Attributes:</p> Name Type Description <code>triplet</code> <code>bool</code> <p>a flag to indicate if the task should generate a triplet of sentences (anchor, positive, negative). Defaults to <code>False</code>.</p> <code>action</code> <code>GenerationAction</code> <p>the action to perform to generate the positive sentence.</p> Input columns <ul> <li>anchor (<code>str</code>): The anchor sentence to generate the positive and negative sentences.</li> </ul> Output columns <ul> <li>positive (<code>str</code>): The positive sentence related to the <code>anchor</code>.</li> <li>negative (<code>str</code>): The negative sentence unrelated to the <code>anchor</code> if <code>triplet=True</code>.</li> <li>model_name (<code>str</code>): The name of the model that was used to generate the sentences.</li> </ul> Categories <ul> <li>embedding</li> </ul> <p>Examples:</p> <pre><code>Paraphrasing:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"paraphrase\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n```\n\nGenerating semantically similar sentences:\n\n```python\nfrom distilabel.llms import InferenceEndpointsLLM\nfrom distilabel.steps.tasks import GenerateSentencePair\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"semantically-similar\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"How does 3D printing work?\"}])\n```\n\nGenerating queries:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"query\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"Argilla is an open-source data curation platform for LLMs. Using Argilla, ...\"}])\n```\n\nGenerating answers:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"answer\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n```\n</code></pre> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>class GenerateSentencePair(Task):\n    \"\"\"Generate a positive and negative (optionally) sentences given an anchor sentence.\n\n    `GenerateSentencePair` is a pre-defined task that given an anchor sentence generates\n    a positive sentence related to the anchor and optionally a negative sentence unrelated\n    to the anchor. This task is useful to generate training datasets for training embeddings\n    models.\n\n    Attributes:\n        triplet: a flag to indicate if the task should generate a triplet of sentences\n            (anchor, positive, negative). Defaults to `False`.\n        action: the action to perform to generate the positive sentence.\n\n    Input columns:\n        - anchor (`str`): The anchor sentence to generate the positive and negative sentences.\n\n    Output columns:\n        - positive (`str`): The positive sentence related to the `anchor`.\n        - negative (`str`): The negative sentence unrelated to the `anchor` if `triplet=True`.\n        - model_name (`str`): The name of the model that was used to generate the sentences.\n\n    Categories:\n        - embedding\n\n    Examples:\n\n        Paraphrasing:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"paraphrase\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n        ```\n\n        Generating semantically similar sentences:\n\n        ```python\n        from distilabel.llms import InferenceEndpointsLLM\n        from distilabel.steps.tasks import GenerateSentencePair\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"semantically-similar\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"How does 3D printing work?\"}])\n        ```\n\n        Generating queries:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"query\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"Argilla is an open-source data curation platform for LLMs. Using Argilla, ...\"}])\n        ```\n\n        Generating answers:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"answer\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n        ```\n    \"\"\"\n\n    triplet: bool = False\n    action: GenerationAction\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"generate-sentence-pair.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task is the `anchor` sentence.\"\"\"\n        return [\"anchor\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The inputs are formatted as a `ChatType`, with a system prompt describing the\n        task of generating a positive and negative sentences for the anchor sentence. The\n        anchor is provided as the first user interaction in the conversation.\n\n        Args:\n            input: The input containing the `anchor` sentence.\n\n        Returns:\n            A list of dictionaries containing the system and user interactions.\n        \"\"\"\n        action_sentence = GENERATION_ACTION_SENTENCES[self.action]\n        system_prompt = (\n            POSITIVE_NEGATIVE_SYSTEM_PROMPT if self.triplet else POSITIVE_SYSTEM_PROMPT\n        ).format(action_sentence=action_sentence)\n\n        return [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": self._template.render(anchor=input[\"anchor\"])},\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the `positive` and `negative` sentences, as well\n        as the `model_name` used to generate the sentences.\"\"\"\n        columns = [\"positive\", \"negative\"] if self.triplet else [\"positive\"]\n        columns += [\"model_name\"]\n        return columns\n\n    def format_output(\n        self, output: Union[str, None], input: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Formats the output of the LLM, to extract the `positive` and `negative` sentences\n        generated. If the output is `None` or the regex doesn't match, then the outputs\n        will be set to `None` as well.\n\n        Args:\n            output: The output of the LLM.\n            input: The input used to generate the output.\n\n        Returns:\n            The formatted output containing the `positive` and `negative` sentences.\n        \"\"\"\n        if output is None:\n            return {\"positive\": None, \"negative\": None}\n\n        match = POSITIVE_NEGATIVE_PAIR_REGEX.match(output)\n        if match is None:\n            formatted_output = {\"positive\": None}\n            if self.triplet:\n                formatted_output[\"negative\"] = None\n            return formatted_output\n\n        groups = match.groups()\n        if self.triplet:\n            return {\n                \"positive\": groups[0].strip(),\n                \"negative\": groups[1].strip()\n                if len(groups) &gt; 1 and groups[1] is not None\n                else None,\n            }\n\n        return {\"positive\": groups[0].strip()}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateSentencePair.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task is the <code>anchor</code> sentence.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateSentencePair.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the <code>positive</code> and <code>negative</code> sentences, as well as the <code>model_name</code> used to generate the sentences.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateSentencePair.format_input","title":"<code>format_input(input)</code>","text":"<p>The inputs are formatted as a <code>ChatType</code>, with a system prompt describing the task of generating a positive and negative sentences for the anchor sentence. The anchor is provided as the first user interaction in the conversation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, Any]</code> <p>The input containing the <code>anchor</code> sentence.</p> required <p>Returns:</p> Type Description <code>ChatType</code> <p>A list of dictionaries containing the system and user interactions.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The inputs are formatted as a `ChatType`, with a system prompt describing the\n    task of generating a positive and negative sentences for the anchor sentence. The\n    anchor is provided as the first user interaction in the conversation.\n\n    Args:\n        input: The input containing the `anchor` sentence.\n\n    Returns:\n        A list of dictionaries containing the system and user interactions.\n    \"\"\"\n    action_sentence = GENERATION_ACTION_SENTENCES[self.action]\n    system_prompt = (\n        POSITIVE_NEGATIVE_SYSTEM_PROMPT if self.triplet else POSITIVE_SYSTEM_PROMPT\n    ).format(action_sentence=action_sentence)\n\n    return [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": self._template.render(anchor=input[\"anchor\"])},\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateSentencePair.format_output","title":"<code>format_output(output, input=None)</code>","text":"<p>Formats the output of the LLM, to extract the <code>positive</code> and <code>negative</code> sentences generated. If the output is <code>None</code> or the regex doesn't match, then the outputs will be set to <code>None</code> as well.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>The output of the LLM.</p> required <code>input</code> <code>Optional[Dict[str, Any]]</code> <p>The input used to generate the output.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The formatted output containing the <code>positive</code> and <code>negative</code> sentences.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Optional[Dict[str, Any]] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Formats the output of the LLM, to extract the `positive` and `negative` sentences\n    generated. If the output is `None` or the regex doesn't match, then the outputs\n    will be set to `None` as well.\n\n    Args:\n        output: The output of the LLM.\n        input: The input used to generate the output.\n\n    Returns:\n        The formatted output containing the `positive` and `negative` sentences.\n    \"\"\"\n    if output is None:\n        return {\"positive\": None, \"negative\": None}\n\n    match = POSITIVE_NEGATIVE_PAIR_REGEX.match(output)\n    if match is None:\n        formatted_output = {\"positive\": None}\n        if self.triplet:\n            formatted_output[\"negative\"] = None\n        return formatted_output\n\n    groups = match.groups()\n    if self.triplet:\n        return {\n            \"positive\": groups[0].strip(),\n            \"negative\": groups[1].strip()\n            if len(groups) &gt; 1 and groups[1] is not None\n            else None,\n        }\n\n    return {\"positive\": groups[0].strip()}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.GenerateSentencePair.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"generate-sentence-pair.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.Genstruct","title":"<code>Genstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate a pair of instruction-response from a document using an <code>LLM</code>.</p> <p><code>Genstruct</code> is a pre-defined task designed to generate valid instructions from a given raw document, with the title and the content, enabling the creation of new, partially synthetic instruction finetuning datasets from any raw-text corpus. The task is based on the Genstruct 7B model by Nous Research, which is inspired in the Ada-Instruct paper.</p> Note <p>The Genstruct prompt i.e. the task, can be used with any model really, but the safest / recommended option is to use <code>NousResearch/Genstruct-7B</code> as the LLM provided to the task, since it was trained for this specific task.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>title (<code>str</code>): The title of the document.</li> <li>content (<code>str</code>): The content of the document.</li> </ul> Output columns <ul> <li>user (<code>str</code>): The user's instruction based on the document.</li> <li>assistant (<code>str</code>): The assistant's response based on the user's instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the <code>feedback</code> and <code>result</code>.</li> </ul> Categories <ul> <li>text-generation</li> <li>instruction</li> <li>response</li> </ul> References <ul> <li>Genstruct 7B by Nous Research</li> <li>Ada-Instruct: Adapting Instruction Generators for Complex Reasoning</li> </ul> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>class Genstruct(Task):\n    \"\"\"Generate a pair of instruction-response from a document using an `LLM`.\n\n    `Genstruct` is a pre-defined task designed to generate valid instructions from a given raw document,\n    with the title and the content, enabling the creation of new, partially synthetic instruction finetuning\n    datasets from any raw-text corpus. The task is based on the Genstruct 7B model by Nous Research, which is\n    inspired in the Ada-Instruct paper.\n\n    Note:\n        The Genstruct prompt i.e. the task, can be used with any model really, but the safest / recommended\n        option is to use `NousResearch/Genstruct-7B` as the LLM provided to the task, since it was trained\n        for this specific task.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - title (`str`): The title of the document.\n        - content (`str`): The content of the document.\n\n    Output columns:\n        - user (`str`): The user's instruction based on the document.\n        - assistant (`str`): The assistant's response based on the user's instruction.\n        - model_name (`str`): The model name used to generate the `feedback` and `result`.\n\n    Categories:\n        - text-generation\n        - instruction\n        - response\n\n    References:\n        - [Genstruct 7B by Nous Research](https://huggingface.co/NousResearch/Genstruct-7B)\n        - [Ada-Instruct: Adapting Instruction Generators for Complex Reasoning](https://arxiv.org/abs/2310.04484)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"genstruct.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the `title` and the `content`.\"\"\"\n        return [\"title\", \"content\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    title=input[\"title\"], content=input[\"content\"]\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `user` instruction based on the provided document\n        and the `assistant` response based on the user's instruction.\"\"\"\n        return [\"user\", \"assistant\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted so that both the user and the assistant messages are\n        captured.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the keys `user` and `assistant` containing the content for each role.\n        \"\"\"\n        if output is None:\n            return {\"user\": None, \"assistant\": None}\n\n        matches = re.search(_PARSE_GENSTRUCT_OUTPUT_REGEX, output, re.DOTALL)\n        if not matches:\n            return {\"user\": None, \"assistant\": None}\n\n        return {\n            \"user\": matches.group(1).strip(),\n            \"assistant\": matches.group(2).strip(),\n        }\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.Genstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the <code>title</code> and the <code>content</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.Genstruct.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>user</code> instruction based on the provided document and the <code>assistant</code> response based on the user's instruction.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.Genstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                title=input[\"title\"], content=input[\"content\"]\n            ),\n        }\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.Genstruct.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted so that both the user and the assistant messages are captured.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the keys <code>user</code> and <code>assistant</code> containing the content for each role.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted so that both the user and the assistant messages are\n    captured.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the keys `user` and `assistant` containing the content for each role.\n    \"\"\"\n    if output is None:\n        return {\"user\": None, \"assistant\": None}\n\n    matches = re.search(_PARSE_GENSTRUCT_OUTPUT_REGEX, output, re.DOTALL)\n    if not matches:\n        return {\"user\": None, \"assistant\": None}\n\n    return {\n        \"user\": matches.group(1).strip(),\n        \"assistant\": matches.group(2).strip(),\n    }\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.Genstruct.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"genstruct.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.InstructionBacktranslation","title":"<code>InstructionBacktranslation</code>","text":"<p>               Bases: <code>Task</code></p> <p>Self-Alignment with Instruction Backtranslation.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Optional[Template]</code> <p>the Jinja2 template to use for the Instruction Backtranslation task.</p> Input columns <ul> <li>instruction (<code>str</code>): The reference instruction to evaluate the text output.</li> <li>generation (<code>str</code>): The text output to evaluate for the given instruction.</li> </ul> Output columns <ul> <li>score (<code>str</code>): The score for the generation based on the given instruction.</li> <li>reason (<code>str</code>): The reason for the provided score.</li> <li>model_name (<code>str</code>): The model name used to score the generation.</li> </ul> Categories <ul> <li>critique</li> </ul> References <ul> <li><code>Self-Alignment with Instruction Backtranslation</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>class InstructionBacktranslation(Task):\n    \"\"\"Self-Alignment with Instruction Backtranslation.\n\n    Attributes:\n        _template: the Jinja2 template to use for the Instruction Backtranslation task.\n\n    Input columns:\n        - instruction (`str`): The reference instruction to evaluate the text output.\n        - generation (`str`): The text output to evaluate for the given instruction.\n\n    Output columns:\n        - score (`str`): The score for the generation based on the given instruction.\n        - reason (`str`): The reason for the provided score.\n        - model_name (`str`): The model name used to score the generation.\n\n    Categories:\n        - critique\n\n    References:\n        - [`Self-Alignment with Instruction Backtranslation`](https://arxiv.org/abs/2308.06259)\n    \"\"\"\n\n    _template: Optional[\"Template\"] = PrivateAttr(default=...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"instruction-backtranslation.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`, and the `generation` for it.\"\"\"\n        return [\"instruction\", \"generation\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], generation=input[\"generation\"]\n                ),\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `score`, `reason` and the `model_name`.\"\"\"\n        return [\"score\", \"reason\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `score` and `reason`. The\n        `model_name` will be automatically included within the `process` method of `Task`.\n\n        Args:\n            output: a string representing the output of the LLM via the `process` method.\n            input: the input to the task, as required by some tasks to format the output.\n\n        Returns:\n            A dictionary containing the `score` and the `reason` for the provided `score`.\n        \"\"\"\n        pattern = r\"(.+?)Score: (\\d)\"\n\n        matches = None\n        if output is not None:\n            matches = re.findall(pattern, output, re.DOTALL)\n        if matches is None:\n            return {\"score\": None, \"reason\": None}\n\n        return {\n            \"score\": int(matches[0][1]),\n            \"reason\": matches[0][0].strip(),\n        }\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.InstructionBacktranslation.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>, and the <code>generation</code> for it.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.InstructionBacktranslation.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>score</code>, <code>reason</code> and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.InstructionBacktranslation.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], generation=input[\"generation\"]\n            ),\n        },\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.InstructionBacktranslation.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>score</code> and <code>reason</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>a string representing the output of the LLM via the <code>process</code> method.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task, as required by some tasks to format the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the <code>score</code> and the <code>reason</code> for the provided <code>score</code>.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `score` and `reason`. The\n    `model_name` will be automatically included within the `process` method of `Task`.\n\n    Args:\n        output: a string representing the output of the LLM via the `process` method.\n        input: the input to the task, as required by some tasks to format the output.\n\n    Returns:\n        A dictionary containing the `score` and the `reason` for the provided `score`.\n    \"\"\"\n    pattern = r\"(.+?)Score: (\\d)\"\n\n    matches = None\n    if output is not None:\n        matches = re.findall(pattern, output, re.DOTALL)\n    if matches is None:\n        return {\"score\": None, \"reason\": None}\n\n    return {\n        \"score\": int(matches[0][1]),\n        \"reason\": matches[0][0].strip(),\n    }\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.InstructionBacktranslation.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"instruction-backtranslation.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PairRM","title":"<code>PairRM</code>","text":"<p>               Bases: <code>Step</code></p> <p>Rank the candidates based on the input using the <code>LLM</code> model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The model to use for the ranking. Defaults to <code>\"llm-blender/PairRM\"</code>.</p> <code>instructions</code> <code>Optional[str]</code> <p>The instructions to use for the model. Defaults to <code>None</code>.</p> Input columns <ul> <li>inputs (<code>List[Dict[str, Any]]</code>): The input text or conversation to rank the candidates for.</li> <li>candidates (<code>List[Dict[str, Any]]</code>): The candidates to rank.</li> </ul> Output columns <ul> <li>ranks (<code>List[int]</code>): The ranks of the candidates based on the input.</li> <li>ranked_candidates (<code>List[Dict[str, Any]]</code>): The candidates ranked based on the input.</li> <li>model_name (<code>str</code>): The model name used to rank the candidate responses. Defaults to <code>\"llm-blender/PairRM\"</code>.</li> </ul> References <ul> <li>LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion.</li> <li>Pair Ranking Model.</li> </ul> Categories <ul> <li>preference</li> </ul> Note <p>This step differs to other tasks as there is a single implementation of this model currently, and we will use a specific <code>LLM</code>.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>class PairRM(Step):\n    \"\"\"Rank the candidates based on the input using the `LLM` model.\n\n    Attributes:\n        model: The model to use for the ranking. Defaults to `\"llm-blender/PairRM\"`.\n        instructions: The instructions to use for the model. Defaults to `None`.\n\n    Input columns:\n        - inputs (`List[Dict[str, Any]]`): The input text or conversation to rank the candidates for.\n        - candidates (`List[Dict[str, Any]]`): The candidates to rank.\n\n    Output columns:\n        - ranks (`List[int]`): The ranks of the candidates based on the input.\n        - ranked_candidates (`List[Dict[str, Any]]`): The candidates ranked based on the input.\n        - model_name (`str`): The model name used to rank the candidate responses. Defaults to `\"llm-blender/PairRM\"`.\n\n    References:\n        - [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://arxiv.org/abs/2306.02561).\n        - [Pair Ranking Model](https://huggingface.co/llm-blender/PairRM).\n\n    Categories:\n        - preference\n\n    Note:\n        This step differs to other tasks as there is a single implementation of this model\n        currently, and we will use a specific `LLM`.\n    \"\"\"\n\n    model: str = \"llm-blender/PairRM\"\n    instructions: Optional[str] = None\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the PairRM model provided via `model` with `llm_blender.Blender`, which is the\n        custom library for running the inference for the PairRM models.\"\"\"\n        try:\n            import llm_blender\n        except ImportError as e:\n            raise ImportError(\n                \"The `llm_blender` package is required to use the `PairRM` class.\"\n                \"Please install it with `pip install git+https://github.com/yuchenlin/LLM-Blender.git`.\"\n            ) from e\n\n        self._blender = llm_blender.Blender()\n        self._blender.loadranker(self.model)\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input columns correspond to the two required arguments from `Blender.rank`:\n        `inputs` and `candidates`.\"\"\"\n        return [\"input\", \"candidates\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs will include the `ranks` and the `ranked_candidates`.\"\"\"\n        return [\"ranks\", \"ranked_candidates\", \"model_name\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"The input is expected to be a dictionary with the keys `input` and `candidates`,\n        where the `input` corresponds to the instruction of a model and `candidates` are a\n        list of responses to be ranked.\n        \"\"\"\n        return {\"input\": input[\"input\"], \"candidates\": input[\"candidates\"]}\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Generates the ranks for the candidates based on the input.\n\n        The ranks are the positions of the candidates, where lower is better,\n        and the ranked candidates correspond to the candidates sorted according to the\n        ranks obtained.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            An iterator with the inputs containing the `ranks`, `ranked_candidates`, and `model_name`.\n        \"\"\"\n        input_texts = []\n        candidates = []\n        for input in inputs:\n            formatted_input = self.format_input(input)\n            input_texts.append(formatted_input[\"input\"])\n            candidates.append(formatted_input[\"candidates\"])\n\n        instructions = (\n            [self.instructions] * len(input_texts) if self.instructions else None\n        )\n\n        ranks = self._blender.rank(\n            input_texts,\n            candidates,\n            instructions=instructions,\n            return_scores=False,\n            batch_size=self.input_batch_size,\n        )\n        # Sort the candidates based on the ranks\n        ranked_candidates = np.take_along_axis(\n            np.array(candidates), ranks - 1, axis=1\n        ).tolist()\n        ranks = ranks.tolist()\n        for input, rank, ranked_candidate in zip(inputs, ranks, ranked_candidates):\n            input[\"ranks\"] = rank\n            input[\"ranked_candidates\"] = ranked_candidate\n            input[\"model_name\"] = self.model\n\n        yield inputs\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PairRM.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input columns correspond to the two required arguments from <code>Blender.rank</code>: <code>inputs</code> and <code>candidates</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PairRM.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs will include the <code>ranks</code> and the <code>ranked_candidates</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PairRM.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is expected to be a dictionary with the keys <code>input</code> and <code>candidates</code>, where the <code>input</code> corresponds to the instruction of a model and <code>candidates</code> are a list of responses to be ranked.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"The input is expected to be a dictionary with the keys `input` and `candidates`,\n    where the `input` corresponds to the instruction of a model and `candidates` are a\n    list of responses to be ranked.\n    \"\"\"\n    return {\"input\": input[\"input\"], \"candidates\": input[\"candidates\"]}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PairRM.load","title":"<code>load()</code>","text":"<p>Loads the PairRM model provided via <code>model</code> with <code>llm_blender.Blender</code>, which is the custom library for running the inference for the PairRM models.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the PairRM model provided via `model` with `llm_blender.Blender`, which is the\n    custom library for running the inference for the PairRM models.\"\"\"\n    try:\n        import llm_blender\n    except ImportError as e:\n        raise ImportError(\n            \"The `llm_blender` package is required to use the `PairRM` class.\"\n            \"Please install it with `pip install git+https://github.com/yuchenlin/LLM-Blender.git`.\"\n        ) from e\n\n    self._blender = llm_blender.Blender()\n    self._blender.loadranker(self.model)\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PairRM.process","title":"<code>process(inputs)</code>","text":"<p>Generates the ranks for the candidates based on the input.</p> <p>The ranks are the positions of the candidates, where lower is better, and the ranked candidates correspond to the candidates sorted according to the ranks obtained.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>An iterator with the inputs containing the <code>ranks</code>, <code>ranked_candidates</code>, and <code>model_name</code>.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Generates the ranks for the candidates based on the input.\n\n    The ranks are the positions of the candidates, where lower is better,\n    and the ranked candidates correspond to the candidates sorted according to the\n    ranks obtained.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        An iterator with the inputs containing the `ranks`, `ranked_candidates`, and `model_name`.\n    \"\"\"\n    input_texts = []\n    candidates = []\n    for input in inputs:\n        formatted_input = self.format_input(input)\n        input_texts.append(formatted_input[\"input\"])\n        candidates.append(formatted_input[\"candidates\"])\n\n    instructions = (\n        [self.instructions] * len(input_texts) if self.instructions else None\n    )\n\n    ranks = self._blender.rank(\n        input_texts,\n        candidates,\n        instructions=instructions,\n        return_scores=False,\n        batch_size=self.input_batch_size,\n    )\n    # Sort the candidates based on the ranks\n    ranked_candidates = np.take_along_axis(\n        np.array(candidates), ranks - 1, axis=1\n    ).tolist()\n    ranks = ranks.tolist()\n    for input, rank, ranked_candidate in zip(inputs, ranks, ranked_candidates):\n        input[\"ranks\"] = rank\n        input[\"ranked_candidates\"] = ranked_candidate\n        input[\"model_name\"] = self.model\n\n    yield inputs\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PrometheusEval","title":"<code>PrometheusEval</code>","text":"<p>               Bases: <code>Task</code></p> <p>Critique and rank the quality of generations from an <code>LLM</code> using Prometheus 2.0.</p> <p><code>PrometheusEval</code> is a task created for Prometheus 2.0, covering both the absolute and relative evaluations.</p> <ul> <li>The absolute evaluation i.e. <code>mode=\"absolute\"</code> is used to evaluate a single generation from     an LLM for a given instruction.</li> <li>The relative evaluation i.e. <code>mode=\"relative\"</code> is used to evaluate two generations from an LLM     for a given instruction.</li> </ul> <p>Both evaluations provide the possibility whether to use a reference answer to compare with or not via the <code>reference</code> attribute, and both are based on a score rubric that critiques the generation/s based on the following default aspects: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, and <code>reasoning</code>, that can be overridden via <code>rubrics</code>, and the selected rubric is set via the attribute <code>rubric</code>.</p> Note <p>The <code>PrometheusEval</code> task is better suited and intended to be used with any of the Prometheus 2.0 models released by Kaist AI, being: https://huggingface.co/prometheus-eval/prometheus-7b-v2.0, and https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0. The critique assessment formatting and quality is not guaranteed if using another model, even though some other models may be able to correctly follow the formatting and generate insightful critiques too.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>Literal['absolute', 'relative']</code> <p>the evaluation mode to use, either <code>absolute</code> or <code>relative</code>. It defines whether the task will evaluate one or two generations.</p> <code>rubric</code> <code>str</code> <p>the score rubric to use within the prompt to run the critique based on different aspects. Can be any existing key in the <code>rubrics</code> attribute, which by default means that it can be: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, or <code>reasoning</code>. Those will only work if using the default <code>rubrics</code>, otherwise, the provided <code>rubrics</code> should be used.</p> <code>rubrics</code> <code>Optional[Dict[str, str]]</code> <p>a dictionary containing the different rubrics to use for the critique, where the keys are the rubric names and the values are the rubric descriptions. The default rubrics are the following: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, and <code>reasoning</code>.</p> <code>reference</code> <code>bool</code> <p>a boolean flag to indicate whether a reference answer / completion will be provided, so that the model critique is based on the comparison with it. It implies that the column <code>reference</code> needs to be provided within the input data in addition to the rest of the inputs.</p> <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to use as reference.</li> <li>generation (<code>str</code>, optional): The generated text from the given <code>instruction</code>. This column is required     if <code>mode=absolute</code>.</li> <li>generations (<code>List[str]</code>, optional): The generated texts from the given <code>instruction</code>. It should     contain 2 generations only. This column is required if <code>mode=relative</code>.</li> <li>reference (<code>str</code>, optional): The reference / golden answer for the <code>instruction</code>, to be used by the LLM     for comparison against.</li> </ul> Output columns <ul> <li>feedback (<code>str</code>): The feedback explaining the result below, as critiqued by the LLM using the     pre-defined score rubric, compared against <code>reference</code> if provided.</li> <li>result (<code>Union[int, Literal[\"A\", \"B\"]]</code>): If <code>mode=absolute</code>, then the result contains the score for the     <code>generation</code> in a likert-scale from 1-5, otherwise, if <code>mode=relative</code>, then the result contains either     \"A\" or \"B\", the \"winning\" one being the generation in the index 0 of <code>generations</code> if <code>result='A'</code> or the     index 1 if <code>result='B'</code>.</li> <li>model_name (<code>str</code>): The model name used to generate the <code>feedback</code> and <code>result</code>.</li> </ul> Categories <ul> <li>critique</li> <li>preference</li> </ul> References <ul> <li>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</li> <li>prometheus-eval: Evaluate your LLM's response with Prometheus \ud83d\udcaf</li> </ul> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>class PrometheusEval(Task):\n    \"\"\"Critique and rank the quality of generations from an `LLM` using Prometheus 2.0.\n\n    `PrometheusEval` is a task created for Prometheus 2.0, covering both the absolute and relative\n    evaluations.\n\n    - The absolute evaluation i.e. `mode=\"absolute\"` is used to evaluate a single generation from\n        an LLM for a given instruction.\n    - The relative evaluation i.e. `mode=\"relative\"` is used to evaluate two generations from an LLM\n        for a given instruction.\n\n    Both evaluations provide the possibility whether to use a reference answer to compare with or not\n    via the `reference` attribute, and both are based on a score rubric that critiques the generation/s\n    based on the following default aspects: `helpfulness`, `harmlessness`, `honesty`, `factual-validity`,\n    and `reasoning`, that can be overridden via `rubrics`, and the selected rubric is set via the attribute\n    `rubric`.\n\n    Note:\n        The `PrometheusEval` task is better suited and intended to be used with any of the Prometheus 2.0\n        models released by Kaist AI, being: https://huggingface.co/prometheus-eval/prometheus-7b-v2.0,\n        and https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0. The critique assessment formatting\n        and quality is not guaranteed if using another model, even though some other models may be able to\n        correctly follow the formatting and generate insightful critiques too.\n\n    Attributes:\n        mode: the evaluation mode to use, either `absolute` or `relative`. It defines whether the task\n            will evaluate one or two generations.\n        rubric: the score rubric to use within the prompt to run the critique based on different aspects.\n            Can be any existing key in the `rubrics` attribute, which by default means that it can be:\n            `helpfulness`, `harmlessness`, `honesty`, `factual-validity`, or `reasoning`. Those will only\n            work if using the default `rubrics`, otherwise, the provided `rubrics` should be used.\n        rubrics: a dictionary containing the different rubrics to use for the critique, where the keys are\n            the rubric names and the values are the rubric descriptions. The default rubrics are the following:\n            `helpfulness`, `harmlessness`, `honesty`, `factual-validity`, and `reasoning`.\n        reference: a boolean flag to indicate whether a reference answer / completion will be provided, so\n            that the model critique is based on the comparison with it. It implies that the column `reference`\n            needs to be provided within the input data in addition to the rest of the inputs.\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instruction (`str`): The instruction to use as reference.\n        - generation (`str`, optional): The generated text from the given `instruction`. This column is required\n            if `mode=absolute`.\n        - generations (`List[str]`, optional): The generated texts from the given `instruction`. It should\n            contain 2 generations only. This column is required if `mode=relative`.\n        - reference (`str`, optional): The reference / golden answer for the `instruction`, to be used by the LLM\n            for comparison against.\n\n    Output columns:\n        - feedback (`str`): The feedback explaining the result below, as critiqued by the LLM using the\n            pre-defined score rubric, compared against `reference` if provided.\n        - result (`Union[int, Literal[\"A\", \"B\"]]`): If `mode=absolute`, then the result contains the score for the\n            `generation` in a likert-scale from 1-5, otherwise, if `mode=relative`, then the result contains either\n            \"A\" or \"B\", the \"winning\" one being the generation in the index 0 of `generations` if `result='A'` or the\n            index 1 if `result='B'`.\n        - model_name (`str`): The model name used to generate the `feedback` and `result`.\n\n    Categories:\n        - critique\n        - preference\n\n    References:\n        - [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535)\n        - [prometheus-eval: Evaluate your LLM's response with Prometheus \ud83d\udcaf](https://github.com/prometheus-eval/prometheus-eval)\n    \"\"\"\n\n    mode: Literal[\"absolute\", \"relative\"]\n    rubric: str\n    rubrics: Optional[Dict[str, str]] = Field(default=_DEFAULT_RUBRICS)\n    reference: bool = False\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    @model_validator(mode=\"after\")\n    def validate_rubric_and_rubrics(self) -&gt; Self:\n        if not isinstance(self.rubrics, dict) or len(self.rubrics) &lt; 1:\n            raise ValueError(\n                \"Provided `rubrics` must be a Python dictionary with string keys and string values.\"\n            )\n\n        def rubric_matches_pattern(rubric: str) -&gt; bool:\n            \"\"\"Checks if the provided rubric matches the pattern of the default rubrics.\"\"\"\n            pattern = r\"^\\[.*?\\]\\n(?:Score [1-4]: .*?\\n){4}(?:Score 5: .*?)\"\n            return bool(re.match(pattern, rubric, re.MULTILINE))\n\n        if not all(rubric_matches_pattern(value) for value in self.rubrics.values()):\n            raise ValueError(\n                \"Provided rubrics should match the format of the default rubrics, which\"\n                \" is as follows: `[&lt;scoring criteria&gt;]\\nScore 1: &lt;description&gt;\\nScore 2: &lt;description&gt;\\n\"\n                \"Score 3: &lt;description&gt;\\nScore 4: &lt;description&gt;\\nScore 5: &lt;description&gt;`; replacing\"\n                \" `&lt;scoring criteria&gt;` and `&lt;description&gt;` with the actual criteria and description\"\n                \" for each or the scores, respectively.\"\n            )\n\n        if self.rubric not in self.rubrics:\n            raise ValueError(\n                f\"Provided rubric '{self.rubric}' is not among the available rubrics: {', '.join(self.rubrics.keys())}.\"\n            )\n\n        return self\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation\n        depending on the `mode` value, and either with or without reference, depending on the\n        value of `reference`.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"prometheus\"\n            / (\n                f\"{self.mode}_without_reference.jinja2\"\n                if self.reference is False\n                else f\"{self.mode}_with_reference.jinja2\"\n            )\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The default inputs for the task are the `instruction` and the `generation`\n        if `reference=False`, otherwise, the inputs are `instruction`, `generation`, and\n        `reference`.\"\"\"\n        if self.mode == \"absolute\":\n            if self.reference:\n                return [\"instruction\", \"generation\", \"reference\"]\n            return [\"instruction\", \"generation\"]\n        else:  # self.mode == \"relative\"\n            if self.reference:\n                return [\"instruction\", \"generations\", \"reference\"]\n            return [\"instruction\", \"generations\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` where the prompt is formatted according\n        to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction\n        from the user, including a pre-defined system prompt.\"\"\"\n        template_kwargs = {\n            \"instruction\": input[\"instruction\"],\n            \"rubric\": self.rubrics[self.rubric],\n        }\n        if self.reference:\n            template_kwargs[\"reference\"] = input[\"reference\"]\n\n        if self.mode == \"absolute\":\n            if not isinstance(input[\"generation\"], str):\n                raise ValueError(\n                    f\"Provided `generation` is of type {type(input['generation'])} but a string\"\n                    \" should be provided instead.\",\n                )\n\n            template_kwargs[\"generation\"] = input[\"generation\"]\n            system_message = (\n                \"You are a fair judge assistant tasked with providing clear, objective feedback based\"\n                \" on specific criteria, ensuring each assessment reflects the absolute standards set\"\n                \" for performance.\"\n            )\n        else:  # self.mode == \"relative\"\n            if (\n                not isinstance(input[\"generations\"], list)\n                or not all(\n                    isinstance(generation, str) for generation in input[\"generations\"]\n                )\n                or len(input[\"generations\"]) != 2\n            ):\n                raise ValueError(\n                    f\"Provided `generations` is of type {type(input['generations'])} but a list of strings with length 2 should be provided instead.\"\n                )\n\n            template_kwargs[\"generations\"] = input[\"generations\"]\n            system_message = (\n                \"You are a fair judge assistant assigned to deliver insightful feedback that compares\"\n                \" individual performances, highlighting how each stands relative to others within the\"\n                \" same cohort.\"\n            )\n\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": system_message,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(**template_kwargs),  # type: ignore\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `feedback` and the `result` generated by Prometheus,\n        as well as the `model_name` which is automatically included based on the `LLM` used.\n        \"\"\"\n        return [\"feedback\", \"result\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dict with the keys `feedback` and `result` captured\n        using a regex from the Prometheus output.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Optionally provided in case it's useful to build the output.\n\n        Returns:\n            A dict with the keys `feedback` and `result` generated by the LLM.\n        \"\"\"\n        if output is None:\n            return {\"feedback\": None, \"result\": None}\n\n        parts = output.split(\"[RESULT]\")\n        if len(parts) != 2:\n            return {\"feedback\": None, \"result\": None}\n\n        feedback, result = parts[0].strip(), parts[1].strip()\n        if feedback.startswith(\"Feedback:\"):\n            feedback = feedback[len(\"Feedback:\") :].strip()\n        if self.mode == \"absolute\":\n            if not result.isdigit() or result not in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n                return {\"feedback\": None, \"result\": None}\n            return {\"feedback\": feedback, \"result\": int(result)}\n        else:  # self.mode == \"relative\"\n            if result not in [\"A\", \"B\"]:\n                return {\"feedback\": None, \"result\": None}\n            return {\"feedback\": feedback, \"result\": result}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PrometheusEval.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The default inputs for the task are the <code>instruction</code> and the <code>generation</code> if <code>reference=False</code>, otherwise, the inputs are <code>instruction</code>, <code>generation</code>, and <code>reference</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PrometheusEval.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>feedback</code> and the <code>result</code> generated by Prometheus, as well as the <code>model_name</code> which is automatically included based on the <code>LLM</code> used.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PrometheusEval.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> where the prompt is formatted according to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction from the user, including a pre-defined system prompt.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` where the prompt is formatted according\n    to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction\n    from the user, including a pre-defined system prompt.\"\"\"\n    template_kwargs = {\n        \"instruction\": input[\"instruction\"],\n        \"rubric\": self.rubrics[self.rubric],\n    }\n    if self.reference:\n        template_kwargs[\"reference\"] = input[\"reference\"]\n\n    if self.mode == \"absolute\":\n        if not isinstance(input[\"generation\"], str):\n            raise ValueError(\n                f\"Provided `generation` is of type {type(input['generation'])} but a string\"\n                \" should be provided instead.\",\n            )\n\n        template_kwargs[\"generation\"] = input[\"generation\"]\n        system_message = (\n            \"You are a fair judge assistant tasked with providing clear, objective feedback based\"\n            \" on specific criteria, ensuring each assessment reflects the absolute standards set\"\n            \" for performance.\"\n        )\n    else:  # self.mode == \"relative\"\n        if (\n            not isinstance(input[\"generations\"], list)\n            or not all(\n                isinstance(generation, str) for generation in input[\"generations\"]\n            )\n            or len(input[\"generations\"]) != 2\n        ):\n            raise ValueError(\n                f\"Provided `generations` is of type {type(input['generations'])} but a list of strings with length 2 should be provided instead.\"\n            )\n\n        template_kwargs[\"generations\"] = input[\"generations\"]\n        system_message = (\n            \"You are a fair judge assistant assigned to deliver insightful feedback that compares\"\n            \" individual performances, highlighting how each stands relative to others within the\"\n            \" same cohort.\"\n        )\n\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": system_message,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(**template_kwargs),  # type: ignore\n        },\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PrometheusEval.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dict with the keys <code>feedback</code> and <code>result</code> captured using a regex from the Prometheus output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Optionally provided in case it's useful to build the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the keys <code>feedback</code> and <code>result</code> generated by the LLM.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dict with the keys `feedback` and `result` captured\n    using a regex from the Prometheus output.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Optionally provided in case it's useful to build the output.\n\n    Returns:\n        A dict with the keys `feedback` and `result` generated by the LLM.\n    \"\"\"\n    if output is None:\n        return {\"feedback\": None, \"result\": None}\n\n    parts = output.split(\"[RESULT]\")\n    if len(parts) != 2:\n        return {\"feedback\": None, \"result\": None}\n\n    feedback, result = parts[0].strip(), parts[1].strip()\n    if feedback.startswith(\"Feedback:\"):\n        feedback = feedback[len(\"Feedback:\") :].strip()\n    if self.mode == \"absolute\":\n        if not result.isdigit() or result not in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n            return {\"feedback\": None, \"result\": None}\n        return {\"feedback\": feedback, \"result\": int(result)}\n    else:  # self.mode == \"relative\"\n        if result not in [\"A\", \"B\"]:\n            return {\"feedback\": None, \"result\": None}\n        return {\"feedback\": feedback, \"result\": result}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.PrometheusEval.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation depending on the <code>mode</code> value, and either with or without reference, depending on the value of <code>reference</code>.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation\n    depending on the `mode` value, and either with or without reference, depending on the\n    value of `reference`.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"prometheus\"\n        / (\n            f\"{self.mode}_without_reference.jinja2\"\n            if self.reference is False\n            else f\"{self.mode}_with_reference.jinja2\"\n        )\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.QualityScorer","title":"<code>QualityScorer</code>","text":"<p>               Bases: <code>Task</code></p> <p>Score responses based on their quality using an <code>LLM</code>.</p> <p><code>QualityScorer</code> is a pre-defined task that defines the <code>instruction</code> as the input and <code>score</code> as the output. This task is used to rate the quality of instructions and responses. It's an implementation of the quality score task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'. The task follows the same scheme as the Complexity Scorer, but the instruction-response pairs are scored in terms of quality, obtaining a quality score for each instruction.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the <code>responses</code>.</li> <li>responses (<code>List[str]</code>): The responses to be scored. Each response forms a pair with the instruction.</li> </ul> Output columns <ul> <li>scores (<code>List[float]</code>): The score for each instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the scores.</li> </ul> Categories <ul> <li>scorer</li> <li>quality</li> <li>response</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>class QualityScorer(Task):\n    \"\"\"Score responses based on their quality using an `LLM`.\n\n    `QualityScorer` is a pre-defined task that defines the `instruction` as the input\n    and `score` as the output. This task is used to rate the quality of instructions and responses.\n    It's an implementation of the quality score task from the paper 'What Makes Good Data\n    for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.\n    The task follows the same scheme as the Complexity Scorer, but the instruction-response pairs\n    are scored in terms of quality, obtaining a quality score for each instruction.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the `responses`.\n        - responses (`List[str]`): The responses to be scored. Each response forms a pair with the instruction.\n\n    Output columns:\n        - scores (`List[float]`): The score for each instruction.\n        - model_name (`str`): The model name used to generate the scores.\n\n    Categories:\n        - scorer\n        - quality\n        - response\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"quality-scorer.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are `instruction` and `responses`.\"\"\"\n        return [\"instruction\", \"responses\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], responses=input[\"responses\"]\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self):\n        \"\"\"The output for the task is a list of `scores` containing the quality score for each\n        response in `responses`.\"\"\"\n        return [\"scores\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the score of each instruction-response pair.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the key `scores` containing the scores for each instruction-response pair.\n        \"\"\"\n        if output is None:\n            return {\"scores\": [None] * len(input[\"responses\"])}\n\n        scores = []\n        score_lines = output.split(\"\\n\")\n\n        for i, line in enumerate(score_lines):\n            match = _PARSE_SCORE_LINE_REGEX.match(line)\n            score = float(match.group(1)) if match else None\n            scores.append(score)\n            if i == len(input[\"responses\"]) - 1:\n                break\n        return {\"scores\": scores}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.QualityScorer.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are <code>instruction</code> and <code>responses</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.QualityScorer.outputs","title":"<code>outputs</code>  <code>property</code>","text":"<p>The output for the task is a list of <code>scores</code> containing the quality score for each response in <code>responses</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.QualityScorer.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], responses=input[\"responses\"]\n            ),\n        }\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.QualityScorer.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a list with the score of each instruction-response pair.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the key <code>scores</code> containing the scores for each instruction-response pair.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the score of each instruction-response pair.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the key `scores` containing the scores for each instruction-response pair.\n    \"\"\"\n    if output is None:\n        return {\"scores\": [None] * len(input[\"responses\"])}\n\n    scores = []\n    score_lines = output.split(\"\\n\")\n\n    for i, line in enumerate(score_lines):\n        match = _PARSE_SCORE_LINE_REGEX.match(line)\n        score = float(match.group(1)) if match else None\n        scores.append(score)\n        if i == len(input[\"responses\"]) - 1:\n            break\n    return {\"scores\": scores}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.QualityScorer.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"quality-scorer.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.SelfInstruct","title":"<code>SelfInstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate instructions based on a given input using an <code>LLM</code>.</p> <p><code>SelfInstruct</code> is a pre-defined task that, given a number of instructions, a certain criteria for query generations, an application description, and an input, generates a number of instruction related to the given input and following what is stated in the criteria for query generation and the application description. It is based in the SelfInstruct framework from the paper \"Self-Instruct: Aligning Language Models with Self-Generated Instructions\".</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <code>int</code> <p>The number of instructions to be generated. Defaults to 5.</p> <code>criteria_for_query_generation</code> <code>str</code> <p>The criteria for the query generation. Defaults to the criteria defined within the paper.</p> <code>application_description</code> <code>str</code> <p>The description of the AI application that one want to build with these instructions. Defaults to <code>AI assistant</code>.</p> Input columns <ul> <li>input (<code>str</code>): The input to generate the instructions. It's also called seed in     the paper.</li> </ul> Output columns <ul> <li>instructions (<code>List[str]</code>): The generated instructions.</li> <li>model_name (<code>str</code>): The model name used to generate the instructions.</li> </ul> Categories <ul> <li>text-generation</li> </ul> Reference <ul> <li><code>Self-Instruct: Aligning Language Models with Self-Generated Instructions</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>class SelfInstruct(Task):\n    \"\"\"Generate instructions based on a given input using an `LLM`.\n\n    `SelfInstruct` is a pre-defined task that, given a number of instructions, a\n    certain criteria for query generations, an application description, and an input,\n    generates a number of instruction related to the given input and following what\n    is stated in the criteria for query generation and the application description.\n    It is based in the SelfInstruct framework from the paper \"Self-Instruct: Aligning\n    Language Models with Self-Generated Instructions\".\n\n    Attributes:\n        num_instructions: The number of instructions to be generated. Defaults to 5.\n        criteria_for_query_generation: The criteria for the query generation. Defaults\n            to the criteria defined within the paper.\n        application_description: The description of the AI application that one want\n            to build with these instructions. Defaults to `AI assistant`.\n\n    Input columns:\n        - input (`str`): The input to generate the instructions. It's also called seed in\n            the paper.\n\n    Output columns:\n        - instructions (`List[str]`): The generated instructions.\n        - model_name (`str`): The model name used to generate the instructions.\n\n    Categories:\n        - text-generation\n\n    Reference:\n        - [`Self-Instruct: Aligning Language Models with Self-Generated Instructions`](https://arxiv.org/abs/2212.10560)\n    \"\"\"\n\n    num_instructions: int = 5\n    criteria_for_query_generation: str = (\n        \"Incorporate a diverse range of verbs, avoiding repetition.\\n\"\n        \"Ensure queries are compatible with AI model's text generation functions and are limited to 1-2 sentences.\\n\"\n        \"Design queries to be self-contained and standalone.\\n\"\n        'Blend interrogative (e.g., \"What is the significance of x?\") and imperative (e.g., \"Detail the process of x.\") styles.'\n    )\n    application_description: str = \"AI assistant\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"self-instruct.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `input` i.e. seed text.\"\"\"\n        return [\"input\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(\n                    input=input[\"input\"],\n                    application_description=self.application_description,\n                    criteria_for_query_generation=self.criteria_for_query_generation,\n                    num_instructions=self.num_instructions,\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self):\n        \"\"\"The output for the task is a list of `instructions` containing the generated instructions.\"\"\"\n        return [\"instructions\", \"model_name\"]\n\n    def format_output(\n        self,\n        output: Union[str, None],\n        input: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the generated instructions.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with containing the generated instructions.\n        \"\"\"\n        if output is None:\n            return {\"instructions\": []}\n        return {\"instructions\": [line for line in output.split(\"\\n\") if line != \"\"]}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.SelfInstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>input</code> i.e. seed text.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.SelfInstruct.outputs","title":"<code>outputs</code>  <code>property</code>","text":"<p>The output for the task is a list of <code>instructions</code> containing the generated instructions.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.SelfInstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(\n                input=input[\"input\"],\n                application_description=self.application_description,\n                criteria_for_query_generation=self.criteria_for_query_generation,\n                num_instructions=self.num_instructions,\n            ),\n        }\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.SelfInstruct.format_output","title":"<code>format_output(output, input=None)</code>","text":"<p>The output is formatted as a list with the generated instructions.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Optional[Dict[str, Any]]</code> <p>the input to the task. Used for obtaining the number of responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with containing the generated instructions.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def format_output(\n    self,\n    output: Union[str, None],\n    input: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the generated instructions.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with containing the generated instructions.\n    \"\"\"\n    if output is None:\n        return {\"instructions\": []}\n    return {\"instructions\": [line for line in output.split(\"\\n\") if line != \"\"]}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.SelfInstruct.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"self-instruct.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.StructuredGeneration","title":"<code>StructuredGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate structured content for a given <code>instruction</code> using an <code>LLM</code>.</p> <p><code>StructuredGeneration</code> is a pre-defined task that defines the <code>instruction</code> and the <code>grammar</code> as the inputs, and <code>generation</code> as the output. This task is used to generate structured content based on the input instruction and following the schema provided within the <code>grammar</code> column per each <code>instruction</code>. The <code>model_name</code> also returned as part of the output in order to enhance it.</p> <p>Attributes:</p> Name Type Description <code>use_system_prompt</code> <code>bool</code> <p>Whether to use the system prompt in the generation. Defaults to <code>True</code>, which means that if the column <code>system_prompt</code> is  defined within the input batch, then the <code>system_prompt</code> will be used, otherwise, it will be ignored.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to generate structured content from.</li> <li>grammar (<code>Dict[str, Any]</code>): The grammar to generate structured content from. It should be a     Python dictionary with the keys <code>type</code> and <code>value</code>, where <code>type</code> should be one of <code>json</code> or     <code>regex</code>, and the <code>value</code> should be either the JSON schema or the regex pattern, respectively.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text matching the provided schema, if possible.</li> <li>model_name (<code>str</code>): The name of the model used to generate the text.</li> </ul> Categories <ul> <li>outlines</li> <li>structured-generation</li> </ul> <p>Examples:</p> <pre><code>from distilabel.steps.tasks import StructuredGeneration\n\ntask = StructuredGeneration(llm=LLM(...))\n</code></pre> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>class StructuredGeneration(Task):\n    \"\"\"Generate structured content for a given `instruction` using an `LLM`.\n\n    `StructuredGeneration` is a pre-defined task that defines the `instruction` and the `grammar`\n    as the inputs, and `generation` as the output. This task is used to generate structured content based on\n    the input instruction and following the schema provided within the `grammar` column per each\n    `instruction`. The `model_name` also returned as part of the output in order to enhance it.\n\n    Attributes:\n        use_system_prompt: Whether to use the system prompt in the generation. Defaults to `True`,\n            which means that if the column `system_prompt` is  defined within the input batch, then\n            the `system_prompt` will be used, otherwise, it will be ignored.\n\n    Input columns:\n        - instruction (`str`): The instruction to generate structured content from.\n        - grammar (`Dict[str, Any]`): The grammar to generate structured content from. It should be a\n            Python dictionary with the keys `type` and `value`, where `type` should be one of `json` or\n            `regex`, and the `value` should be either the JSON schema or the regex pattern, respectively.\n\n    Output columns:\n        - generation (`str`): The generated text matching the provided schema, if possible.\n        - model_name (`str`): The name of the model used to generate the text.\n\n    Categories:\n        - outlines\n        - structured-generation\n\n    Examples:\n        ```python\n        from distilabel.steps.tasks import StructuredGeneration\n\n        task = StructuredGeneration(llm=LLM(...))\n        ```\n    \"\"\"\n\n    use_system_prompt: bool = False\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `instruction` and the `grammar`.\n        Optionally, if the `use_system_prompt` flag is set to True, then the\n        `system_prompt` will be used too.\"\"\"\n        columns = [\"instruction\", \"grammar\"]\n        if self.use_system_prompt:\n            columns = [\"system_prompt\"] + columns\n        return columns\n\n    def format_input(self, input: Dict[str, Any]) -&gt; StructuredInput:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        if not isinstance(input[\"instruction\"], str):\n            raise ValueError(\n                f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n            )\n\n        messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n        if self.use_system_prompt:\n            if \"system_prompt\" in input:\n                messages.insert(\n                    0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n                )\n            else:\n                warnings.warn(\n                    \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n        return (messages, input.get(\"grammar\", None))  # type: ignore\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`. Note that even\n        if the `grammar` is defined to produce a JSON schema, this method will return the raw\n        output i.e. a string without any parsing.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.StructuredGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>instruction</code> and the <code>grammar</code>. Optionally, if the <code>use_system_prompt</code> flag is set to True, then the <code>system_prompt</code> will be used too.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.StructuredGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.StructuredGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; StructuredInput:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    if not isinstance(input[\"instruction\"], str):\n        raise ValueError(\n            f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n        )\n\n    messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n    if self.use_system_prompt:\n        if \"system_prompt\" in input:\n            messages.insert(\n                0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n            )\n        else:\n            warnings.warn(\n                \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    return (messages, input.get(\"grammar\", None))  # type: ignore\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.StructuredGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>. Note that even if the <code>grammar</code> is defined to produce a JSON schema, this method will return the raw output i.e. a string without any parsing.</p> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`. Note that even\n    if the `grammar` is defined to produce a JSON schema, this method will return the raw\n    output i.e. a string without any parsing.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.TextGeneration","title":"<code>TextGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Simple text generation with an <code>LLM</code> given an instruction.</p> <p><code>TextGeneration</code> is a pre-defined task that defines the <code>instruction</code> as the input and <code>generation</code> as the output. This task is used to generate text based on the input instruction. The model_name is also returned as part of the output in order to enhance it.</p> <p>Attributes:</p> Name Type Description <code>use_system_prompt</code> <code>bool</code> <p>Whether to use the system prompt in the generation. Defaults to <code>True</code>, which means that if the column <code>system_prompt</code> is defined within the input batch, then the <code>system_prompt</code> will be used, otherwise, it will be ignored.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to generate text from.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text.</li> <li>model_name (<code>str</code>): The name of the model used to generate the text.</li> </ul> Categories <ul> <li>text-generation</li> </ul> <p>Examples:</p> <pre><code>from distilabel.steps.tasks import TextGeneration\n\ntask = TextGeneration(llm=LLM(...))\n</code></pre> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>class TextGeneration(Task):\n    \"\"\"Simple text generation with an `LLM` given an instruction.\n\n    `TextGeneration` is a pre-defined task that defines the `instruction` as the input\n    and `generation` as the output. This task is used to generate text based on the input\n    instruction. The model_name is also returned as part of the output in order to enhance it.\n\n    Attributes:\n        use_system_prompt: Whether to use the system prompt in the generation. Defaults to `True`,\n            which means that if the column `system_prompt` is defined within the input batch, then\n            the `system_prompt` will be used, otherwise, it will be ignored.\n\n    Input columns:\n        - instruction (`str`): The instruction to generate text from.\n\n    Output columns:\n        - generation (`str`): The generated text.\n        - model_name (`str`): The name of the model used to generate the text.\n\n    Categories:\n        - text-generation\n\n    Examples:\n        ```python\n        from distilabel.steps.tasks import TextGeneration\n\n        task = TextGeneration(llm=LLM(...))\n        ```\n    \"\"\"\n\n    use_system_prompt: bool = True\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`.\"\"\"\n        return [\"instruction\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n\n        if is_openai_format(input[\"instruction\"]):\n            raise ValueError(\n                \"Providing `instruction` formatted as an OpenAI chat / conversation is\"\n                \" deprecated, you should use `ChatGeneration` with `messages` as input instead.\",\n            )\n\n        if not isinstance(input[\"instruction\"], str):\n            raise ValueError(\n                f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n            )\n\n        messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n        if self.use_system_prompt:\n            if \"system_prompt\" in input:\n                messages.insert(\n                    0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n                )\n            else:\n                warnings.warn(\n                    \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        return messages  # type: ignore\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.TextGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.TextGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.TextGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n\n    if is_openai_format(input[\"instruction\"]):\n        raise ValueError(\n            \"Providing `instruction` formatted as an OpenAI chat / conversation is\"\n            \" deprecated, you should use `ChatGeneration` with `messages` as input instead.\",\n        )\n\n    if not isinstance(input[\"instruction\"], str):\n        raise ValueError(\n            f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n        )\n\n    messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n    if self.use_system_prompt:\n        if \"system_prompt\" in input:\n            messages.insert(\n                0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n            )\n        else:\n            warnings.warn(\n                \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    return messages  # type: ignore\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.TextGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.UltraFeedback","title":"<code>UltraFeedback</code>","text":"<p>               Bases: <code>Task</code></p> <p>Rank generations focusing on different aspects using an <code>LLM</code>.</p> <p>UltraFeedback: Boosting Language Models with High-quality Feedback.</p> <p>Attributes:</p> Name Type Description <code>aspect</code> <code>Literal['helpfulness', 'honesty', 'instruction-following', 'truthfulness', 'overall-rating']</code> <p>The aspect to perform with the <code>UltraFeedback</code> model. The available aspects are: - <code>helpfulness</code>: Evaluate text outputs based on helpfulness. - <code>honesty</code>: Evaluate text outputs based on honesty. - <code>instruction-following</code>: Evaluate text outputs based on given instructions. - <code>truthfulness</code>: Evaluate text outputs based on truthfulness. Additionally, a custom aspect has been defined by Argilla, so as to evaluate the overall assessment of the text outputs within a single prompt. The custom aspect is: - <code>overall-rating</code>: Evaluate text outputs based on an overall assessment.</p> Input columns <ul> <li>instruction (<code>str</code>): The reference instruction to evaluate the text outputs.</li> <li>generations (<code>List[str]</code>): The text outputs to evaluate for the given instruction.</li> </ul> Output columns <ul> <li>ratings (<code>List[float]</code>): The ratings for each of the provided text outputs.</li> <li>rationales (<code>List[str]</code>): The rationales for each of the provided text outputs.</li> <li>model_name (<code>str</code>): The name of the model used to generate the ratings and rationales.</li> </ul> Categories <ul> <li>preference</li> </ul> References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>UltraFeedback - GitHub Repository</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>class UltraFeedback(Task):\n    \"\"\"Rank generations focusing on different aspects using an `LLM`.\n\n    UltraFeedback: Boosting Language Models with High-quality Feedback.\n\n    Attributes:\n        aspect: The aspect to perform with the `UltraFeedback` model. The available aspects are:\n            - `helpfulness`: Evaluate text outputs based on helpfulness.\n            - `honesty`: Evaluate text outputs based on honesty.\n            - `instruction-following`: Evaluate text outputs based on given instructions.\n            - `truthfulness`: Evaluate text outputs based on truthfulness.\n            Additionally, a custom aspect has been defined by Argilla, so as to evaluate the overall\n            assessment of the text outputs within a single prompt. The custom aspect is:\n            - `overall-rating`: Evaluate text outputs based on an overall assessment.\n\n    Input columns:\n        - instruction (`str`): The reference instruction to evaluate the text outputs.\n        - generations (`List[str]`): The text outputs to evaluate for the given instruction.\n\n    Output columns:\n        - ratings (`List[float]`): The ratings for each of the provided text outputs.\n        - rationales (`List[str]`): The rationales for each of the provided text outputs.\n        - model_name (`str`): The name of the model used to generate the ratings and rationales.\n\n    Categories:\n        - preference\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`UltraFeedback - GitHub Repository`](https://github.com/OpenBMB/UltraFeedback)\n    \"\"\"\n\n    aspect: Literal[\n        \"helpfulness\",\n        \"honesty\",\n        \"instruction-following\",\n        \"truthfulness\",\n        # Custom aspects\n        \"overall-rating\",\n    ]\n\n    _system_prompt: str = PrivateAttr(\n        default=(\n            \"Your role is to evaluate text quality based on given criteria.\\n\"\n            'You\\'ll receive an instructional description (\"Instruction\") and {no_texts} text outputs (\"Text\").\\n'\n            \"Understand and interpret instructions to evaluate effectively.\\n\"\n            \"Provide annotations for each text with a rating and rationale.\\n\"\n            \"The {no_texts} texts given are independent, and should be evaluated separately.\\n\"\n        )\n    )\n    _template: Optional[\"Template\"] = PrivateAttr(default=...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template for the given `aspect`.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"ultrafeedback\"\n            / f\"{self.aspect}.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`, and the `generations` for it.\"\"\"\n        return [\"instruction\", \"generations\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": self._system_prompt.format(\n                    no_texts=len(input[\"generations\"])\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], generations=input[\"generations\"]\n                ),\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        columns = []\n        if self.aspect in [\"honesty\", \"instruction-following\", \"overall-rating\"]:\n            columns = [\"ratings\", \"rationales\"]\n        elif self.aspect in [\"helpfulness\", \"truthfulness\"]:\n            columns = [\"types\", \"rationales\", \"ratings\", \"rationales-for-ratings\"]\n        return columns + [\"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `ratings` and `rationales` for\n        each of the provided `generations` for the given `instruction`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\n\n        Args:\n            output: a string representing the output of the LLM via the `process` method.\n            input: the input to the task, as required by some tasks to format the output.\n\n        Returns:\n            A dictionary containing either the `ratings` and `rationales` for each of the provided\n            `generations` for the given `instruction` if the provided aspect is either `honesty`,\n            `instruction-following`, or `overall-rating`; or the `types`, `rationales`,\n            `ratings`, and `rationales-for-ratings` for each of the provided `generations` for the\n            given `instruction` if the provided aspect is either `helpfulness` or `truthfulness`.\n        \"\"\"\n        if self.aspect in [\n            \"honesty\",\n            \"instruction-following\",\n            \"overall-rating\",\n        ]:\n            return self._format_ratings_rationales_output(output, input)\n        return self._format_types_ratings_rationales_output(output, input)\n\n    def _format_ratings_rationales_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, List[Any]]:\n        \"\"\"Formats the output when the aspect is either `honesty`, `instruction-following`, or `overall-rating`.\"\"\"\n        if output is None:\n            return {\n                \"ratings\": [None] * len(input[\"generations\"]),\n                \"rationales\": [None] * len(input[\"generations\"]),\n            }\n\n        pattern = r\"Rating: (.+?)\\nRationale: (.+)\"\n        sections = output.split(\"\\n\\n\")\n\n        formatted_outputs = []\n        for section in sections:\n            matches = None\n            if section is not None and section != \"\":\n                matches = re.search(pattern, section, re.DOTALL)\n            if not matches:\n                formatted_outputs.append({\"ratings\": None, \"rationales\": None})\n                continue\n\n            formatted_outputs.append(\n                {\n                    \"ratings\": int(re.findall(r\"\\b\\d+\\b\", matches.group(1))[0])\n                    if matches.group(1) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales\": matches.group(2),\n                }\n            )\n        return combine_dicts(*formatted_outputs)\n\n    def _format_types_ratings_rationales_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, List[Any]]:\n        \"\"\"Formats the output when the aspect is either `helpfulness` or `truthfulness`.\"\"\"\n        if output is None:\n            return {\n                \"types\": [None] * len(input[\"generations\"]),\n                \"rationales\": [None] * len(input[\"generations\"]),\n                \"ratings\": [None] * len(input[\"generations\"]),\n                \"rationales-for-ratings\": [None] * len(input[\"generations\"]),\n            }\n\n        pattern = r\"Type: (.+?)\\nRationale: (.+?)\\nRating: (.+?)\\nRationale: (.+)\"\n\n        sections = output.split(\"\\n\\n\")\n\n        formatted_outputs = []\n        for section in sections:\n            matches = None\n            if section is not None and section != \"\":\n                matches = re.search(pattern, section, re.DOTALL)\n            if not matches:\n                formatted_outputs.append(\n                    {\n                        \"types\": None,\n                        \"rationales\": None,\n                        \"ratings\": None,\n                        \"rationales-for-ratings\": None,\n                    }\n                )\n                continue\n\n            formatted_outputs.append(\n                {\n                    \"types\": int(re.findall(r\"\\b\\d+\\b\", matches.group(1))[0])\n                    if matches.group(1) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales\": matches.group(2),\n                    \"ratings\": int(re.findall(r\"\\b\\d+\\b\", matches.group(3))[0])\n                    if matches.group(3) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales-for-ratings\": matches.group(4),\n                }\n            )\n        return combine_dicts(*formatted_outputs)\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.UltraFeedback.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>, and the <code>generations</code> for it.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.UltraFeedback.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"api/task_gallery/#distilabel.steps.tasks.UltraFeedback._format_ratings_rationales_output","title":"<code>_format_ratings_rationales_output(output, input)</code>","text":"<p>Formats the output when the aspect is either <code>honesty</code>, <code>instruction-following</code>, or <code>overall-rating</code>.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def _format_ratings_rationales_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, List[Any]]:\n    \"\"\"Formats the output when the aspect is either `honesty`, `instruction-following`, or `overall-rating`.\"\"\"\n    if output is None:\n        return {\n            \"ratings\": [None] * len(input[\"generations\"]),\n            \"rationales\": [None] * len(input[\"generations\"]),\n        }\n\n    pattern = r\"Rating: (.+?)\\nRationale: (.+)\"\n    sections = output.split(\"\\n\\n\")\n\n    formatted_outputs = []\n    for section in sections:\n        matches = None\n        if section is not None and section != \"\":\n            matches = re.search(pattern, section, re.DOTALL)\n        if not matches:\n            formatted_outputs.append({\"ratings\": None, \"rationales\": None})\n            continue\n\n        formatted_outputs.append(\n            {\n                \"ratings\": int(re.findall(r\"\\b\\d+\\b\", matches.group(1))[0])\n                if matches.group(1) not in [\"None\", \"N/A\"]\n                else None,\n                \"rationales\": matches.group(2),\n            }\n        )\n    return combine_dicts(*formatted_outputs)\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.UltraFeedback._format_types_ratings_rationales_output","title":"<code>_format_types_ratings_rationales_output(output, input)</code>","text":"<p>Formats the output when the aspect is either <code>helpfulness</code> or <code>truthfulness</code>.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def _format_types_ratings_rationales_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, List[Any]]:\n    \"\"\"Formats the output when the aspect is either `helpfulness` or `truthfulness`.\"\"\"\n    if output is None:\n        return {\n            \"types\": [None] * len(input[\"generations\"]),\n            \"rationales\": [None] * len(input[\"generations\"]),\n            \"ratings\": [None] * len(input[\"generations\"]),\n            \"rationales-for-ratings\": [None] * len(input[\"generations\"]),\n        }\n\n    pattern = r\"Type: (.+?)\\nRationale: (.+?)\\nRating: (.+?)\\nRationale: (.+)\"\n\n    sections = output.split(\"\\n\\n\")\n\n    formatted_outputs = []\n    for section in sections:\n        matches = None\n        if section is not None and section != \"\":\n            matches = re.search(pattern, section, re.DOTALL)\n        if not matches:\n            formatted_outputs.append(\n                {\n                    \"types\": None,\n                    \"rationales\": None,\n                    \"ratings\": None,\n                    \"rationales-for-ratings\": None,\n                }\n            )\n            continue\n\n        formatted_outputs.append(\n            {\n                \"types\": int(re.findall(r\"\\b\\d+\\b\", matches.group(1))[0])\n                if matches.group(1) not in [\"None\", \"N/A\"]\n                else None,\n                \"rationales\": matches.group(2),\n                \"ratings\": int(re.findall(r\"\\b\\d+\\b\", matches.group(3))[0])\n                if matches.group(3) not in [\"None\", \"N/A\"]\n                else None,\n                \"rationales-for-ratings\": matches.group(4),\n            }\n        )\n    return combine_dicts(*formatted_outputs)\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.UltraFeedback.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": self._system_prompt.format(\n                no_texts=len(input[\"generations\"])\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], generations=input[\"generations\"]\n            ),\n        },\n    ]\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.UltraFeedback.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>ratings</code> and <code>rationales</code> for each of the provided <code>generations</code> for the given <code>instruction</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>a string representing the output of the LLM via the <code>process</code> method.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task, as required by some tasks to format the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing either the <code>ratings</code> and <code>rationales</code> for each of the provided</p> <code>Dict[str, Any]</code> <p><code>generations</code> for the given <code>instruction</code> if the provided aspect is either <code>honesty</code>,</p> <code>Dict[str, Any]</code> <p><code>instruction-following</code>, or <code>overall-rating</code>; or the <code>types</code>, <code>rationales</code>,</p> <code>Dict[str, Any]</code> <p><code>ratings</code>, and <code>rationales-for-ratings</code> for each of the provided <code>generations</code> for the</p> <code>Dict[str, Any]</code> <p>given <code>instruction</code> if the provided aspect is either <code>helpfulness</code> or <code>truthfulness</code>.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `ratings` and `rationales` for\n    each of the provided `generations` for the given `instruction`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\n\n    Args:\n        output: a string representing the output of the LLM via the `process` method.\n        input: the input to the task, as required by some tasks to format the output.\n\n    Returns:\n        A dictionary containing either the `ratings` and `rationales` for each of the provided\n        `generations` for the given `instruction` if the provided aspect is either `honesty`,\n        `instruction-following`, or `overall-rating`; or the `types`, `rationales`,\n        `ratings`, and `rationales-for-ratings` for each of the provided `generations` for the\n        given `instruction` if the provided aspect is either `helpfulness` or `truthfulness`.\n    \"\"\"\n    if self.aspect in [\n        \"honesty\",\n        \"instruction-following\",\n        \"overall-rating\",\n    ]:\n        return self._format_ratings_rationales_output(output, input)\n    return self._format_types_ratings_rationales_output(output, input)\n</code></pre>"},{"location":"api/task_gallery/#distilabel.steps.tasks.UltraFeedback.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template for the given <code>aspect</code>.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template for the given `aspect`.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"ultrafeedback\"\n        / f\"{self.aspect}.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>distilabel<ul> <li>cli<ul> <li>app</li> <li>pipeline<ul> <li>app</li> <li>utils</li> </ul> </li> </ul> </li> <li>distiset</li> <li>llms<ul> <li>anthropic</li> <li>anyscale</li> <li>azure</li> <li>base</li> <li>chat_templates</li> <li>cohere</li> <li>groq</li> <li>huggingface<ul> <li>inference_endpoints</li> <li>transformers</li> </ul> </li> <li>litellm</li> <li>llamacpp</li> <li>mistral</li> <li>mixins</li> <li>ollama</li> <li>openai</li> <li>together</li> <li>typing</li> <li>vertexai</li> <li>vllm</li> </ul> </li> <li>mixins<ul> <li>runtime_parameters</li> </ul> </li> <li>pipeline<ul> <li>_dag</li> <li>base</li> <li>constants</li> <li>local</li> <li>routing_batch_function</li> <li>typing</li> <li>utils</li> </ul> </li> <li>steps<ul> <li>argilla<ul> <li>base</li> <li>preference</li> <li>text_generation</li> </ul> </li> <li>base</li> <li>combine</li> <li>constants</li> <li>decorator</li> <li>deita</li> <li>expand</li> <li>formatting<ul> <li>conversation</li> <li>dpo</li> <li>sft</li> </ul> </li> <li>generators<ul> <li>data</li> <li>huggingface</li> </ul> </li> <li>globals<ul> <li>huggingface</li> </ul> </li> <li>keep</li> <li>tasks<ul> <li>base</li> <li>complexity_scorer</li> <li>evol_instruct<ul> <li>base</li> <li>evol_complexity<ul> <li>base</li> <li>generator</li> <li>utils</li> </ul> </li> <li>generator</li> <li>utils</li> </ul> </li> <li>evol_quality<ul> <li>base</li> <li>utils</li> </ul> </li> <li>generate_embeddings</li> <li>genstruct</li> <li>instruction_backtranslation</li> <li>pair_rm</li> <li>prometheus_eval</li> <li>quality_scorer</li> <li>self_instruct</li> <li>sentence_transformers</li> <li>structured_generation</li> <li>structured_outputs<ul> <li>instructor</li> <li>outlines</li> <li>utils</li> </ul> </li> <li>text_generation</li> <li>typing</li> <li>ultrafeedback</li> </ul> </li> <li>typing</li> </ul> </li> <li>utils<ul> <li>card<ul> <li>dataset_card</li> </ul> </li> <li>chat</li> <li>dicts</li> <li>docstring</li> <li>export_components_info</li> <li>files</li> <li>itertools</li> <li>lists</li> <li>logging</li> <li>mkdocs<ul> <li>components_gallery</li> </ul> </li> <li>notebook</li> <li>serialization</li> <li>typing_</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/distilabel/","title":"Index","text":""},{"location":"reference/distilabel/distiset/","title":"Distiset","text":""},{"location":"reference/distilabel/distiset/#distilabel.distiset.Distiset","title":"<code>Distiset</code>","text":"<p>               Bases: <code>dict</code></p> <p>Convenient wrapper around <code>datasets.Dataset</code> to push to the Hugging Face Hub.</p> <p>It's a dictionary where the keys correspond to the different leaf_steps from the internal <code>DAG</code> and the values are <code>datasets.Dataset</code>.</p> <p>Attributes:</p> Name Type Description <code>pipeline_path</code> <code>Union[Path, None]</code> <p>Optional path to the pipeline.yaml file that generated the dataset.</p> <code>log_filename_path</code> <code>Union[Path, None]</code> <p>Optional path to the pipeline.log file that generated was written by the pipeline.</p> Source code in <code>src/distilabel/distiset.py</code> <pre><code>class Distiset(dict):\n    \"\"\"Convenient wrapper around `datasets.Dataset` to push to the Hugging Face Hub.\n\n    It's a dictionary where the keys correspond to the different leaf_steps from the internal\n    `DAG` and the values are `datasets.Dataset`.\n\n    Attributes:\n        pipeline_path: Optional path to the pipeline.yaml file that generated the dataset.\n        log_filename_path: Optional path to the pipeline.log file that generated was written by the\n            pipeline.\n    \"\"\"\n\n    _pipeline_path: Optional[Path] = None\n    _log_filename_path: Optional[Path] = None\n\n    def push_to_hub(\n        self,\n        repo_id: str,\n        private: bool = False,\n        token: Optional[str] = None,\n        generate_card: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Pushes the `Distiset` to the Hugging Face Hub, each dataset will be pushed as a different configuration\n        corresponding to the leaf step that generated it.\n\n        Args:\n            repo_id:\n                The ID of the repository to push to in the following format: `&lt;user&gt;/&lt;dataset_name&gt;` or\n                `&lt;org&gt;/&lt;dataset_name&gt;`. Also accepts `&lt;dataset_name&gt;`, which will default to the namespace\n                of the logged-in user.\n            private:\n                Whether the dataset repository should be set to private or not. Only affects repository creation:\n                a repository that already exists will not be affected by that parameter.\n            token:\n                An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n                to the token saved locally when logging in with `huggingface-cli login`. Will raise an error\n                if no token is passed and the user is not logged-in.\n            generate_card:\n                Whether to generate a dataset card or not. Defaults to True.\n            **kwargs:\n                Additional keyword arguments to pass to the `push_to_hub` method of the `datasets.Dataset` object.\n        \"\"\"\n        for name, dataset in self.items():\n            dataset.push_to_hub(\n                repo_id=repo_id,\n                config_name=name,\n                private=private,\n                token=token,\n                **kwargs,\n            )\n\n        if generate_card:\n            self._generate_card(repo_id, token)\n\n    def _get_card(\n        self, repo_id: str, token: Optional[str] = None\n    ) -&gt; DistilabelDatasetCard:\n        \"\"\"Generates the dataset card for the `Distiset`.\n\n        Note:\n            If `repo_id` and `token` are provided, it will extract the metadata from the README.md file\n            on the hub.\n\n        Args:\n            repo_id: Name of the repository to push to, or the path for the distiset if saved to disk.\n            token: The token to authenticate with the Hugging Face Hub.\n                We assume that if it's provided, the dataset will be in the Hugging Face Hub,\n                so the README metadata will be extracted from there.\n\n        Returns:\n            The dataset card for the `Distiset`.\n        \"\"\"\n        sample_records = {}\n        for name, dataset in self.items():\n            sample_records[name] = (\n                dataset[0] if not isinstance(dataset, dict) else dataset[\"train\"][0]\n            )\n\n        readme_metadata = {}\n        if repo_id and token:\n            readme_metadata = self._extract_readme_metadata(repo_id, token)\n\n        metadata = {\n            **readme_metadata,\n            \"size_categories\": size_categories_parser(\n                max(len(dataset) for dataset in self.values())\n            ),\n            \"tags\": [\"synthetic\", \"distilabel\", \"rlaif\"],\n        }\n\n        card = DistilabelDatasetCard.from_template(\n            card_data=DatasetCardData(**metadata),\n            repo_id=repo_id,\n            sample_records=sample_records,\n        )\n\n        return card\n\n    def _extract_readme_metadata(\n        self, repo_id: str, token: Optional[str]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Extracts the metadata from the README.md file of the dataset repository.\n\n        We have to download the previous README.md file in the repo, extract the metadata from it,\n        and generate a dict again to be passed thorough the `DatasetCardData` object.\n\n        Args:\n            repo_id: The ID of the repository to push to, from the `push_to_hub` method.\n            token: The token to authenticate with the Hugging Face Hub, from the `push_to_hub` method.\n\n        Returns:\n            The metadata extracted from the README.md file of the dataset repository as a dict.\n        \"\"\"\n        readme_path = Path(\n            hf_hub_download(repo_id, \"README.md\", repo_type=\"dataset\", token=token)\n        )\n        # Remove the '---' from the metadata\n        metadata = re.findall(r\"---\\n(.*?)\\n---\", readme_path.read_text(), re.DOTALL)[0]\n        metadata = yaml.safe_load(metadata)\n        return metadata\n\n    def _generate_card(self, repo_id: str, token: str) -&gt; None:\n        \"\"\"Generates a dataset card and pushes it to the Hugging Face Hub, and\n        if the `pipeline.yaml` path is available in the `Distiset`, uploads that\n        to the same repository.\n\n        Args:\n            repo_id: The ID of the repository to push to, from the `push_to_hub` method.\n            token: The token to authenticate with the Hugging Face Hub, from the `push_to_hub` method.\n        \"\"\"\n        card = self._get_card(repo_id=repo_id, token=token)\n\n        card.push_to_hub(\n            repo_id,\n            repo_type=\"dataset\",\n            token=token,\n        )\n        if self.pipeline_path:\n            # If the pipeline.yaml is available, upload it to the Hugging Face Hub as well.\n            HfApi().upload_file(\n                path_or_fileobj=self.pipeline_path,\n                path_in_repo=PIPELINE_CONFIG_FILENAME,\n                repo_id=repo_id,\n                repo_type=\"dataset\",\n                token=token,\n            )\n        if self.log_filename_path:\n            # The same we had with \"pipeline.yaml\" but with the log file.\n            HfApi().upload_file(\n                path_or_fileobj=self.log_filename_path,\n                path_in_repo=PIPELINE_LOG_FILENAME,\n                repo_id=repo_id,\n                repo_type=\"dataset\",\n                token=token,\n            )\n\n    def train_test_split(\n        self,\n        train_size: float,\n        shuffle: bool = True,\n        seed: Optional[int] = None,\n    ) -&gt; Self:\n        \"\"\"Return a `Distiset` whose values will be a `datasets.DatasetDict` with two random train and test subsets.\n        Splits are created from the dataset according to `train_size` and `shuffle`.\n\n        Args:\n            train_size:\n                Float between `0.0` and `1.0` representing the proportion of the dataset to include in the test split.\n                It will be applied to all the datasets in the `Distiset`.\n            shuffle: Whether or not to shuffle the data before splitting\n            seed:\n                A seed to initialize the default BitGenerator, passed to the underlying method.\n\n        Returns:\n            The `Distiset` with the train-test split applied to all the datasets.\n        \"\"\"\n        assert 0 &lt; train_size &lt; 1, \"train_size must be a float between 0 and 1\"\n        for name, dataset in self.items():\n            self[name] = dataset.train_test_split(\n                train_size=train_size,\n                shuffle=shuffle,\n                seed=seed,\n            )\n        return self\n\n    def save_to_disk(\n        self,\n        distiset_path: PathLike,\n        max_shard_size: Optional[Union[str, int]] = None,\n        num_shards: Optional[int] = None,\n        num_proc: Optional[int] = None,\n        storage_options: Optional[dict] = None,\n        save_card: bool = True,\n        save_pipeline_config: bool = True,\n        save_pipeline_log: bool = True,\n    ) -&gt; None:\n        r\"\"\"\n        Saves a `Distiset` to a dataset directory, or in a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n\n        In case you want to save the `Distiset` in a remote filesystem, you can pass the `storage_options` parameter\n        as you would do with `datasets`'s `Dataset.save_to_disk` method: [see example](https://huggingface.co/docs/datasets/filesystems#saving-serialized-datasets)\n\n        Args:\n            distiset_path: Path where you want to save the `Distiset`. It can be a local path\n                (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n            max_shard_size: The maximum size of the dataset shards to be uploaded to the hub.\n                If expressed as a string, needs to be digits followed by a unit (like `\"50MB\"`).\n                Defaults to `None`.\n            num_shards: Number of shards to write. By default the number of shards depends on\n                `max_shard_size` and `num_proc`. Defaults to `None`.\n            num_proc: Number of processes when downloading and generating the dataset locally.\n                Multiprocessing is disabled by default. Defaults to `None`.\n            storage_options: Key/value pairs to be passed on to the file-system backend, if any.\n                Defaults to `None`.\n            save_card: Whether to save the dataset card. Defaults to `True`.\n            save_pipeline_config: Whether to save the pipeline configuration file (aka the `pipeline.yaml` file).\n                Defaults to `True`.\n            save_pipeline_log: Whether to save the pipeline log file (aka the `pipeline.log` file).\n                Defaults to `True`.\n\n        Examples:\n            ```python\n            # Save your distiset in a local folder:\n            &gt;&gt;&gt; distiset.save_to_disk(dataset_path=\"my-distiset\")\n            # Save your distiset in a remote storage:\n            &gt;&gt;&gt; storage_options = {\n            ...     \"key\": os.environ[\"S3_ACCESS_KEY\"],\n            ...     \"secret\": os.environ[\"S3_SECRET_KEY\"],\n            ...     \"client_kwargs\": {\n            ...         \"endpoint_url\": os.environ[\"S3_ENDPOINT_URL\"],\n            ...         \"region_name\": os.environ[\"S3_REGION\"],\n            ...     },\n            ... }\n            &gt;&gt;&gt; distiset.save_to_disk(dataset_path=\"my-distiset\", storage_options=storage_options)\n            ```\n        \"\"\"\n        distiset_path = str(distiset_path)\n        for name, dataset in self.items():\n            dataset.save_to_disk(\n                f\"{distiset_path}/{name}\",\n                max_shard_size=max_shard_size,\n                num_shards=num_shards,\n                num_proc=num_proc,\n                storage_options=storage_options,\n            )\n\n        distiset_config_folder = posixpath.join(distiset_path, DISTISET_CONFIG_FOLDER)\n\n        fs: fsspec.AbstractFileSystem\n        fs, _, _ = fsspec.get_fs_token_paths(\n            distiset_config_folder, storage_options=storage_options\n        )\n        fs.makedirs(distiset_config_folder, exist_ok=True)\n\n        if save_card:\n            # NOTE:\u00a0Currently the card is not the same if we write to disk or push to the HF hub,\n            # as we aren't generating the README copying/updating the data from the dataset repo.\n            card = self._get_card(repo_id=Path(distiset_path).stem, token=None)\n            new_filename = posixpath.join(distiset_config_folder, \"README.md\")\n            if storage_options:\n                # Write the card the same way as DatasetCard.save does:\n                with fs.open(new_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n                    f.write(str(card))\n            else:\n                card.save(new_filename)\n\n        # Write our internal files to the distiset folder by copying them to the distiset folder.\n        if save_pipeline_config and self.pipeline_path:\n            new_filename = posixpath.join(\n                distiset_config_folder, PIPELINE_CONFIG_FILENAME\n            )\n            if self.pipeline_path.exists() and (not fs.isfile(new_filename)):\n                data = yaml.safe_load(self.pipeline_path.read_text())\n                with fs.open(new_filename, \"w\", encoding=\"utf-8\") as f:\n                    yaml.dump(data, f, default_flow_style=False)\n\n        if save_pipeline_log and self.log_filename_path:\n            new_filename = posixpath.join(distiset_config_folder, PIPELINE_LOG_FILENAME)\n            if self.log_filename_path.exists() and (not fs.isfile(new_filename)):\n                data = self.log_filename_path.read_text()\n                with fs.open(new_filename, \"w\", encoding=\"utf-8\") as f:\n                    f.write(data)\n\n    @classmethod\n    def load_from_disk(\n        cls,\n        distiset_path: PathLike,\n        keep_in_memory: Optional[bool] = None,\n        storage_options: Optional[Dict[str, Any]] = None,\n        download_dir: Optional[PathLike] = None,\n    ) -&gt; Self:\n        \"\"\"Loads a dataset that was previously saved using `Distiset.save_to_disk` from a dataset\n        directory, or from a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n\n        Args:\n            distiset_path: Path (\"dataset/train\") or remote URI (\"s3://bucket/dataset/train\").\n            keep_in_memory: Whether to copy the dataset in-memory, see `datasets.Dataset.load_from_disk``\n                for more information. Defaults to `None`.\n            storage_options: Key/value pairs to be passed on to the file-system backend, if any.\n                Defaults to `None`.\n            download_dir: Optional directory to download the dataset to. Defaults to None,\n                in which case it will create a temporary directory.\n\n        Returns:\n            A `Distiset` loaded from disk, it should be a `Distiset` object created using `Distiset.save_to_disk`.\n        \"\"\"\n        original_distiset_path = str(distiset_path)\n\n        fs: fsspec.AbstractFileSystem\n        fs, _, [distiset_path] = fsspec.get_fs_token_paths(\n            original_distiset_path, storage_options=storage_options\n        )\n        dest_distiset_path = distiset_path\n\n        assert fs.isdir(\n            original_distiset_path\n        ), \"`distiset_path` must be a `PathLike` object pointing to a folder or a URI of a remote filesystem.\"\n\n        has_config = False\n        distiset = cls()\n\n        if is_remote_filesystem(fs):\n            src_dataset_path = distiset_path\n            if download_dir:\n                dest_distiset_path = download_dir\n            else:\n                dest_distiset_path = Dataset._build_local_temp_path(src_dataset_path)\n            fs.download(src_dataset_path, dest_distiset_path.as_posix(), recursive=True)\n\n        # Now we should have the distiset locally, so we can read those files\n        for folder in Path(dest_distiset_path).iterdir():\n            if folder.stem == DISTISET_CONFIG_FOLDER:\n                has_config = True\n                continue\n            distiset[folder.stem] = load_from_disk(\n                str(folder),\n                keep_in_memory=keep_in_memory,\n            )\n        # From the config folder we just need to point to the files. Once downloaded we set the path\n        # to wherever they are.\n        if has_config:\n            distiset_config_folder = posixpath.join(\n                dest_distiset_path, DISTISET_CONFIG_FOLDER\n            )\n\n            pipeline_path = posixpath.join(\n                distiset_config_folder, PIPELINE_CONFIG_FILENAME\n            )\n            if Path(pipeline_path).exists():\n                distiset.pipeline_path = Path(pipeline_path)\n\n            log_filename_path = posixpath.join(\n                distiset_config_folder, PIPELINE_LOG_FILENAME\n            )\n            if Path(log_filename_path).exists():\n                distiset.log_filename_path = Path(log_filename_path)\n\n        return distiset\n\n    @property\n    def pipeline_path(self) -&gt; Union[Path, None]:\n        \"\"\"Returns the path to the `pipeline.yaml` file that generated the `Pipeline`.\"\"\"\n        return self._pipeline_path\n\n    @pipeline_path.setter\n    def pipeline_path(self, path: PathLike) -&gt; None:\n        self._pipeline_path = Path(path)\n\n    @property\n    def log_filename_path(self) -&gt; Union[Path, None]:\n        \"\"\"Returns the path to the `pipeline.log` file that generated the `Pipeline`.\"\"\"\n        return self._log_filename_path\n\n    @log_filename_path.setter\n    def log_filename_path(self, path: PathLike) -&gt; None:\n        self._log_filename_path = Path(path)\n\n    def __repr__(self):\n        # Copy from `datasets.DatasetDict.__repr__`.\n        repr = \"\\n\".join([f\"{k}: {v}\" for k, v in self.items()])\n        repr = re.sub(r\"^\", \" \" * 4, repr, count=0, flags=re.M)\n        return f\"Distiset({{\\n{repr}\\n}})\"\n</code></pre>"},{"location":"reference/distilabel/distiset/#distilabel.distiset.Distiset.log_filename_path","title":"<code>log_filename_path: Union[Path, None]</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the path to the <code>pipeline.log</code> file that generated the <code>Pipeline</code>.</p>"},{"location":"reference/distilabel/distiset/#distilabel.distiset.Distiset.pipeline_path","title":"<code>pipeline_path: Union[Path, None]</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the path to the <code>pipeline.yaml</code> file that generated the <code>Pipeline</code>.</p>"},{"location":"reference/distilabel/distiset/#distilabel.distiset.Distiset.load_from_disk","title":"<code>load_from_disk(distiset_path, keep_in_memory=None, storage_options=None, download_dir=None)</code>  <code>classmethod</code>","text":"<p>Loads a dataset that was previously saved using <code>Distiset.save_to_disk</code> from a dataset directory, or from a filesystem using any implementation of <code>fsspec.spec.AbstractFileSystem</code>.</p> <p>Parameters:</p> Name Type Description Default <code>distiset_path</code> <code>PathLike</code> <p>Path (\"dataset/train\") or remote URI (\"s3://bucket/dataset/train\").</p> required <code>keep_in_memory</code> <code>Optional[bool]</code> <p>Whether to copy the dataset in-memory, see <code>datasets.Dataset.load_from_disk`` for more information. Defaults to</code>None`.</p> <code>None</code> <code>storage_options</code> <code>Optional[Dict[str, Any]]</code> <p>Key/value pairs to be passed on to the file-system backend, if any. Defaults to <code>None</code>.</p> <code>None</code> <code>download_dir</code> <code>Optional[PathLike]</code> <p>Optional directory to download the dataset to. Defaults to None, in which case it will create a temporary directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A <code>Distiset</code> loaded from disk, it should be a <code>Distiset</code> object created using <code>Distiset.save_to_disk</code>.</p> Source code in <code>src/distilabel/distiset.py</code> <pre><code>@classmethod\ndef load_from_disk(\n    cls,\n    distiset_path: PathLike,\n    keep_in_memory: Optional[bool] = None,\n    storage_options: Optional[Dict[str, Any]] = None,\n    download_dir: Optional[PathLike] = None,\n) -&gt; Self:\n    \"\"\"Loads a dataset that was previously saved using `Distiset.save_to_disk` from a dataset\n    directory, or from a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n\n    Args:\n        distiset_path: Path (\"dataset/train\") or remote URI (\"s3://bucket/dataset/train\").\n        keep_in_memory: Whether to copy the dataset in-memory, see `datasets.Dataset.load_from_disk``\n            for more information. Defaults to `None`.\n        storage_options: Key/value pairs to be passed on to the file-system backend, if any.\n            Defaults to `None`.\n        download_dir: Optional directory to download the dataset to. Defaults to None,\n            in which case it will create a temporary directory.\n\n    Returns:\n        A `Distiset` loaded from disk, it should be a `Distiset` object created using `Distiset.save_to_disk`.\n    \"\"\"\n    original_distiset_path = str(distiset_path)\n\n    fs: fsspec.AbstractFileSystem\n    fs, _, [distiset_path] = fsspec.get_fs_token_paths(\n        original_distiset_path, storage_options=storage_options\n    )\n    dest_distiset_path = distiset_path\n\n    assert fs.isdir(\n        original_distiset_path\n    ), \"`distiset_path` must be a `PathLike` object pointing to a folder or a URI of a remote filesystem.\"\n\n    has_config = False\n    distiset = cls()\n\n    if is_remote_filesystem(fs):\n        src_dataset_path = distiset_path\n        if download_dir:\n            dest_distiset_path = download_dir\n        else:\n            dest_distiset_path = Dataset._build_local_temp_path(src_dataset_path)\n        fs.download(src_dataset_path, dest_distiset_path.as_posix(), recursive=True)\n\n    # Now we should have the distiset locally, so we can read those files\n    for folder in Path(dest_distiset_path).iterdir():\n        if folder.stem == DISTISET_CONFIG_FOLDER:\n            has_config = True\n            continue\n        distiset[folder.stem] = load_from_disk(\n            str(folder),\n            keep_in_memory=keep_in_memory,\n        )\n    # From the config folder we just need to point to the files. Once downloaded we set the path\n    # to wherever they are.\n    if has_config:\n        distiset_config_folder = posixpath.join(\n            dest_distiset_path, DISTISET_CONFIG_FOLDER\n        )\n\n        pipeline_path = posixpath.join(\n            distiset_config_folder, PIPELINE_CONFIG_FILENAME\n        )\n        if Path(pipeline_path).exists():\n            distiset.pipeline_path = Path(pipeline_path)\n\n        log_filename_path = posixpath.join(\n            distiset_config_folder, PIPELINE_LOG_FILENAME\n        )\n        if Path(log_filename_path).exists():\n            distiset.log_filename_path = Path(log_filename_path)\n\n    return distiset\n</code></pre>"},{"location":"reference/distilabel/distiset/#distilabel.distiset.Distiset.push_to_hub","title":"<code>push_to_hub(repo_id, private=False, token=None, generate_card=True, **kwargs)</code>","text":"<p>Pushes the <code>Distiset</code> to the Hugging Face Hub, each dataset will be pushed as a different configuration corresponding to the leaf step that generated it.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The ID of the repository to push to in the following format: <code>&lt;user&gt;/&lt;dataset_name&gt;</code> or <code>&lt;org&gt;/&lt;dataset_name&gt;</code>. Also accepts <code>&lt;dataset_name&gt;</code>, which will default to the namespace of the logged-in user.</p> required <code>private</code> <code>bool</code> <p>Whether the dataset repository should be set to private or not. Only affects repository creation: a repository that already exists will not be affected by that parameter.</p> <code>False</code> <code>token</code> <code>Optional[str]</code> <p>An optional authentication token for the Hugging Face Hub. If no token is passed, will default to the token saved locally when logging in with <code>huggingface-cli login</code>. Will raise an error if no token is passed and the user is not logged-in.</p> <code>None</code> <code>generate_card</code> <code>bool</code> <p>Whether to generate a dataset card or not. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>push_to_hub</code> method of the <code>datasets.Dataset</code> object.</p> <code>{}</code> Source code in <code>src/distilabel/distiset.py</code> <pre><code>def push_to_hub(\n    self,\n    repo_id: str,\n    private: bool = False,\n    token: Optional[str] = None,\n    generate_card: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Pushes the `Distiset` to the Hugging Face Hub, each dataset will be pushed as a different configuration\n    corresponding to the leaf step that generated it.\n\n    Args:\n        repo_id:\n            The ID of the repository to push to in the following format: `&lt;user&gt;/&lt;dataset_name&gt;` or\n            `&lt;org&gt;/&lt;dataset_name&gt;`. Also accepts `&lt;dataset_name&gt;`, which will default to the namespace\n            of the logged-in user.\n        private:\n            Whether the dataset repository should be set to private or not. Only affects repository creation:\n            a repository that already exists will not be affected by that parameter.\n        token:\n            An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n            to the token saved locally when logging in with `huggingface-cli login`. Will raise an error\n            if no token is passed and the user is not logged-in.\n        generate_card:\n            Whether to generate a dataset card or not. Defaults to True.\n        **kwargs:\n            Additional keyword arguments to pass to the `push_to_hub` method of the `datasets.Dataset` object.\n    \"\"\"\n    for name, dataset in self.items():\n        dataset.push_to_hub(\n            repo_id=repo_id,\n            config_name=name,\n            private=private,\n            token=token,\n            **kwargs,\n        )\n\n    if generate_card:\n        self._generate_card(repo_id, token)\n</code></pre>"},{"location":"reference/distilabel/distiset/#distilabel.distiset.Distiset.save_to_disk","title":"<code>save_to_disk(distiset_path, max_shard_size=None, num_shards=None, num_proc=None, storage_options=None, save_card=True, save_pipeline_config=True, save_pipeline_log=True)</code>","text":"<p>Saves a <code>Distiset</code> to a dataset directory, or in a filesystem using any implementation of <code>fsspec.spec.AbstractFileSystem</code>.</p> <p>In case you want to save the <code>Distiset</code> in a remote filesystem, you can pass the <code>storage_options</code> parameter as you would do with <code>datasets</code>'s <code>Dataset.save_to_disk</code> method: see example</p> <p>Parameters:</p> Name Type Description Default <code>distiset_path</code> <code>PathLike</code> <p>Path where you want to save the <code>Distiset</code>. It can be a local path (e.g. <code>dataset/train</code>) or remote URI (e.g. <code>s3://my-bucket/dataset/train</code>)</p> required <code>max_shard_size</code> <code>Optional[Union[str, int]]</code> <p>The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit (like <code>\"50MB\"</code>). Defaults to <code>None</code>.</p> <code>None</code> <code>num_shards</code> <code>Optional[int]</code> <p>Number of shards to write. By default the number of shards depends on <code>max_shard_size</code> and <code>num_proc</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>num_proc</code> <code>Optional[int]</code> <p>Number of processes when downloading and generating the dataset locally. Multiprocessing is disabled by default. Defaults to <code>None</code>.</p> <code>None</code> <code>storage_options</code> <code>Optional[dict]</code> <p>Key/value pairs to be passed on to the file-system backend, if any. Defaults to <code>None</code>.</p> <code>None</code> <code>save_card</code> <code>bool</code> <p>Whether to save the dataset card. Defaults to <code>True</code>.</p> <code>True</code> <code>save_pipeline_config</code> <code>bool</code> <p>Whether to save the pipeline configuration file (aka the <code>pipeline.yaml</code> file). Defaults to <code>True</code>.</p> <code>True</code> <code>save_pipeline_log</code> <code>bool</code> <p>Whether to save the pipeline log file (aka the <code>pipeline.log</code> file). Defaults to <code>True</code>.</p> <code>True</code> <p>Examples:</p> <pre><code># Save your distiset in a local folder:\n&gt;&gt;&gt; distiset.save_to_disk(dataset_path=\"my-distiset\")\n# Save your distiset in a remote storage:\n&gt;&gt;&gt; storage_options = {\n...     \"key\": os.environ[\"S3_ACCESS_KEY\"],\n...     \"secret\": os.environ[\"S3_SECRET_KEY\"],\n...     \"client_kwargs\": {\n...         \"endpoint_url\": os.environ[\"S3_ENDPOINT_URL\"],\n...         \"region_name\": os.environ[\"S3_REGION\"],\n...     },\n... }\n&gt;&gt;&gt; distiset.save_to_disk(dataset_path=\"my-distiset\", storage_options=storage_options)\n</code></pre> Source code in <code>src/distilabel/distiset.py</code> <pre><code>def save_to_disk(\n    self,\n    distiset_path: PathLike,\n    max_shard_size: Optional[Union[str, int]] = None,\n    num_shards: Optional[int] = None,\n    num_proc: Optional[int] = None,\n    storage_options: Optional[dict] = None,\n    save_card: bool = True,\n    save_pipeline_config: bool = True,\n    save_pipeline_log: bool = True,\n) -&gt; None:\n    r\"\"\"\n    Saves a `Distiset` to a dataset directory, or in a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n\n    In case you want to save the `Distiset` in a remote filesystem, you can pass the `storage_options` parameter\n    as you would do with `datasets`'s `Dataset.save_to_disk` method: [see example](https://huggingface.co/docs/datasets/filesystems#saving-serialized-datasets)\n\n    Args:\n        distiset_path: Path where you want to save the `Distiset`. It can be a local path\n            (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n        max_shard_size: The maximum size of the dataset shards to be uploaded to the hub.\n            If expressed as a string, needs to be digits followed by a unit (like `\"50MB\"`).\n            Defaults to `None`.\n        num_shards: Number of shards to write. By default the number of shards depends on\n            `max_shard_size` and `num_proc`. Defaults to `None`.\n        num_proc: Number of processes when downloading and generating the dataset locally.\n            Multiprocessing is disabled by default. Defaults to `None`.\n        storage_options: Key/value pairs to be passed on to the file-system backend, if any.\n            Defaults to `None`.\n        save_card: Whether to save the dataset card. Defaults to `True`.\n        save_pipeline_config: Whether to save the pipeline configuration file (aka the `pipeline.yaml` file).\n            Defaults to `True`.\n        save_pipeline_log: Whether to save the pipeline log file (aka the `pipeline.log` file).\n            Defaults to `True`.\n\n    Examples:\n        ```python\n        # Save your distiset in a local folder:\n        &gt;&gt;&gt; distiset.save_to_disk(dataset_path=\"my-distiset\")\n        # Save your distiset in a remote storage:\n        &gt;&gt;&gt; storage_options = {\n        ...     \"key\": os.environ[\"S3_ACCESS_KEY\"],\n        ...     \"secret\": os.environ[\"S3_SECRET_KEY\"],\n        ...     \"client_kwargs\": {\n        ...         \"endpoint_url\": os.environ[\"S3_ENDPOINT_URL\"],\n        ...         \"region_name\": os.environ[\"S3_REGION\"],\n        ...     },\n        ... }\n        &gt;&gt;&gt; distiset.save_to_disk(dataset_path=\"my-distiset\", storage_options=storage_options)\n        ```\n    \"\"\"\n    distiset_path = str(distiset_path)\n    for name, dataset in self.items():\n        dataset.save_to_disk(\n            f\"{distiset_path}/{name}\",\n            max_shard_size=max_shard_size,\n            num_shards=num_shards,\n            num_proc=num_proc,\n            storage_options=storage_options,\n        )\n\n    distiset_config_folder = posixpath.join(distiset_path, DISTISET_CONFIG_FOLDER)\n\n    fs: fsspec.AbstractFileSystem\n    fs, _, _ = fsspec.get_fs_token_paths(\n        distiset_config_folder, storage_options=storage_options\n    )\n    fs.makedirs(distiset_config_folder, exist_ok=True)\n\n    if save_card:\n        # NOTE:\u00a0Currently the card is not the same if we write to disk or push to the HF hub,\n        # as we aren't generating the README copying/updating the data from the dataset repo.\n        card = self._get_card(repo_id=Path(distiset_path).stem, token=None)\n        new_filename = posixpath.join(distiset_config_folder, \"README.md\")\n        if storage_options:\n            # Write the card the same way as DatasetCard.save does:\n            with fs.open(new_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n                f.write(str(card))\n        else:\n            card.save(new_filename)\n\n    # Write our internal files to the distiset folder by copying them to the distiset folder.\n    if save_pipeline_config and self.pipeline_path:\n        new_filename = posixpath.join(\n            distiset_config_folder, PIPELINE_CONFIG_FILENAME\n        )\n        if self.pipeline_path.exists() and (not fs.isfile(new_filename)):\n            data = yaml.safe_load(self.pipeline_path.read_text())\n            with fs.open(new_filename, \"w\", encoding=\"utf-8\") as f:\n                yaml.dump(data, f, default_flow_style=False)\n\n    if save_pipeline_log and self.log_filename_path:\n        new_filename = posixpath.join(distiset_config_folder, PIPELINE_LOG_FILENAME)\n        if self.log_filename_path.exists() and (not fs.isfile(new_filename)):\n            data = self.log_filename_path.read_text()\n            with fs.open(new_filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(data)\n</code></pre>"},{"location":"reference/distilabel/distiset/#distilabel.distiset.Distiset.train_test_split","title":"<code>train_test_split(train_size, shuffle=True, seed=None)</code>","text":"<p>Return a <code>Distiset</code> whose values will be a <code>datasets.DatasetDict</code> with two random train and test subsets. Splits are created from the dataset according to <code>train_size</code> and <code>shuffle</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_size</code> <code>float</code> <p>Float between <code>0.0</code> and <code>1.0</code> representing the proportion of the dataset to include in the test split. It will be applied to all the datasets in the <code>Distiset</code>.</p> required <code>shuffle</code> <code>bool</code> <p>Whether or not to shuffle the data before splitting</p> <code>True</code> <code>seed</code> <code>Optional[int]</code> <p>A seed to initialize the default BitGenerator, passed to the underlying method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>The <code>Distiset</code> with the train-test split applied to all the datasets.</p> Source code in <code>src/distilabel/distiset.py</code> <pre><code>def train_test_split(\n    self,\n    train_size: float,\n    shuffle: bool = True,\n    seed: Optional[int] = None,\n) -&gt; Self:\n    \"\"\"Return a `Distiset` whose values will be a `datasets.DatasetDict` with two random train and test subsets.\n    Splits are created from the dataset according to `train_size` and `shuffle`.\n\n    Args:\n        train_size:\n            Float between `0.0` and `1.0` representing the proportion of the dataset to include in the test split.\n            It will be applied to all the datasets in the `Distiset`.\n        shuffle: Whether or not to shuffle the data before splitting\n        seed:\n            A seed to initialize the default BitGenerator, passed to the underlying method.\n\n    Returns:\n        The `Distiset` with the train-test split applied to all the datasets.\n    \"\"\"\n    assert 0 &lt; train_size &lt; 1, \"train_size must be a float between 0 and 1\"\n    for name, dataset in self.items():\n        self[name] = dataset.train_test_split(\n            train_size=train_size,\n            shuffle=shuffle,\n            seed=seed,\n        )\n    return self\n</code></pre>"},{"location":"reference/distilabel/distiset/#distilabel.distiset.create_distiset","title":"<code>create_distiset(data_dir, pipeline_path=None, log_filename_path=None, enable_metadata=False)</code>","text":"<p>Creates a <code>Distiset</code> from the buffer folder.</p> <p>This function is intended to be used as a helper to create a <code>Distiset</code> from from the folder where the cached data was written by the <code>_WriteBuffer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>Folder where the data buffers were written by the <code>_WriteBuffer</code>. It should correspond to <code>CacheLocation.data</code>.</p> required <code>pipeline_path</code> <code>Optional[Path]</code> <p>Optional path to the pipeline.yaml file that generated the dataset. Internally this will be passed to the <code>Distiset</code> object on creation to allow uploading the <code>pipeline.yaml</code> file to the repo upon <code>Distiset.push_to_hub</code>.</p> <code>None</code> <code>log_filename_path</code> <code>Optional[Path]</code> <p>Optional path to the pipeline.log file that was generated during the pipeline run. Internally this will be passed to the <code>Distiset</code> object on creation to allow uploading the <code>pipeline.log</code> file to the repo upon <code>Distiset.push_to_hub</code>.</p> <code>None</code> <code>enable_metadata</code> <code>bool</code> <p>Whether to include the distilabel metadata column in the dataset or not. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distiset</code> <p>The dataset created from the buffer folder, where the different leaf steps will</p> <code>Distiset</code> <p>correspond to different configurations of the dataset.</p> <p>Examples:</p> <pre><code>```python\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; distiset = create_distiset(Path.home() / \".cache/distilabel/pipelines/path-to-pipe-hashname\")\n```\n</code></pre> Source code in <code>src/distilabel/distiset.py</code> <pre><code>def create_distiset(  # noqa: C901\n    data_dir: Path,\n    pipeline_path: Optional[Path] = None,\n    log_filename_path: Optional[Path] = None,\n    enable_metadata: bool = False,\n) -&gt; Distiset:\n    \"\"\"Creates a `Distiset` from the buffer folder.\n\n    This function is intended to be used as a helper to create a `Distiset` from from the folder\n    where the cached data was written by the `_WriteBuffer`.\n\n    Args:\n        data_dir: Folder where the data buffers were written by the `_WriteBuffer`.\n            It should correspond to `CacheLocation.data`.\n        pipeline_path: Optional path to the pipeline.yaml file that generated the dataset.\n            Internally this will be passed to the `Distiset` object on creation to allow\n            uploading the `pipeline.yaml` file to the repo upon `Distiset.push_to_hub`.\n        log_filename_path: Optional path to the pipeline.log file that was generated during the pipeline run.\n            Internally this will be passed to the `Distiset` object on creation to allow\n            uploading the `pipeline.log` file to the repo upon `Distiset.push_to_hub`.\n        enable_metadata: Whether to include the distilabel metadata column in the dataset or not.\n            Defaults to `False`.\n\n    Returns:\n        The dataset created from the buffer folder, where the different leaf steps will\n        correspond to different configurations of the dataset.\n\n    Examples:\n\n        ```python\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; distiset = create_distiset(Path.home() / \".cache/distilabel/pipelines/path-to-pipe-hashname\")\n        ```\n    \"\"\"\n    from distilabel.steps.constants import DISTILABEL_METADATA_KEY\n\n    logger = logging.getLogger(\"distilabel.distiset\")\n\n    data_dir = Path(data_dir)\n\n    distiset = Distiset()\n    for file in data_dir.iterdir():\n        if file.is_file():\n            continue\n\n        files = [str(file) for file in list_files_in_dir(file)]\n        if files:\n            try:\n                ds = load_dataset(\n                    \"parquet\", name=file.stem, data_files={\"train\": files}\n                )\n                if not enable_metadata and DISTILABEL_METADATA_KEY in ds.column_names:\n                    ds = ds.remove_columns(DISTILABEL_METADATA_KEY)\n                distiset[file.stem] = ds\n            except ArrowInvalid:\n                logger.warning(f\"\u274c Failed to load the subset from '{file}' directory.\")\n                continue\n        else:\n            logger.warning(\n                f\"No output files for step '{file.stem}', can't create a dataset.\"\n                \" Did the step produce any data?\"\n            )\n\n    # If there's only one dataset i.e. one config, then set the config name to `default`\n    if len(distiset.keys()) == 1:\n        distiset[\"default\"] = distiset.pop(list(distiset.keys())[0])\n\n    if pipeline_path:\n        distiset.pipeline_path = pipeline_path\n    else:\n        # If the pipeline path is not provided, try to find it in the parent directory\n        # and assume that's the wanted file.\n        pipeline_path = data_dir.parent / \"pipeline.yaml\"\n        if pipeline_path.exists():\n            distiset.pipeline_path = pipeline_path\n\n    if log_filename_path:\n        distiset.log_filename_path = log_filename_path\n    else:\n        log_filename_path = data_dir.parent / \"pipeline.log\"\n        if log_filename_path.exists():\n            distiset.log_filename_path = log_filename_path\n\n    return distiset\n</code></pre>"},{"location":"reference/distilabel/cli/","title":"Index","text":""},{"location":"reference/distilabel/cli/app/","title":"App","text":""},{"location":"reference/distilabel/cli/pipeline/","title":"Index","text":""},{"location":"reference/distilabel/cli/pipeline/app/","title":"App","text":""},{"location":"reference/distilabel/cli/pipeline/utils/","title":"Utils","text":""},{"location":"reference/distilabel/cli/pipeline/utils/#distilabel.cli.pipeline.utils.display_pipeline_information","title":"<code>display_pipeline_information(pipeline)</code>","text":"<p>Displays the pipeline information to the console.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BasePipeline</code> <p>The pipeline.</p> required Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def display_pipeline_information(pipeline: \"BasePipeline\") -&gt; None:\n    \"\"\"Displays the pipeline information to the console.\n\n    Args:\n        pipeline: The pipeline.\n    \"\"\"\n    from rich.console import Console\n\n    Console().print(_build_pipeline_panel(pipeline))\n</code></pre>"},{"location":"reference/distilabel/cli/pipeline/utils/#distilabel.cli.pipeline.utils.get_config_from_url","title":"<code>get_config_from_url(url)</code>","text":"<p>Loads the pipeline configuration from a URL pointing to a JSON or YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL pointing to the pipeline configuration file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The pipeline configuration as a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported.</p> Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def get_config_from_url(url: str) -&gt; Dict[str, Any]:\n    \"\"\"Loads the pipeline configuration from a URL pointing to a JSON or YAML file.\n\n    Args:\n        url: The URL pointing to the pipeline configuration file.\n\n    Returns:\n        The pipeline configuration as a dictionary.\n\n    Raises:\n        ValueError: If the file format is not supported.\n    \"\"\"\n    if not url.endswith((\".json\", \".yaml\", \".yml\")):\n        raise ValueError(\n            f\"Unsupported file format for '{url}'. Only JSON and YAML are supported\"\n        )\n    if \"huggingface.co\" in url and \"HF_TOKEN\" in os.environ:\n        headers = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n    else:\n        headers = None\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n\n    if url.endswith((\".yaml\", \".yml\")):\n        content = response.content.decode(\"utf-8\")\n        return yaml.safe_load(content)\n\n    return response.json()\n</code></pre>"},{"location":"reference/distilabel/cli/pipeline/utils/#distilabel.cli.pipeline.utils.get_pipeline","title":"<code>get_pipeline(config)</code>","text":"<p>Get a pipeline from a configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>The path or URL to the pipeline configuration file.</p> required <p>Returns:</p> Type Description <code>BasePipeline</code> <p>The pipeline.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported.</p> <code>FileNotFoundError</code> <p>If the configuration file does not exist.</p> Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def get_pipeline(config: str) -&gt; \"BasePipeline\":\n    \"\"\"Get a pipeline from a configuration file.\n\n    Args:\n        config: The path or URL to the pipeline configuration file.\n\n    Returns:\n        The pipeline.\n\n    Raises:\n        ValueError: If the file format is not supported.\n        FileNotFoundError: If the configuration file does not exist.\n    \"\"\"\n    if valid_http_url(config):\n        return Pipeline.from_dict(get_config_from_url(config))\n\n    if Path(config).is_file():\n        return Pipeline.from_file(config)\n\n    raise FileNotFoundError(f\"Config file '{config}' does not exist.\")\n</code></pre>"},{"location":"reference/distilabel/cli/pipeline/utils/#distilabel.cli.pipeline.utils.parse_runtime_parameters","title":"<code>parse_runtime_parameters(params)</code>","text":"<p>Parses the runtime parameters from the CLI format to the format expected by the <code>Pipeline.run</code> method. The CLI format is a list of tuples, where the first element is a list of keys and the second element is the value.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>List[Tuple[List[str], str]]</code> <p>A list of tuples, where the first element is a list of keys and the second element is the value.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary with the runtime parameters in the format expected by the</p> <code>Dict[str, Dict[str, Any]]</code> <p><code>Pipeline.run</code> method.</p> Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def parse_runtime_parameters(\n    params: List[Tuple[List[str], str]],\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Parses the runtime parameters from the CLI format to the format expected by the\n    `Pipeline.run` method. The CLI format is a list of tuples, where the first element is\n    a list of keys and the second element is the value.\n\n    Args:\n        params: A list of tuples, where the first element is a list of keys and the\n            second element is the value.\n\n    Returns:\n        A dictionary with the runtime parameters in the format expected by the\n        `Pipeline.run` method.\n    \"\"\"\n    runtime_params = {}\n    for keys, value in params:\n        current = runtime_params\n        for i, key in enumerate(keys):\n            if i == len(keys) - 1:\n                current[key] = value\n            else:\n                current = current.setdefault(key, {})\n    return runtime_params\n</code></pre>"},{"location":"reference/distilabel/cli/pipeline/utils/#distilabel.cli.pipeline.utils.valid_http_url","title":"<code>valid_http_url(url)</code>","text":"<p>Check if the URL is a valid HTTP URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code>, if the URL is a valid HTTP URL. <code>False</code>, otherwise.</p> Source code in <code>src/distilabel/cli/pipeline/utils.py</code> <pre><code>def valid_http_url(url: str) -&gt; bool:\n    \"\"\"Check if the URL is a valid HTTP URL.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        `True`, if the URL is a valid HTTP URL. `False`, otherwise.\n    \"\"\"\n    try:\n        TypeAdapter(HttpUrl).validate_python(url)  # type: ignore\n    except ValidationError:\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/distilabel/llms/","title":"Index","text":""},{"location":"reference/distilabel/llms/#distilabel.llms.AnthropicLLM","title":"<code>AnthropicLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Anthropic LLM implementation running the Async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the name of the model to use for the LLM e.g. \"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", etc. Available models can be checked here: Anthropic: Models overview.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Anthropic API. If not provided, it will be read from <code>ANTHROPIC_API_KEY</code> environment variable.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Anthropic API. Defaults to <code>None</code> which means that <code>https://api.anthropic.com</code> will be used internally.</p> <code>timeout</code> <code>RuntimeParameter[float]</code> <p>the maximum time in seconds to wait for a response. Defaults to <code>600.0</code>.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>The maximum number of times to retry the request before failing. Defaults to <code>6</code>.</p> <code>http_client</code> <code>Optional[AsyncClient]</code> <p>if provided, an alternative HTTP client to use for calling Anthropic API. Defaults to <code>None</code>.</p> <code>structured_output</code> <code>Optional[AsyncClient]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. Defaults to None.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> <code>_aclient</code> <code>Optional[AsyncAnthropic]</code> <p>the <code>AsyncAnthropic</code> client to use for the Anthropic API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>api_key</code>: the API key to authenticate the requests to the Anthropic API. If not     provided, it will be read from <code>ANTHROPIC_API_KEY</code> environment variable.</li> <li><code>base_url</code>: the base URL to use for the Anthropic API. Defaults to <code>\"https://api.anthropic.com\"</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response. Defaults to <code>600.0</code>.</li> <li><code>max_retries</code>: the maximum number of times to retry the request before failing.     Defaults to <code>6</code>.</li> </ul> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>class AnthropicLLM(AsyncLLM):\n    \"\"\"Anthropic LLM implementation running the Async API client.\n\n    Attributes:\n        model: the name of the model to use for the LLM e.g. \"claude-3-opus-20240229\",\n            \"claude-3-sonnet-20240229\", etc. Available models can be checked here:\n            [Anthropic: Models overview](https://docs.anthropic.com/claude/docs/models-overview).\n        api_key: the API key to authenticate the requests to the Anthropic API. If not provided,\n            it will be read from `ANTHROPIC_API_KEY` environment variable.\n        base_url: the base URL to use for the Anthropic API. Defaults to `None` which means\n            that `https://api.anthropic.com` will be used internally.\n        timeout: the maximum time in seconds to wait for a response. Defaults to `600.0`.\n        max_retries: The maximum number of times to retry the request before failing. Defaults\n            to `6`.\n        http_client: if provided, an alternative HTTP client to use for calling Anthropic\n            API. Defaults to `None`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. Defaults to None.\n        _api_key_env_var: the name of the environment variable to use for the API key. It\n            is meant to be used internally.\n        _aclient: the `AsyncAnthropic` client to use for the Anthropic API. It is meant\n            to be used internally. Set in the `load` method.\n\n    Runtime parameters:\n        - `api_key`: the API key to authenticate the requests to the Anthropic API. If not\n            provided, it will be read from `ANTHROPIC_API_KEY` environment variable.\n        - `base_url`: the base URL to use for the Anthropic API. Defaults to `\"https://api.anthropic.com\"`.\n        - `timeout`: the maximum time in seconds to wait for a response. Defaults to `600.0`.\n        - `max_retries`: the maximum number of times to retry the request before failing.\n            Defaults to `6`.\n    \"\"\"\n\n    model: str\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"ANTHROPIC_BASE_URL\", \"https://api.anthropic.com\"\n        ),\n        description=\"The base URL to use for the Anthropic API.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_ANTHROPIC_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Anthropic API.\",\n    )\n    timeout: RuntimeParameter[float] = Field(\n        default=600.0,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    http_client: Optional[AsyncClient] = Field(default=None, exclude=True)\n\n    _api_key_env_var: str = PrivateAttr(default=_ANTHROPIC_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncAnthropic\"] = PrivateAttr(...)\n\n    def _check_model_exists(self) -&gt; None:\n        \"\"\"Checks if the specified model exists in the available models.\"\"\"\n        from anthropic import AsyncAnthropic\n\n        annotation = get_type_hints(AsyncAnthropic().messages.create).get(\"model\", None)\n        models = [\n            value\n            for type_ in get_args(annotation)\n            if get_origin(type_) is Literal\n            for value in get_args(type_)\n        ]\n\n        if self.model not in models:\n            raise ValueError(\n                f\"Model {self.model} does not exist among available models. \"\n                f\"The available models are {', '.join(models)}\"\n            )\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncAnthropic` client to use the Anthropic async API.\"\"\"\n        super().load()\n\n        try:\n            from anthropic import AsyncAnthropic\n        except ImportError as ie:\n            raise ImportError(\n                \"Anthropic Python client is not installed. Please install it using\"\n                \" `pip install anthropic`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._check_model_exists()\n\n        self._aclient = AsyncAnthropic(\n            api_key=self.api_key.get_secret_value(),\n            base_url=self.base_url,\n            timeout=self.timeout,\n            http_client=self.http_client,\n            max_retries=self.max_retries,\n        )\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"anthropic\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        max_tokens: int = 128,\n        stop_sequences: Union[List[str], None] = None,\n        temperature: float = 1.0,\n        top_p: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates a response asynchronously, using the [Anthropic Async API definition](https://github.com/anthropics/anthropic-sdk-python).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_tokens: the maximum number of new tokens that the model will generate. Defaults to `128`.\n            stop_sequences: custom text sequences that will cause the model to stop generating. Defaults to `NOT_GIVEN`.\n            temperature: the temperature to use for the generation. Set only if top_p is None. Defaults to `1.0`.\n            top_p: the top-p value to use for the generation. Defaults to `NOT_GIVEN`.\n            top_k: the top-k value to use for the generation. Defaults to `NOT_GIVEN`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        from anthropic._types import NOT_GIVEN\n\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"system\": (\n                input.pop(0)[\"content\"]\n                if input and input[0][\"role\"] == \"system\"\n                else NOT_GIVEN\n            ),\n            \"max_tokens\": max_tokens,\n            \"stream\": False,\n            \"stop_sequences\": NOT_GIVEN if stop_sequences is None else stop_sequences,\n            \"temperature\": temperature,\n            \"top_p\": NOT_GIVEN if top_p is None else top_p,\n            \"top_k\": NOT_GIVEN if top_k is None else top_k,\n        }\n\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n\n        completion = await self._aclient.messages.create(**kwargs)  # type: ignore\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        if (content := completion.content[0].text) is None:\n            self._logger.warning(\n                f\"Received no response using Anthropic client (model: '{self.model}').\"\n                f\" Finish reason was: {completion.stop_reason}\"\n            )\n        generations.append(content)\n        return generations\n\n    # TODO: remove this function once Anthropic client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AnthropicLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.AnthropicLLM.agenerate","title":"<code>agenerate(input, max_tokens=128, stop_sequences=None, temperature=1.0, top_p=None, top_k=None)</code>  <code>async</code>","text":"<p>Generates a response asynchronously, using the Anthropic Async API definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>stop_sequences</code> <code>Union[List[str], None]</code> <p>custom text sequences that will cause the model to stop generating. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Set only if top_p is None. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_p</code> <code>Union[float, None]</code> <p>the top-p value to use for the generation. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <code>top_k</code> <code>Union[int, None]</code> <p>the top-k value to use for the generation. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    max_tokens: int = 128,\n    stop_sequences: Union[List[str], None] = None,\n    temperature: float = 1.0,\n    top_p: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates a response asynchronously, using the [Anthropic Async API definition](https://github.com/anthropics/anthropic-sdk-python).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_tokens: the maximum number of new tokens that the model will generate. Defaults to `128`.\n        stop_sequences: custom text sequences that will cause the model to stop generating. Defaults to `NOT_GIVEN`.\n        temperature: the temperature to use for the generation. Set only if top_p is None. Defaults to `1.0`.\n        top_p: the top-p value to use for the generation. Defaults to `NOT_GIVEN`.\n        top_k: the top-k value to use for the generation. Defaults to `NOT_GIVEN`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    from anthropic._types import NOT_GIVEN\n\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"system\": (\n            input.pop(0)[\"content\"]\n            if input and input[0][\"role\"] == \"system\"\n            else NOT_GIVEN\n        ),\n        \"max_tokens\": max_tokens,\n        \"stream\": False,\n        \"stop_sequences\": NOT_GIVEN if stop_sequences is None else stop_sequences,\n        \"temperature\": temperature,\n        \"top_p\": NOT_GIVEN if top_p is None else top_p,\n        \"top_k\": NOT_GIVEN if top_k is None else top_k,\n    }\n\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n\n    completion = await self._aclient.messages.create(**kwargs)  # type: ignore\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    if (content := completion.content[0].text) is None:\n        self._logger.warning(\n            f\"Received no response using Anthropic client (model: '{self.model}').\"\n            f\" Finish reason was: {completion.stop_reason}\"\n        )\n    generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AnthropicLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AnthropicLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncAnthropic</code> client to use the Anthropic async API.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncAnthropic` client to use the Anthropic async API.\"\"\"\n    super().load()\n\n    try:\n        from anthropic import AsyncAnthropic\n    except ImportError as ie:\n        raise ImportError(\n            \"Anthropic Python client is not installed. Please install it using\"\n            \" `pip install anthropic`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._check_model_exists()\n\n    self._aclient = AsyncAnthropic(\n        api_key=self.api_key.get_secret_value(),\n        base_url=self.base_url,\n        timeout=self.timeout,\n        http_client=self.http_client,\n        max_retries=self.max_retries,\n    )\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"anthropic\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AnyscaleLLM","title":"<code>AnyscaleLLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>Anyscale LLM implementation running the async API client of OpenAI.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM, e.g., <code>google/gemma-7b-it</code>. See the supported models under the \"Text Generation -&gt; Supported Models\" section here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Anyscale API requests. Defaults to <code>None</code>, which means that the value set for the environment variable <code>ANYSCALE_BASE_URL</code> will be used, or \"https://api.endpoints.anyscale.com/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Anyscale API. Defaults to <code>None</code> which means that the value set for the environment variable <code>ANYSCALE_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> Source code in <code>src/distilabel/llms/anyscale.py</code> <pre><code>class AnyscaleLLM(OpenAILLM):\n    \"\"\"Anyscale LLM implementation running the async API client of OpenAI.\n\n    Attributes:\n        model: the model name to use for the LLM, e.g., `google/gemma-7b-it`. See the\n            supported models under the \"Text Generation -&gt; Supported Models\" section\n            [here](https://docs.endpoints.anyscale.com/).\n        base_url: the base URL to use for the Anyscale API requests. Defaults to `None`, which\n            means that the value set for the environment variable `ANYSCALE_BASE_URL` will be used, or\n            \"https://api.endpoints.anyscale.com/v1\" if not set.\n        api_key: the API key to authenticate the requests to the Anyscale API. Defaults to `None` which\n            means that the value set for the environment variable `ANYSCALE_API_KEY` will be used, or\n            `None` if not set.\n        _api_key_env_var: the name of the environment variable to use for the API key.\n            It is meant to be used internally.\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"ANYSCALE_BASE_URL\", \"https://api.endpoints.anyscale.com/v1\"\n        ),\n        description=\"The base URL to use for the Anyscale API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_ANYSCALE_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Anyscale API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_ANYSCALE_API_KEY_ENV_VAR_NAME)\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AsyncLLM","title":"<code>AsyncLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Abstract class for asynchronous LLMs, so as to benefit from the async capabilities of each LLM implementation. This class is meant to be subclassed by each LLM, and the method <code>agenerate</code> needs to be implemented to provide the asynchronous generation of responses.</p> <p>Attributes:</p> Name Type Description <code>_event_loop</code> <code>AbstractEventLoop</code> <p>the event loop to be used for the asynchronous generation of responses.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>class AsyncLLM(LLM):\n    \"\"\"Abstract class for asynchronous LLMs, so as to benefit from the async capabilities\n    of each LLM implementation. This class is meant to be subclassed by each LLM, and the\n    method `agenerate` needs to be implemented to provide the asynchronous generation of\n    responses.\n\n    Attributes:\n        _event_loop: the event loop to be used for the asynchronous generation of responses.\n    \"\"\"\n\n    _event_loop: \"asyncio.AbstractEventLoop\" = PrivateAttr(default=None)\n    _new_event_loop: bool = PrivateAttr(default=False)\n\n    @property\n    def generate_parameters(self) -&gt; List[inspect.Parameter]:\n        \"\"\"Returns the parameters of the `agenerate` method.\n\n        Returns:\n            A list containing the parameters of the `agenerate` method.\n        \"\"\"\n        return list(inspect.signature(self.agenerate).parameters.values())\n\n    @cached_property\n    def generate_parsed_docstring(self) -&gt; \"Docstring\":\n        \"\"\"Returns the parsed docstring of the `agenerate` method.\n\n        Returns:\n            The parsed docstring of the `agenerate` method.\n        \"\"\"\n        return parse_google_docstring(self.agenerate)\n\n    @property\n    def event_loop(self) -&gt; \"asyncio.AbstractEventLoop\":\n        if self._event_loop is None:\n            try:\n                self._event_loop = asyncio.get_running_loop()\n                if self._event_loop.is_closed():\n                    self._event_loop = asyncio.new_event_loop()  # type: ignore\n                    self._new_event_loop = True\n            except RuntimeError:\n                self._event_loop = asyncio.new_event_loop()\n                self._new_event_loop = True\n        asyncio.set_event_loop(self._event_loop)\n        return self._event_loop\n\n    @abstractmethod\n    async def agenerate(\n        self, input: \"FormattedInput\", num_generations: int = 1, **kwargs: Any\n    ) -&gt; List[Union[str, None]]:\n        \"\"\"Method to generate a `num_generations` responses for a given input asynchronously,\n        and executed concurrently in `generate` method.\n        \"\"\"\n        pass\n\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"FormattedInput\"], **kwargs: Any\n        ) -&gt; List[List[Union[str, None]]]:\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(\n                    self.agenerate(\n                        input=input, num_generations=num_generations, **kwargs\n                    )\n                )\n                for input in inputs\n            ]\n            return await asyncio.gather(*tasks)\n\n        return self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n\n    def __del__(self) -&gt; None:\n        \"\"\"Closes the event loop when the object is deleted.\"\"\"\n        if sys.meta_path is None:\n            return\n\n        if self._new_event_loop:\n            if self._event_loop.is_running():\n                self._event_loop.stop()\n            self._event_loop.close()\n\n    @staticmethod\n    def _prepare_structured_output(\n        structured_output: \"InstructorStructuredOutputType\",\n        client: Any = None,\n        framework: Optional[str] = None,\n    ) -&gt; Dict[str, Union[str, Any]]:\n        \"\"\"Wraps the client and updates the schema to work store it internally as a json schema.\n\n        Args:\n            structured_output: The configuration dict to prepare the structured output.\n            client: The client to wrap to generate structured output. Implemented to work\n                with `instructor`.\n            framework: The name of the framework.\n\n        Returns:\n            A dictionary containing the wrapped client and the schema to update the structured_output\n            variable in case it is a pydantic model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.instructor import (\n            prepare_instructor,\n        )\n\n        result = {}\n        client = prepare_instructor(\n            client,\n            mode=structured_output.get(\"mode\"),\n            framework=framework,\n        )\n        result[\"client\"] = client\n\n        schema = structured_output.get(\"schema\")\n        if not schema:\n            raise ValueError(\n                f\"The `structured_output` argument must contain a schema: {structured_output}\"\n            )\n        if issubclass(schema, BaseModel):\n            # We want a json schema for the serialization, but instructor wants a pydantic BaseModel.\n            structured_output[\"schema\"] = schema.model_json_schema()\n            result[\"structured_output\"] = structured_output\n\n        return result\n\n    @staticmethod\n    def _prepare_kwargs(\n        arguments: Dict[str, Any], structured_output: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Helper method to update the kwargs with the structured output configuration,\n        used in case they are defined.\n\n        Args:\n            arguments: The arguments that would be passed to the LLM as **kwargs.\n                to update with the structured output configuration.\n            structured_outputs: The structured output configuration to update the arguments.\n\n        Returns:\n            kwargs updated with the special arguments used by `instructor`.\n        \"\"\"\n        # We can deal with json schema or BaseModel, but we need to convert it to a BaseModel\n        # for the Instructor client.\n        schema = structured_output.get(\"schema\")\n        if not issubclass(schema, BaseModel):\n            from distilabel.steps.tasks.structured_outputs.utils import (\n                json_schema_to_model,\n            )\n\n            try:\n                schema = json_schema_to_model(schema)\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to convert the schema to a pydantic model, the model is too complex currently: {e}\"\n                ) from e\n\n        arguments.update(\n            **{\n                \"response_model\": schema,\n                \"max_retries\": structured_output.get(\"max_retries\", 1),\n            },\n        )\n        return arguments\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AsyncLLM.generate_parameters","title":"<code>generate_parameters: List[inspect.Parameter]</code>  <code>property</code>","text":"<p>Returns the parameters of the <code>agenerate</code> method.</p> <p>Returns:</p> Type Description <code>List[Parameter]</code> <p>A list containing the parameters of the <code>agenerate</code> method.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.AsyncLLM.generate_parsed_docstring","title":"<code>generate_parsed_docstring: Docstring</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the parsed docstring of the <code>agenerate</code> method.</p> <p>Returns:</p> Type Description <code>Docstring</code> <p>The parsed docstring of the <code>agenerate</code> method.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.AsyncLLM.__del__","title":"<code>__del__()</code>","text":"<p>Closes the event loop when the object is deleted.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Closes the event loop when the object is deleted.\"\"\"\n    if sys.meta_path is None:\n        return\n\n    if self._new_event_loop:\n        if self._event_loop.is_running():\n            self._event_loop.stop()\n        self._event_loop.close()\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AsyncLLM.agenerate","title":"<code>agenerate(input, num_generations=1, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Method to generate a <code>num_generations</code> responses for a given input asynchronously, and executed concurrently in <code>generate</code> method.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>@abstractmethod\nasync def agenerate(\n    self, input: \"FormattedInput\", num_generations: int = 1, **kwargs: Any\n) -&gt; List[Union[str, None]]:\n    \"\"\"Method to generate a `num_generations` responses for a given input asynchronously,\n    and executed concurrently in `generate` method.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AsyncLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"FormattedInput\"], **kwargs: Any\n    ) -&gt; List[List[Union[str, None]]]:\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(\n                self.agenerate(\n                    input=input, num_generations=num_generations, **kwargs\n                )\n            )\n            for input in inputs\n        ]\n        return await asyncio.gather(*tasks)\n\n    return self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AzureOpenAILLM","title":"<code>AzureOpenAILLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>Azure OpenAI LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM i.e. the name of the Azure deployment.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Azure OpenAI API can be set with <code>AZURE_OPENAI_ENDPOINT</code>. Defaults to <code>None</code> which means that the value set for the environment variable <code>AZURE_OPENAI_ENDPOINT</code> will be used, or <code>None</code> if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Azure OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>AZURE_OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>api_version</code> <code>Optional[RuntimeParameter[str]]</code> <p>the API version to use for the Azure OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_VERSION</code> will be used, or <code>None</code> if not set.</p> Icon <p><code>:simple-microsoftazure:</code></p> Source code in <code>src/distilabel/llms/azure.py</code> <pre><code>class AzureOpenAILLM(OpenAILLM):\n    \"\"\"Azure OpenAI LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM i.e. the name of the Azure deployment.\n        base_url: the base URL to use for the Azure OpenAI API can be set with `AZURE_OPENAI_ENDPOINT`.\n            Defaults to `None` which means that the value set for the environment variable\n            `AZURE_OPENAI_ENDPOINT` will be used, or `None` if not set.\n        api_key: the API key to authenticate the requests to the Azure OpenAI API. Defaults to `None`\n            which means that the value set for the environment variable `AZURE_OPENAI_API_KEY` will be\n            used, or `None` if not set.\n        api_version: the API version to use for the Azure OpenAI API. Defaults to `None` which means\n            that the value set for the environment variable `OPENAI_API_VERSION` will be used, or\n            `None` if not set.\n\n    Icon:\n        `:simple-microsoftazure:`\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(_AZURE_OPENAI_ENDPOINT_ENV_VAR_NAME),\n        description=\"The base URL to use for the Azure OpenAI API requests i.e. the Azure OpenAI endpoint.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_AZURE_OPENAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Azure OpenAI API.\",\n    )\n\n    api_version: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\"OPENAI_API_VERSION\"),\n        description=\"The API version to use for the Azure OpenAI API.\",\n    )\n\n    _base_url_env_var: str = PrivateAttr(_AZURE_OPENAI_ENDPOINT_ENV_VAR_NAME)\n    _api_key_env_var: str = PrivateAttr(_AZURE_OPENAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncAzureOpenAI\"] = PrivateAttr(...)  # type: ignore\n\n    @override\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncAzureOpenAI` client to benefit from async requests.\"\"\"\n        # This is a workaround to avoid the `OpenAILLM` calling the _prepare_structured_output\n        # in the load method before we have the proper client.\n        with patch(\"OpenAILLM._prepare_structured_output\", lambda x: x):\n            super().load()\n\n        try:\n            from openai import AsyncAzureOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        # TODO: May be worth adding the AD auth too? Also the `organization`?\n        self._aclient = AsyncAzureOpenAI(  # type: ignore\n            azure_endpoint=self.base_url,  # type: ignore\n            azure_deployment=self.model,\n            api_version=self.api_version,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            self._prepare_structured_output(self.structured_output)\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.AzureOpenAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncAzureOpenAI</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/azure.py</code> <pre><code>@override\ndef load(self) -&gt; None:\n    \"\"\"Loads the `AsyncAzureOpenAI` client to benefit from async requests.\"\"\"\n    # This is a workaround to avoid the `OpenAILLM` calling the _prepare_structured_output\n    # in the load method before we have the proper client.\n    with patch(\"OpenAILLM._prepare_structured_output\", lambda x: x):\n        super().load()\n\n    try:\n        from openai import AsyncAzureOpenAI\n    except ImportError as ie:\n        raise ImportError(\n            \"OpenAI Python client is not installed. Please install it using\"\n            \" `pip install openai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    # TODO: May be worth adding the AD auth too? Also the `organization`?\n    self._aclient = AsyncAzureOpenAI(  # type: ignore\n        azure_endpoint=self.base_url,  # type: ignore\n        azure_deployment=self.model,\n        api_version=self.api_version,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        self._prepare_structured_output(self.structured_output)\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.CohereLLM","title":"<code>CohereLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Cohere API implementation using the async client for concurrent text generation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the name of the model from the Cohere API to use for the generation.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Cohere API requests. Defaults to <code>\"https://api.cohere.ai/v1\"</code>.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Cohere API. Defaults to the value of the <code>COHERE_API_KEY</code> environment variable.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response from the API. Defaults to <code>120</code>.</p> <code>client_name</code> <code>RuntimeParameter[str]</code> <p>the name of the client to use for the API requests. Defaults to <code>\"distilabel\"</code>.</p> <code>structured_output</code> <code>RuntimeParameter[str]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> <code>_ChatMessage</code> <code>Type[ChatMessage]</code> <p>the <code>ChatMessage</code> class from the <code>cohere</code> package.</p> <code>_aclient</code> <code>AsyncClient</code> <p>the <code>AsyncClient</code> client from the <code>cohere</code> package.</p> Runtime parameters <ul> <li><code>base_url</code>: the base URL to use for the Cohere API requests. Defaults to     <code>\"https://api.cohere.ai/v1\"</code>.</li> <li><code>api_key</code>: the API key to authenticate the requests to the Cohere API. Defaults     to the value of the <code>COHERE_API_KEY</code> environment variable.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response from the API. Defaults     to <code>120</code>.</li> <li><code>client_name</code>: the name of the client to use for the API requests. Defaults to     <code>\"distilabel\"</code>.</li> </ul> Source code in <code>src/distilabel/llms/cohere.py</code> <pre><code>class CohereLLM(AsyncLLM):\n    \"\"\"Cohere API implementation using the async client for concurrent text generation.\n\n    Attributes:\n        model: the name of the model from the Cohere API to use for the generation.\n        base_url: the base URL to use for the Cohere API requests. Defaults to\n            `\"https://api.cohere.ai/v1\"`.\n        api_key: the API key to authenticate the requests to the Cohere API. Defaults to\n            the value of the `COHERE_API_KEY` environment variable.\n        timeout: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        client_name: the name of the client to use for the API requests. Defaults to\n            `\"distilabel\"`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n        _ChatMessage: the `ChatMessage` class from the `cohere` package.\n        _aclient: the `AsyncClient` client from the `cohere` package.\n\n    Runtime parameters:\n        - `base_url`: the base URL to use for the Cohere API requests. Defaults to\n            `\"https://api.cohere.ai/v1\"`.\n        - `api_key`: the API key to authenticate the requests to the Cohere API. Defaults\n            to the value of the `COHERE_API_KEY` environment variable.\n        - `timeout`: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        - `client_name`: the name of the client to use for the API requests. Defaults to\n            `\"distilabel\"`.\n    \"\"\"\n\n    model: str\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"COHERE_BASE_URL\", \"https://api.cohere.ai/v1\"\n        ),\n        description=\"The base URL to use for the Cohere API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_COHERE_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Cohere API.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n    client_name: RuntimeParameter[str] = Field(\n        default=\"distilabel\",\n        description=\"The name of the client to use for the API requests.\",\n    )\n\n    _ChatMessage: Type[\"ChatMessage\"] = PrivateAttr(...)\n    _aclient: \"AsyncClient\" = PrivateAttr(...)\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncClient` client from the `cohere` package.\"\"\"\n\n        super().load()\n\n        try:\n            from cohere import AsyncClient, ChatMessage\n        except ImportError as ie:\n            raise ImportError(\n                \"The `cohere` package is required to use the `CohereLLM` class.\"\n            ) from ie\n\n        self._ChatMessage = ChatMessage\n\n        self._aclient = AsyncClient(\n            api_key=self.api_key.get_secret_value(),  # type: ignore\n            client_name=self.client_name,\n            base_url=self.base_url,\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"cohere\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    def _format_chat_to_cohere(\n        self, input: \"StandardInput\"\n    ) -&gt; Tuple[Union[str, None], List[\"ChatMessage\"], str]:\n        \"\"\"Formats the chat input to the Cohere Chat API conversational format.\n\n        Args:\n            input: The chat input to format.\n\n        Returns:\n            A tuple containing the system, chat history, and message.\n        \"\"\"\n        system = None\n        message = None\n        chat_history = []\n        for item in input:\n            role = item[\"role\"]\n            content = item[\"content\"]\n            if role == \"system\":\n                system = content\n            elif role == \"user\":\n                message = content\n            elif role == \"assistant\":\n                if message is None:\n                    raise ValueError(\n                        \"An assistant message but be preceded by a user message.\"\n                    )\n                chat_history.append(self._ChatMessage(role=\"USER\", message=message))  # type: ignore\n                chat_history.append(self._ChatMessage(role=\"CHATBOT\", message=content))\n                message = None\n\n        if message is None:\n            raise ValueError(\"The chat input must end with a user message.\")\n\n        return system, chat_history, message\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        k: Optional[int] = None,\n        p: Optional[float] = None,\n        seed: Optional[float] = None,\n        stop_sequences: Optional[Sequence[str]] = None,\n        frequency_penalty: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        raw_prompting: Optional[bool] = None,\n    ) -&gt; Union[str, None]:\n        \"\"\"Generates a response from the LLM given an input.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            temperature: the temperature to use for the generation. Defaults to `None`.\n            max_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `None`.\n            k: the number of highest probability vocabulary tokens to keep for the generation.\n                Defaults to `None`.\n            p: the nucleus sampling probability to use for the generation. Defaults to\n                `None`.\n            seed: the seed to use for the generation. Defaults to `None`.\n            stop_sequences: a list of sequences to use as stopping criteria for the generation.\n                Defaults to `None`.\n            frequency_penalty: the frequency penalty to use for the generation. Defaults\n                to `None`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `None`.\n            raw_prompting: a flag to use raw prompting for the generation. Defaults to\n                `None`.\n\n        Returns:\n            The generated response from the Cohere API model.\n        \"\"\"\n        system, chat_history, message = self._format_chat_to_cohere(input)\n\n        kwargs = {\n            \"message\": message,\n            \"model\": self.model,\n            \"preamble\": system,\n            \"chat_history\": chat_history,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"k\": k,\n            \"p\": p,\n            \"seed\": seed,\n            \"stop_sequences\": stop_sequences,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"raw_prompting\": raw_prompting,\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        response = await self._aclient.chat(**kwargs)  # type: ignore\n\n        if self.structured_output:\n            return response.model_dump_json()\n\n        if (text := response.text) == \"\":\n            self._logger.warning(\n                f\"Received no response using Cohere client (model: '{self.model}').\"\n                f\" Finish reason was: {response.finish_reason}\"\n            )\n            return None\n\n        return text\n\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return await asyncio.gather(*tasks)\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.CohereLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.CohereLLM.agenerate","title":"<code>agenerate(input, temperature=None, max_tokens=None, k=None, p=None, seed=None, stop_sequences=None, frequency_penalty=None, presence_penalty=None, raw_prompting=None)</code>  <code>async</code>","text":"<p>Generates a response from the LLM given an input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>None</code>.</p> <code>None</code> <code>k</code> <code>Optional[int]</code> <p>the number of highest probability vocabulary tokens to keep for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>p</code> <code>Optional[float]</code> <p>the nucleus sampling probability to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>seed</code> <code>Optional[float]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[Sequence[str]]</code> <p>a list of sequences to use as stopping criteria for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>the frequency penalty to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>the presence penalty to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>raw_prompting</code> <code>Optional[bool]</code> <p>a flag to use raw prompting for the generation. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>The generated response from the Cohere API model.</p> Source code in <code>src/distilabel/llms/cohere.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    k: Optional[int] = None,\n    p: Optional[float] = None,\n    seed: Optional[float] = None,\n    stop_sequences: Optional[Sequence[str]] = None,\n    frequency_penalty: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    raw_prompting: Optional[bool] = None,\n) -&gt; Union[str, None]:\n    \"\"\"Generates a response from the LLM given an input.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        temperature: the temperature to use for the generation. Defaults to `None`.\n        max_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `None`.\n        k: the number of highest probability vocabulary tokens to keep for the generation.\n            Defaults to `None`.\n        p: the nucleus sampling probability to use for the generation. Defaults to\n            `None`.\n        seed: the seed to use for the generation. Defaults to `None`.\n        stop_sequences: a list of sequences to use as stopping criteria for the generation.\n            Defaults to `None`.\n        frequency_penalty: the frequency penalty to use for the generation. Defaults\n            to `None`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `None`.\n        raw_prompting: a flag to use raw prompting for the generation. Defaults to\n            `None`.\n\n    Returns:\n        The generated response from the Cohere API model.\n    \"\"\"\n    system, chat_history, message = self._format_chat_to_cohere(input)\n\n    kwargs = {\n        \"message\": message,\n        \"model\": self.model,\n        \"preamble\": system,\n        \"chat_history\": chat_history,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens,\n        \"k\": k,\n        \"p\": p,\n        \"seed\": seed,\n        \"stop_sequences\": stop_sequences,\n        \"frequency_penalty\": frequency_penalty,\n        \"presence_penalty\": presence_penalty,\n        \"raw_prompting\": raw_prompting,\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    response = await self._aclient.chat(**kwargs)  # type: ignore\n\n    if self.structured_output:\n        return response.model_dump_json()\n\n    if (text := response.text) == \"\":\n        self._logger.warning(\n            f\"Received no response using Cohere client (model: '{self.model}').\"\n            f\" Finish reason was: {response.finish_reason}\"\n        )\n        return None\n\n    return text\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.CohereLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/cohere.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return await asyncio.gather(*tasks)\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.CohereLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncClient</code> client from the <code>cohere</code> package.</p> Source code in <code>src/distilabel/llms/cohere.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncClient` client from the `cohere` package.\"\"\"\n\n    super().load()\n\n    try:\n        from cohere import AsyncClient, ChatMessage\n    except ImportError as ie:\n        raise ImportError(\n            \"The `cohere` package is required to use the `CohereLLM` class.\"\n        ) from ie\n\n    self._ChatMessage = ChatMessage\n\n    self._aclient = AsyncClient(\n        api_key=self.api_key.get_secret_value(),  # type: ignore\n        client_name=self.client_name,\n        base_url=self.base_url,\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"cohere\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.CudaDevicePlacementMixin","title":"<code>CudaDevicePlacementMixin</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Mixin class to assign CUDA devices to the <code>LLM</code> based on the <code>cuda_devices</code> attribute and the device placement information provided in <code>_device_llm_placement_map</code>. Providing the device placement information is optional, but if it is provided, it will be used to assign CUDA devices to the <code>LLM</code>s, trying to avoid using the same device for different <code>LLM</code>s.</p> <p>Attributes:</p> Name Type Description <code>cuda_devices</code> <code>Union[List[int], Literal['auto']]</code> <p>a list with the ID of the CUDA devices to be used by the <code>LLM</code>. If set to \"auto\", the devices will be automatically assigned based on the device placement information provided in <code>_device_llm_placement_map</code>. If set to a list of devices, it will be checked if the devices are available to be used by the <code>LLM</code>. If not, a warning will be logged.</p> <code>_llm_identifier</code> <code>Union[str, None]</code> <p>the identifier of the <code>LLM</code> to be used as key in <code>_device_llm_placement_map</code>.</p> <code>_device_llm_placement_map</code> <code>Union[DictProxy[str, Any], None]</code> <p>a dictionary with the device placement information for each <code>LLM</code>.</p> Source code in <code>src/distilabel/llms/mixins.py</code> <pre><code>class CudaDevicePlacementMixin(BaseModel):\n    \"\"\"Mixin class to assign CUDA devices to the `LLM` based on the `cuda_devices` attribute\n    and the device placement information provided in `_device_llm_placement_map`. Providing\n    the device placement information is optional, but if it is provided, it will be used to\n    assign CUDA devices to the `LLM`s, trying to avoid using the same device for different\n    `LLM`s.\n\n    Attributes:\n        cuda_devices: a list with the ID of the CUDA devices to be used by the `LLM`. If set\n            to \"auto\", the devices will be automatically assigned based on the device\n            placement information provided in `_device_llm_placement_map`. If set to a list\n            of devices, it will be checked if the devices are available to be used by the\n            `LLM`. If not, a warning will be logged.\n        _llm_identifier: the identifier of the `LLM` to be used as key in `_device_llm_placement_map`.\n        _device_llm_placement_map: a dictionary with the device placement information for each\n            `LLM`.\n    \"\"\"\n\n    # TODO: this should be a runtime parameter\n    cuda_devices: Union[List[int], Literal[\"auto\"]] = Field(default=\"auto\")\n\n    _llm_identifier: Union[str, None] = PrivateAttr(default=None)\n    _device_llm_placement_map: Union[\"DictProxy[str, Any]\", None] = PrivateAttr(\n        default=None\n    )\n    _device_llm_placement_lock: Union[\"Lock\", None] = PrivateAttr(default=None)\n    _available_cuda_devices: Union[List[int], None] = PrivateAttr(default=None)\n    _can_check_cuda_devices: bool = PrivateAttr(default=False)\n\n    def load(self) -&gt; None:\n        \"\"\"Assign CUDA devices to the LLM based on the device placement information provided\n        in `_device_llm_placement_map`.\"\"\"\n\n        try:\n            import pynvml\n\n            pynvml.nvmlInit()\n            device_count = pynvml.nvmlDeviceGetCount()\n            self._available_cuda_devices = list(range(device_count))\n            self._can_check_cuda_devices = True\n        except ImportError as ie:\n            if self.cuda_devices == \"auto\":\n                raise ImportError(\n                    \"The 'pynvml' library is not installed. It is required to automatically\"\n                    \" assign CUDA devices to the `LLM`s. Please, install it and try again.\"\n                ) from ie\n\n            if self.cuda_devices:\n                self._logger.warning(  # type: ignore\n                    \"The 'pynvml' library is not installed. It is recommended to install it\"\n                    \" to check if the CUDA devices assigned to the LLM are available.\"\n                )\n\n        self._assign_cuda_devices()\n\n    def set_device_placement_info(\n        self,\n        llm_identifier: str,\n        device_llm_placement_map: \"DictProxy[str, Any]\",\n        device_llm_placement_lock: \"Lock\",\n    ) -&gt; None:\n        \"\"\"Sets the value of `_device_llm_placement_map` to be used to assign CUDA devices\n        to the LLM.\n\n        Args:\n            llm_identifier: the identifier of the LLM to be used as key in the device\n                placement information.\n            device_llm_placement_map: a dictionary with the device placement information for\n                each LLM. It should have two keys. The first key is \"lock\" and its value is\n                a lock object to be used to synchronize the access to the device placement\n                information. The second key is \"value\" and its value is a dictionary with the\n                device placement information for each LLM.\n            device_llm_placement_lock: a lock object to be used to synchronize the access to\n                `_device_llm_placement_map`.\n        \"\"\"\n        self._llm_identifier = llm_identifier\n        self._device_llm_placement_map = device_llm_placement_map\n        self._device_llm_placement_lock = device_llm_placement_lock\n\n    def _assign_cuda_devices(self) -&gt; None:\n        \"\"\"Assigns CUDA devices to the LLM based on the device placement information provided\n        in `_device_llm_placement_map`. If the `cuda_devices` attribute is set to \"auto\", it\n        will be set to the first available CUDA device that is not going to be used by any\n        other LLM. If the `cuda_devices` attribute is set to a list of devices, it will be\n        checked if the devices are available to be used by the LLM. If not, a warning will be\n        logged.\"\"\"\n\n        if self._device_llm_placement_map is not None:\n            with self._device_llm_placement_lock:  # type: ignore\n                if self.cuda_devices == \"auto\":\n                    self.cuda_devices = [\n                        self._get_cuda_device(self._device_llm_placement_map)\n                    ]\n                else:\n                    self._check_cuda_devices(self._device_llm_placement_map)\n\n                self._device_llm_placement_map[self._llm_identifier] = self.cuda_devices  # type: ignore\n\n        # `_device_llm_placement_map` was not provided and user didn't set the `cuda_devices`\n        # attribute. In this case, the `cuda_devices` attribute will be set to an empty list.\n        if self.cuda_devices == \"auto\":\n            self.cuda_devices = []\n\n        self._set_cuda_visible_devices()\n\n    def _check_cuda_devices(self, device_map: Dict[str, List[int]]) -&gt; None:\n        \"\"\"Checks if the CUDA devices assigned to the LLM are also assigned to other LLMs.\n\n        Args:\n            device_map: a dictionary with the device placement information for each LLM.\n        \"\"\"\n        for device in self.cuda_devices:\n            for llm, devices in device_map.items():\n                if device in devices:\n                    self._logger.warning(\n                        f\"LLM with identifier '{llm}' is also going to use CUDA device \"\n                        f\"'{device}'. This may lead to performance issues or running out\"\n                        \" of memory depending on the device capabilities and the loaded\"\n                        \" models.\"\n                    )\n\n    def _get_cuda_device(self, device_map: Dict[str, List[int]]) -&gt; int:\n        \"\"\"Returns the first available CUDA device to be used by the LLM that is not going\n        to be used by any other LLM.\n\n        Args:\n            device_map: a dictionary with the device placement information for each LLM.\n\n        Returns:\n            The first available CUDA device to be used by the LLM.\n\n        Raises:\n            RuntimeError: if there is no available CUDA device to be used by the LLM.\n        \"\"\"\n        for device in self._available_cuda_devices:\n            if all(device not in devices for devices in device_map.values()):\n                return device\n\n        raise RuntimeError(\n            \"Couldn't find an available CUDA device automatically to be used by the LLM\"\n            f\" '{self._llm_identifier}'. For forcing the use of a specific device, set the\"\n            \" `cuda_devices` attribute to a list with the desired device(s).\"\n        )\n\n    def _set_cuda_visible_devices(self) -&gt; None:\n        \"\"\"Sets the `CUDA_VISIBLE_DEVICES` environment variable to the list of CUDA devices\n        to be used by the LLM.\n        \"\"\"\n        if not self.cuda_devices:\n            return\n\n        if self._can_check_cuda_devices and not all(\n            device in self._available_cuda_devices for device in self.cuda_devices\n        ):\n            raise RuntimeError(\n                f\"Invalid CUDA devices for LLM '{self._llm_identifier}': {self.cuda_devices}.\"\n                f\" The available devices are: {self._available_cuda_devices}. Please, review\"\n                \" the 'cuda_devices' attribute and try again.\"\n            )\n\n        cuda_devices = \",\".join([str(device) for device in self.cuda_devices])\n        self._logger.info(\n            f\"\ud83c\udfae LLM '{self._llm_identifier}' is going to use the following CUDA devices:\"\n            f\" {self.cuda_devices}.\"\n        )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_devices\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.CudaDevicePlacementMixin.load","title":"<code>load()</code>","text":"<p>Assign CUDA devices to the LLM based on the device placement information provided in <code>_device_llm_placement_map</code>.</p> Source code in <code>src/distilabel/llms/mixins.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Assign CUDA devices to the LLM based on the device placement information provided\n    in `_device_llm_placement_map`.\"\"\"\n\n    try:\n        import pynvml\n\n        pynvml.nvmlInit()\n        device_count = pynvml.nvmlDeviceGetCount()\n        self._available_cuda_devices = list(range(device_count))\n        self._can_check_cuda_devices = True\n    except ImportError as ie:\n        if self.cuda_devices == \"auto\":\n            raise ImportError(\n                \"The 'pynvml' library is not installed. It is required to automatically\"\n                \" assign CUDA devices to the `LLM`s. Please, install it and try again.\"\n            ) from ie\n\n        if self.cuda_devices:\n            self._logger.warning(  # type: ignore\n                \"The 'pynvml' library is not installed. It is recommended to install it\"\n                \" to check if the CUDA devices assigned to the LLM are available.\"\n            )\n\n    self._assign_cuda_devices()\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.CudaDevicePlacementMixin.set_device_placement_info","title":"<code>set_device_placement_info(llm_identifier, device_llm_placement_map, device_llm_placement_lock)</code>","text":"<p>Sets the value of <code>_device_llm_placement_map</code> to be used to assign CUDA devices to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>llm_identifier</code> <code>str</code> <p>the identifier of the LLM to be used as key in the device placement information.</p> required <code>device_llm_placement_map</code> <code>DictProxy[str, Any]</code> <p>a dictionary with the device placement information for each LLM. It should have two keys. The first key is \"lock\" and its value is a lock object to be used to synchronize the access to the device placement information. The second key is \"value\" and its value is a dictionary with the device placement information for each LLM.</p> required <code>device_llm_placement_lock</code> <code>Lock</code> <p>a lock object to be used to synchronize the access to <code>_device_llm_placement_map</code>.</p> required Source code in <code>src/distilabel/llms/mixins.py</code> <pre><code>def set_device_placement_info(\n    self,\n    llm_identifier: str,\n    device_llm_placement_map: \"DictProxy[str, Any]\",\n    device_llm_placement_lock: \"Lock\",\n) -&gt; None:\n    \"\"\"Sets the value of `_device_llm_placement_map` to be used to assign CUDA devices\n    to the LLM.\n\n    Args:\n        llm_identifier: the identifier of the LLM to be used as key in the device\n            placement information.\n        device_llm_placement_map: a dictionary with the device placement information for\n            each LLM. It should have two keys. The first key is \"lock\" and its value is\n            a lock object to be used to synchronize the access to the device placement\n            information. The second key is \"value\" and its value is a dictionary with the\n            device placement information for each LLM.\n        device_llm_placement_lock: a lock object to be used to synchronize the access to\n            `_device_llm_placement_map`.\n    \"\"\"\n    self._llm_identifier = llm_identifier\n    self._device_llm_placement_map = device_llm_placement_map\n    self._device_llm_placement_lock = device_llm_placement_lock\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.GroqLLM","title":"<code>GroqLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Groq API implementation using the async client for concurrent text generation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the name of the model from the Groq API to use for the generation.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Groq API requests. Defaults to <code>\"https://api.groq.com\"</code>.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Groq API. Defaults to the value of the <code>GROQ_API_KEY</code> environment variable.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of times to retry the request to the API before failing. Defaults to <code>2</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response from the API. Defaults to <code>120</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key.</p> <code>_aclient</code> <code>Optional[AsyncGroq]</code> <p>the <code>AsyncGroq</code> client from the <code>groq</code> package.</p> Runtime parameters <ul> <li><code>base_url</code>: the base URL to use for the Groq API requests. Defaults to     <code>\"https://api.groq.com\"</code>.</li> <li><code>api_key</code>: the API key to authenticate the requests to the Groq API. Defaults to     the value of the <code>GROQ_API_KEY</code> environment variable.</li> <li><code>max_retries</code>: the maximum number of times to retry the request to the API before     failing. Defaults to <code>2</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response from the API. Defaults     to <code>120</code>.</li> </ul> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>class GroqLLM(AsyncLLM):\n    \"\"\"Groq API implementation using the async client for concurrent text generation.\n\n    Attributes:\n        model: the name of the model from the Groq API to use for the generation.\n        base_url: the base URL to use for the Groq API requests. Defaults to\n            `\"https://api.groq.com\"`.\n        api_key: the API key to authenticate the requests to the Groq API. Defaults to\n            the value of the `GROQ_API_KEY` environment variable.\n        max_retries: the maximum number of times to retry the request to the API before\n            failing. Defaults to `2`.\n        timeout: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n        _api_key_env_var: the name of the environment variable to use for the API key.\n        _aclient: the `AsyncGroq` client from the `groq` package.\n\n    Runtime parameters:\n        - `base_url`: the base URL to use for the Groq API requests. Defaults to\n            `\"https://api.groq.com\"`.\n        - `api_key`: the API key to authenticate the requests to the Groq API. Defaults to\n            the value of the `GROQ_API_KEY` environment variable.\n        - `max_retries`: the maximum number of times to retry the request to the API before\n            failing. Defaults to `2`.\n        - `timeout`: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n    \"\"\"\n\n    model: str\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            _GROQ_API_BASE_URL_ENV_VAR_NAME, \"https://api.groq.com\"\n        ),\n        description=\"The base URL to use for the Groq API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_GROQ_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Groq API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=2,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_GROQ_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncGroq\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncGroq` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from groq import AsyncGroq\n        except ImportError as ie:\n            raise ImportError(\n                \"Groq Python client is not installed. Please install it using\"\n                ' `pip install groq` or from the extras as `pip install \"distilabel[groq]\"`.'\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = AsyncGroq(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"groq\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        seed: Optional[int] = None,\n        max_new_tokens: int = 128,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        stop: Optional[str] = None,\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Generates `num_generations` responses for the given input using the Groq async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            seed: the seed to use for the generation. Defaults to `None`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: the stop sequence to use for the generation. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n\n        References:\n            - https://console.groq.com/docs/text-chat\n        \"\"\"\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"seed\": seed,\n            \"temperature\": temperature,\n            \"max_tokens\": max_new_tokens,\n            \"top_p\": top_p,\n            \"stream\": False,\n            \"stop\": stop,\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n        completion = await self._aclient.chat.completions.create(**kwargs)  # type: ignore\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(  # type: ignore\n                    f\"Received no response using the Groq client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n\n    # TODO: remove this function once Groq client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.GroqLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.GroqLLM.agenerate","title":"<code>agenerate(input, seed=None, max_new_tokens=128, temperature=1.0, top_p=1.0, stop=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the Groq async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>seed</code> <code>Optional[int]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[str]</code> <p>the stop sequence to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> References <ul> <li>https://console.groq.com/docs/text-chat</li> </ul> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    seed: Optional[int] = None,\n    max_new_tokens: int = 128,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stop: Optional[str] = None,\n) -&gt; \"GenerateOutput\":\n    \"\"\"Generates `num_generations` responses for the given input using the Groq async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        seed: the seed to use for the generation. Defaults to `None`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: the stop sequence to use for the generation. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n\n    References:\n        - https://console.groq.com/docs/text-chat\n    \"\"\"\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"seed\": seed,\n        \"temperature\": temperature,\n        \"max_tokens\": max_new_tokens,\n        \"top_p\": top_p,\n        \"stream\": False,\n        \"stop\": stop,\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n    completion = await self._aclient.chat.completions.create(**kwargs)  # type: ignore\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(  # type: ignore\n                f\"Received no response using the Groq client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.GroqLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.GroqLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncGroq</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncGroq` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from groq import AsyncGroq\n    except ImportError as ie:\n        raise ImportError(\n            \"Groq Python client is not installed. Please install it using\"\n            ' `pip install groq` or from the extras as `pip install \"distilabel[groq]\"`.'\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = AsyncGroq(\n        base_url=self.base_url,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"groq\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.InferenceEndpointsLLM","title":"<code>InferenceEndpointsLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>InferenceEndpoints LLM implementation running the async API client.</p> <p>This LLM will internally use <code>huggingface_hub.AsyncInferenceClient</code> or <code>openai.AsyncOpenAI</code> depending on the <code>use_openai_client</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>Optional[str]</code> <p>the model ID to use for the LLM as available in the Hugging Face Hub, which will be used to resolve the base URL for the serverless Inference Endpoints API requests. Defaults to <code>None</code>.</p> <code>endpoint_name</code> <code>Optional[RuntimeParameter[str]]</code> <p>the name of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> <code>endpoint_namespace</code> <code>Optional[RuntimeParameter[str]]</code> <p>the namespace of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Inference Endpoints API requests.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Inference Endpoints API.</p> <code>tokenizer_id</code> <code>Optional[str]</code> <p>the tokenizer ID to use for the LLM as available in the Hugging Face Hub. Defaults to <code>None</code>, but defining one is recommended to properly format the prompt.</p> <code>model_display_name</code> <code>Optional[str]</code> <p>the model display name to use for the LLM. Defaults to <code>None</code>.</p> <code>use_openai_client</code> <code>bool</code> <p>whether to use the OpenAI client instead of the Hugging Face client.</p> Icon <p><code>:hugging:</code></p> <p>Examples:</p> <pre><code>Free serverless Inference API:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n\nDedicated Inference Endpoints:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    endpoint_namespace=\"&lt;USER|ORG&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n\nDedicated Inference Endpoints or TGI:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    base_url=\"&lt;BASE_URL&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n</code></pre> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>class InferenceEndpointsLLM(AsyncLLM):\n    \"\"\"InferenceEndpoints LLM implementation running the async API client.\n\n    This LLM will internally use `huggingface_hub.AsyncInferenceClient` or `openai.AsyncOpenAI`\n    depending on the `use_openai_client` attribute.\n\n    Attributes:\n        model_id: the model ID to use for the LLM as available in the Hugging Face Hub, which\n            will be used to resolve the base URL for the serverless Inference Endpoints API requests.\n            Defaults to `None`.\n        endpoint_name: the name of the Inference Endpoint to use for the LLM. Defaults to `None`.\n        endpoint_namespace: the namespace of the Inference Endpoint to use for the LLM. Defaults to `None`.\n        base_url: the base URL to use for the Inference Endpoints API requests.\n        api_key: the API key to authenticate the requests to the Inference Endpoints API.\n        tokenizer_id: the tokenizer ID to use for the LLM as available in the Hugging Face Hub.\n            Defaults to `None`, but defining one is recommended to properly format the prompt.\n        model_display_name: the model display name to use for the LLM. Defaults to `None`.\n        use_openai_client: whether to use the OpenAI client instead of the Hugging Face client.\n\n    Icon:\n        `:hugging:`\n\n    Examples:\n\n        Free serverless Inference API:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n\n        Dedicated Inference Endpoints:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n            api_key=\"&lt;HF_API_KEY&gt;\",\n            endpoint_namespace=\"&lt;USER|ORG&gt;\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n\n        Dedicated Inference Endpoints or TGI:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            api_key=\"&lt;HF_API_KEY&gt;\",\n            base_url=\"&lt;BASE_URL&gt;\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n    \"\"\"\n\n    model_id: Optional[str] = None\n\n    endpoint_name: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The name of the Inference Endpoint to use for the LLM.\",\n    )\n    endpoint_namespace: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The namespace of the Inference Endpoint to use for the LLM.\",\n    )\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The base URL to use for the Inference Endpoints API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default=os.getenv(_INFERENCE_ENDPOINTS_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Inference Endpoints API.\",\n    )\n\n    tokenizer_id: Optional[str] = None\n    model_display_name: Optional[str] = None\n    use_openai_client: bool = False\n\n    grammar: Optional[RuntimeParameter[Grammar]] = Field(\n        default=None,\n        description=\"The grammar to use across all the generations.\",\n    )\n\n    _model_name: Optional[str] = PrivateAttr(default=None)\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(default=None)\n    _api_key_env_var: str = PrivateAttr(_INFERENCE_ENDPOINTS_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[Union[\"AsyncInferenceClient\", \"AsyncOpenAI\"]] = PrivateAttr(...)\n\n    @model_validator(mode=\"after\")  # type: ignore\n    def only_one_of_model_id_endpoint_name_or_base_url_provided(\n        self,\n    ) -&gt; \"InferenceEndpointsLLM\":\n        \"\"\"Validates that only one of `model_id` or `endpoint_name` is provided; and if `base_url` is also\n        provided, a warning will be shown informing the user that the provided `base_url` will be ignored in\n        favour of the dynamically calculated one..\"\"\"\n\n        if self.base_url and (self.model_id or self.endpoint_name):\n            self._logger.warning(  # type: ignore\n                f\"Since the `base_url={self.base_url}` is available and either one of `model_id` or `endpoint_name`\"\n                \" is also provided, the `base_url` will either be ignored or overwritten with the one generated\"\n                \" from either of those args, for serverless or dedicated inference endpoints, respectively.\"\n            )\n\n        if self.base_url and not (self.model_id or self.endpoint_name):\n            return self\n\n        if self.model_id and not self.endpoint_name:\n            return self\n\n        if self.endpoint_name and not self.model_id:\n            return self\n\n        raise ValidationError(\n            \"Only one of `model_id` or `endpoint_name` must be provided. If `base_url` is provided too,\"\n            \" it will be overwritten instead. Found `model_id`={self.model_id}, `endpoint_name`={self.endpoint_name},\"\n            f\" and `base_url`={self.base_url}.\"\n        )\n\n    def load(self) -&gt; None:  # noqa: C901\n        \"\"\"Loads the either the `AsyncInferenceClient` or the `AsyncOpenAI` client to benefit\n        from async requests, running the Hugging Face Inference Endpoint underneath via the\n        `/v1/chat/completions` endpoint, exposed for the models running on TGI using the\n        `text-generation` task.\n\n        Raises:\n            ImportError: if the `openai` Python client is not installed.\n            ImportError: if the `huggingface-hub` Python client is not installed.\n            ValueError: if the model is not currently deployed or is not running the TGI framework.\n            ImportError: if the `transformers` Python client is not installed.\n        \"\"\"\n        super().load()\n\n        try:\n            from huggingface_hub import (\n                AsyncInferenceClient,\n                InferenceClient,\n                constants,\n                get_inference_endpoint,\n            )\n        except ImportError as ie:\n            raise ImportError(\n                \"Hugging Face Hub Python client is not installed. Please install it using\"\n                \" `pip install huggingface-hub`.\"\n            ) from ie\n\n        if self.api_key is None:\n            if not Path(constants.HF_TOKEN_PATH).exists():\n                raise ValueError(\n                    f\"To use `{self.__class__.__name__}` an API key must be provided via\"\n                    \" `api_key` attribute or runtime parameter, set the environment variable\"\n                    f\" `{self._api_key_env_var}` or use the `huggingface-hub` CLI to login\"\n                    \" with `huggingface-cli login`.\"\n                )\n            self.api_key = SecretStr(open(constants.HF_TOKEN_PATH).read().strip())\n\n        if self.model_id is not None:\n            client = InferenceClient()\n            status = client.get_model_status(self.model_id)\n\n            if (\n                status.state not in {\"Loadable\", \"Loaded\"}\n                and status.framework != \"text-generation-inference\"\n            ):\n                raise ValueError(\n                    f\"Model {self.model_id} is not currently deployed or is not running the TGI framework\"\n                )\n\n            self.base_url = client._resolve_url(\n                model=self.model_id, task=\"text-generation\"\n            )\n\n        if self.endpoint_name is not None:\n            client = get_inference_endpoint(\n                name=self.endpoint_name,\n                namespace=self.endpoint_namespace,\n                token=self.api_key.get_secret_value(),\n            )\n            if client.status in [\"paused\", \"scaledToZero\"]:\n                client.resume().wait(timeout=300)\n            elif client.status in [\"initializing\"]:\n                client.wait(timeout=300)\n\n            self.base_url = client.url\n            self._model_name = client.repository\n\n        if self.use_openai_client:\n            try:\n                from openai import AsyncOpenAI\n            except ImportError as ie:\n                raise ImportError(\n                    \"OpenAI Python client is not installed. Please install it using\"\n                    \" `pip install openai`.\"\n                ) from ie\n\n            self._aclient = AsyncOpenAI(\n                base_url=self.base_url,\n                api_key=self.api_key.get_secret_value(),\n                max_retries=6,\n            )\n        else:\n            self._aclient = AsyncInferenceClient(\n                model=self.base_url,\n                token=self.api_key.get_secret_value(),\n            )\n\n        if self.tokenizer_id:\n            try:\n                from transformers import AutoTokenizer\n            except ImportError as ie:\n                raise ImportError(\n                    \"Transformers Python client is not installed. Please install it using\"\n                    \" `pip install transformers`.\"\n                ) from ie\n\n            self._tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n\n    @property\n    @override\n    def model_name(self) -&gt; Union[str, None]:  # type: ignore\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return (\n            self.model_display_name\n            or self._model_name\n            or self.model_id\n            or self.endpoint_name\n            or self.base_url\n        )\n\n    async def _openai_agenerate(\n        self,\n        input: \"StandardInput\",\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: Optional[float] = None,\n        stop: Optional[Union[str, List[str]]] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates completions for the given input using the OpenAI async client.\"\"\"\n        completion = await self._aclient.chat.completions.create(  # type: ignore\n            messages=input,  # type: ignore\n            model=\"tgi\",\n            max_tokens=max_new_tokens,\n            n=1,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop,\n            timeout=50,\n        )\n        if completion.choices[0].message.content is None:\n            self._logger.warning(  # type: ignore\n                f\"\u26a0\ufe0f Received no response using OpenAI client (model: '{self.model_name}').\"\n                f\" Finish reason was: {completion.choices[0].finish_reason}\"\n            )\n        return [completion.choices[0].message.content]\n\n    # TODO: add `num_generations` parameter once either TGI or `AsyncInferenceClient` allows `n` parameter\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: \"FormattedInput\",\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        repetition_penalty: Optional[float] = None,\n        temperature: float = 1.0,\n        do_sample: bool = False,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        typical_p: Optional[float] = None,\n        stop_sequences: Optional[Union[str, List[str]]] = None,\n        return_full_text: bool = False,\n        seed: Optional[int] = None,\n        watermark: bool = False,\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Generates completions for the given input using the OpenAI async client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`. Only applies if `use_openai_client=True`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`. Only applies if `use_openai_client=True`.\n            repetition_penalty: the repetition penalty to use for the generation. Defaults\n                to `None`. Only applies if `use_openai_client=False`.\n            temperature: the temperature to use for the generation. Defaults to `1.0`.\n            do_sample: whether to use sampling for the generation. Defaults to `False`.\n                Only applies if `use_openai_client=False`.\n            top_k: the top-k value to use for the generation. Defaults to `0.8`, since neither\n                `0.0` nor `1.0` are valid values in TGI.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            typical_p: the typical-p value to use for the generation. Defaults to `0.5`.\n            stop_sequences: either a single string or a list of strings containing the sequences\n                to stop the generation at. Defaults to `None`, but will be set to the\n                `tokenizer.eos_token` if available.\n            return_full_text: whether to return the full text of the completion or just the\n                generated text. Defaults to `False`, meaning that only the generated text will be\n                returned.\n            seed: the seed to use for the generation. Defaults to `None`.\n            watermark: whether to add the watermark to the generated text. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        if stop_sequences is not None:\n            if isinstance(stop_sequences, str):\n                stop_sequences = [stop_sequences]\n            if len(stop_sequences) &gt; 4:\n                warnings.warn(\n                    \"Only up to 4 stop sequences are allowed, so keeping the first 4 items only.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                stop_sequences = stop_sequences[:4]\n\n        grammar = None\n        if isinstance(input, tuple):\n            input, grammar = input\n\n        if self.use_openai_client:\n            return await self._openai_agenerate(\n                input=input,\n                max_new_tokens=max_new_tokens,\n                frequency_penalty=frequency_penalty,\n                presence_penalty=presence_penalty,\n                temperature=temperature,\n                top_p=top_p,\n                stop=stop_sequences,\n            )\n\n        if self._tokenizer is not None:\n            prompt = self._tokenizer.apply_chat_template(  # type: ignore\n                conversation=input,  # type: ignore\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        else:\n            # TODO: should we apply a default chat template here instead? e.g. ChatML\n            prompt = \"\\n\".join([message[\"content\"] for message in input])\n\n        try:\n            completion = await self._aclient.text_generation(  # type: ignore\n                prompt=prompt,  # type: ignore\n                max_new_tokens=max_new_tokens,\n                do_sample=do_sample,\n                typical_p=typical_p,\n                repetition_penalty=repetition_penalty,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                stop_sequences=stop_sequences,\n                return_full_text=return_full_text,\n                watermark=watermark,\n                # NOTE: `self.grammar` applies to all the generations, while `grammar` is intended\n                # to be different per each input, and those are not intended to be used together\n                grammar=grammar or self.grammar,  # type: ignore\n                # NOTE: here to ensure that the cache is not used and a different response is\n                # generated every time\n                seed=seed or random.randint(0, 2147483647),\n            )\n            return [completion]\n        except Exception as e:\n            self._logger.warning(  # type: ignore\n                f\"\u26a0\ufe0f Received no response using Inference Client (model: '{self.model_name}').\"\n                f\" Finish reason was: {e}\"\n            )\n            return [None]\n\n    # TODO: remove this function once `AsyncInferenceClient` allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"FormattedInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.InferenceEndpointsLLM.model_name","title":"<code>model_name: Union[str, None]</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.InferenceEndpointsLLM.agenerate","title":"<code>agenerate(input, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, repetition_penalty=None, temperature=1.0, do_sample=False, top_k=None, top_p=None, typical_p=None, stop_sequences=None, return_full_text=False, seed=None, watermark=False)</code>  <code>async</code>","text":"<p>Generates completions for the given input using the OpenAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>FormattedInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>. Only applies if <code>use_openai_client=True</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>. Only applies if <code>use_openai_client=True</code>.</p> <code>0.0</code> <code>repetition_penalty</code> <code>Optional[float]</code> <p>the repetition penalty to use for the generation. Defaults to <code>None</code>. Only applies if <code>use_openai_client=False</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>do_sample</code> <code>bool</code> <p>whether to use sampling for the generation. Defaults to <code>False</code>. Only applies if <code>use_openai_client=False</code>.</p> <code>False</code> <code>top_k</code> <code>Optional[int]</code> <p>the top-k value to use for the generation. Defaults to <code>0.8</code>, since neither <code>0.0</code> nor <code>1.0</code> are valid values in TGI.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>None</code> <code>typical_p</code> <code>Optional[float]</code> <p>the typical-p value to use for the generation. Defaults to <code>0.5</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[Union[str, List[str]]]</code> <p>either a single string or a list of strings containing the sequences to stop the generation at. Defaults to <code>None</code>, but will be set to the <code>tokenizer.eos_token</code> if available.</p> <code>None</code> <code>return_full_text</code> <code>bool</code> <p>whether to return the full text of the completion or just the generated text. Defaults to <code>False</code>, meaning that only the generated text will be returned.</p> <code>False</code> <code>seed</code> <code>Optional[int]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>watermark</code> <code>bool</code> <p>whether to add the watermark to the generated text. Defaults to <code>None</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: \"FormattedInput\",\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    repetition_penalty: Optional[float] = None,\n    temperature: float = 1.0,\n    do_sample: bool = False,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    typical_p: Optional[float] = None,\n    stop_sequences: Optional[Union[str, List[str]]] = None,\n    return_full_text: bool = False,\n    seed: Optional[int] = None,\n    watermark: bool = False,\n) -&gt; \"GenerateOutput\":\n    \"\"\"Generates completions for the given input using the OpenAI async client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`. Only applies if `use_openai_client=True`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`. Only applies if `use_openai_client=True`.\n        repetition_penalty: the repetition penalty to use for the generation. Defaults\n            to `None`. Only applies if `use_openai_client=False`.\n        temperature: the temperature to use for the generation. Defaults to `1.0`.\n        do_sample: whether to use sampling for the generation. Defaults to `False`.\n            Only applies if `use_openai_client=False`.\n        top_k: the top-k value to use for the generation. Defaults to `0.8`, since neither\n            `0.0` nor `1.0` are valid values in TGI.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        typical_p: the typical-p value to use for the generation. Defaults to `0.5`.\n        stop_sequences: either a single string or a list of strings containing the sequences\n            to stop the generation at. Defaults to `None`, but will be set to the\n            `tokenizer.eos_token` if available.\n        return_full_text: whether to return the full text of the completion or just the\n            generated text. Defaults to `False`, meaning that only the generated text will be\n            returned.\n        seed: the seed to use for the generation. Defaults to `None`.\n        watermark: whether to add the watermark to the generated text. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    if stop_sequences is not None:\n        if isinstance(stop_sequences, str):\n            stop_sequences = [stop_sequences]\n        if len(stop_sequences) &gt; 4:\n            warnings.warn(\n                \"Only up to 4 stop sequences are allowed, so keeping the first 4 items only.\",\n                UserWarning,\n                stacklevel=2,\n            )\n            stop_sequences = stop_sequences[:4]\n\n    grammar = None\n    if isinstance(input, tuple):\n        input, grammar = input\n\n    if self.use_openai_client:\n        return await self._openai_agenerate(\n            input=input,\n            max_new_tokens=max_new_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop_sequences,\n        )\n\n    if self._tokenizer is not None:\n        prompt = self._tokenizer.apply_chat_template(  # type: ignore\n            conversation=input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n    else:\n        # TODO: should we apply a default chat template here instead? e.g. ChatML\n        prompt = \"\\n\".join([message[\"content\"] for message in input])\n\n    try:\n        completion = await self._aclient.text_generation(  # type: ignore\n            prompt=prompt,  # type: ignore\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            typical_p=typical_p,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop_sequences=stop_sequences,\n            return_full_text=return_full_text,\n            watermark=watermark,\n            # NOTE: `self.grammar` applies to all the generations, while `grammar` is intended\n            # to be different per each input, and those are not intended to be used together\n            grammar=grammar or self.grammar,  # type: ignore\n            # NOTE: here to ensure that the cache is not used and a different response is\n            # generated every time\n            seed=seed or random.randint(0, 2147483647),\n        )\n        return [completion]\n    except Exception as e:\n        self._logger.warning(  # type: ignore\n            f\"\u26a0\ufe0f Received no response using Inference Client (model: '{self.model_name}').\"\n            f\" Finish reason was: {e}\"\n        )\n        return [None]\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.InferenceEndpointsLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"FormattedInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.InferenceEndpointsLLM.load","title":"<code>load()</code>","text":"<p>Loads the either the <code>AsyncInferenceClient</code> or the <code>AsyncOpenAI</code> client to benefit from async requests, running the Hugging Face Inference Endpoint underneath via the <code>/v1/chat/completions</code> endpoint, exposed for the models running on TGI using the <code>text-generation</code> task.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>if the <code>openai</code> Python client is not installed.</p> <code>ImportError</code> <p>if the <code>huggingface-hub</code> Python client is not installed.</p> <code>ValueError</code> <p>if the model is not currently deployed or is not running the TGI framework.</p> <code>ImportError</code> <p>if the <code>transformers</code> Python client is not installed.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>def load(self) -&gt; None:  # noqa: C901\n    \"\"\"Loads the either the `AsyncInferenceClient` or the `AsyncOpenAI` client to benefit\n    from async requests, running the Hugging Face Inference Endpoint underneath via the\n    `/v1/chat/completions` endpoint, exposed for the models running on TGI using the\n    `text-generation` task.\n\n    Raises:\n        ImportError: if the `openai` Python client is not installed.\n        ImportError: if the `huggingface-hub` Python client is not installed.\n        ValueError: if the model is not currently deployed or is not running the TGI framework.\n        ImportError: if the `transformers` Python client is not installed.\n    \"\"\"\n    super().load()\n\n    try:\n        from huggingface_hub import (\n            AsyncInferenceClient,\n            InferenceClient,\n            constants,\n            get_inference_endpoint,\n        )\n    except ImportError as ie:\n        raise ImportError(\n            \"Hugging Face Hub Python client is not installed. Please install it using\"\n            \" `pip install huggingface-hub`.\"\n        ) from ie\n\n    if self.api_key is None:\n        if not Path(constants.HF_TOKEN_PATH).exists():\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via\"\n                \" `api_key` attribute or runtime parameter, set the environment variable\"\n                f\" `{self._api_key_env_var}` or use the `huggingface-hub` CLI to login\"\n                \" with `huggingface-cli login`.\"\n            )\n        self.api_key = SecretStr(open(constants.HF_TOKEN_PATH).read().strip())\n\n    if self.model_id is not None:\n        client = InferenceClient()\n        status = client.get_model_status(self.model_id)\n\n        if (\n            status.state not in {\"Loadable\", \"Loaded\"}\n            and status.framework != \"text-generation-inference\"\n        ):\n            raise ValueError(\n                f\"Model {self.model_id} is not currently deployed or is not running the TGI framework\"\n            )\n\n        self.base_url = client._resolve_url(\n            model=self.model_id, task=\"text-generation\"\n        )\n\n    if self.endpoint_name is not None:\n        client = get_inference_endpoint(\n            name=self.endpoint_name,\n            namespace=self.endpoint_namespace,\n            token=self.api_key.get_secret_value(),\n        )\n        if client.status in [\"paused\", \"scaledToZero\"]:\n            client.resume().wait(timeout=300)\n        elif client.status in [\"initializing\"]:\n            client.wait(timeout=300)\n\n        self.base_url = client.url\n        self._model_name = client.repository\n\n    if self.use_openai_client:\n        try:\n            from openai import AsyncOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        self._aclient = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=6,\n        )\n    else:\n        self._aclient = AsyncInferenceClient(\n            model=self.base_url,\n            token=self.api_key.get_secret_value(),\n        )\n\n    if self.tokenizer_id:\n        try:\n            from transformers import AutoTokenizer\n        except ImportError as ie:\n            raise ImportError(\n                \"Transformers Python client is not installed. Please install it using\"\n                \" `pip install transformers`.\"\n            ) from ie\n\n        self._tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.InferenceEndpointsLLM.only_one_of_model_id_endpoint_name_or_base_url_provided","title":"<code>only_one_of_model_id_endpoint_name_or_base_url_provided()</code>","text":"<p>Validates that only one of <code>model_id</code> or <code>endpoint_name</code> is provided; and if <code>base_url</code> is also provided, a warning will be shown informing the user that the provided <code>base_url</code> will be ignored in favour of the dynamically calculated one..</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@model_validator(mode=\"after\")  # type: ignore\ndef only_one_of_model_id_endpoint_name_or_base_url_provided(\n    self,\n) -&gt; \"InferenceEndpointsLLM\":\n    \"\"\"Validates that only one of `model_id` or `endpoint_name` is provided; and if `base_url` is also\n    provided, a warning will be shown informing the user that the provided `base_url` will be ignored in\n    favour of the dynamically calculated one..\"\"\"\n\n    if self.base_url and (self.model_id or self.endpoint_name):\n        self._logger.warning(  # type: ignore\n            f\"Since the `base_url={self.base_url}` is available and either one of `model_id` or `endpoint_name`\"\n            \" is also provided, the `base_url` will either be ignored or overwritten with the one generated\"\n            \" from either of those args, for serverless or dedicated inference endpoints, respectively.\"\n        )\n\n    if self.base_url and not (self.model_id or self.endpoint_name):\n        return self\n\n    if self.model_id and not self.endpoint_name:\n        return self\n\n    if self.endpoint_name and not self.model_id:\n        return self\n\n    raise ValidationError(\n        \"Only one of `model_id` or `endpoint_name` must be provided. If `base_url` is provided too,\"\n        \" it will be overwritten instead. Found `model_id`={self.model_id}, `endpoint_name`={self.endpoint_name},\"\n        f\" and `base_url`={self.base_url}.\"\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM","title":"<code>LLM</code>","text":"<p>               Bases: <code>RuntimeParametersMixin</code>, <code>BaseModel</code>, <code>_Serializable</code>, <code>ABC</code></p> <p>Base class for <code>LLM</code>s to be used in <code>distilabel</code> framework.</p> <p>To implement an <code>LLM</code> subclass, you need to subclass this class and implement:     - <code>load</code> method to load the <code>LLM</code> if needed. Don't forget to call <code>super().load()</code>,         so the <code>_logger</code> attribute is initialized.     - <code>model_name</code> property to return the model name used for the LLM.     - <code>generate</code> method to generate <code>num_generations</code> per input in <code>inputs</code>.</p> <p>Attributes:</p> Name Type Description <code>generation_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>the kwargs to be propagated to either <code>generate</code> or <code>agenerate</code> methods within each <code>LLM</code>.</p> <code>structured_output</code> <code>Optional[Any]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>_logger</code> <code>Union[Logger, None]</code> <p>the logger to be used for the <code>LLM</code>. It will be initialized when the <code>load</code> method is called.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>class LLM(RuntimeParametersMixin, BaseModel, _Serializable, ABC):\n    \"\"\"Base class for `LLM`s to be used in `distilabel` framework.\n\n    To implement an `LLM` subclass, you need to subclass this class and implement:\n        - `load` method to load the `LLM` if needed. Don't forget to call `super().load()`,\n            so the `_logger` attribute is initialized.\n        - `model_name` property to return the model name used for the LLM.\n        - `generate` method to generate `num_generations` per input in `inputs`.\n\n    Attributes:\n        generation_kwargs: the kwargs to be propagated to either `generate` or `agenerate`\n            methods within each `LLM`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        _logger: the logger to be used for the `LLM`. It will be initialized when the `load`\n            method is called.\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        protected_namespaces=(),\n        validate_default=True,\n        validate_assignment=True,\n        extra=\"forbid\",\n    )\n\n    generation_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"The kwargs to be propagated to either `generate` or `agenerate`\"\n        \" methods within each `LLM`.\",\n    )\n    structured_output: Optional[Any] = None\n\n    _logger: Union[logging.Logger, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Method to be called to initialize the `LLM`, its logger and optionally the structured output generator.\"\"\"\n        self._logger = logging.getLogger(f\"distilabel.llm.{self.model_name}\")\n\n    @property\n    @abstractmethod\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        pass\n\n    @abstractmethod\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Abstract method to be implemented by each LLM to generate `num_generations`\n        per input in `inputs`.\n\n        Args:\n            inputs: the list of inputs to generate responses for which follows OpenAI's\n                API format:\n\n                ```python\n                [\n                    {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n                    {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n                    {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n                    {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n                ]\n                ```\n            num_generations: the number of generations to generate per input.\n            **kwargs: the additional kwargs to be used for the generation.\n        \"\"\"\n        pass\n\n    @property\n    def generate_parameters(self) -&gt; List[\"inspect.Parameter\"]:\n        \"\"\"Returns the parameters of the `generate` method.\n\n        Returns:\n            A list containing the parameters of the `generate` method.\n        \"\"\"\n        return list(inspect.signature(self.generate).parameters.values())\n\n    @property\n    def runtime_parameters_names(self) -&gt; \"RuntimeParametersNames\":\n        \"\"\"Returns the runtime parameters of the `LLM`, which are combination of the\n        attributes of the `LLM` type hinted with `RuntimeParameter` and the parameters\n        of the `generate` method that are not `input` and `num_generations`.\n\n        Returns:\n            A dictionary with the name of the runtime parameters as keys and a boolean\n            indicating if the parameter is optional or not.\n        \"\"\"\n        runtime_parameters = super().runtime_parameters_names\n        runtime_parameters[\"generation_kwargs\"] = {}\n\n        # runtime parameters from the `generate` method\n        for param in self.generate_parameters:\n            if param.name in [\"input\", \"inputs\", \"num_generations\"]:\n                continue\n            is_optional = param.default != inspect.Parameter.empty\n            runtime_parameters[\"generation_kwargs\"][param.name] = is_optional\n\n        return runtime_parameters\n\n    def get_runtime_parameters_info(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Gets the information of the runtime parameters of the `LLM` such as the name\n        and the description. This function is meant to include the information of the runtime\n        parameters in the serialized data of the `LLM`.\n\n        Returns:\n            A list containing the information for each runtime parameter of the `LLM`.\n        \"\"\"\n        runtime_parameters_info = super().get_runtime_parameters_info()\n\n        generation_kwargs_info = next(\n            runtime_parameter_info\n            for runtime_parameter_info in runtime_parameters_info\n            if runtime_parameter_info[\"name\"] == \"generation_kwargs\"\n        )\n\n        generate_docstring_args = self.generate_parsed_docstring[\"args\"]\n\n        generation_kwargs_info[\"keys\"] = []\n        for key, value in generation_kwargs_info[\"optional\"].items():\n            info = {\"name\": key, \"optional\": value}\n            if description := generate_docstring_args.get(key):\n                info[\"description\"] = description\n            generation_kwargs_info[\"keys\"].append(info)\n\n        generation_kwargs_info.pop(\"optional\")\n\n        return runtime_parameters_info\n\n    @cached_property\n    def generate_parsed_docstring(self) -&gt; \"Docstring\":\n        \"\"\"Returns the parsed docstring of the `generate` method.\n\n        Returns:\n            The parsed docstring of the `generate` method.\n        \"\"\"\n        return parse_google_docstring(self.generate)\n\n    def get_last_hidden_states(\n        self, inputs: List[\"StandardInput\"]\n    ) -&gt; List[\"HiddenState\"]:\n        \"\"\"Method to get the last hidden states of the model for a list of inputs.\n\n        Args:\n            inputs: the list of inputs to get the last hidden states from.\n\n        Returns:\n            A list containing the last hidden state for each sequence using a NumPy array\n                with shape [num_tokens, hidden_size].\n        \"\"\"\n        raise NotImplementedError(\n            f\"Method `get_last_hidden_states` is not implemented for `{self.__class__.__name__}`\"\n        )\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Any, None]:\n        \"\"\"Method in charge of preparing the structured output generator.\n\n        By default will raise a `NotImplementedError`, subclasses that allow it must override this\n        method with the implementation.\n\n        Args:\n            structured_output: the config to prepare the guided generation.\n\n        Returns:\n            The structure to be used for the guided generation.\n        \"\"\"\n        raise NotImplementedError(\n            f\"Guided generation is not implemented for `{type(self).__name__}`\"\n        )\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM.generate_parameters","title":"<code>generate_parameters: List[inspect.Parameter]</code>  <code>property</code>","text":"<p>Returns the parameters of the <code>generate</code> method.</p> <p>Returns:</p> Type Description <code>List[Parameter]</code> <p>A list containing the parameters of the <code>generate</code> method.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM.generate_parsed_docstring","title":"<code>generate_parsed_docstring: Docstring</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the parsed docstring of the <code>generate</code> method.</p> <p>Returns:</p> Type Description <code>Docstring</code> <p>The parsed docstring of the <code>generate</code> method.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM.model_name","title":"<code>model_name: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM.runtime_parameters_names","title":"<code>runtime_parameters_names: RuntimeParametersNames</code>  <code>property</code>","text":"<p>Returns the runtime parameters of the <code>LLM</code>, which are combination of the attributes of the <code>LLM</code> type hinted with <code>RuntimeParameter</code> and the parameters of the <code>generate</code> method that are not <code>input</code> and <code>num_generations</code>.</p> <p>Returns:</p> Type Description <code>RuntimeParametersNames</code> <p>A dictionary with the name of the runtime parameters as keys and a boolean</p> <code>RuntimeParametersNames</code> <p>indicating if the parameter is optional or not.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be implemented by each LLM to generate <code>num_generations</code> per input in <code>inputs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[FormattedInput]</code> <p>the list of inputs to generate responses for which follows OpenAI's API format:</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n    {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n    {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n    {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n]\n</code></pre> required <code>num_generations</code> <code>int</code> <p>the number of generations to generate per input.</p> <code>1</code> <code>**kwargs</code> <code>Any</code> <p>the additional kwargs to be used for the generation.</p> <code>{}</code> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Abstract method to be implemented by each LLM to generate `num_generations`\n    per input in `inputs`.\n\n    Args:\n        inputs: the list of inputs to generate responses for which follows OpenAI's\n            API format:\n\n            ```python\n            [\n                {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n                {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n                {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n                {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n            ]\n            ```\n        num_generations: the number of generations to generate per input.\n        **kwargs: the additional kwargs to be used for the generation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM.get_last_hidden_states","title":"<code>get_last_hidden_states(inputs)</code>","text":"<p>Method to get the last hidden states of the model for a list of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>the list of inputs to get the last hidden states from.</p> required <p>Returns:</p> Type Description <code>List[HiddenState]</code> <p>A list containing the last hidden state for each sequence using a NumPy array with shape [num_tokens, hidden_size].</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def get_last_hidden_states(\n    self, inputs: List[\"StandardInput\"]\n) -&gt; List[\"HiddenState\"]:\n    \"\"\"Method to get the last hidden states of the model for a list of inputs.\n\n    Args:\n        inputs: the list of inputs to get the last hidden states from.\n\n    Returns:\n        A list containing the last hidden state for each sequence using a NumPy array\n            with shape [num_tokens, hidden_size].\n    \"\"\"\n    raise NotImplementedError(\n        f\"Method `get_last_hidden_states` is not implemented for `{self.__class__.__name__}`\"\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM.get_runtime_parameters_info","title":"<code>get_runtime_parameters_info()</code>","text":"<p>Gets the information of the runtime parameters of the <code>LLM</code> such as the name and the description. This function is meant to include the information of the runtime parameters in the serialized data of the <code>LLM</code>.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list containing the information for each runtime parameter of the <code>LLM</code>.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def get_runtime_parameters_info(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Gets the information of the runtime parameters of the `LLM` such as the name\n    and the description. This function is meant to include the information of the runtime\n    parameters in the serialized data of the `LLM`.\n\n    Returns:\n        A list containing the information for each runtime parameter of the `LLM`.\n    \"\"\"\n    runtime_parameters_info = super().get_runtime_parameters_info()\n\n    generation_kwargs_info = next(\n        runtime_parameter_info\n        for runtime_parameter_info in runtime_parameters_info\n        if runtime_parameter_info[\"name\"] == \"generation_kwargs\"\n    )\n\n    generate_docstring_args = self.generate_parsed_docstring[\"args\"]\n\n    generation_kwargs_info[\"keys\"] = []\n    for key, value in generation_kwargs_info[\"optional\"].items():\n        info = {\"name\": key, \"optional\": value}\n        if description := generate_docstring_args.get(key):\n            info[\"description\"] = description\n        generation_kwargs_info[\"keys\"].append(info)\n\n    generation_kwargs_info.pop(\"optional\")\n\n    return runtime_parameters_info\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LLM.load","title":"<code>load()</code>","text":"<p>Method to be called to initialize the <code>LLM</code>, its logger and optionally the structured output generator.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Method to be called to initialize the `LLM`, its logger and optionally the structured output generator.\"\"\"\n    self._logger = logging.getLogger(f\"distilabel.llm.{self.model_name}\")\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LiteLLM","title":"<code>LiteLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>LiteLLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gpt-3.5-turbo\" or \"mistral/mistral-large\", etc.</p> <code>verbose</code> <code>RuntimeParameter[bool]</code> <p>whether to log the LiteLLM client's logs. Defaults to <code>False</code>.</p> <code>structured_output</code> <code>RuntimeParameter[bool]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> Runtime parameters <ul> <li><code>verbose</code>: whether to log the LiteLLM client's logs. Defaults to <code>False</code>.</li> </ul> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>class LiteLLM(AsyncLLM):\n    \"\"\"LiteLLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gpt-3.5-turbo\" or \"mistral/mistral-large\",\n            etc.\n        verbose: whether to log the LiteLLM client's logs. Defaults to `False`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n\n    Runtime parameters:\n        - `verbose`: whether to log the LiteLLM client's logs. Defaults to `False`.\n    \"\"\"\n\n    model: str\n    verbose: RuntimeParameter[bool] = Field(\n        default=False, description=\"Whether to log the LiteLLM client's logs.\"\n    )\n\n    _aclient: Optional[Callable] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"\n        Loads the `acompletion` LiteLLM client to benefit from async requests.\n        \"\"\"\n        super().load()\n\n        try:\n            import litellm\n\n            litellm.telemetry = False\n        except ImportError as e:\n            raise ImportError(\n                \"LiteLLM Python client is not installed. Please install it using\"\n                \" `pip install litellm`.\"\n            ) from e\n        self._aclient = litellm.acompletion\n\n        if not self.verbose:\n            litellm.suppress_debug_info = True\n            for key in logging.Logger.manager.loggerDict.keys():\n                if \"litellm\" not in key.lower():\n                    continue\n                logging.getLogger(key).setLevel(logging.CRITICAL)\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"litellm\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        temperature: Optional[float] = 1.0,\n        top_p: Optional[float] = 1.0,\n        stop: Optional[Union[str, list]] = None,\n        max_tokens: Optional[int] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        metadata: Optional[dict] = None,\n        api_base: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,\n        mock_response: Optional[str] = None,\n        force_timeout: Optional[int] = 600,\n        custom_llm_provider: Optional[str] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the [LiteLLM async client](https://github.com/BerriAI/litellm).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            functions: a list of functions to apply to the conversation messages. Defaults to\n                `None`.\n            function_call: the name of the function to call within the conversation. Defaults\n                to `None`.\n            temperature: the temperature to use for the generation. Defaults to `1.0`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: Up to 4 sequences where the LLM API will stop generating further tokens.\n                Defaults to `None`.\n            max_tokens: The maximum number of tokens in the generated completion. Defaults to\n                `None`.\n            presence_penalty: It is used to penalize new tokens based on their existence in the\n                text so far. Defaults to `None`.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the\n                text so far. Defaults to `None`.\n            logit_bias: Used to modify the probability of specific tokens appearing in the\n                completion. Defaults to `None`.\n            user: A unique identifier representing your end-user. This can help the LLM provider\n                to monitor and detect abuse. Defaults to `None`.\n            metadata: Pass in additional metadata to tag your completion calls - eg. prompt\n                version, details, etc. Defaults to `None`.\n            api_base: Base URL for the API. Defaults to `None`.\n            api_version: API version. Defaults to `None`.\n            api_key: API key. Defaults to `None`.\n            model_list: List of api base, version, keys. Defaults to `None`.\n            mock_response: If provided, return a mock completion response for testing or debugging\n                purposes. Defaults to `None`.\n            force_timeout: The maximum execution time in seconds for the completion request.\n                Defaults to `600`.\n            custom_llm_provider: Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable)\n                model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to\n                `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        import litellm\n\n        kwargs = {\n            \"model\": self.model,\n            \"messages\": input,\n            \"n\": num_generations,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"stream\": False,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"metadata\": metadata,\n            \"api_base\": api_base,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"mock_response\": mock_response,\n            \"force_timeout\": force_timeout,\n            \"custom_llm_provider\": custom_llm_provider,\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        async def _call_aclient_until_n_choices() -&gt; List[\"Choices\"]:\n            choices = []\n            while len(choices) &lt; num_generations:\n                completion = await self._aclient(**kwargs)  # type: ignore\n                if not self.structured_output:\n                    completion = completion.choices\n                choices.extend(completion)\n            return choices\n\n        # litellm.drop_params is used to en/disable sending **kwargs parameters to the API if they cannot be used\n        try:\n            litellm.drop_params = False\n            choices = await _call_aclient_until_n_choices()\n        except litellm.exceptions.APIError as e:\n            if \"does not support parameters\" in str(e):\n                litellm.drop_params = True\n                choices = await _call_aclient_until_n_choices()\n            else:\n                raise e\n\n        generations = []\n\n        if self.structured_output:\n            generations.append([choice.model_dump_json() for choice in choices])\n            return generations\n\n        for choice in choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(\n                    f\"Received no response using LiteLLM client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LiteLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.LiteLLM.agenerate","title":"<code>agenerate(input, num_generations=1, functions=None, function_call=None, temperature=1.0, top_p=1.0, stop=None, max_tokens=None, presence_penalty=None, frequency_penalty=None, logit_bias=None, user=None, metadata=None, api_base=None, api_version=None, api_key=None, model_list=None, mock_response=None, force_timeout=600, custom_llm_provider=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the LiteLLM async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>functions</code> <code>Optional[List]</code> <p>a list of functions to apply to the conversation messages. Defaults to <code>None</code>.</p> <code>None</code> <code>function_call</code> <code>Optional[str]</code> <p>the name of the function to call within the conversation. Defaults to <code>None</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[Union[str, list]]</code> <p>Up to 4 sequences where the LLM API will stop generating further tokens. Defaults to <code>None</code>.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens in the generated completion. Defaults to <code>None</code>.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>It is used to penalize new tokens based on their existence in the text so far. Defaults to <code>None</code>.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>It is used to penalize new tokens based on their frequency in the text so far. Defaults to <code>None</code>.</p> <code>None</code> <code>logit_bias</code> <code>Optional[dict]</code> <p>Used to modify the probability of specific tokens appearing in the completion. Defaults to <code>None</code>.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse. Defaults to <code>None</code>.</p> <code>None</code> <code>metadata</code> <code>Optional[dict]</code> <p>Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc. Defaults to <code>None</code>.</p> <code>None</code> <code>api_base</code> <code>Optional[str]</code> <p>Base URL for the API. Defaults to <code>None</code>.</p> <code>None</code> <code>api_version</code> <code>Optional[str]</code> <p>API version. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key. Defaults to <code>None</code>.</p> <code>None</code> <code>model_list</code> <code>Optional[list]</code> <p>List of api base, version, keys. Defaults to <code>None</code>.</p> <code>None</code> <code>mock_response</code> <code>Optional[str]</code> <p>If provided, return a mock completion response for testing or debugging purposes. Defaults to <code>None</code>.</p> <code>None</code> <code>force_timeout</code> <code>Optional[int]</code> <p>The maximum execution time in seconds for the completion request. Defaults to <code>600</code>.</p> <code>600</code> <code>custom_llm_provider</code> <code>Optional[str]</code> <p>Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable) model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    functions: Optional[List] = None,\n    function_call: Optional[str] = None,\n    temperature: Optional[float] = 1.0,\n    top_p: Optional[float] = 1.0,\n    stop: Optional[Union[str, list]] = None,\n    max_tokens: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[dict] = None,\n    user: Optional[str] = None,\n    metadata: Optional[dict] = None,\n    api_base: Optional[str] = None,\n    api_version: Optional[str] = None,\n    api_key: Optional[str] = None,\n    model_list: Optional[list] = None,\n    mock_response: Optional[str] = None,\n    force_timeout: Optional[int] = 600,\n    custom_llm_provider: Optional[str] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the [LiteLLM async client](https://github.com/BerriAI/litellm).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        functions: a list of functions to apply to the conversation messages. Defaults to\n            `None`.\n        function_call: the name of the function to call within the conversation. Defaults\n            to `None`.\n        temperature: the temperature to use for the generation. Defaults to `1.0`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: Up to 4 sequences where the LLM API will stop generating further tokens.\n            Defaults to `None`.\n        max_tokens: The maximum number of tokens in the generated completion. Defaults to\n            `None`.\n        presence_penalty: It is used to penalize new tokens based on their existence in the\n            text so far. Defaults to `None`.\n        frequency_penalty: It is used to penalize new tokens based on their frequency in the\n            text so far. Defaults to `None`.\n        logit_bias: Used to modify the probability of specific tokens appearing in the\n            completion. Defaults to `None`.\n        user: A unique identifier representing your end-user. This can help the LLM provider\n            to monitor and detect abuse. Defaults to `None`.\n        metadata: Pass in additional metadata to tag your completion calls - eg. prompt\n            version, details, etc. Defaults to `None`.\n        api_base: Base URL for the API. Defaults to `None`.\n        api_version: API version. Defaults to `None`.\n        api_key: API key. Defaults to `None`.\n        model_list: List of api base, version, keys. Defaults to `None`.\n        mock_response: If provided, return a mock completion response for testing or debugging\n            purposes. Defaults to `None`.\n        force_timeout: The maximum execution time in seconds for the completion request.\n            Defaults to `600`.\n        custom_llm_provider: Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable)\n            model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to\n            `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    import litellm\n\n    kwargs = {\n        \"model\": self.model,\n        \"messages\": input,\n        \"n\": num_generations,\n        \"functions\": functions,\n        \"function_call\": function_call,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"stream\": False,\n        \"stop\": stop,\n        \"max_tokens\": max_tokens,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"logit_bias\": logit_bias,\n        \"user\": user,\n        \"metadata\": metadata,\n        \"api_base\": api_base,\n        \"api_version\": api_version,\n        \"api_key\": api_key,\n        \"model_list\": model_list,\n        \"mock_response\": mock_response,\n        \"force_timeout\": force_timeout,\n        \"custom_llm_provider\": custom_llm_provider,\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    async def _call_aclient_until_n_choices() -&gt; List[\"Choices\"]:\n        choices = []\n        while len(choices) &lt; num_generations:\n            completion = await self._aclient(**kwargs)  # type: ignore\n            if not self.structured_output:\n                completion = completion.choices\n            choices.extend(completion)\n        return choices\n\n    # litellm.drop_params is used to en/disable sending **kwargs parameters to the API if they cannot be used\n    try:\n        litellm.drop_params = False\n        choices = await _call_aclient_until_n_choices()\n    except litellm.exceptions.APIError as e:\n        if \"does not support parameters\" in str(e):\n            litellm.drop_params = True\n            choices = await _call_aclient_until_n_choices()\n        else:\n            raise e\n\n    generations = []\n\n    if self.structured_output:\n        generations.append([choice.model_dump_json() for choice in choices])\n        return generations\n\n    for choice in choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(\n                f\"Received no response using LiteLLM client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LiteLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>acompletion</code> LiteLLM client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"\n    Loads the `acompletion` LiteLLM client to benefit from async requests.\n    \"\"\"\n    super().load()\n\n    try:\n        import litellm\n\n        litellm.telemetry = False\n    except ImportError as e:\n        raise ImportError(\n            \"LiteLLM Python client is not installed. Please install it using\"\n            \" `pip install litellm`.\"\n        ) from e\n    self._aclient = litellm.acompletion\n\n    if not self.verbose:\n        litellm.suppress_debug_info = True\n        for key in logging.Logger.manager.loggerDict.keys():\n            if \"litellm\" not in key.lower():\n                continue\n            logging.getLogger(key).setLevel(logging.CRITICAL)\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"litellm\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LlamaCppLLM","title":"<code>LlamaCppLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>llama.cpp LLM implementation running the Python bindings for the C++ code.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>RuntimeParameter[FilePath]</code> <p>contains the path to the GGUF quantized model, compatible with the installed version of the <code>llama.cpp</code> Python bindings.</p> <code>n_gpu_layers</code> <code>RuntimeParameter[int]</code> <p>the number of layers to use for the GPU. Defaults to <code>-1</code>, meaning that the available GPU device will be used.</p> <code>chat_format</code> <code>Optional[RuntimeParameter[str]]</code> <p>the chat format to use for the model. Defaults to <code>None</code>, which means the Llama format will be used.</p> <code>n_ctx</code> <code>int</code> <p>the context size to use for the model. Defaults to <code>512</code>.</p> <code>n_batch</code> <code>int</code> <p>the prompt processing maximum batch size to use for the model. Defaults to <code>512</code>.</p> <code>seed</code> <code>int</code> <p>random seed to use for the generation. Defaults to <code>4294967295</code>.</p> <code>verbose</code> <code>RuntimeParameter[bool]</code> <p>whether to print verbose output. Defaults to <code>False</code>.</p> <code>structured_output</code> <code>RuntimeParameter[bool]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>extra_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>Llama</code> class of <code>llama_cpp</code> library. Defaults to <code>{}</code>.</p> <code>_model</code> <code>Optional[Llama]</code> <p>the Llama model instance. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>model_path</code>: the path to the GGUF quantized model.</li> <li><code>n_gpu_layers</code>: the number of layers to use for the GPU. Defaults to <code>-1</code>.</li> <li><code>chat_format</code>: the chat format to use for the model. Defaults to <code>None</code>.</li> <li><code>verbose</code>: whether to print verbose output. Defaults to <code>False</code>.</li> <li><code>extra_kwargs</code>: additional dictionary of keyword arguments that will be passed to the     <code>Llama</code> class of <code>llama_cpp</code> library. Defaults to <code>{}</code>.</li> </ul> References <ul> <li><code>llama.cpp</code></li> <li><code>llama-cpp-python</code></li> </ul> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>class LlamaCppLLM(LLM):\n    \"\"\"llama.cpp LLM implementation running the Python bindings for the C++ code.\n\n    Attributes:\n        model_path: contains the path to the GGUF quantized model, compatible with the\n            installed version of the `llama.cpp` Python bindings.\n        n_gpu_layers: the number of layers to use for the GPU. Defaults to `-1`, meaning that\n            the available GPU device will be used.\n        chat_format: the chat format to use for the model. Defaults to `None`, which means the\n            Llama format will be used.\n        n_ctx: the context size to use for the model. Defaults to `512`.\n        n_batch: the prompt processing maximum batch size to use for the model. Defaults to `512`.\n        seed: random seed to use for the generation. Defaults to `4294967295`.\n        verbose: whether to print verbose output. Defaults to `False`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        extra_kwargs: additional dictionary of keyword arguments that will be passed to the\n            `Llama` class of `llama_cpp` library. Defaults to `{}`.\n        _model: the Llama model instance. This attribute is meant to be used internally and\n            should not be accessed directly. It will be set in the `load` method.\n\n    Runtime parameters:\n        - `model_path`: the path to the GGUF quantized model.\n        - `n_gpu_layers`: the number of layers to use for the GPU. Defaults to `-1`.\n        - `chat_format`: the chat format to use for the model. Defaults to `None`.\n        - `verbose`: whether to print verbose output. Defaults to `False`.\n        - `extra_kwargs`: additional dictionary of keyword arguments that will be passed to the\n            `Llama` class of `llama_cpp` library. Defaults to `{}`.\n\n    References:\n        - [`llama.cpp`](https://github.com/ggerganov/llama.cpp)\n        - [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python)\n    \"\"\"\n\n    model_path: RuntimeParameter[FilePath] = Field(\n        default=None, description=\"The path to the GGUF quantized model.\", exclude=True\n    )\n    n_gpu_layers: RuntimeParameter[int] = Field(\n        default=-1,\n        description=\"The number of layers that will be loaded in the GPU.\",\n    )\n    chat_format: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The chat format to use for the model. Defaults to `None`, which means the Llama format will be used.\",\n    )\n\n    n_ctx: int = 512\n    n_batch: int = 512\n    seed: int = 4294967295\n\n    verbose: RuntimeParameter[bool] = Field(\n        default=False,\n        description=\"Whether to print verbose output from llama.cpp library.\",\n    )\n    extra_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"Additional dictionary of keyword arguments that will be passed to the\"\n        \" `Llama` class of `llama_cpp` library. See all the supported arguments at: \"\n        \"https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__init__\",\n    )\n\n    _logits_processor: Optional[\"LogitsProcessorList\"] = PrivateAttr(default=None)\n    _model: Optional[\"Llama\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `Llama` model from the `model_path`.\"\"\"\n        try:\n            from llama_cpp import Llama\n        except ImportError as ie:\n            raise ImportError(\n                \"The `llama_cpp` package is required to use the `LlamaCppLLM` class.\"\n            ) from ie\n\n        self._model = Llama(\n            model_path=self.model_path.as_posix(),  # type: ignore\n            seed=self.seed,\n            n_ctx=self.n_ctx,\n            n_batch=self.n_batch,\n            chat_format=self.chat_format,\n            n_gpu_layers=self.n_gpu_layers,\n            verbose=self.verbose,\n            **self.extra_kwargs,\n        )\n\n        if self.structured_output:\n            self._logits_processor = self._prepare_structured_output(\n                self.structured_output\n            )\n\n        # NOTE: Here because of the custom `logging` interface used, since it will create the logging name\n        # out of the model name, which won't be available until the `Llama` instance is created.\n        super().load()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self._model.model_path  # type: ignore\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        extra_generation_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for the given input using the Llama model.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            extra_generation_kwargs: dictionary with additional arguments to be passed to\n                the `create_chat_completion` method. Reference at\n                https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n\n        batch_outputs = []\n        for input in inputs:\n            outputs = []\n            for _ in range(num_generations):\n                # NOTE(plaguss): There seems to be a bug in how the logits processor\n                # is used. Basically it consumes the FSM internally, and it isn't reinitialized\n                # after each generation, so subsequent calls yield nothing. This is a workaround\n                # until is fixed in the `llama_cpp` or `outlines` libraries.\n                if self.structured_output:\n                    self._logits_processor = self._prepare_structured_output(\n                        self.structured_output\n                    )\n                chat_completions: \"CreateChatCompletionResponse\" = (\n                    self._model.create_chat_completion(  # type: ignore\n                        messages=input,  # type: ignore\n                        max_tokens=max_new_tokens,\n                        frequency_penalty=frequency_penalty,\n                        presence_penalty=presence_penalty,\n                        temperature=temperature,\n                        top_p=top_p,\n                        logits_processor=self._logits_processor,\n                        **(extra_generation_kwargs or {}),\n                    )\n                )\n                outputs.append(chat_completions[\"choices\"][0][\"message\"][\"content\"])\n            batch_outputs.append(outputs)\n        return batch_outputs\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[\"LogitsProcessorList\", None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(structured_output, \"llamacpp\", self._model)\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LlamaCppLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.LlamaCppLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, extra_generation_kwargs=None)</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the Llama model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>extra_generation_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary with additional arguments to be passed to the <code>create_chat_completion</code> method. Reference at https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion</p> <code>None</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    extra_generation_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for the given input using the Llama model.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        extra_generation_kwargs: dictionary with additional arguments to be passed to\n            the `create_chat_completion` method. Reference at\n            https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n\n    batch_outputs = []\n    for input in inputs:\n        outputs = []\n        for _ in range(num_generations):\n            # NOTE(plaguss): There seems to be a bug in how the logits processor\n            # is used. Basically it consumes the FSM internally, and it isn't reinitialized\n            # after each generation, so subsequent calls yield nothing. This is a workaround\n            # until is fixed in the `llama_cpp` or `outlines` libraries.\n            if self.structured_output:\n                self._logits_processor = self._prepare_structured_output(\n                    self.structured_output\n                )\n            chat_completions: \"CreateChatCompletionResponse\" = (\n                self._model.create_chat_completion(  # type: ignore\n                    messages=input,  # type: ignore\n                    max_tokens=max_new_tokens,\n                    frequency_penalty=frequency_penalty,\n                    presence_penalty=presence_penalty,\n                    temperature=temperature,\n                    top_p=top_p,\n                    logits_processor=self._logits_processor,\n                    **(extra_generation_kwargs or {}),\n                )\n            )\n            outputs.append(chat_completions[\"choices\"][0][\"message\"][\"content\"])\n        batch_outputs.append(outputs)\n    return batch_outputs\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.LlamaCppLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>Llama</code> model from the <code>model_path</code>.</p> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `Llama` model from the `model_path`.\"\"\"\n    try:\n        from llama_cpp import Llama\n    except ImportError as ie:\n        raise ImportError(\n            \"The `llama_cpp` package is required to use the `LlamaCppLLM` class.\"\n        ) from ie\n\n    self._model = Llama(\n        model_path=self.model_path.as_posix(),  # type: ignore\n        seed=self.seed,\n        n_ctx=self.n_ctx,\n        n_batch=self.n_batch,\n        chat_format=self.chat_format,\n        n_gpu_layers=self.n_gpu_layers,\n        verbose=self.verbose,\n        **self.extra_kwargs,\n    )\n\n    if self.structured_output:\n        self._logits_processor = self._prepare_structured_output(\n            self.structured_output\n        )\n\n    # NOTE: Here because of the custom `logging` interface used, since it will create the logging name\n    # out of the model name, which won't be available until the `Llama` instance is created.\n    super().load()\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.MistralLLM","title":"<code>MistralLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Mistral LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"mistral-tiny\", \"mistral-large\", etc.</p> <code>endpoint</code> <code>str</code> <p>the endpoint to use for the Mistral API. Defaults to \"https://api.mistral.ai\".</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Mistral API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of retries to attempt when a request fails. Defaults to <code>5</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response. Defaults to <code>120</code>.</p> <code>max_concurrent_requests</code> <code>RuntimeParameter[int]</code> <p>the maximum number of concurrent requests to send. Defaults to <code>64</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> <code>_aclient</code> <code>Optional[MistralAsyncClient]</code> <p>the <code>MistralAsyncClient</code> to use for the Mistral API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>api_key</code>: the API key to authenticate the requests to the Mistral API.</li> <li><code>max_retries</code>: the maximum number of retries to attempt when a request fails.     Defaults to <code>5</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response. Defaults to <code>120</code>.</li> <li><code>max_concurrent_requests</code>: the maximum number of concurrent requests to send.     Defaults to <code>64</code>.</li> </ul> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>class MistralLLM(AsyncLLM):\n    \"\"\"Mistral LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"mistral-tiny\", \"mistral-large\", etc.\n        endpoint: the endpoint to use for the Mistral API. Defaults to \"https://api.mistral.ai\".\n        api_key: the API key to authenticate the requests to the Mistral API. Defaults to `None` which\n            means that the value set for the environment variable `OPENAI_API_KEY` will be used, or\n            `None` if not set.\n        max_retries: the maximum number of retries to attempt when a request fails. Defaults to `5`.\n        timeout: the maximum time in seconds to wait for a response. Defaults to `120`.\n        max_concurrent_requests: the maximum number of concurrent requests to send. Defaults\n            to `64`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n        _api_key_env_var: the name of the environment variable to use for the API key. It is meant to\n            be used internally.\n        _aclient: the `MistralAsyncClient` to use for the Mistral API. It is meant to be used internally.\n            Set in the `load` method.\n\n    Runtime parameters:\n        - `api_key`: the API key to authenticate the requests to the Mistral API.\n        - `max_retries`: the maximum number of retries to attempt when a request fails.\n            Defaults to `5`.\n        - `timeout`: the maximum time in seconds to wait for a response. Defaults to `120`.\n        - `max_concurrent_requests`: the maximum number of concurrent requests to send.\n            Defaults to `64`.\n    \"\"\"\n\n    model: str\n    endpoint: str = \"https://api.mistral.ai\"\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_MISTRALAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Mistral API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n    max_concurrent_requests: RuntimeParameter[int] = Field(\n        default=64, description=\"The maximum number of concurrent requests to send.\"\n    )\n\n    _api_key_env_var: str = PrivateAttr(_MISTRALAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"MistralAsyncClient\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `MistralAsyncClient` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from mistralai.async_client import MistralAsyncClient\n        except ImportError as ie:\n            raise ImportError(\n                \"MistralAI Python client is not installed. Please install it using\"\n                \" `pip install mistralai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = MistralAsyncClient(\n            api_key=self.api_key.get_secret_value(),\n            endpoint=self.endpoint,\n            max_retries=self.max_retries,\n            timeout=self.timeout,\n            max_concurrent_requests=self.max_concurrent_requests,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"mistral\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    # TODO: add `num_generations` parameter once Mistral client allows `n` parameter\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        max_new_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the MistralAI async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"max_tokens\": max_new_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n        }\n        generations = []\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n            # TODO:\u00a0This should work just with the _aclient.chat method, but it's not working.\n            # We need to check instructor and see if we can create a PR.\n            completion = await self._aclient.chat.completions.create(**kwargs)\n        else:\n            completion = await self._aclient.chat(**kwargs)\n\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(\n                    f\"Received no response using MistralAI client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n\n    # TODO: remove this function once Mistral client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.MistralLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.MistralLLM.agenerate","title":"<code>agenerate(input, max_new_tokens=None, temperature=None, top_p=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the MistralAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_new_tokens</code> <code>Optional[int]</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    max_new_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the MistralAI async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"max_tokens\": max_new_tokens,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n    }\n    generations = []\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n        # TODO:\u00a0This should work just with the _aclient.chat method, but it's not working.\n        # We need to check instructor and see if we can create a PR.\n        completion = await self._aclient.chat.completions.create(**kwargs)\n    else:\n        completion = await self._aclient.chat(**kwargs)\n\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(\n                f\"Received no response using MistralAI client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.MistralLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.MistralLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>MistralAsyncClient</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `MistralAsyncClient` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from mistralai.async_client import MistralAsyncClient\n    except ImportError as ie:\n        raise ImportError(\n            \"MistralAI Python client is not installed. Please install it using\"\n            \" `pip install mistralai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = MistralAsyncClient(\n        api_key=self.api_key.get_secret_value(),\n        endpoint=self.endpoint,\n        max_retries=self.max_retries,\n        timeout=self.timeout,\n        max_concurrent_requests=self.max_concurrent_requests,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"mistral\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.OllamaLLM","title":"<code>OllamaLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Ollama LLM implementation running the Async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"notus\".</p> <code>host</code> <code>Optional[RuntimeParameter[str]]</code> <p>the Ollama server host.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the timeout for the LLM. Defaults to <code>120</code>.</p> <code>_aclient</code> <code>Optional[AsyncClient]</code> <p>the <code>AsyncClient</code> to use for the Ollama API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>host</code>: the Ollama server host.</li> <li><code>timeout</code>: the client timeout for the Ollama API. Defaults to <code>120</code>.</li> </ul> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>class OllamaLLM(AsyncLLM):\n    \"\"\"Ollama LLM implementation running the Async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"notus\".\n        host: the Ollama server host.\n        timeout: the timeout for the LLM. Defaults to `120`.\n        _aclient: the `AsyncClient` to use for the Ollama API. It is meant to be used internally.\n            Set in the `load` method.\n\n    Runtime parameters:\n        - `host`: the Ollama server host.\n        - `timeout`: the client timeout for the Ollama API. Defaults to `120`.\n    \"\"\"\n\n    model: str\n    host: Optional[RuntimeParameter[str]] = Field(\n        default=None, description=\"The host of the Ollama API.\"\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120, description=\"The timeout for the Ollama API.\"\n    )\n    follow_redirects: bool = True\n\n    _aclient: Optional[\"AsyncClient\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncClient` to use Ollama async API.\"\"\"\n        super().load()\n\n        try:\n            from ollama import AsyncClient\n\n            self._aclient = AsyncClient(\n                host=self.host,\n                timeout=self.timeout,\n                follow_redirects=self.follow_redirects,\n            )\n        except ImportError as e:\n            raise ImportError(\n                \"Ollama Python client is not installed. Please install it using\"\n                \" `pip install ollama`.\"\n            ) from e\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        format: Literal[\"\", \"json\"] = \"\",\n        # TODO: include relevant options from `Options` in `agenerate` method.\n        options: Union[Options, None] = None,\n        keep_alive: Union[bool, None] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Generates a response asynchronously, using the [Ollama Async API definition](https://github.com/ollama/ollama-python).\n\n        Args:\n            input: the input to use for the generation.\n            num_generations: the number of generations to produce. Defaults to `1`.\n            format: the format to use for the generation. Defaults to `\"\"`.\n            options: the options to use for the generation. Defaults to `None`.\n            keep_alive: whether to keep the connection alive. Defaults to `None`.\n\n        Returns:\n            A list of strings as completion for the given input.\n        \"\"\"\n        generations = []\n        # TODO: remove this for-loop and override the `generate` method\n        for _ in range(num_generations):\n            completion = await self._aclient.chat(  # type: ignore\n                model=self.model,\n                messages=input,  # type: ignore\n                stream=False,\n                format=format,\n                options=options,\n                keep_alive=keep_alive,\n            )\n            # TODO: improve error handling\n            generations.append(completion[\"message\"][\"content\"])\n\n        return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.OllamaLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.OllamaLLM.agenerate","title":"<code>agenerate(input, num_generations=1, format='', options=None, keep_alive=None)</code>  <code>async</code>","text":"<p>Generates a response asynchronously, using the Ollama Async API definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>the input to use for the generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to produce. Defaults to <code>1</code>.</p> <code>1</code> <code>format</code> <code>Literal['', 'json']</code> <p>the format to use for the generation. Defaults to <code>\"\"</code>.</p> <code>''</code> <code>options</code> <code>Union[Options, None]</code> <p>the options to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>keep_alive</code> <code>Union[bool, None]</code> <p>whether to keep the connection alive. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings as completion for the given input.</p> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    format: Literal[\"\", \"json\"] = \"\",\n    # TODO: include relevant options from `Options` in `agenerate` method.\n    options: Union[Options, None] = None,\n    keep_alive: Union[bool, None] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Generates a response asynchronously, using the [Ollama Async API definition](https://github.com/ollama/ollama-python).\n\n    Args:\n        input: the input to use for the generation.\n        num_generations: the number of generations to produce. Defaults to `1`.\n        format: the format to use for the generation. Defaults to `\"\"`.\n        options: the options to use for the generation. Defaults to `None`.\n        keep_alive: whether to keep the connection alive. Defaults to `None`.\n\n    Returns:\n        A list of strings as completion for the given input.\n    \"\"\"\n    generations = []\n    # TODO: remove this for-loop and override the `generate` method\n    for _ in range(num_generations):\n        completion = await self._aclient.chat(  # type: ignore\n            model=self.model,\n            messages=input,  # type: ignore\n            stream=False,\n            format=format,\n            options=options,\n            keep_alive=keep_alive,\n        )\n        # TODO: improve error handling\n        generations.append(completion[\"message\"][\"content\"])\n\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.OllamaLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncClient</code> to use Ollama async API.</p> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncClient` to use Ollama async API.\"\"\"\n    super().load()\n\n    try:\n        from ollama import AsyncClient\n\n        self._aclient = AsyncClient(\n            host=self.host,\n            timeout=self.timeout,\n            follow_redirects=self.follow_redirects,\n        )\n    except ImportError as e:\n        raise ImportError(\n            \"Ollama Python client is not installed. Please install it using\"\n            \" `pip install ollama`.\"\n        ) from e\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.OpenAILLM","title":"<code>OpenAILLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>OpenAI LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gpt-3.5-turbo\", \"gpt-4\", etc. Supported models can be found here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the OpenAI API requests. Defaults to <code>None</code>, which means that the value set for the environment variable <code>OPENAI_BASE_URL</code> will be used, or \"https://api.openai.com/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of times to retry the request to the API before failing. Defaults to <code>6</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response from the API. Defaults to <code>120</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> Runtime parameters <ul> <li><code>base_url</code>: the base URL to use for the OpenAI API requests. Defaults to <code>None</code>.</li> <li><code>api_key</code>: the API key to authenticate the requests to the OpenAI API. Defaults     to <code>None</code>.</li> <li><code>max_retries</code>: the maximum number of times to retry the request to the API before     failing. Defaults to <code>6</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response from the API. Defaults     to <code>120</code>.</li> </ul> Icon <p><code>:simple-openai:</code></p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>class OpenAILLM(AsyncLLM):\n    \"\"\"OpenAI LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gpt-3.5-turbo\", \"gpt-4\", etc.\n            Supported models can be found [here](https://platform.openai.com/docs/guides/text-generation).\n        base_url: the base URL to use for the OpenAI API requests. Defaults to `None`, which\n            means that the value set for the environment variable `OPENAI_BASE_URL` will\n            be used, or \"https://api.openai.com/v1\" if not set.\n        api_key: the API key to authenticate the requests to the OpenAI API. Defaults to\n            `None` which means that the value set for the environment variable `OPENAI_API_KEY`\n            will be used, or `None` if not set.\n        max_retries: the maximum number of times to retry the request to the API before\n            failing. Defaults to `6`.\n        timeout: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n\n    Runtime parameters:\n        - `base_url`: the base URL to use for the OpenAI API requests. Defaults to `None`.\n        - `api_key`: the API key to authenticate the requests to the OpenAI API. Defaults\n            to `None`.\n        - `max_retries`: the maximum number of times to retry the request to the API before\n            failing. Defaults to `6`.\n        - `timeout`: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n\n    Icon:\n        `:simple-openai:`\n    \"\"\"\n\n    model: str\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"OPENAI_BASE_URL\", \"https://api.openai.com/v1\"\n        ),\n        description=\"The base URL to use for the OpenAI API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_OPENAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the OpenAI API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_OPENAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncOpenAI\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncOpenAI` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from openai import AsyncOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"openai\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        stop: Optional[Union[str, List[str]]] = None,\n        response_format: str = \"text\",\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the OpenAI async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: a string or a list of strings to use as a stop sequence for the generation.\n                Defaults to `None`.\n            response_format: the format of the response to return. Must be one of\n                \"text\" or \"json\". Read the documentation [here](https://platform.openai.com/docs/guides/text-generation/json-mode)\n                for more information on how to use the JSON model from OpenAI. Defaults to `text`.\n\n        Note:\n            If response_format\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        if response_format == \"json\":\n            response_format = \"json_object\"\n        elif response_format != \"text\":\n            raise ValueError(\n                f\"Invalid response format '{response_format}'. Must be either 'text' or 'json'.\"\n            )\n\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"max_tokens\": max_new_tokens,\n            \"n\": num_generations,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"stop\": stop,\n            \"timeout\": 50,\n            \"response_format\": {\"type\": response_format},\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n        completion = await self._aclient.chat.completions.create(**kwargs)\n\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(  # type: ignore\n                    f\"Received no response using OpenAI client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.OpenAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.OpenAILLM.agenerate","title":"<code>agenerate(input, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, stop=None, response_format='text')</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the OpenAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>a string or a list of strings to use as a stop sequence for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>response_format</code> <code>str</code> <p>the format of the response to return. Must be one of \"text\" or \"json\". Read the documentation here for more information on how to use the JSON model from OpenAI. Defaults to <code>text</code>.</p> <code>'text'</code> Note <p>If response_format</p> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stop: Optional[Union[str, List[str]]] = None,\n    response_format: str = \"text\",\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the OpenAI async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: a string or a list of strings to use as a stop sequence for the generation.\n            Defaults to `None`.\n        response_format: the format of the response to return. Must be one of\n            \"text\" or \"json\". Read the documentation [here](https://platform.openai.com/docs/guides/text-generation/json-mode)\n            for more information on how to use the JSON model from OpenAI. Defaults to `text`.\n\n    Note:\n        If response_format\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    if response_format == \"json\":\n        response_format = \"json_object\"\n    elif response_format != \"text\":\n        raise ValueError(\n            f\"Invalid response format '{response_format}'. Must be either 'text' or 'json'.\"\n        )\n\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"max_tokens\": max_new_tokens,\n        \"n\": num_generations,\n        \"frequency_penalty\": frequency_penalty,\n        \"presence_penalty\": presence_penalty,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"stop\": stop,\n        \"timeout\": 50,\n        \"response_format\": {\"type\": response_format},\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n    completion = await self._aclient.chat.completions.create(**kwargs)\n\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(  # type: ignore\n                f\"Received no response using OpenAI client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.OpenAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncOpenAI</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncOpenAI` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from openai import AsyncOpenAI\n    except ImportError as ie:\n        raise ImportError(\n            \"OpenAI Python client is not installed. Please install it using\"\n            \" `pip install openai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = AsyncOpenAI(\n        base_url=self.base_url,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"openai\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.TogetherLLM","title":"<code>TogetherLLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>TogetherLLM LLM implementation running the async API client of OpenAI.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM e.g. \"mistralai/Mixtral-8x7B-Instruct-v0.1\". Supported models can be found here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Together API can be set with <code>TOGETHER_BASE_URL</code>. Defaults to <code>None</code> which means that the value set for the environment variable <code>TOGETHER_BASE_URL</code> will be used, or \"https://api.together.xyz/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Together API. Defaults to <code>None</code> which means that the value set for the environment variable <code>TOGETHER_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> Source code in <code>src/distilabel/llms/together.py</code> <pre><code>class TogetherLLM(OpenAILLM):\n    \"\"\"TogetherLLM LLM implementation running the async API client of OpenAI.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"mistralai/Mixtral-8x7B-Instruct-v0.1\".\n            Supported models can be found [here](https://api.together.xyz/models).\n        base_url: the base URL to use for the Together API can be set with `TOGETHER_BASE_URL`.\n            Defaults to `None` which means that the value set for the environment variable\n            `TOGETHER_BASE_URL` will be used, or \"https://api.together.xyz/v1\" if not set.\n        api_key: the API key to authenticate the requests to the Together API. Defaults to `None`\n            which means that the value set for the environment variable `TOGETHER_API_KEY` will be\n            used, or `None` if not set.\n        _api_key_env_var: the name of the environment variable to use for the API key. It\n            is meant to be used internally.\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"TOGETHER_BASE_URL\", \"https://api.together.xyz/v1\"\n        ),\n        description=\"The base URL to use for the Together API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_TOGETHER_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Together API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_TOGETHER_API_KEY_ENV_VAR_NAME)\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.TransformersLLM","title":"<code>TransformersLLM</code>","text":"<p>               Bases: <code>LLM</code>, <code>CudaDevicePlacementMixin</code></p> <p>Hugging Face <code>transformers</code> library LLM implementation using the text generation pipeline.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model Hugging Face Hub repo id or a path to a directory containing the model weights and configuration files.</p> <code>revision</code> <code>str</code> <p>if <code>model</code> refers to a Hugging Face Hub repository, then the revision (e.g. a branch name or a commit id) to use. Defaults to <code>\"main\"</code>.</p> <code>torch_dtype</code> <code>str</code> <p>the torch dtype to use for the model e.g. \"float16\", \"float32\", etc. Defaults to <code>\"auto\"</code>.</p> <code>trust_remote_code</code> <code>bool</code> <p>whether to trust or not remote (code in the Hugging Face Hub repository) code to load the model. Defaults to <code>False</code>.</p> <code>model_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>from_pretrained</code> method of the model.</p> <code>tokenizer</code> <code>Optional[str]</code> <p>the tokenizer Hugging Face Hub repo id or a path to a directory containing the tokenizer config files. If not provided, the one associated to the <code>model</code> will be used. Defaults to <code>None</code>.</p> <code>use_fast</code> <code>bool</code> <p>whether to use a fast tokenizer or not. Defaults to <code>True</code>.</p> <code>chat_template</code> <code>Optional[str]</code> <p>a chat template that will be used to build the prompts before sending them to the model. If not provided, the chat template defined in the tokenizer config will be used. If not provided and the tokenizer doesn't have a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> <code>device</code> <code>Optional[Union[str, int]]</code> <p>the name or index of the device where the model will be loaded. Defaults to <code>None</code>.</p> <code>device_map</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>a dictionary mapping each layer of the model to a device, or a mode like <code>\"sequential\"</code> or <code>\"auto\"</code>. Defaults to <code>None</code>.</p> <code>token</code> <code>Optional[str]</code> <p>the Hugging Face Hub token that will be used to authenticate to the Hugging Face Hub. If not provided, the <code>HF_TOKEN</code> environment or <code>huggingface_hub</code> package local configuration will be used. Defaults to <code>None</code>.</p> Icon <p><code>:hugging:</code></p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>class TransformersLLM(LLM, CudaDevicePlacementMixin):\n    \"\"\"Hugging Face `transformers` library LLM implementation using the text generation\n    pipeline.\n\n    Attributes:\n        model: the model Hugging Face Hub repo id or a path to a directory containing the\n            model weights and configuration files.\n        revision: if `model` refers to a Hugging Face Hub repository, then the revision\n            (e.g. a branch name or a commit id) to use. Defaults to `\"main\"`.\n        torch_dtype: the torch dtype to use for the model e.g. \"float16\", \"float32\", etc.\n            Defaults to `\"auto\"`.\n        trust_remote_code: whether to trust or not remote (code in the Hugging Face Hub\n            repository) code to load the model. Defaults to `False`.\n        model_kwargs: additional dictionary of keyword arguments that will be passed to\n            the `from_pretrained` method of the model.\n        tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing\n            the tokenizer config files. If not provided, the one associated to the `model`\n            will be used. Defaults to `None`.\n        use_fast: whether to use a fast tokenizer or not. Defaults to `True`.\n        chat_template: a chat template that will be used to build the prompts before\n            sending them to the model. If not provided, the chat template defined in the\n            tokenizer config will be used. If not provided and the tokenizer doesn't have\n            a chat template, then ChatML template will be used. Defaults to `None`.\n        device: the name or index of the device where the model will be loaded. Defaults\n            to `None`.\n        device_map: a dictionary mapping each layer of the model to a device, or a mode\n            like `\"sequential\"` or `\"auto\"`. Defaults to `None`.\n        token: the Hugging Face Hub token that will be used to authenticate to the Hugging\n            Face Hub. If not provided, the `HF_TOKEN` environment or `huggingface_hub` package\n            local configuration will be used. Defaults to `None`.\n\n    Icon:\n        `:hugging:`\n    \"\"\"\n\n    model: str\n    revision: str = \"main\"\n    torch_dtype: str = \"auto\"\n    trust_remote_code: bool = False\n    model_kwargs: Optional[Dict[str, Any]] = None\n    tokenizer: Optional[str] = None\n    use_fast: bool = True\n    chat_template: Optional[str] = None\n    device: Optional[Union[str, int]] = None\n    device_map: Optional[Union[str, Dict[str, Any]]] = None\n    token: Optional[str] = None\n\n    _pipeline: Optional[\"Pipeline\"] = PrivateAttr(...)\n    _prefix_allowed_tokens_fn: Union[Callable, None] = PrivateAttr(default=None)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the model and tokenizer and creates the text generation pipeline. In addition,\n        it will configure the tokenizer chat template.\"\"\"\n        if self.device == \"cuda\":\n            CudaDevicePlacementMixin.load(self)\n\n        try:\n            from transformers import pipeline\n        except ImportError as ie:\n            raise ImportError(\n                \"Transformers is not installed. Please install it using `pip install transformers`.\"\n            ) from ie\n\n        self._pipeline = pipeline(\n            \"text-generation\",\n            model=self.model,\n            revision=self.revision,\n            torch_dtype=self.torch_dtype,\n            trust_remote_code=self.trust_remote_code,\n            model_kwargs=self.model_kwargs or {},\n            tokenizer=self.tokenizer or self.model,\n            use_fast=self.use_fast,\n            device=self.device,\n            device_map=self.device_map,\n            token=self.token or os.getenv(\"HF_TOKEN\"),\n            return_full_text=False,\n        )\n\n        if self.chat_template is not None:\n            self._pipeline.tokenizer.chat_template = self.chat_template  # type: ignore\n        elif (\n            self._pipeline.tokenizer.chat_template is None  # type: ignore\n            and self._pipeline.tokenizer.default_chat_template is None  # type: ignore\n        ):\n            self._pipeline.tokenizer.chat_template = CHATML_TEMPLATE  # type: ignore\n\n        if self.structured_output:\n            self._prefix_allowed_tokens_fn = self._prepare_structured_output(\n                self.structured_output\n            )\n\n        super().load()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def prepare_input(self, input: \"StandardInput\") -&gt; str:\n        \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n        as an OpenAI conversation, and adding the generation prompt.\n        \"\"\"\n        return self._pipeline.tokenizer.apply_chat_template(  # type: ignore\n            input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        temperature: float = 0.1,\n        repetition_penalty: float = 1.1,\n        top_p: float = 1.0,\n        top_k: int = 0,\n        do_sample: bool = True,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for each input using the text generation\n        pipeline.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            repetition_penalty: the repetition penalty to use for the generation. Defaults\n                to `1.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            top_k: the top-k value to use for the generation. Defaults to `0`.\n            do_sample: whether to use sampling or not. Defaults to `True`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        prepared_inputs = [self.prepare_input(input=input) for input in inputs]\n\n        outputs: List[List[Dict[str, str]]] = self._pipeline(  # type: ignore\n            prepared_inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            repetition_penalty=repetition_penalty,\n            top_p=top_p,\n            top_k=top_k,\n            do_sample=do_sample,\n            num_return_sequences=num_generations,\n            prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n        )\n        return [\n            [generation[\"generated_text\"] for generation in output]\n            for output in outputs\n        ]\n\n    def get_last_hidden_states(\n        self, inputs: List[\"StandardInput\"]\n    ) -&gt; List[\"HiddenState\"]:\n        \"\"\"Gets the last `hidden_states` of the model for the given inputs. It doesn't\n        execute the task head.\n\n        Args:\n            inputs: a list of inputs in chat format to generate the embeddings for.\n\n        Returns:\n            A list containing the last hidden state for each sequence using a NumPy array\n            with shape [num_tokens, hidden_size].\n        \"\"\"\n        model: \"PreTrainedModel\" = (\n            self._pipeline.model.model  # type: ignore\n            if hasattr(self._pipeline.model, \"model\")  # type: ignore\n            else next(self._pipeline.model.children())  # type: ignore\n        )\n        tokenizer: \"PreTrainedTokenizer\" = self._pipeline.tokenizer  # type: ignore\n        input_ids = tokenizer(\n            [self.prepare_input(input) for input in inputs],  # type: ignore\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(model.device)\n        last_hidden_states = model(**input_ids)[\"last_hidden_state\"]\n\n        return [\n            seq_last_hidden_state[attention_mask.bool(), :].detach().cpu().numpy()\n            for seq_last_hidden_state, attention_mask in zip(\n                last_hidden_states,\n                input_ids[\"attention_mask\"],  # type: ignore\n            )\n        ]\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Callable, None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(\n            structured_output, \"transformers\", self._pipeline\n        )\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.TransformersLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.TransformersLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, temperature=0.1, repetition_penalty=1.1, top_p=1.0, top_k=0, do_sample=True)</code>","text":"<p>Generates <code>num_generations</code> responses for each input using the text generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>0.1</code> <code>repetition_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>1.1</code>.</p> <code>1.1</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to use for the generation. Defaults to <code>0</code>.</p> <code>0</code> <code>do_sample</code> <code>bool</code> <p>whether to use sampling or not. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    temperature: float = 0.1,\n    repetition_penalty: float = 1.1,\n    top_p: float = 1.0,\n    top_k: int = 0,\n    do_sample: bool = True,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for each input using the text generation\n    pipeline.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        repetition_penalty: the repetition penalty to use for the generation. Defaults\n            to `1.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        top_k: the top-k value to use for the generation. Defaults to `0`.\n        do_sample: whether to use sampling or not. Defaults to `True`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    prepared_inputs = [self.prepare_input(input=input) for input in inputs]\n\n    outputs: List[List[Dict[str, str]]] = self._pipeline(  # type: ignore\n        prepared_inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        top_p=top_p,\n        top_k=top_k,\n        do_sample=do_sample,\n        num_return_sequences=num_generations,\n        prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n    )\n    return [\n        [generation[\"generated_text\"] for generation in output]\n        for output in outputs\n    ]\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.TransformersLLM.get_last_hidden_states","title":"<code>get_last_hidden_states(inputs)</code>","text":"<p>Gets the last <code>hidden_states</code> of the model for the given inputs. It doesn't execute the task head.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate the embeddings for.</p> required <p>Returns:</p> Type Description <code>List[HiddenState]</code> <p>A list containing the last hidden state for each sequence using a NumPy array</p> <code>List[HiddenState]</code> <p>with shape [num_tokens, hidden_size].</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def get_last_hidden_states(\n    self, inputs: List[\"StandardInput\"]\n) -&gt; List[\"HiddenState\"]:\n    \"\"\"Gets the last `hidden_states` of the model for the given inputs. It doesn't\n    execute the task head.\n\n    Args:\n        inputs: a list of inputs in chat format to generate the embeddings for.\n\n    Returns:\n        A list containing the last hidden state for each sequence using a NumPy array\n        with shape [num_tokens, hidden_size].\n    \"\"\"\n    model: \"PreTrainedModel\" = (\n        self._pipeline.model.model  # type: ignore\n        if hasattr(self._pipeline.model, \"model\")  # type: ignore\n        else next(self._pipeline.model.children())  # type: ignore\n    )\n    tokenizer: \"PreTrainedTokenizer\" = self._pipeline.tokenizer  # type: ignore\n    input_ids = tokenizer(\n        [self.prepare_input(input) for input in inputs],  # type: ignore\n        return_tensors=\"pt\",\n        padding=True,\n    ).to(model.device)\n    last_hidden_states = model(**input_ids)[\"last_hidden_state\"]\n\n    return [\n        seq_last_hidden_state[attention_mask.bool(), :].detach().cpu().numpy()\n        for seq_last_hidden_state, attention_mask in zip(\n            last_hidden_states,\n            input_ids[\"attention_mask\"],  # type: ignore\n        )\n    ]\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.TransformersLLM.load","title":"<code>load()</code>","text":"<p>Loads the model and tokenizer and creates the text generation pipeline. In addition, it will configure the tokenizer chat template.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the model and tokenizer and creates the text generation pipeline. In addition,\n    it will configure the tokenizer chat template.\"\"\"\n    if self.device == \"cuda\":\n        CudaDevicePlacementMixin.load(self)\n\n    try:\n        from transformers import pipeline\n    except ImportError as ie:\n        raise ImportError(\n            \"Transformers is not installed. Please install it using `pip install transformers`.\"\n        ) from ie\n\n    self._pipeline = pipeline(\n        \"text-generation\",\n        model=self.model,\n        revision=self.revision,\n        torch_dtype=self.torch_dtype,\n        trust_remote_code=self.trust_remote_code,\n        model_kwargs=self.model_kwargs or {},\n        tokenizer=self.tokenizer or self.model,\n        use_fast=self.use_fast,\n        device=self.device,\n        device_map=self.device_map,\n        token=self.token or os.getenv(\"HF_TOKEN\"),\n        return_full_text=False,\n    )\n\n    if self.chat_template is not None:\n        self._pipeline.tokenizer.chat_template = self.chat_template  # type: ignore\n    elif (\n        self._pipeline.tokenizer.chat_template is None  # type: ignore\n        and self._pipeline.tokenizer.default_chat_template is None  # type: ignore\n    ):\n        self._pipeline.tokenizer.chat_template = CHATML_TEMPLATE  # type: ignore\n\n    if self.structured_output:\n        self._prefix_allowed_tokens_fn = self._prepare_structured_output(\n            self.structured_output\n        )\n\n    super().load()\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.TransformersLLM.prepare_input","title":"<code>prepare_input(input)</code>","text":"<p>Prepares the input by applying the chat template to the input, which is formatted as an OpenAI conversation, and adding the generation prompt.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def prepare_input(self, input: \"StandardInput\") -&gt; str:\n    \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n    as an OpenAI conversation, and adding the generation prompt.\n    \"\"\"\n    return self._pipeline.tokenizer.apply_chat_template(  # type: ignore\n        input,  # type: ignore\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.VertexAILLM","title":"<code>VertexAILLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>VertexAI LLM implementation running the async API clients for Gemini.</p> <ul> <li>Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini</li> </ul> <p>To use the <code>VertexAILLM</code> is necessary to have configured the Google Cloud authentication using one of these methods:</p> <ul> <li>Setting <code>GOOGLE_CLOUD_CREDENTIALS</code> environment variable</li> <li>Using <code>gcloud auth application-default login</code> command</li> <li>Using <code>vertexai.init</code> function from the <code>google-cloud-aiplatform</code> library</li> </ul> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gemini-1.0-pro\". Supported models.</p> <code>_aclient</code> <code>Optional[GenerativeModel]</code> <p>the <code>GenerativeModel</code> to use for the Vertex AI Gemini API. It is meant to be used internally. Set in the <code>load</code> method.</p> Icon <p><code>:simple-googlecloud:</code></p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>class VertexAILLM(AsyncLLM):\n    \"\"\"VertexAI LLM implementation running the async API clients for Gemini.\n\n    - Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n\n    To use the `VertexAILLM` is necessary to have configured the Google Cloud authentication\n    using one of these methods:\n\n    - Setting `GOOGLE_CLOUD_CREDENTIALS` environment variable\n    - Using `gcloud auth application-default login` command\n    - Using `vertexai.init` function from the `google-cloud-aiplatform` library\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gemini-1.0-pro\". [Supported models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models).\n        _aclient: the `GenerativeModel` to use for the Vertex AI Gemini API. It is meant\n            to be used internally. Set in the `load` method.\n\n    Icon:\n        `:simple-googlecloud:`\n    \"\"\"\n\n    model: str\n\n    _aclient: Optional[\"GenerativeModel\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `GenerativeModel` class which has access to `generate_content_async` to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from vertexai.generative_models import GenerationConfig, GenerativeModel\n\n            self._generation_config_class = GenerationConfig\n        except ImportError as e:\n            raise ImportError(\n                \"vertexai is not installed. Please install it using\"\n                \" `pip install google-cloud-aiplatform`.\"\n            ) from e\n\n        if _is_gemini_model(self.model):\n            self._aclient = GenerativeModel(model_name=self.model)\n        else:\n            raise NotImplementedError(\n                \"`VertexAILLM` is only implemented for `gemini` models that allow for `ChatType` data.\"\n            )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def _chattype_to_content(self, input: \"StandardInput\") -&gt; List[\"Content\"]:\n        \"\"\"Converts a chat type to a list of content items expected by the API.\n\n        Args:\n            input: the chat type to be converted.\n\n        Returns:\n            List[str]: a list of content items expected by the API.\n        \"\"\"\n        from vertexai.generative_models import Content, Part\n\n        contents = []\n        for message in input:\n            if message[\"role\"] not in [\"user\", \"model\"]:\n                raise ValueError(\n                    \"`VertexAILLM only supports the roles 'user' or 'model'.\"\n                )\n            contents.append(\n                Content(\n                    role=message[\"role\"], parts=[Part.from_text(message[\"content\"])]\n                )\n            )\n        return contents\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        max_output_tokens: Optional[int] = None,\n        stop_sequences: Optional[List[str]] = None,\n        safety_settings: Optional[Dict[str, Any]] = None,\n        tools: Optional[List[Dict[str, Any]]] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the [VertexAI async client definition](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            temperature: Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to `None`.\n            top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to `None`.\n            top_k: If specified, top-k sampling will be used. Defaults to `None`.\n            max_output_tokens: The maximum number of output tokens to generate per message. Defaults to `None`.\n            stop_sequences: A list of stop sequences. Defaults to `None`.\n            safety_settings: Safety configuration for returned content from the API. Defaults to `None`.\n            tools: A potential list of tools that can be used by the API. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        from vertexai.generative_models import GenerationConfig\n\n        contents = self._chattype_to_content(input)\n        generations = []\n        # TODO: remove this for-loop and override `generate`\n        for _ in range(num_generations):\n            content = await self._aclient.generate_content_async(  # type: ignore\n                contents=contents,\n                generation_config=GenerationConfig(\n                    candidate_count=1,  # only one candidate allowed per call\n                    temperature=temperature,\n                    top_k=top_k,\n                    top_p=top_p,\n                    max_output_tokens=max_output_tokens,\n                    stop_sequences=stop_sequences,\n                ),\n                safety_settings=safety_settings,\n                tools=tools,\n                stream=False,\n            )\n\n            text = None\n            try:\n                text = content.candidates[0].text\n            except ValueError:\n                self._logger.warning(\n                    f\"Received no response using VertexAI client (model: '{self.model}').\"\n                    f\" Finish reason was: '{content.candidates[0].finish_reason}'.\"\n                )\n            generations.append(text)\n\n        return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.VertexAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.VertexAILLM.agenerate","title":"<code>agenerate(input, num_generations=1, temperature=None, top_p=None, top_k=None, max_output_tokens=None, stop_sequences=None, safety_settings=None, tools=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the VertexAI async client definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>temperature</code> <code>Optional[float]</code> <p>Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to <code>None</code>.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to <code>None</code>.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>If specified, top-k sampling will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_output_tokens</code> <code>Optional[int]</code> <p>The maximum number of output tokens to generate per message. Defaults to <code>None</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>A list of stop sequences. Defaults to <code>None</code>.</p> <code>None</code> <code>safety_settings</code> <code>Optional[Dict[str, Any]]</code> <p>Safety configuration for returned content from the API. Defaults to <code>None</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Dict[str, Any]]]</code> <p>A potential list of tools that can be used by the API. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    safety_settings: Optional[Dict[str, Any]] = None,\n    tools: Optional[List[Dict[str, Any]]] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the [VertexAI async client definition](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to `None`.\n        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to `None`.\n        top_k: If specified, top-k sampling will be used. Defaults to `None`.\n        max_output_tokens: The maximum number of output tokens to generate per message. Defaults to `None`.\n        stop_sequences: A list of stop sequences. Defaults to `None`.\n        safety_settings: Safety configuration for returned content from the API. Defaults to `None`.\n        tools: A potential list of tools that can be used by the API. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    from vertexai.generative_models import GenerationConfig\n\n    contents = self._chattype_to_content(input)\n    generations = []\n    # TODO: remove this for-loop and override `generate`\n    for _ in range(num_generations):\n        content = await self._aclient.generate_content_async(  # type: ignore\n            contents=contents,\n            generation_config=GenerationConfig(\n                candidate_count=1,  # only one candidate allowed per call\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                max_output_tokens=max_output_tokens,\n                stop_sequences=stop_sequences,\n            ),\n            safety_settings=safety_settings,\n            tools=tools,\n            stream=False,\n        )\n\n        text = None\n        try:\n            text = content.candidates[0].text\n        except ValueError:\n            self._logger.warning(\n                f\"Received no response using VertexAI client (model: '{self.model}').\"\n                f\" Finish reason was: '{content.candidates[0].finish_reason}'.\"\n            )\n        generations.append(text)\n\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.VertexAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>GenerativeModel</code> class which has access to <code>generate_content_async</code> to benefit from async requests.</p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `GenerativeModel` class which has access to `generate_content_async` to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from vertexai.generative_models import GenerationConfig, GenerativeModel\n\n        self._generation_config_class = GenerationConfig\n    except ImportError as e:\n        raise ImportError(\n            \"vertexai is not installed. Please install it using\"\n            \" `pip install google-cloud-aiplatform`.\"\n        ) from e\n\n    if _is_gemini_model(self.model):\n        self._aclient = GenerativeModel(model_name=self.model)\n    else:\n        raise NotImplementedError(\n            \"`VertexAILLM` is only implemented for `gemini` models that allow for `ChatType` data.\"\n        )\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.vLLM","title":"<code>vLLM</code>","text":"<p>               Bases: <code>LLM</code>, <code>CudaDevicePlacementMixin</code></p> <p><code>vLLM</code> library LLM implementation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model Hugging Face Hub repo id or a path to a directory containing the model weights and configuration files.</p> <code>dtype</code> <code>str</code> <p>the data type to use for the model. Defaults to <code>auto</code>.</p> <code>trust_remote_code</code> <code>bool</code> <p>whether to trust the remote code when loading the model. Defaults to <code>False</code>.</p> <code>quantization</code> <code>Optional[str]</code> <p>the quantization mode to use for the model. Defaults to <code>None</code>.</p> <code>revision</code> <code>Optional[str]</code> <p>the revision of the model to load. Defaults to <code>None</code>.</p> <code>tokenizer</code> <code>Optional[str]</code> <p>the tokenizer Hugging Face Hub repo id or a path to a directory containing the tokenizer files. If not provided, the tokenizer will be loaded from the model directory. Defaults to <code>None</code>.</p> <code>tokenizer_mode</code> <code>Literal['auto', 'slow']</code> <p>the mode to use for the tokenizer. Defaults to <code>auto</code>.</p> <code>tokenizer_revision</code> <code>Optional[str]</code> <p>the revision of the tokenizer to load. Defaults to <code>None</code>.</p> <code>skip_tokenizer_init</code> <code>bool</code> <p>whether to skip the initialization of the tokenizer. Defaults to <code>False</code>.</p> <code>chat_template</code> <code>Optional[str]</code> <p>a chat template that will be used to build the prompts before sending them to the model. If not provided, the chat template defined in the tokenizer config will be used. If not provided and the tokenizer doesn't have a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> <code>structured_output</code> <code>Optional[str]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>seed</code> <code>int</code> <p>the seed to use for the random number generator. Defaults to <code>0</code>.</p> <code>extra_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>LLM</code> class of <code>vllm</code> library. Defaults to <code>{}</code>.</p> <code>_model</code> <code>Optional[LLM]</code> <p>the <code>vLLM</code> model instance. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> <code>_tokenizer</code> <code>Optional[PreTrainedTokenizer]</code> <p>the tokenizer instance used to format the prompt before passing it to the <code>LLM</code>. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> References <ul> <li>https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py</li> </ul> Runtime parameters <ul> <li><code>extra_kwargs</code>: additional dictionary of keyword arguments that will be passed to     the <code>LLM</code> class of <code>vllm</code> library.</li> </ul> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>class vLLM(LLM, CudaDevicePlacementMixin):\n    \"\"\"`vLLM` library LLM implementation.\n\n    Attributes:\n        model: the model Hugging Face Hub repo id or a path to a directory containing the\n            model weights and configuration files.\n        dtype: the data type to use for the model. Defaults to `auto`.\n        trust_remote_code: whether to trust the remote code when loading the model. Defaults\n            to `False`.\n        quantization: the quantization mode to use for the model. Defaults to `None`.\n        revision: the revision of the model to load. Defaults to `None`.\n        tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing\n            the tokenizer files. If not provided, the tokenizer will be loaded from the\n            model directory. Defaults to `None`.\n        tokenizer_mode: the mode to use for the tokenizer. Defaults to `auto`.\n        tokenizer_revision: the revision of the tokenizer to load. Defaults to `None`.\n        skip_tokenizer_init: whether to skip the initialization of the tokenizer. Defaults\n            to `False`.\n        chat_template: a chat template that will be used to build the prompts before\n            sending them to the model. If not provided, the chat template defined in the\n            tokenizer config will be used. If not provided and the tokenizer doesn't have\n            a chat template, then ChatML template will be used. Defaults to `None`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        seed: the seed to use for the random number generator. Defaults to `0`.\n        extra_kwargs: additional dictionary of keyword arguments that will be passed to the\n            `LLM` class of `vllm` library. Defaults to `{}`.\n        _model: the `vLLM` model instance. This attribute is meant to be used internally\n            and should not be accessed directly. It will be set in the `load` method.\n        _tokenizer: the tokenizer instance used to format the prompt before passing it to\n            the `LLM`. This attribute is meant to be used internally and should not be\n            accessed directly. It will be set in the `load` method.\n\n    References:\n        - https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\n\n    Runtime parameters:\n        - `extra_kwargs`: additional dictionary of keyword arguments that will be passed to\n            the `LLM` class of `vllm` library.\n    \"\"\"\n\n    model: str\n    dtype: str = \"auto\"\n    trust_remote_code: bool = False\n    quantization: Optional[str] = None\n    revision: Optional[str] = None\n\n    tokenizer: Optional[str] = None\n    tokenizer_mode: Literal[\"auto\", \"slow\"] = \"auto\"\n    tokenizer_revision: Optional[str] = None\n    skip_tokenizer_init: bool = False\n    chat_template: Optional[str] = None\n\n    seed: int = 0\n\n    extra_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"Additional dictionary of keyword arguments that will be passed to the\"\n        \" `vLLM` class of `vllm` library. See all the supported arguments at: \"\n        \"https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\",\n    )\n\n    _model: Optional[\"_vLLM\"] = PrivateAttr(...)\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(...)\n    _logits_processor: Optional[Callable] = PrivateAttr(default=None)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `vLLM` model using either the path or the Hugging Face Hub repository id.\n        Additionally, this method also sets the `chat_template` for the tokenizer, so as to properly\n        parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the\n        default value is ChatML format, unless explicitly provided.\n        \"\"\"\n        super().load()\n\n        CudaDevicePlacementMixin.load(self)\n\n        try:\n            from vllm import LLM as _vLLM\n            from vllm import SamplingParams as _SamplingParams\n\n            global SamplingParams\n            SamplingParams = _SamplingParams\n        except ImportError as ie:\n            raise ImportError(\n                \"vLLM is not installed. Please install it using `pip install vllm`.\"\n            ) from ie\n\n        self._model = _vLLM(\n            self.model,\n            dtype=self.dtype,\n            trust_remote_code=self.trust_remote_code,\n            quantization=self.quantization,\n            revision=self.revision,\n            tokenizer=self.tokenizer,\n            tokenizer_mode=self.tokenizer_mode,\n            tokenizer_revision=self.tokenizer_revision,\n            skip_tokenizer_init=self.skip_tokenizer_init,\n            seed=self.seed,\n            **self.extra_kwargs,\n        )\n\n        self._tokenizer = self._model.get_tokenizer()  # type: ignore\n        if self.chat_template is not None:\n            self._tokenizer.chat_template = self.chat_template  # type: ignore\n        elif (\n            self._tokenizer.chat_template is None  # type: ignore\n            and self._tokenizer.default_chat_template is None  # type: ignore\n        ):\n            self._tokenizer.chat_template = CHATML_TEMPLATE\n\n        if self.structured_output:\n            self._logits_processor = self._prepare_structured_output(\n                self.structured_output\n            )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def prepare_input(self, input: \"StandardInput\") -&gt; str:\n        \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n        as an OpenAI conversation, and adding the generation prompt.\n        \"\"\"\n        return self._tokenizer.apply_chat_template(  # type: ignore\n            input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,  # type: ignore\n        )\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        extra_sampling_params: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for each input using the text generation\n        pipeline.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            top_k: the top-k value to use for the generation. Defaults to `0`.\n            extra_sampling_params: dictionary with additional arguments to be passed to\n                the `SamplingParams` class from `vllm`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        prepared_inputs = [self.prepare_input(input) for input in inputs]\n\n        if extra_sampling_params is None:\n            extra_sampling_params = {}\n\n        sampling_params = SamplingParams(  # type: ignore\n            n=num_generations,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            max_tokens=max_new_tokens,\n            logits_processors=(\n                [self._logits_processor] if self._logits_processor else None\n            ),\n            **extra_sampling_params,\n        )\n\n        batch_outputs = self._model.generate(  # type: ignore\n            prepared_inputs,\n            sampling_params,\n            use_tqdm=False,  # type: ignore\n        )\n        return [\n            [output.text for output in outputs.outputs] for outputs in batch_outputs\n        ]\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Callable, None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(structured_output, \"vllm\", self._model)\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.vLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/#distilabel.llms.vLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, extra_sampling_params=None)</code>","text":"<p>Generates <code>num_generations</code> responses for each input using the text generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to use for the generation. Defaults to <code>0</code>.</p> <code>-1</code> <code>extra_sampling_params</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary with additional arguments to be passed to the <code>SamplingParams</code> class from <code>vllm</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = -1,\n    extra_sampling_params: Optional[Dict[str, Any]] = None,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for each input using the text generation\n    pipeline.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        top_k: the top-k value to use for the generation. Defaults to `0`.\n        extra_sampling_params: dictionary with additional arguments to be passed to\n            the `SamplingParams` class from `vllm`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    prepared_inputs = [self.prepare_input(input) for input in inputs]\n\n    if extra_sampling_params is None:\n        extra_sampling_params = {}\n\n    sampling_params = SamplingParams(  # type: ignore\n        n=num_generations,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        max_tokens=max_new_tokens,\n        logits_processors=(\n            [self._logits_processor] if self._logits_processor else None\n        ),\n        **extra_sampling_params,\n    )\n\n    batch_outputs = self._model.generate(  # type: ignore\n        prepared_inputs,\n        sampling_params,\n        use_tqdm=False,  # type: ignore\n    )\n    return [\n        [output.text for output in outputs.outputs] for outputs in batch_outputs\n    ]\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.vLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>vLLM</code> model using either the path or the Hugging Face Hub repository id. Additionally, this method also sets the <code>chat_template</code> for the tokenizer, so as to properly parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the default value is ChatML format, unless explicitly provided.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `vLLM` model using either the path or the Hugging Face Hub repository id.\n    Additionally, this method also sets the `chat_template` for the tokenizer, so as to properly\n    parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the\n    default value is ChatML format, unless explicitly provided.\n    \"\"\"\n    super().load()\n\n    CudaDevicePlacementMixin.load(self)\n\n    try:\n        from vllm import LLM as _vLLM\n        from vllm import SamplingParams as _SamplingParams\n\n        global SamplingParams\n        SamplingParams = _SamplingParams\n    except ImportError as ie:\n        raise ImportError(\n            \"vLLM is not installed. Please install it using `pip install vllm`.\"\n        ) from ie\n\n    self._model = _vLLM(\n        self.model,\n        dtype=self.dtype,\n        trust_remote_code=self.trust_remote_code,\n        quantization=self.quantization,\n        revision=self.revision,\n        tokenizer=self.tokenizer,\n        tokenizer_mode=self.tokenizer_mode,\n        tokenizer_revision=self.tokenizer_revision,\n        skip_tokenizer_init=self.skip_tokenizer_init,\n        seed=self.seed,\n        **self.extra_kwargs,\n    )\n\n    self._tokenizer = self._model.get_tokenizer()  # type: ignore\n    if self.chat_template is not None:\n        self._tokenizer.chat_template = self.chat_template  # type: ignore\n    elif (\n        self._tokenizer.chat_template is None  # type: ignore\n        and self._tokenizer.default_chat_template is None  # type: ignore\n    ):\n        self._tokenizer.chat_template = CHATML_TEMPLATE\n\n    if self.structured_output:\n        self._logits_processor = self._prepare_structured_output(\n            self.structured_output\n        )\n</code></pre>"},{"location":"reference/distilabel/llms/#distilabel.llms.vLLM.prepare_input","title":"<code>prepare_input(input)</code>","text":"<p>Prepares the input by applying the chat template to the input, which is formatted as an OpenAI conversation, and adding the generation prompt.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>def prepare_input(self, input: \"StandardInput\") -&gt; str:\n    \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n    as an OpenAI conversation, and adding the generation prompt.\n    \"\"\"\n    return self._tokenizer.apply_chat_template(  # type: ignore\n        input,  # type: ignore\n        tokenize=False,\n        add_generation_prompt=True,  # type: ignore\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/anthropic/","title":"Anthropic","text":""},{"location":"reference/distilabel/llms/anthropic/#distilabel.llms.anthropic.AnthropicLLM","title":"<code>AnthropicLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Anthropic LLM implementation running the Async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the name of the model to use for the LLM e.g. \"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", etc. Available models can be checked here: Anthropic: Models overview.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Anthropic API. If not provided, it will be read from <code>ANTHROPIC_API_KEY</code> environment variable.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Anthropic API. Defaults to <code>None</code> which means that <code>https://api.anthropic.com</code> will be used internally.</p> <code>timeout</code> <code>RuntimeParameter[float]</code> <p>the maximum time in seconds to wait for a response. Defaults to <code>600.0</code>.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>The maximum number of times to retry the request before failing. Defaults to <code>6</code>.</p> <code>http_client</code> <code>Optional[AsyncClient]</code> <p>if provided, an alternative HTTP client to use for calling Anthropic API. Defaults to <code>None</code>.</p> <code>structured_output</code> <code>Optional[AsyncClient]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. Defaults to None.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> <code>_aclient</code> <code>Optional[AsyncAnthropic]</code> <p>the <code>AsyncAnthropic</code> client to use for the Anthropic API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>api_key</code>: the API key to authenticate the requests to the Anthropic API. If not     provided, it will be read from <code>ANTHROPIC_API_KEY</code> environment variable.</li> <li><code>base_url</code>: the base URL to use for the Anthropic API. Defaults to <code>\"https://api.anthropic.com\"</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response. Defaults to <code>600.0</code>.</li> <li><code>max_retries</code>: the maximum number of times to retry the request before failing.     Defaults to <code>6</code>.</li> </ul> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>class AnthropicLLM(AsyncLLM):\n    \"\"\"Anthropic LLM implementation running the Async API client.\n\n    Attributes:\n        model: the name of the model to use for the LLM e.g. \"claude-3-opus-20240229\",\n            \"claude-3-sonnet-20240229\", etc. Available models can be checked here:\n            [Anthropic: Models overview](https://docs.anthropic.com/claude/docs/models-overview).\n        api_key: the API key to authenticate the requests to the Anthropic API. If not provided,\n            it will be read from `ANTHROPIC_API_KEY` environment variable.\n        base_url: the base URL to use for the Anthropic API. Defaults to `None` which means\n            that `https://api.anthropic.com` will be used internally.\n        timeout: the maximum time in seconds to wait for a response. Defaults to `600.0`.\n        max_retries: The maximum number of times to retry the request before failing. Defaults\n            to `6`.\n        http_client: if provided, an alternative HTTP client to use for calling Anthropic\n            API. Defaults to `None`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. Defaults to None.\n        _api_key_env_var: the name of the environment variable to use for the API key. It\n            is meant to be used internally.\n        _aclient: the `AsyncAnthropic` client to use for the Anthropic API. It is meant\n            to be used internally. Set in the `load` method.\n\n    Runtime parameters:\n        - `api_key`: the API key to authenticate the requests to the Anthropic API. If not\n            provided, it will be read from `ANTHROPIC_API_KEY` environment variable.\n        - `base_url`: the base URL to use for the Anthropic API. Defaults to `\"https://api.anthropic.com\"`.\n        - `timeout`: the maximum time in seconds to wait for a response. Defaults to `600.0`.\n        - `max_retries`: the maximum number of times to retry the request before failing.\n            Defaults to `6`.\n    \"\"\"\n\n    model: str\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"ANTHROPIC_BASE_URL\", \"https://api.anthropic.com\"\n        ),\n        description=\"The base URL to use for the Anthropic API.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_ANTHROPIC_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Anthropic API.\",\n    )\n    timeout: RuntimeParameter[float] = Field(\n        default=600.0,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    http_client: Optional[AsyncClient] = Field(default=None, exclude=True)\n\n    _api_key_env_var: str = PrivateAttr(default=_ANTHROPIC_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncAnthropic\"] = PrivateAttr(...)\n\n    def _check_model_exists(self) -&gt; None:\n        \"\"\"Checks if the specified model exists in the available models.\"\"\"\n        from anthropic import AsyncAnthropic\n\n        annotation = get_type_hints(AsyncAnthropic().messages.create).get(\"model\", None)\n        models = [\n            value\n            for type_ in get_args(annotation)\n            if get_origin(type_) is Literal\n            for value in get_args(type_)\n        ]\n\n        if self.model not in models:\n            raise ValueError(\n                f\"Model {self.model} does not exist among available models. \"\n                f\"The available models are {', '.join(models)}\"\n            )\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncAnthropic` client to use the Anthropic async API.\"\"\"\n        super().load()\n\n        try:\n            from anthropic import AsyncAnthropic\n        except ImportError as ie:\n            raise ImportError(\n                \"Anthropic Python client is not installed. Please install it using\"\n                \" `pip install anthropic`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._check_model_exists()\n\n        self._aclient = AsyncAnthropic(\n            api_key=self.api_key.get_secret_value(),\n            base_url=self.base_url,\n            timeout=self.timeout,\n            http_client=self.http_client,\n            max_retries=self.max_retries,\n        )\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"anthropic\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        max_tokens: int = 128,\n        stop_sequences: Union[List[str], None] = None,\n        temperature: float = 1.0,\n        top_p: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates a response asynchronously, using the [Anthropic Async API definition](https://github.com/anthropics/anthropic-sdk-python).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_tokens: the maximum number of new tokens that the model will generate. Defaults to `128`.\n            stop_sequences: custom text sequences that will cause the model to stop generating. Defaults to `NOT_GIVEN`.\n            temperature: the temperature to use for the generation. Set only if top_p is None. Defaults to `1.0`.\n            top_p: the top-p value to use for the generation. Defaults to `NOT_GIVEN`.\n            top_k: the top-k value to use for the generation. Defaults to `NOT_GIVEN`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        from anthropic._types import NOT_GIVEN\n\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"system\": (\n                input.pop(0)[\"content\"]\n                if input and input[0][\"role\"] == \"system\"\n                else NOT_GIVEN\n            ),\n            \"max_tokens\": max_tokens,\n            \"stream\": False,\n            \"stop_sequences\": NOT_GIVEN if stop_sequences is None else stop_sequences,\n            \"temperature\": temperature,\n            \"top_p\": NOT_GIVEN if top_p is None else top_p,\n            \"top_k\": NOT_GIVEN if top_k is None else top_k,\n        }\n\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n\n        completion = await self._aclient.messages.create(**kwargs)  # type: ignore\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        if (content := completion.content[0].text) is None:\n            self._logger.warning(\n                f\"Received no response using Anthropic client (model: '{self.model}').\"\n                f\" Finish reason was: {completion.stop_reason}\"\n            )\n        generations.append(content)\n        return generations\n\n    # TODO: remove this function once Anthropic client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/anthropic/#distilabel.llms.anthropic.AnthropicLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/anthropic/#distilabel.llms.anthropic.AnthropicLLM.agenerate","title":"<code>agenerate(input, max_tokens=128, stop_sequences=None, temperature=1.0, top_p=None, top_k=None)</code>  <code>async</code>","text":"<p>Generates a response asynchronously, using the Anthropic Async API definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>stop_sequences</code> <code>Union[List[str], None]</code> <p>custom text sequences that will cause the model to stop generating. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Set only if top_p is None. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_p</code> <code>Union[float, None]</code> <p>the top-p value to use for the generation. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <code>top_k</code> <code>Union[int, None]</code> <p>the top-k value to use for the generation. Defaults to <code>NOT_GIVEN</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    max_tokens: int = 128,\n    stop_sequences: Union[List[str], None] = None,\n    temperature: float = 1.0,\n    top_p: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates a response asynchronously, using the [Anthropic Async API definition](https://github.com/anthropics/anthropic-sdk-python).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_tokens: the maximum number of new tokens that the model will generate. Defaults to `128`.\n        stop_sequences: custom text sequences that will cause the model to stop generating. Defaults to `NOT_GIVEN`.\n        temperature: the temperature to use for the generation. Set only if top_p is None. Defaults to `1.0`.\n        top_p: the top-p value to use for the generation. Defaults to `NOT_GIVEN`.\n        top_k: the top-k value to use for the generation. Defaults to `NOT_GIVEN`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    from anthropic._types import NOT_GIVEN\n\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"system\": (\n            input.pop(0)[\"content\"]\n            if input and input[0][\"role\"] == \"system\"\n            else NOT_GIVEN\n        ),\n        \"max_tokens\": max_tokens,\n        \"stream\": False,\n        \"stop_sequences\": NOT_GIVEN if stop_sequences is None else stop_sequences,\n        \"temperature\": temperature,\n        \"top_p\": NOT_GIVEN if top_p is None else top_p,\n        \"top_k\": NOT_GIVEN if top_k is None else top_k,\n    }\n\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n\n    completion = await self._aclient.messages.create(**kwargs)  # type: ignore\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    if (content := completion.content[0].text) is None:\n        self._logger.warning(\n            f\"Received no response using Anthropic client (model: '{self.model}').\"\n            f\" Finish reason was: {completion.stop_reason}\"\n        )\n    generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/anthropic/#distilabel.llms.anthropic.AnthropicLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/anthropic/#distilabel.llms.anthropic.AnthropicLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncAnthropic</code> client to use the Anthropic async API.</p> Source code in <code>src/distilabel/llms/anthropic.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncAnthropic` client to use the Anthropic async API.\"\"\"\n    super().load()\n\n    try:\n        from anthropic import AsyncAnthropic\n    except ImportError as ie:\n        raise ImportError(\n            \"Anthropic Python client is not installed. Please install it using\"\n            \" `pip install anthropic`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._check_model_exists()\n\n    self._aclient = AsyncAnthropic(\n        api_key=self.api_key.get_secret_value(),\n        base_url=self.base_url,\n        timeout=self.timeout,\n        http_client=self.http_client,\n        max_retries=self.max_retries,\n    )\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"anthropic\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/anyscale/","title":"Anyscale","text":""},{"location":"reference/distilabel/llms/anyscale/#distilabel.llms.anyscale.AnyscaleLLM","title":"<code>AnyscaleLLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>Anyscale LLM implementation running the async API client of OpenAI.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM, e.g., <code>google/gemma-7b-it</code>. See the supported models under the \"Text Generation -&gt; Supported Models\" section here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Anyscale API requests. Defaults to <code>None</code>, which means that the value set for the environment variable <code>ANYSCALE_BASE_URL</code> will be used, or \"https://api.endpoints.anyscale.com/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Anyscale API. Defaults to <code>None</code> which means that the value set for the environment variable <code>ANYSCALE_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> Source code in <code>src/distilabel/llms/anyscale.py</code> <pre><code>class AnyscaleLLM(OpenAILLM):\n    \"\"\"Anyscale LLM implementation running the async API client of OpenAI.\n\n    Attributes:\n        model: the model name to use for the LLM, e.g., `google/gemma-7b-it`. See the\n            supported models under the \"Text Generation -&gt; Supported Models\" section\n            [here](https://docs.endpoints.anyscale.com/).\n        base_url: the base URL to use for the Anyscale API requests. Defaults to `None`, which\n            means that the value set for the environment variable `ANYSCALE_BASE_URL` will be used, or\n            \"https://api.endpoints.anyscale.com/v1\" if not set.\n        api_key: the API key to authenticate the requests to the Anyscale API. Defaults to `None` which\n            means that the value set for the environment variable `ANYSCALE_API_KEY` will be used, or\n            `None` if not set.\n        _api_key_env_var: the name of the environment variable to use for the API key.\n            It is meant to be used internally.\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"ANYSCALE_BASE_URL\", \"https://api.endpoints.anyscale.com/v1\"\n        ),\n        description=\"The base URL to use for the Anyscale API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_ANYSCALE_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Anyscale API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_ANYSCALE_API_KEY_ENV_VAR_NAME)\n</code></pre>"},{"location":"reference/distilabel/llms/azure/","title":"Azure","text":""},{"location":"reference/distilabel/llms/azure/#distilabel.llms.azure.AzureOpenAILLM","title":"<code>AzureOpenAILLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>Azure OpenAI LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM i.e. the name of the Azure deployment.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Azure OpenAI API can be set with <code>AZURE_OPENAI_ENDPOINT</code>. Defaults to <code>None</code> which means that the value set for the environment variable <code>AZURE_OPENAI_ENDPOINT</code> will be used, or <code>None</code> if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Azure OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>AZURE_OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>api_version</code> <code>Optional[RuntimeParameter[str]]</code> <p>the API version to use for the Azure OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_VERSION</code> will be used, or <code>None</code> if not set.</p> Icon <p><code>:simple-microsoftazure:</code></p> Source code in <code>src/distilabel/llms/azure.py</code> <pre><code>class AzureOpenAILLM(OpenAILLM):\n    \"\"\"Azure OpenAI LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM i.e. the name of the Azure deployment.\n        base_url: the base URL to use for the Azure OpenAI API can be set with `AZURE_OPENAI_ENDPOINT`.\n            Defaults to `None` which means that the value set for the environment variable\n            `AZURE_OPENAI_ENDPOINT` will be used, or `None` if not set.\n        api_key: the API key to authenticate the requests to the Azure OpenAI API. Defaults to `None`\n            which means that the value set for the environment variable `AZURE_OPENAI_API_KEY` will be\n            used, or `None` if not set.\n        api_version: the API version to use for the Azure OpenAI API. Defaults to `None` which means\n            that the value set for the environment variable `OPENAI_API_VERSION` will be used, or\n            `None` if not set.\n\n    Icon:\n        `:simple-microsoftazure:`\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(_AZURE_OPENAI_ENDPOINT_ENV_VAR_NAME),\n        description=\"The base URL to use for the Azure OpenAI API requests i.e. the Azure OpenAI endpoint.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_AZURE_OPENAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Azure OpenAI API.\",\n    )\n\n    api_version: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\"OPENAI_API_VERSION\"),\n        description=\"The API version to use for the Azure OpenAI API.\",\n    )\n\n    _base_url_env_var: str = PrivateAttr(_AZURE_OPENAI_ENDPOINT_ENV_VAR_NAME)\n    _api_key_env_var: str = PrivateAttr(_AZURE_OPENAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncAzureOpenAI\"] = PrivateAttr(...)  # type: ignore\n\n    @override\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncAzureOpenAI` client to benefit from async requests.\"\"\"\n        # This is a workaround to avoid the `OpenAILLM` calling the _prepare_structured_output\n        # in the load method before we have the proper client.\n        with patch(\"OpenAILLM._prepare_structured_output\", lambda x: x):\n            super().load()\n\n        try:\n            from openai import AsyncAzureOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        # TODO: May be worth adding the AD auth too? Also the `organization`?\n        self._aclient = AsyncAzureOpenAI(  # type: ignore\n            azure_endpoint=self.base_url,  # type: ignore\n            azure_deployment=self.model,\n            api_version=self.api_version,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            self._prepare_structured_output(self.structured_output)\n</code></pre>"},{"location":"reference/distilabel/llms/azure/#distilabel.llms.azure.AzureOpenAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncAzureOpenAI</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/azure.py</code> <pre><code>@override\ndef load(self) -&gt; None:\n    \"\"\"Loads the `AsyncAzureOpenAI` client to benefit from async requests.\"\"\"\n    # This is a workaround to avoid the `OpenAILLM` calling the _prepare_structured_output\n    # in the load method before we have the proper client.\n    with patch(\"OpenAILLM._prepare_structured_output\", lambda x: x):\n        super().load()\n\n    try:\n        from openai import AsyncAzureOpenAI\n    except ImportError as ie:\n        raise ImportError(\n            \"OpenAI Python client is not installed. Please install it using\"\n            \" `pip install openai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    # TODO: May be worth adding the AD auth too? Also the `organization`?\n    self._aclient = AsyncAzureOpenAI(  # type: ignore\n        azure_endpoint=self.base_url,  # type: ignore\n        azure_deployment=self.model,\n        api_version=self.api_version,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        self._prepare_structured_output(self.structured_output)\n</code></pre>"},{"location":"reference/distilabel/llms/base/","title":"Base","text":""},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.AsyncLLM","title":"<code>AsyncLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Abstract class for asynchronous LLMs, so as to benefit from the async capabilities of each LLM implementation. This class is meant to be subclassed by each LLM, and the method <code>agenerate</code> needs to be implemented to provide the asynchronous generation of responses.</p> <p>Attributes:</p> Name Type Description <code>_event_loop</code> <code>AbstractEventLoop</code> <p>the event loop to be used for the asynchronous generation of responses.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>class AsyncLLM(LLM):\n    \"\"\"Abstract class for asynchronous LLMs, so as to benefit from the async capabilities\n    of each LLM implementation. This class is meant to be subclassed by each LLM, and the\n    method `agenerate` needs to be implemented to provide the asynchronous generation of\n    responses.\n\n    Attributes:\n        _event_loop: the event loop to be used for the asynchronous generation of responses.\n    \"\"\"\n\n    _event_loop: \"asyncio.AbstractEventLoop\" = PrivateAttr(default=None)\n    _new_event_loop: bool = PrivateAttr(default=False)\n\n    @property\n    def generate_parameters(self) -&gt; List[inspect.Parameter]:\n        \"\"\"Returns the parameters of the `agenerate` method.\n\n        Returns:\n            A list containing the parameters of the `agenerate` method.\n        \"\"\"\n        return list(inspect.signature(self.agenerate).parameters.values())\n\n    @cached_property\n    def generate_parsed_docstring(self) -&gt; \"Docstring\":\n        \"\"\"Returns the parsed docstring of the `agenerate` method.\n\n        Returns:\n            The parsed docstring of the `agenerate` method.\n        \"\"\"\n        return parse_google_docstring(self.agenerate)\n\n    @property\n    def event_loop(self) -&gt; \"asyncio.AbstractEventLoop\":\n        if self._event_loop is None:\n            try:\n                self._event_loop = asyncio.get_running_loop()\n                if self._event_loop.is_closed():\n                    self._event_loop = asyncio.new_event_loop()  # type: ignore\n                    self._new_event_loop = True\n            except RuntimeError:\n                self._event_loop = asyncio.new_event_loop()\n                self._new_event_loop = True\n        asyncio.set_event_loop(self._event_loop)\n        return self._event_loop\n\n    @abstractmethod\n    async def agenerate(\n        self, input: \"FormattedInput\", num_generations: int = 1, **kwargs: Any\n    ) -&gt; List[Union[str, None]]:\n        \"\"\"Method to generate a `num_generations` responses for a given input asynchronously,\n        and executed concurrently in `generate` method.\n        \"\"\"\n        pass\n\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"FormattedInput\"], **kwargs: Any\n        ) -&gt; List[List[Union[str, None]]]:\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(\n                    self.agenerate(\n                        input=input, num_generations=num_generations, **kwargs\n                    )\n                )\n                for input in inputs\n            ]\n            return await asyncio.gather(*tasks)\n\n        return self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n\n    def __del__(self) -&gt; None:\n        \"\"\"Closes the event loop when the object is deleted.\"\"\"\n        if sys.meta_path is None:\n            return\n\n        if self._new_event_loop:\n            if self._event_loop.is_running():\n                self._event_loop.stop()\n            self._event_loop.close()\n\n    @staticmethod\n    def _prepare_structured_output(\n        structured_output: \"InstructorStructuredOutputType\",\n        client: Any = None,\n        framework: Optional[str] = None,\n    ) -&gt; Dict[str, Union[str, Any]]:\n        \"\"\"Wraps the client and updates the schema to work store it internally as a json schema.\n\n        Args:\n            structured_output: The configuration dict to prepare the structured output.\n            client: The client to wrap to generate structured output. Implemented to work\n                with `instructor`.\n            framework: The name of the framework.\n\n        Returns:\n            A dictionary containing the wrapped client and the schema to update the structured_output\n            variable in case it is a pydantic model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.instructor import (\n            prepare_instructor,\n        )\n\n        result = {}\n        client = prepare_instructor(\n            client,\n            mode=structured_output.get(\"mode\"),\n            framework=framework,\n        )\n        result[\"client\"] = client\n\n        schema = structured_output.get(\"schema\")\n        if not schema:\n            raise ValueError(\n                f\"The `structured_output` argument must contain a schema: {structured_output}\"\n            )\n        if issubclass(schema, BaseModel):\n            # We want a json schema for the serialization, but instructor wants a pydantic BaseModel.\n            structured_output[\"schema\"] = schema.model_json_schema()\n            result[\"structured_output\"] = structured_output\n\n        return result\n\n    @staticmethod\n    def _prepare_kwargs(\n        arguments: Dict[str, Any], structured_output: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Helper method to update the kwargs with the structured output configuration,\n        used in case they are defined.\n\n        Args:\n            arguments: The arguments that would be passed to the LLM as **kwargs.\n                to update with the structured output configuration.\n            structured_outputs: The structured output configuration to update the arguments.\n\n        Returns:\n            kwargs updated with the special arguments used by `instructor`.\n        \"\"\"\n        # We can deal with json schema or BaseModel, but we need to convert it to a BaseModel\n        # for the Instructor client.\n        schema = structured_output.get(\"schema\")\n        if not issubclass(schema, BaseModel):\n            from distilabel.steps.tasks.structured_outputs.utils import (\n                json_schema_to_model,\n            )\n\n            try:\n                schema = json_schema_to_model(schema)\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to convert the schema to a pydantic model, the model is too complex currently: {e}\"\n                ) from e\n\n        arguments.update(\n            **{\n                \"response_model\": schema,\n                \"max_retries\": structured_output.get(\"max_retries\", 1),\n            },\n        )\n        return arguments\n</code></pre>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.AsyncLLM.generate_parameters","title":"<code>generate_parameters: List[inspect.Parameter]</code>  <code>property</code>","text":"<p>Returns the parameters of the <code>agenerate</code> method.</p> <p>Returns:</p> Type Description <code>List[Parameter]</code> <p>A list containing the parameters of the <code>agenerate</code> method.</p>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.AsyncLLM.generate_parsed_docstring","title":"<code>generate_parsed_docstring: Docstring</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the parsed docstring of the <code>agenerate</code> method.</p> <p>Returns:</p> Type Description <code>Docstring</code> <p>The parsed docstring of the <code>agenerate</code> method.</p>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.AsyncLLM.__del__","title":"<code>__del__()</code>","text":"<p>Closes the event loop when the object is deleted.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Closes the event loop when the object is deleted.\"\"\"\n    if sys.meta_path is None:\n        return\n\n    if self._new_event_loop:\n        if self._event_loop.is_running():\n            self._event_loop.stop()\n        self._event_loop.close()\n</code></pre>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.AsyncLLM.agenerate","title":"<code>agenerate(input, num_generations=1, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Method to generate a <code>num_generations</code> responses for a given input asynchronously, and executed concurrently in <code>generate</code> method.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>@abstractmethod\nasync def agenerate(\n    self, input: \"FormattedInput\", num_generations: int = 1, **kwargs: Any\n) -&gt; List[Union[str, None]]:\n    \"\"\"Method to generate a `num_generations` responses for a given input asynchronously,\n    and executed concurrently in `generate` method.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.AsyncLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"FormattedInput\"], **kwargs: Any\n    ) -&gt; List[List[Union[str, None]]]:\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(\n                self.agenerate(\n                    input=input, num_generations=num_generations, **kwargs\n                )\n            )\n            for input in inputs\n        ]\n        return await asyncio.gather(*tasks)\n\n    return self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n</code></pre>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM","title":"<code>LLM</code>","text":"<p>               Bases: <code>RuntimeParametersMixin</code>, <code>BaseModel</code>, <code>_Serializable</code>, <code>ABC</code></p> <p>Base class for <code>LLM</code>s to be used in <code>distilabel</code> framework.</p> <p>To implement an <code>LLM</code> subclass, you need to subclass this class and implement:     - <code>load</code> method to load the <code>LLM</code> if needed. Don't forget to call <code>super().load()</code>,         so the <code>_logger</code> attribute is initialized.     - <code>model_name</code> property to return the model name used for the LLM.     - <code>generate</code> method to generate <code>num_generations</code> per input in <code>inputs</code>.</p> <p>Attributes:</p> Name Type Description <code>generation_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>the kwargs to be propagated to either <code>generate</code> or <code>agenerate</code> methods within each <code>LLM</code>.</p> <code>structured_output</code> <code>Optional[Any]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>_logger</code> <code>Union[Logger, None]</code> <p>the logger to be used for the <code>LLM</code>. It will be initialized when the <code>load</code> method is called.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>class LLM(RuntimeParametersMixin, BaseModel, _Serializable, ABC):\n    \"\"\"Base class for `LLM`s to be used in `distilabel` framework.\n\n    To implement an `LLM` subclass, you need to subclass this class and implement:\n        - `load` method to load the `LLM` if needed. Don't forget to call `super().load()`,\n            so the `_logger` attribute is initialized.\n        - `model_name` property to return the model name used for the LLM.\n        - `generate` method to generate `num_generations` per input in `inputs`.\n\n    Attributes:\n        generation_kwargs: the kwargs to be propagated to either `generate` or `agenerate`\n            methods within each `LLM`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        _logger: the logger to be used for the `LLM`. It will be initialized when the `load`\n            method is called.\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        protected_namespaces=(),\n        validate_default=True,\n        validate_assignment=True,\n        extra=\"forbid\",\n    )\n\n    generation_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"The kwargs to be propagated to either `generate` or `agenerate`\"\n        \" methods within each `LLM`.\",\n    )\n    structured_output: Optional[Any] = None\n\n    _logger: Union[logging.Logger, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Method to be called to initialize the `LLM`, its logger and optionally the structured output generator.\"\"\"\n        self._logger = logging.getLogger(f\"distilabel.llm.{self.model_name}\")\n\n    @property\n    @abstractmethod\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        pass\n\n    @abstractmethod\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Abstract method to be implemented by each LLM to generate `num_generations`\n        per input in `inputs`.\n\n        Args:\n            inputs: the list of inputs to generate responses for which follows OpenAI's\n                API format:\n\n                ```python\n                [\n                    {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n                    {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n                    {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n                    {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n                ]\n                ```\n            num_generations: the number of generations to generate per input.\n            **kwargs: the additional kwargs to be used for the generation.\n        \"\"\"\n        pass\n\n    @property\n    def generate_parameters(self) -&gt; List[\"inspect.Parameter\"]:\n        \"\"\"Returns the parameters of the `generate` method.\n\n        Returns:\n            A list containing the parameters of the `generate` method.\n        \"\"\"\n        return list(inspect.signature(self.generate).parameters.values())\n\n    @property\n    def runtime_parameters_names(self) -&gt; \"RuntimeParametersNames\":\n        \"\"\"Returns the runtime parameters of the `LLM`, which are combination of the\n        attributes of the `LLM` type hinted with `RuntimeParameter` and the parameters\n        of the `generate` method that are not `input` and `num_generations`.\n\n        Returns:\n            A dictionary with the name of the runtime parameters as keys and a boolean\n            indicating if the parameter is optional or not.\n        \"\"\"\n        runtime_parameters = super().runtime_parameters_names\n        runtime_parameters[\"generation_kwargs\"] = {}\n\n        # runtime parameters from the `generate` method\n        for param in self.generate_parameters:\n            if param.name in [\"input\", \"inputs\", \"num_generations\"]:\n                continue\n            is_optional = param.default != inspect.Parameter.empty\n            runtime_parameters[\"generation_kwargs\"][param.name] = is_optional\n\n        return runtime_parameters\n\n    def get_runtime_parameters_info(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Gets the information of the runtime parameters of the `LLM` such as the name\n        and the description. This function is meant to include the information of the runtime\n        parameters in the serialized data of the `LLM`.\n\n        Returns:\n            A list containing the information for each runtime parameter of the `LLM`.\n        \"\"\"\n        runtime_parameters_info = super().get_runtime_parameters_info()\n\n        generation_kwargs_info = next(\n            runtime_parameter_info\n            for runtime_parameter_info in runtime_parameters_info\n            if runtime_parameter_info[\"name\"] == \"generation_kwargs\"\n        )\n\n        generate_docstring_args = self.generate_parsed_docstring[\"args\"]\n\n        generation_kwargs_info[\"keys\"] = []\n        for key, value in generation_kwargs_info[\"optional\"].items():\n            info = {\"name\": key, \"optional\": value}\n            if description := generate_docstring_args.get(key):\n                info[\"description\"] = description\n            generation_kwargs_info[\"keys\"].append(info)\n\n        generation_kwargs_info.pop(\"optional\")\n\n        return runtime_parameters_info\n\n    @cached_property\n    def generate_parsed_docstring(self) -&gt; \"Docstring\":\n        \"\"\"Returns the parsed docstring of the `generate` method.\n\n        Returns:\n            The parsed docstring of the `generate` method.\n        \"\"\"\n        return parse_google_docstring(self.generate)\n\n    def get_last_hidden_states(\n        self, inputs: List[\"StandardInput\"]\n    ) -&gt; List[\"HiddenState\"]:\n        \"\"\"Method to get the last hidden states of the model for a list of inputs.\n\n        Args:\n            inputs: the list of inputs to get the last hidden states from.\n\n        Returns:\n            A list containing the last hidden state for each sequence using a NumPy array\n                with shape [num_tokens, hidden_size].\n        \"\"\"\n        raise NotImplementedError(\n            f\"Method `get_last_hidden_states` is not implemented for `{self.__class__.__name__}`\"\n        )\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Any, None]:\n        \"\"\"Method in charge of preparing the structured output generator.\n\n        By default will raise a `NotImplementedError`, subclasses that allow it must override this\n        method with the implementation.\n\n        Args:\n            structured_output: the config to prepare the guided generation.\n\n        Returns:\n            The structure to be used for the guided generation.\n        \"\"\"\n        raise NotImplementedError(\n            f\"Guided generation is not implemented for `{type(self).__name__}`\"\n        )\n</code></pre>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM.generate_parameters","title":"<code>generate_parameters: List[inspect.Parameter]</code>  <code>property</code>","text":"<p>Returns the parameters of the <code>generate</code> method.</p> <p>Returns:</p> Type Description <code>List[Parameter]</code> <p>A list containing the parameters of the <code>generate</code> method.</p>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM.generate_parsed_docstring","title":"<code>generate_parsed_docstring: Docstring</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the parsed docstring of the <code>generate</code> method.</p> <p>Returns:</p> Type Description <code>Docstring</code> <p>The parsed docstring of the <code>generate</code> method.</p>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM.model_name","title":"<code>model_name: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM.runtime_parameters_names","title":"<code>runtime_parameters_names: RuntimeParametersNames</code>  <code>property</code>","text":"<p>Returns the runtime parameters of the <code>LLM</code>, which are combination of the attributes of the <code>LLM</code> type hinted with <code>RuntimeParameter</code> and the parameters of the <code>generate</code> method that are not <code>input</code> and <code>num_generations</code>.</p> <p>Returns:</p> Type Description <code>RuntimeParametersNames</code> <p>A dictionary with the name of the runtime parameters as keys and a boolean</p> <code>RuntimeParametersNames</code> <p>indicating if the parameter is optional or not.</p>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be implemented by each LLM to generate <code>num_generations</code> per input in <code>inputs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[FormattedInput]</code> <p>the list of inputs to generate responses for which follows OpenAI's API format:</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n    {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n    {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n    {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n]\n</code></pre> required <code>num_generations</code> <code>int</code> <p>the number of generations to generate per input.</p> <code>1</code> <code>**kwargs</code> <code>Any</code> <p>the additional kwargs to be used for the generation.</p> <code>{}</code> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Abstract method to be implemented by each LLM to generate `num_generations`\n    per input in `inputs`.\n\n    Args:\n        inputs: the list of inputs to generate responses for which follows OpenAI's\n            API format:\n\n            ```python\n            [\n                {\"role\": \"system\", \"content\": \"You're a helpful assistant...\"},\n                {\"role\": \"user\", \"content\": \"Give a template email for B2B communications...\"},\n                {\"role\": \"assistant\", \"content\": \"Sure, here's a template you can use...\"},\n                {\"role\": \"user\", \"content\": \"Modify the second paragraph...\"}\n            ]\n            ```\n        num_generations: the number of generations to generate per input.\n        **kwargs: the additional kwargs to be used for the generation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM.get_last_hidden_states","title":"<code>get_last_hidden_states(inputs)</code>","text":"<p>Method to get the last hidden states of the model for a list of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>the list of inputs to get the last hidden states from.</p> required <p>Returns:</p> Type Description <code>List[HiddenState]</code> <p>A list containing the last hidden state for each sequence using a NumPy array with shape [num_tokens, hidden_size].</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def get_last_hidden_states(\n    self, inputs: List[\"StandardInput\"]\n) -&gt; List[\"HiddenState\"]:\n    \"\"\"Method to get the last hidden states of the model for a list of inputs.\n\n    Args:\n        inputs: the list of inputs to get the last hidden states from.\n\n    Returns:\n        A list containing the last hidden state for each sequence using a NumPy array\n            with shape [num_tokens, hidden_size].\n    \"\"\"\n    raise NotImplementedError(\n        f\"Method `get_last_hidden_states` is not implemented for `{self.__class__.__name__}`\"\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM.get_runtime_parameters_info","title":"<code>get_runtime_parameters_info()</code>","text":"<p>Gets the information of the runtime parameters of the <code>LLM</code> such as the name and the description. This function is meant to include the information of the runtime parameters in the serialized data of the <code>LLM</code>.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list containing the information for each runtime parameter of the <code>LLM</code>.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def get_runtime_parameters_info(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Gets the information of the runtime parameters of the `LLM` such as the name\n    and the description. This function is meant to include the information of the runtime\n    parameters in the serialized data of the `LLM`.\n\n    Returns:\n        A list containing the information for each runtime parameter of the `LLM`.\n    \"\"\"\n    runtime_parameters_info = super().get_runtime_parameters_info()\n\n    generation_kwargs_info = next(\n        runtime_parameter_info\n        for runtime_parameter_info in runtime_parameters_info\n        if runtime_parameter_info[\"name\"] == \"generation_kwargs\"\n    )\n\n    generate_docstring_args = self.generate_parsed_docstring[\"args\"]\n\n    generation_kwargs_info[\"keys\"] = []\n    for key, value in generation_kwargs_info[\"optional\"].items():\n        info = {\"name\": key, \"optional\": value}\n        if description := generate_docstring_args.get(key):\n            info[\"description\"] = description\n        generation_kwargs_info[\"keys\"].append(info)\n\n    generation_kwargs_info.pop(\"optional\")\n\n    return runtime_parameters_info\n</code></pre>"},{"location":"reference/distilabel/llms/base/#distilabel.llms.base.LLM.load","title":"<code>load()</code>","text":"<p>Method to be called to initialize the <code>LLM</code>, its logger and optionally the structured output generator.</p> Source code in <code>src/distilabel/llms/base.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Method to be called to initialize the `LLM`, its logger and optionally the structured output generator.\"\"\"\n    self._logger = logging.getLogger(f\"distilabel.llm.{self.model_name}\")\n</code></pre>"},{"location":"reference/distilabel/llms/chat_templates/","title":"Chat templates","text":""},{"location":"reference/distilabel/llms/cohere/","title":"Cohere","text":""},{"location":"reference/distilabel/llms/cohere/#distilabel.llms.cohere.CohereLLM","title":"<code>CohereLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Cohere API implementation using the async client for concurrent text generation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the name of the model from the Cohere API to use for the generation.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Cohere API requests. Defaults to <code>\"https://api.cohere.ai/v1\"</code>.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Cohere API. Defaults to the value of the <code>COHERE_API_KEY</code> environment variable.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response from the API. Defaults to <code>120</code>.</p> <code>client_name</code> <code>RuntimeParameter[str]</code> <p>the name of the client to use for the API requests. Defaults to <code>\"distilabel\"</code>.</p> <code>structured_output</code> <code>RuntimeParameter[str]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> <code>_ChatMessage</code> <code>Type[ChatMessage]</code> <p>the <code>ChatMessage</code> class from the <code>cohere</code> package.</p> <code>_aclient</code> <code>AsyncClient</code> <p>the <code>AsyncClient</code> client from the <code>cohere</code> package.</p> Runtime parameters <ul> <li><code>base_url</code>: the base URL to use for the Cohere API requests. Defaults to     <code>\"https://api.cohere.ai/v1\"</code>.</li> <li><code>api_key</code>: the API key to authenticate the requests to the Cohere API. Defaults     to the value of the <code>COHERE_API_KEY</code> environment variable.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response from the API. Defaults     to <code>120</code>.</li> <li><code>client_name</code>: the name of the client to use for the API requests. Defaults to     <code>\"distilabel\"</code>.</li> </ul> Source code in <code>src/distilabel/llms/cohere.py</code> <pre><code>class CohereLLM(AsyncLLM):\n    \"\"\"Cohere API implementation using the async client for concurrent text generation.\n\n    Attributes:\n        model: the name of the model from the Cohere API to use for the generation.\n        base_url: the base URL to use for the Cohere API requests. Defaults to\n            `\"https://api.cohere.ai/v1\"`.\n        api_key: the API key to authenticate the requests to the Cohere API. Defaults to\n            the value of the `COHERE_API_KEY` environment variable.\n        timeout: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        client_name: the name of the client to use for the API requests. Defaults to\n            `\"distilabel\"`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n        _ChatMessage: the `ChatMessage` class from the `cohere` package.\n        _aclient: the `AsyncClient` client from the `cohere` package.\n\n    Runtime parameters:\n        - `base_url`: the base URL to use for the Cohere API requests. Defaults to\n            `\"https://api.cohere.ai/v1\"`.\n        - `api_key`: the API key to authenticate the requests to the Cohere API. Defaults\n            to the value of the `COHERE_API_KEY` environment variable.\n        - `timeout`: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        - `client_name`: the name of the client to use for the API requests. Defaults to\n            `\"distilabel\"`.\n    \"\"\"\n\n    model: str\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"COHERE_BASE_URL\", \"https://api.cohere.ai/v1\"\n        ),\n        description=\"The base URL to use for the Cohere API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_COHERE_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Cohere API.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n    client_name: RuntimeParameter[str] = Field(\n        default=\"distilabel\",\n        description=\"The name of the client to use for the API requests.\",\n    )\n\n    _ChatMessage: Type[\"ChatMessage\"] = PrivateAttr(...)\n    _aclient: \"AsyncClient\" = PrivateAttr(...)\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncClient` client from the `cohere` package.\"\"\"\n\n        super().load()\n\n        try:\n            from cohere import AsyncClient, ChatMessage\n        except ImportError as ie:\n            raise ImportError(\n                \"The `cohere` package is required to use the `CohereLLM` class.\"\n            ) from ie\n\n        self._ChatMessage = ChatMessage\n\n        self._aclient = AsyncClient(\n            api_key=self.api_key.get_secret_value(),  # type: ignore\n            client_name=self.client_name,\n            base_url=self.base_url,\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"cohere\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    def _format_chat_to_cohere(\n        self, input: \"StandardInput\"\n    ) -&gt; Tuple[Union[str, None], List[\"ChatMessage\"], str]:\n        \"\"\"Formats the chat input to the Cohere Chat API conversational format.\n\n        Args:\n            input: The chat input to format.\n\n        Returns:\n            A tuple containing the system, chat history, and message.\n        \"\"\"\n        system = None\n        message = None\n        chat_history = []\n        for item in input:\n            role = item[\"role\"]\n            content = item[\"content\"]\n            if role == \"system\":\n                system = content\n            elif role == \"user\":\n                message = content\n            elif role == \"assistant\":\n                if message is None:\n                    raise ValueError(\n                        \"An assistant message but be preceded by a user message.\"\n                    )\n                chat_history.append(self._ChatMessage(role=\"USER\", message=message))  # type: ignore\n                chat_history.append(self._ChatMessage(role=\"CHATBOT\", message=content))\n                message = None\n\n        if message is None:\n            raise ValueError(\"The chat input must end with a user message.\")\n\n        return system, chat_history, message\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        k: Optional[int] = None,\n        p: Optional[float] = None,\n        seed: Optional[float] = None,\n        stop_sequences: Optional[Sequence[str]] = None,\n        frequency_penalty: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        raw_prompting: Optional[bool] = None,\n    ) -&gt; Union[str, None]:\n        \"\"\"Generates a response from the LLM given an input.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            temperature: the temperature to use for the generation. Defaults to `None`.\n            max_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `None`.\n            k: the number of highest probability vocabulary tokens to keep for the generation.\n                Defaults to `None`.\n            p: the nucleus sampling probability to use for the generation. Defaults to\n                `None`.\n            seed: the seed to use for the generation. Defaults to `None`.\n            stop_sequences: a list of sequences to use as stopping criteria for the generation.\n                Defaults to `None`.\n            frequency_penalty: the frequency penalty to use for the generation. Defaults\n                to `None`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `None`.\n            raw_prompting: a flag to use raw prompting for the generation. Defaults to\n                `None`.\n\n        Returns:\n            The generated response from the Cohere API model.\n        \"\"\"\n        system, chat_history, message = self._format_chat_to_cohere(input)\n\n        kwargs = {\n            \"message\": message,\n            \"model\": self.model,\n            \"preamble\": system,\n            \"chat_history\": chat_history,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"k\": k,\n            \"p\": p,\n            \"seed\": seed,\n            \"stop_sequences\": stop_sequences,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"raw_prompting\": raw_prompting,\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        response = await self._aclient.chat(**kwargs)  # type: ignore\n\n        if self.structured_output:\n            return response.model_dump_json()\n\n        if (text := response.text) == \"\":\n            self._logger.warning(\n                f\"Received no response using Cohere client (model: '{self.model}').\"\n                f\" Finish reason was: {response.finish_reason}\"\n            )\n            return None\n\n        return text\n\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return await asyncio.gather(*tasks)\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/cohere/#distilabel.llms.cohere.CohereLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/cohere/#distilabel.llms.cohere.CohereLLM.agenerate","title":"<code>agenerate(input, temperature=None, max_tokens=None, k=None, p=None, seed=None, stop_sequences=None, frequency_penalty=None, presence_penalty=None, raw_prompting=None)</code>  <code>async</code>","text":"<p>Generates a response from the LLM given an input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>None</code>.</p> <code>None</code> <code>k</code> <code>Optional[int]</code> <p>the number of highest probability vocabulary tokens to keep for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>p</code> <code>Optional[float]</code> <p>the nucleus sampling probability to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>seed</code> <code>Optional[float]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[Sequence[str]]</code> <p>a list of sequences to use as stopping criteria for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>the frequency penalty to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>the presence penalty to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>raw_prompting</code> <code>Optional[bool]</code> <p>a flag to use raw prompting for the generation. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>The generated response from the Cohere API model.</p> Source code in <code>src/distilabel/llms/cohere.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    k: Optional[int] = None,\n    p: Optional[float] = None,\n    seed: Optional[float] = None,\n    stop_sequences: Optional[Sequence[str]] = None,\n    frequency_penalty: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    raw_prompting: Optional[bool] = None,\n) -&gt; Union[str, None]:\n    \"\"\"Generates a response from the LLM given an input.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        temperature: the temperature to use for the generation. Defaults to `None`.\n        max_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `None`.\n        k: the number of highest probability vocabulary tokens to keep for the generation.\n            Defaults to `None`.\n        p: the nucleus sampling probability to use for the generation. Defaults to\n            `None`.\n        seed: the seed to use for the generation. Defaults to `None`.\n        stop_sequences: a list of sequences to use as stopping criteria for the generation.\n            Defaults to `None`.\n        frequency_penalty: the frequency penalty to use for the generation. Defaults\n            to `None`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `None`.\n        raw_prompting: a flag to use raw prompting for the generation. Defaults to\n            `None`.\n\n    Returns:\n        The generated response from the Cohere API model.\n    \"\"\"\n    system, chat_history, message = self._format_chat_to_cohere(input)\n\n    kwargs = {\n        \"message\": message,\n        \"model\": self.model,\n        \"preamble\": system,\n        \"chat_history\": chat_history,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens,\n        \"k\": k,\n        \"p\": p,\n        \"seed\": seed,\n        \"stop_sequences\": stop_sequences,\n        \"frequency_penalty\": frequency_penalty,\n        \"presence_penalty\": presence_penalty,\n        \"raw_prompting\": raw_prompting,\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    response = await self._aclient.chat(**kwargs)  # type: ignore\n\n    if self.structured_output:\n        return response.model_dump_json()\n\n    if (text := response.text) == \"\":\n        self._logger.warning(\n            f\"Received no response using Cohere client (model: '{self.model}').\"\n            f\" Finish reason was: {response.finish_reason}\"\n        )\n        return None\n\n    return text\n</code></pre>"},{"location":"reference/distilabel/llms/cohere/#distilabel.llms.cohere.CohereLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/cohere.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return await asyncio.gather(*tasks)\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/cohere/#distilabel.llms.cohere.CohereLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncClient</code> client from the <code>cohere</code> package.</p> Source code in <code>src/distilabel/llms/cohere.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncClient` client from the `cohere` package.\"\"\"\n\n    super().load()\n\n    try:\n        from cohere import AsyncClient, ChatMessage\n    except ImportError as ie:\n        raise ImportError(\n            \"The `cohere` package is required to use the `CohereLLM` class.\"\n        ) from ie\n\n    self._ChatMessage = ChatMessage\n\n    self._aclient = AsyncClient(\n        api_key=self.api_key.get_secret_value(),  # type: ignore\n        client_name=self.client_name,\n        base_url=self.base_url,\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"cohere\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/groq/","title":"Groq","text":""},{"location":"reference/distilabel/llms/groq/#distilabel.llms.groq.GroqLLM","title":"<code>GroqLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Groq API implementation using the async client for concurrent text generation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the name of the model from the Groq API to use for the generation.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Groq API requests. Defaults to <code>\"https://api.groq.com\"</code>.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Groq API. Defaults to the value of the <code>GROQ_API_KEY</code> environment variable.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of times to retry the request to the API before failing. Defaults to <code>2</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response from the API. Defaults to <code>120</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key.</p> <code>_aclient</code> <code>Optional[AsyncGroq]</code> <p>the <code>AsyncGroq</code> client from the <code>groq</code> package.</p> Runtime parameters <ul> <li><code>base_url</code>: the base URL to use for the Groq API requests. Defaults to     <code>\"https://api.groq.com\"</code>.</li> <li><code>api_key</code>: the API key to authenticate the requests to the Groq API. Defaults to     the value of the <code>GROQ_API_KEY</code> environment variable.</li> <li><code>max_retries</code>: the maximum number of times to retry the request to the API before     failing. Defaults to <code>2</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response from the API. Defaults     to <code>120</code>.</li> </ul> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>class GroqLLM(AsyncLLM):\n    \"\"\"Groq API implementation using the async client for concurrent text generation.\n\n    Attributes:\n        model: the name of the model from the Groq API to use for the generation.\n        base_url: the base URL to use for the Groq API requests. Defaults to\n            `\"https://api.groq.com\"`.\n        api_key: the API key to authenticate the requests to the Groq API. Defaults to\n            the value of the `GROQ_API_KEY` environment variable.\n        max_retries: the maximum number of times to retry the request to the API before\n            failing. Defaults to `2`.\n        timeout: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n        _api_key_env_var: the name of the environment variable to use for the API key.\n        _aclient: the `AsyncGroq` client from the `groq` package.\n\n    Runtime parameters:\n        - `base_url`: the base URL to use for the Groq API requests. Defaults to\n            `\"https://api.groq.com\"`.\n        - `api_key`: the API key to authenticate the requests to the Groq API. Defaults to\n            the value of the `GROQ_API_KEY` environment variable.\n        - `max_retries`: the maximum number of times to retry the request to the API before\n            failing. Defaults to `2`.\n        - `timeout`: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n    \"\"\"\n\n    model: str\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            _GROQ_API_BASE_URL_ENV_VAR_NAME, \"https://api.groq.com\"\n        ),\n        description=\"The base URL to use for the Groq API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_GROQ_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Groq API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=2,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_GROQ_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncGroq\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncGroq` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from groq import AsyncGroq\n        except ImportError as ie:\n            raise ImportError(\n                \"Groq Python client is not installed. Please install it using\"\n                ' `pip install groq` or from the extras as `pip install \"distilabel[groq]\"`.'\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = AsyncGroq(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"groq\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        seed: Optional[int] = None,\n        max_new_tokens: int = 128,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        stop: Optional[str] = None,\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Generates `num_generations` responses for the given input using the Groq async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            seed: the seed to use for the generation. Defaults to `None`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: the stop sequence to use for the generation. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n\n        References:\n            - https://console.groq.com/docs/text-chat\n        \"\"\"\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"seed\": seed,\n            \"temperature\": temperature,\n            \"max_tokens\": max_new_tokens,\n            \"top_p\": top_p,\n            \"stream\": False,\n            \"stop\": stop,\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n        completion = await self._aclient.chat.completions.create(**kwargs)  # type: ignore\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(  # type: ignore\n                    f\"Received no response using the Groq client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n\n    # TODO: remove this function once Groq client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/groq/#distilabel.llms.groq.GroqLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/groq/#distilabel.llms.groq.GroqLLM.agenerate","title":"<code>agenerate(input, seed=None, max_new_tokens=128, temperature=1.0, top_p=1.0, stop=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the Groq async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>seed</code> <code>Optional[int]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[str]</code> <p>the stop sequence to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> References <ul> <li>https://console.groq.com/docs/text-chat</li> </ul> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    seed: Optional[int] = None,\n    max_new_tokens: int = 128,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stop: Optional[str] = None,\n) -&gt; \"GenerateOutput\":\n    \"\"\"Generates `num_generations` responses for the given input using the Groq async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        seed: the seed to use for the generation. Defaults to `None`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: the stop sequence to use for the generation. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n\n    References:\n        - https://console.groq.com/docs/text-chat\n    \"\"\"\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"seed\": seed,\n        \"temperature\": temperature,\n        \"max_tokens\": max_new_tokens,\n        \"top_p\": top_p,\n        \"stream\": False,\n        \"stop\": stop,\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n    completion = await self._aclient.chat.completions.create(**kwargs)  # type: ignore\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(  # type: ignore\n                f\"Received no response using the Groq client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/groq/#distilabel.llms.groq.GroqLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/groq/#distilabel.llms.groq.GroqLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncGroq</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/groq.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncGroq` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from groq import AsyncGroq\n    except ImportError as ie:\n        raise ImportError(\n            \"Groq Python client is not installed. Please install it using\"\n            ' `pip install groq` or from the extras as `pip install \"distilabel[groq]\"`.'\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = AsyncGroq(\n        base_url=self.base_url,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"groq\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/litellm/","title":"Litellm","text":""},{"location":"reference/distilabel/llms/litellm/#distilabel.llms.litellm.LiteLLM","title":"<code>LiteLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>LiteLLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gpt-3.5-turbo\" or \"mistral/mistral-large\", etc.</p> <code>verbose</code> <code>RuntimeParameter[bool]</code> <p>whether to log the LiteLLM client's logs. Defaults to <code>False</code>.</p> <code>structured_output</code> <code>RuntimeParameter[bool]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> Runtime parameters <ul> <li><code>verbose</code>: whether to log the LiteLLM client's logs. Defaults to <code>False</code>.</li> </ul> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>class LiteLLM(AsyncLLM):\n    \"\"\"LiteLLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gpt-3.5-turbo\" or \"mistral/mistral-large\",\n            etc.\n        verbose: whether to log the LiteLLM client's logs. Defaults to `False`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n\n    Runtime parameters:\n        - `verbose`: whether to log the LiteLLM client's logs. Defaults to `False`.\n    \"\"\"\n\n    model: str\n    verbose: RuntimeParameter[bool] = Field(\n        default=False, description=\"Whether to log the LiteLLM client's logs.\"\n    )\n\n    _aclient: Optional[Callable] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"\n        Loads the `acompletion` LiteLLM client to benefit from async requests.\n        \"\"\"\n        super().load()\n\n        try:\n            import litellm\n\n            litellm.telemetry = False\n        except ImportError as e:\n            raise ImportError(\n                \"LiteLLM Python client is not installed. Please install it using\"\n                \" `pip install litellm`.\"\n            ) from e\n        self._aclient = litellm.acompletion\n\n        if not self.verbose:\n            litellm.suppress_debug_info = True\n            for key in logging.Logger.manager.loggerDict.keys():\n                if \"litellm\" not in key.lower():\n                    continue\n                logging.getLogger(key).setLevel(logging.CRITICAL)\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"litellm\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        temperature: Optional[float] = 1.0,\n        top_p: Optional[float] = 1.0,\n        stop: Optional[Union[str, list]] = None,\n        max_tokens: Optional[int] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        metadata: Optional[dict] = None,\n        api_base: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,\n        mock_response: Optional[str] = None,\n        force_timeout: Optional[int] = 600,\n        custom_llm_provider: Optional[str] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the [LiteLLM async client](https://github.com/BerriAI/litellm).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            functions: a list of functions to apply to the conversation messages. Defaults to\n                `None`.\n            function_call: the name of the function to call within the conversation. Defaults\n                to `None`.\n            temperature: the temperature to use for the generation. Defaults to `1.0`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: Up to 4 sequences where the LLM API will stop generating further tokens.\n                Defaults to `None`.\n            max_tokens: The maximum number of tokens in the generated completion. Defaults to\n                `None`.\n            presence_penalty: It is used to penalize new tokens based on their existence in the\n                text so far. Defaults to `None`.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the\n                text so far. Defaults to `None`.\n            logit_bias: Used to modify the probability of specific tokens appearing in the\n                completion. Defaults to `None`.\n            user: A unique identifier representing your end-user. This can help the LLM provider\n                to monitor and detect abuse. Defaults to `None`.\n            metadata: Pass in additional metadata to tag your completion calls - eg. prompt\n                version, details, etc. Defaults to `None`.\n            api_base: Base URL for the API. Defaults to `None`.\n            api_version: API version. Defaults to `None`.\n            api_key: API key. Defaults to `None`.\n            model_list: List of api base, version, keys. Defaults to `None`.\n            mock_response: If provided, return a mock completion response for testing or debugging\n                purposes. Defaults to `None`.\n            force_timeout: The maximum execution time in seconds for the completion request.\n                Defaults to `600`.\n            custom_llm_provider: Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable)\n                model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to\n                `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        import litellm\n\n        kwargs = {\n            \"model\": self.model,\n            \"messages\": input,\n            \"n\": num_generations,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"stream\": False,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"metadata\": metadata,\n            \"api_base\": api_base,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"mock_response\": mock_response,\n            \"force_timeout\": force_timeout,\n            \"custom_llm_provider\": custom_llm_provider,\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        async def _call_aclient_until_n_choices() -&gt; List[\"Choices\"]:\n            choices = []\n            while len(choices) &lt; num_generations:\n                completion = await self._aclient(**kwargs)  # type: ignore\n                if not self.structured_output:\n                    completion = completion.choices\n                choices.extend(completion)\n            return choices\n\n        # litellm.drop_params is used to en/disable sending **kwargs parameters to the API if they cannot be used\n        try:\n            litellm.drop_params = False\n            choices = await _call_aclient_until_n_choices()\n        except litellm.exceptions.APIError as e:\n            if \"does not support parameters\" in str(e):\n                litellm.drop_params = True\n                choices = await _call_aclient_until_n_choices()\n            else:\n                raise e\n\n        generations = []\n\n        if self.structured_output:\n            generations.append([choice.model_dump_json() for choice in choices])\n            return generations\n\n        for choice in choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(\n                    f\"Received no response using LiteLLM client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n</code></pre>"},{"location":"reference/distilabel/llms/litellm/#distilabel.llms.litellm.LiteLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/litellm/#distilabel.llms.litellm.LiteLLM.agenerate","title":"<code>agenerate(input, num_generations=1, functions=None, function_call=None, temperature=1.0, top_p=1.0, stop=None, max_tokens=None, presence_penalty=None, frequency_penalty=None, logit_bias=None, user=None, metadata=None, api_base=None, api_version=None, api_key=None, model_list=None, mock_response=None, force_timeout=600, custom_llm_provider=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the LiteLLM async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>functions</code> <code>Optional[List]</code> <p>a list of functions to apply to the conversation messages. Defaults to <code>None</code>.</p> <code>None</code> <code>function_call</code> <code>Optional[str]</code> <p>the name of the function to call within the conversation. Defaults to <code>None</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[Union[str, list]]</code> <p>Up to 4 sequences where the LLM API will stop generating further tokens. Defaults to <code>None</code>.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens in the generated completion. Defaults to <code>None</code>.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>It is used to penalize new tokens based on their existence in the text so far. Defaults to <code>None</code>.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>It is used to penalize new tokens based on their frequency in the text so far. Defaults to <code>None</code>.</p> <code>None</code> <code>logit_bias</code> <code>Optional[dict]</code> <p>Used to modify the probability of specific tokens appearing in the completion. Defaults to <code>None</code>.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse. Defaults to <code>None</code>.</p> <code>None</code> <code>metadata</code> <code>Optional[dict]</code> <p>Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc. Defaults to <code>None</code>.</p> <code>None</code> <code>api_base</code> <code>Optional[str]</code> <p>Base URL for the API. Defaults to <code>None</code>.</p> <code>None</code> <code>api_version</code> <code>Optional[str]</code> <p>API version. Defaults to <code>None</code>.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key. Defaults to <code>None</code>.</p> <code>None</code> <code>model_list</code> <code>Optional[list]</code> <p>List of api base, version, keys. Defaults to <code>None</code>.</p> <code>None</code> <code>mock_response</code> <code>Optional[str]</code> <p>If provided, return a mock completion response for testing or debugging purposes. Defaults to <code>None</code>.</p> <code>None</code> <code>force_timeout</code> <code>Optional[int]</code> <p>The maximum execution time in seconds for the completion request. Defaults to <code>600</code>.</p> <code>600</code> <code>custom_llm_provider</code> <code>Optional[str]</code> <p>Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable) model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    functions: Optional[List] = None,\n    function_call: Optional[str] = None,\n    temperature: Optional[float] = 1.0,\n    top_p: Optional[float] = 1.0,\n    stop: Optional[Union[str, list]] = None,\n    max_tokens: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[dict] = None,\n    user: Optional[str] = None,\n    metadata: Optional[dict] = None,\n    api_base: Optional[str] = None,\n    api_version: Optional[str] = None,\n    api_key: Optional[str] = None,\n    model_list: Optional[list] = None,\n    mock_response: Optional[str] = None,\n    force_timeout: Optional[int] = 600,\n    custom_llm_provider: Optional[str] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the [LiteLLM async client](https://github.com/BerriAI/litellm).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        functions: a list of functions to apply to the conversation messages. Defaults to\n            `None`.\n        function_call: the name of the function to call within the conversation. Defaults\n            to `None`.\n        temperature: the temperature to use for the generation. Defaults to `1.0`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: Up to 4 sequences where the LLM API will stop generating further tokens.\n            Defaults to `None`.\n        max_tokens: The maximum number of tokens in the generated completion. Defaults to\n            `None`.\n        presence_penalty: It is used to penalize new tokens based on their existence in the\n            text so far. Defaults to `None`.\n        frequency_penalty: It is used to penalize new tokens based on their frequency in the\n            text so far. Defaults to `None`.\n        logit_bias: Used to modify the probability of specific tokens appearing in the\n            completion. Defaults to `None`.\n        user: A unique identifier representing your end-user. This can help the LLM provider\n            to monitor and detect abuse. Defaults to `None`.\n        metadata: Pass in additional metadata to tag your completion calls - eg. prompt\n            version, details, etc. Defaults to `None`.\n        api_base: Base URL for the API. Defaults to `None`.\n        api_version: API version. Defaults to `None`.\n        api_key: API key. Defaults to `None`.\n        model_list: List of api base, version, keys. Defaults to `None`.\n        mock_response: If provided, return a mock completion response for testing or debugging\n            purposes. Defaults to `None`.\n        force_timeout: The maximum execution time in seconds for the completion request.\n            Defaults to `600`.\n        custom_llm_provider: Used for Non-OpenAI LLMs, Example usage for bedrock, set(iterable)\n            model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\". Defaults to\n            `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    import litellm\n\n    kwargs = {\n        \"model\": self.model,\n        \"messages\": input,\n        \"n\": num_generations,\n        \"functions\": functions,\n        \"function_call\": function_call,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"stream\": False,\n        \"stop\": stop,\n        \"max_tokens\": max_tokens,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"logit_bias\": logit_bias,\n        \"user\": user,\n        \"metadata\": metadata,\n        \"api_base\": api_base,\n        \"api_version\": api_version,\n        \"api_key\": api_key,\n        \"model_list\": model_list,\n        \"mock_response\": mock_response,\n        \"force_timeout\": force_timeout,\n        \"custom_llm_provider\": custom_llm_provider,\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    async def _call_aclient_until_n_choices() -&gt; List[\"Choices\"]:\n        choices = []\n        while len(choices) &lt; num_generations:\n            completion = await self._aclient(**kwargs)  # type: ignore\n            if not self.structured_output:\n                completion = completion.choices\n            choices.extend(completion)\n        return choices\n\n    # litellm.drop_params is used to en/disable sending **kwargs parameters to the API if they cannot be used\n    try:\n        litellm.drop_params = False\n        choices = await _call_aclient_until_n_choices()\n    except litellm.exceptions.APIError as e:\n        if \"does not support parameters\" in str(e):\n            litellm.drop_params = True\n            choices = await _call_aclient_until_n_choices()\n        else:\n            raise e\n\n    generations = []\n\n    if self.structured_output:\n        generations.append([choice.model_dump_json() for choice in choices])\n        return generations\n\n    for choice in choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(\n                f\"Received no response using LiteLLM client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/litellm/#distilabel.llms.litellm.LiteLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>acompletion</code> LiteLLM client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/litellm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"\n    Loads the `acompletion` LiteLLM client to benefit from async requests.\n    \"\"\"\n    super().load()\n\n    try:\n        import litellm\n\n        litellm.telemetry = False\n    except ImportError as e:\n        raise ImportError(\n            \"LiteLLM Python client is not installed. Please install it using\"\n            \" `pip install litellm`.\"\n        ) from e\n    self._aclient = litellm.acompletion\n\n    if not self.verbose:\n        litellm.suppress_debug_info = True\n        for key in logging.Logger.manager.loggerDict.keys():\n            if \"litellm\" not in key.lower():\n                continue\n            logging.getLogger(key).setLevel(logging.CRITICAL)\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"litellm\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/llamacpp/","title":"Llamacpp","text":""},{"location":"reference/distilabel/llms/llamacpp/#distilabel.llms.llamacpp.LlamaCppLLM","title":"<code>LlamaCppLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>llama.cpp LLM implementation running the Python bindings for the C++ code.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>RuntimeParameter[FilePath]</code> <p>contains the path to the GGUF quantized model, compatible with the installed version of the <code>llama.cpp</code> Python bindings.</p> <code>n_gpu_layers</code> <code>RuntimeParameter[int]</code> <p>the number of layers to use for the GPU. Defaults to <code>-1</code>, meaning that the available GPU device will be used.</p> <code>chat_format</code> <code>Optional[RuntimeParameter[str]]</code> <p>the chat format to use for the model. Defaults to <code>None</code>, which means the Llama format will be used.</p> <code>n_ctx</code> <code>int</code> <p>the context size to use for the model. Defaults to <code>512</code>.</p> <code>n_batch</code> <code>int</code> <p>the prompt processing maximum batch size to use for the model. Defaults to <code>512</code>.</p> <code>seed</code> <code>int</code> <p>random seed to use for the generation. Defaults to <code>4294967295</code>.</p> <code>verbose</code> <code>RuntimeParameter[bool]</code> <p>whether to print verbose output. Defaults to <code>False</code>.</p> <code>structured_output</code> <code>RuntimeParameter[bool]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>extra_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>Llama</code> class of <code>llama_cpp</code> library. Defaults to <code>{}</code>.</p> <code>_model</code> <code>Optional[Llama]</code> <p>the Llama model instance. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>model_path</code>: the path to the GGUF quantized model.</li> <li><code>n_gpu_layers</code>: the number of layers to use for the GPU. Defaults to <code>-1</code>.</li> <li><code>chat_format</code>: the chat format to use for the model. Defaults to <code>None</code>.</li> <li><code>verbose</code>: whether to print verbose output. Defaults to <code>False</code>.</li> <li><code>extra_kwargs</code>: additional dictionary of keyword arguments that will be passed to the     <code>Llama</code> class of <code>llama_cpp</code> library. Defaults to <code>{}</code>.</li> </ul> References <ul> <li><code>llama.cpp</code></li> <li><code>llama-cpp-python</code></li> </ul> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>class LlamaCppLLM(LLM):\n    \"\"\"llama.cpp LLM implementation running the Python bindings for the C++ code.\n\n    Attributes:\n        model_path: contains the path to the GGUF quantized model, compatible with the\n            installed version of the `llama.cpp` Python bindings.\n        n_gpu_layers: the number of layers to use for the GPU. Defaults to `-1`, meaning that\n            the available GPU device will be used.\n        chat_format: the chat format to use for the model. Defaults to `None`, which means the\n            Llama format will be used.\n        n_ctx: the context size to use for the model. Defaults to `512`.\n        n_batch: the prompt processing maximum batch size to use for the model. Defaults to `512`.\n        seed: random seed to use for the generation. Defaults to `4294967295`.\n        verbose: whether to print verbose output. Defaults to `False`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        extra_kwargs: additional dictionary of keyword arguments that will be passed to the\n            `Llama` class of `llama_cpp` library. Defaults to `{}`.\n        _model: the Llama model instance. This attribute is meant to be used internally and\n            should not be accessed directly. It will be set in the `load` method.\n\n    Runtime parameters:\n        - `model_path`: the path to the GGUF quantized model.\n        - `n_gpu_layers`: the number of layers to use for the GPU. Defaults to `-1`.\n        - `chat_format`: the chat format to use for the model. Defaults to `None`.\n        - `verbose`: whether to print verbose output. Defaults to `False`.\n        - `extra_kwargs`: additional dictionary of keyword arguments that will be passed to the\n            `Llama` class of `llama_cpp` library. Defaults to `{}`.\n\n    References:\n        - [`llama.cpp`](https://github.com/ggerganov/llama.cpp)\n        - [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python)\n    \"\"\"\n\n    model_path: RuntimeParameter[FilePath] = Field(\n        default=None, description=\"The path to the GGUF quantized model.\", exclude=True\n    )\n    n_gpu_layers: RuntimeParameter[int] = Field(\n        default=-1,\n        description=\"The number of layers that will be loaded in the GPU.\",\n    )\n    chat_format: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The chat format to use for the model. Defaults to `None`, which means the Llama format will be used.\",\n    )\n\n    n_ctx: int = 512\n    n_batch: int = 512\n    seed: int = 4294967295\n\n    verbose: RuntimeParameter[bool] = Field(\n        default=False,\n        description=\"Whether to print verbose output from llama.cpp library.\",\n    )\n    extra_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"Additional dictionary of keyword arguments that will be passed to the\"\n        \" `Llama` class of `llama_cpp` library. See all the supported arguments at: \"\n        \"https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__init__\",\n    )\n\n    _logits_processor: Optional[\"LogitsProcessorList\"] = PrivateAttr(default=None)\n    _model: Optional[\"Llama\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `Llama` model from the `model_path`.\"\"\"\n        try:\n            from llama_cpp import Llama\n        except ImportError as ie:\n            raise ImportError(\n                \"The `llama_cpp` package is required to use the `LlamaCppLLM` class.\"\n            ) from ie\n\n        self._model = Llama(\n            model_path=self.model_path.as_posix(),  # type: ignore\n            seed=self.seed,\n            n_ctx=self.n_ctx,\n            n_batch=self.n_batch,\n            chat_format=self.chat_format,\n            n_gpu_layers=self.n_gpu_layers,\n            verbose=self.verbose,\n            **self.extra_kwargs,\n        )\n\n        if self.structured_output:\n            self._logits_processor = self._prepare_structured_output(\n                self.structured_output\n            )\n\n        # NOTE: Here because of the custom `logging` interface used, since it will create the logging name\n        # out of the model name, which won't be available until the `Llama` instance is created.\n        super().load()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self._model.model_path  # type: ignore\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        extra_generation_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for the given input using the Llama model.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            extra_generation_kwargs: dictionary with additional arguments to be passed to\n                the `create_chat_completion` method. Reference at\n                https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n\n        batch_outputs = []\n        for input in inputs:\n            outputs = []\n            for _ in range(num_generations):\n                # NOTE(plaguss): There seems to be a bug in how the logits processor\n                # is used. Basically it consumes the FSM internally, and it isn't reinitialized\n                # after each generation, so subsequent calls yield nothing. This is a workaround\n                # until is fixed in the `llama_cpp` or `outlines` libraries.\n                if self.structured_output:\n                    self._logits_processor = self._prepare_structured_output(\n                        self.structured_output\n                    )\n                chat_completions: \"CreateChatCompletionResponse\" = (\n                    self._model.create_chat_completion(  # type: ignore\n                        messages=input,  # type: ignore\n                        max_tokens=max_new_tokens,\n                        frequency_penalty=frequency_penalty,\n                        presence_penalty=presence_penalty,\n                        temperature=temperature,\n                        top_p=top_p,\n                        logits_processor=self._logits_processor,\n                        **(extra_generation_kwargs or {}),\n                    )\n                )\n                outputs.append(chat_completions[\"choices\"][0][\"message\"][\"content\"])\n            batch_outputs.append(outputs)\n        return batch_outputs\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[\"LogitsProcessorList\", None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(structured_output, \"llamacpp\", self._model)\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"reference/distilabel/llms/llamacpp/#distilabel.llms.llamacpp.LlamaCppLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/llamacpp/#distilabel.llms.llamacpp.LlamaCppLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, extra_generation_kwargs=None)</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the Llama model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>extra_generation_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary with additional arguments to be passed to the <code>create_chat_completion</code> method. Reference at https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion</p> <code>None</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    extra_generation_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for the given input using the Llama model.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        extra_generation_kwargs: dictionary with additional arguments to be passed to\n            the `create_chat_completion` method. Reference at\n            https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n\n    batch_outputs = []\n    for input in inputs:\n        outputs = []\n        for _ in range(num_generations):\n            # NOTE(plaguss): There seems to be a bug in how the logits processor\n            # is used. Basically it consumes the FSM internally, and it isn't reinitialized\n            # after each generation, so subsequent calls yield nothing. This is a workaround\n            # until is fixed in the `llama_cpp` or `outlines` libraries.\n            if self.structured_output:\n                self._logits_processor = self._prepare_structured_output(\n                    self.structured_output\n                )\n            chat_completions: \"CreateChatCompletionResponse\" = (\n                self._model.create_chat_completion(  # type: ignore\n                    messages=input,  # type: ignore\n                    max_tokens=max_new_tokens,\n                    frequency_penalty=frequency_penalty,\n                    presence_penalty=presence_penalty,\n                    temperature=temperature,\n                    top_p=top_p,\n                    logits_processor=self._logits_processor,\n                    **(extra_generation_kwargs or {}),\n                )\n            )\n            outputs.append(chat_completions[\"choices\"][0][\"message\"][\"content\"])\n        batch_outputs.append(outputs)\n    return batch_outputs\n</code></pre>"},{"location":"reference/distilabel/llms/llamacpp/#distilabel.llms.llamacpp.LlamaCppLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>Llama</code> model from the <code>model_path</code>.</p> Source code in <code>src/distilabel/llms/llamacpp.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `Llama` model from the `model_path`.\"\"\"\n    try:\n        from llama_cpp import Llama\n    except ImportError as ie:\n        raise ImportError(\n            \"The `llama_cpp` package is required to use the `LlamaCppLLM` class.\"\n        ) from ie\n\n    self._model = Llama(\n        model_path=self.model_path.as_posix(),  # type: ignore\n        seed=self.seed,\n        n_ctx=self.n_ctx,\n        n_batch=self.n_batch,\n        chat_format=self.chat_format,\n        n_gpu_layers=self.n_gpu_layers,\n        verbose=self.verbose,\n        **self.extra_kwargs,\n    )\n\n    if self.structured_output:\n        self._logits_processor = self._prepare_structured_output(\n            self.structured_output\n        )\n\n    # NOTE: Here because of the custom `logging` interface used, since it will create the logging name\n    # out of the model name, which won't be available until the `Llama` instance is created.\n    super().load()\n</code></pre>"},{"location":"reference/distilabel/llms/mistral/","title":"Mistral","text":""},{"location":"reference/distilabel/llms/mistral/#distilabel.llms.mistral.MistralLLM","title":"<code>MistralLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Mistral LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"mistral-tiny\", \"mistral-large\", etc.</p> <code>endpoint</code> <code>str</code> <p>the endpoint to use for the Mistral API. Defaults to \"https://api.mistral.ai\".</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Mistral API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of retries to attempt when a request fails. Defaults to <code>5</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response. Defaults to <code>120</code>.</p> <code>max_concurrent_requests</code> <code>RuntimeParameter[int]</code> <p>the maximum number of concurrent requests to send. Defaults to <code>64</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> <code>_aclient</code> <code>Optional[MistralAsyncClient]</code> <p>the <code>MistralAsyncClient</code> to use for the Mistral API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>api_key</code>: the API key to authenticate the requests to the Mistral API.</li> <li><code>max_retries</code>: the maximum number of retries to attempt when a request fails.     Defaults to <code>5</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response. Defaults to <code>120</code>.</li> <li><code>max_concurrent_requests</code>: the maximum number of concurrent requests to send.     Defaults to <code>64</code>.</li> </ul> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>class MistralLLM(AsyncLLM):\n    \"\"\"Mistral LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"mistral-tiny\", \"mistral-large\", etc.\n        endpoint: the endpoint to use for the Mistral API. Defaults to \"https://api.mistral.ai\".\n        api_key: the API key to authenticate the requests to the Mistral API. Defaults to `None` which\n            means that the value set for the environment variable `OPENAI_API_KEY` will be used, or\n            `None` if not set.\n        max_retries: the maximum number of retries to attempt when a request fails. Defaults to `5`.\n        timeout: the maximum time in seconds to wait for a response. Defaults to `120`.\n        max_concurrent_requests: the maximum number of concurrent requests to send. Defaults\n            to `64`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n        _api_key_env_var: the name of the environment variable to use for the API key. It is meant to\n            be used internally.\n        _aclient: the `MistralAsyncClient` to use for the Mistral API. It is meant to be used internally.\n            Set in the `load` method.\n\n    Runtime parameters:\n        - `api_key`: the API key to authenticate the requests to the Mistral API.\n        - `max_retries`: the maximum number of retries to attempt when a request fails.\n            Defaults to `5`.\n        - `timeout`: the maximum time in seconds to wait for a response. Defaults to `120`.\n        - `max_concurrent_requests`: the maximum number of concurrent requests to send.\n            Defaults to `64`.\n    \"\"\"\n\n    model: str\n    endpoint: str = \"https://api.mistral.ai\"\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_MISTRALAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Mistral API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n    max_concurrent_requests: RuntimeParameter[int] = Field(\n        default=64, description=\"The maximum number of concurrent requests to send.\"\n    )\n\n    _api_key_env_var: str = PrivateAttr(_MISTRALAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"MistralAsyncClient\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `MistralAsyncClient` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from mistralai.async_client import MistralAsyncClient\n        except ImportError as ie:\n            raise ImportError(\n                \"MistralAI Python client is not installed. Please install it using\"\n                \" `pip install mistralai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = MistralAsyncClient(\n            api_key=self.api_key.get_secret_value(),\n            endpoint=self.endpoint,\n            max_retries=self.max_retries,\n            timeout=self.timeout,\n            max_concurrent_requests=self.max_concurrent_requests,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"mistral\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    # TODO: add `num_generations` parameter once Mistral client allows `n` parameter\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        max_new_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the MistralAI async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"max_tokens\": max_new_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n        }\n        generations = []\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n            # TODO:\u00a0This should work just with the _aclient.chat method, but it's not working.\n            # We need to check instructor and see if we can create a PR.\n            completion = await self._aclient.chat.completions.create(**kwargs)\n        else:\n            completion = await self._aclient.chat(**kwargs)\n\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(\n                    f\"Received no response using MistralAI client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n\n    # TODO: remove this function once Mistral client allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"StandardInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"StandardInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/mistral/#distilabel.llms.mistral.MistralLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/mistral/#distilabel.llms.mistral.MistralLLM.agenerate","title":"<code>agenerate(input, max_new_tokens=None, temperature=None, top_p=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the MistralAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_new_tokens</code> <code>Optional[int]</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    max_new_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the MistralAI async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"max_tokens\": max_new_tokens,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n    }\n    generations = []\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n        # TODO:\u00a0This should work just with the _aclient.chat method, but it's not working.\n        # We need to check instructor and see if we can create a PR.\n        completion = await self._aclient.chat.completions.create(**kwargs)\n    else:\n        completion = await self._aclient.chat(**kwargs)\n\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(\n                f\"Received no response using MistralAI client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/mistral/#distilabel.llms.mistral.MistralLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"StandardInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"StandardInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/mistral/#distilabel.llms.mistral.MistralLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>MistralAsyncClient</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/mistral.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `MistralAsyncClient` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from mistralai.async_client import MistralAsyncClient\n    except ImportError as ie:\n        raise ImportError(\n            \"MistralAI Python client is not installed. Please install it using\"\n            \" `pip install mistralai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = MistralAsyncClient(\n        api_key=self.api_key.get_secret_value(),\n        endpoint=self.endpoint,\n        max_retries=self.max_retries,\n        timeout=self.timeout,\n        max_concurrent_requests=self.max_concurrent_requests,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"mistral\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/mixins/","title":"Mixins","text":""},{"location":"reference/distilabel/llms/mixins/#distilabel.llms.mixins.CudaDevicePlacementMixin","title":"<code>CudaDevicePlacementMixin</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Mixin class to assign CUDA devices to the <code>LLM</code> based on the <code>cuda_devices</code> attribute and the device placement information provided in <code>_device_llm_placement_map</code>. Providing the device placement information is optional, but if it is provided, it will be used to assign CUDA devices to the <code>LLM</code>s, trying to avoid using the same device for different <code>LLM</code>s.</p> <p>Attributes:</p> Name Type Description <code>cuda_devices</code> <code>Union[List[int], Literal['auto']]</code> <p>a list with the ID of the CUDA devices to be used by the <code>LLM</code>. If set to \"auto\", the devices will be automatically assigned based on the device placement information provided in <code>_device_llm_placement_map</code>. If set to a list of devices, it will be checked if the devices are available to be used by the <code>LLM</code>. If not, a warning will be logged.</p> <code>_llm_identifier</code> <code>Union[str, None]</code> <p>the identifier of the <code>LLM</code> to be used as key in <code>_device_llm_placement_map</code>.</p> <code>_device_llm_placement_map</code> <code>Union[DictProxy[str, Any], None]</code> <p>a dictionary with the device placement information for each <code>LLM</code>.</p> Source code in <code>src/distilabel/llms/mixins.py</code> <pre><code>class CudaDevicePlacementMixin(BaseModel):\n    \"\"\"Mixin class to assign CUDA devices to the `LLM` based on the `cuda_devices` attribute\n    and the device placement information provided in `_device_llm_placement_map`. Providing\n    the device placement information is optional, but if it is provided, it will be used to\n    assign CUDA devices to the `LLM`s, trying to avoid using the same device for different\n    `LLM`s.\n\n    Attributes:\n        cuda_devices: a list with the ID of the CUDA devices to be used by the `LLM`. If set\n            to \"auto\", the devices will be automatically assigned based on the device\n            placement information provided in `_device_llm_placement_map`. If set to a list\n            of devices, it will be checked if the devices are available to be used by the\n            `LLM`. If not, a warning will be logged.\n        _llm_identifier: the identifier of the `LLM` to be used as key in `_device_llm_placement_map`.\n        _device_llm_placement_map: a dictionary with the device placement information for each\n            `LLM`.\n    \"\"\"\n\n    # TODO: this should be a runtime parameter\n    cuda_devices: Union[List[int], Literal[\"auto\"]] = Field(default=\"auto\")\n\n    _llm_identifier: Union[str, None] = PrivateAttr(default=None)\n    _device_llm_placement_map: Union[\"DictProxy[str, Any]\", None] = PrivateAttr(\n        default=None\n    )\n    _device_llm_placement_lock: Union[\"Lock\", None] = PrivateAttr(default=None)\n    _available_cuda_devices: Union[List[int], None] = PrivateAttr(default=None)\n    _can_check_cuda_devices: bool = PrivateAttr(default=False)\n\n    def load(self) -&gt; None:\n        \"\"\"Assign CUDA devices to the LLM based on the device placement information provided\n        in `_device_llm_placement_map`.\"\"\"\n\n        try:\n            import pynvml\n\n            pynvml.nvmlInit()\n            device_count = pynvml.nvmlDeviceGetCount()\n            self._available_cuda_devices = list(range(device_count))\n            self._can_check_cuda_devices = True\n        except ImportError as ie:\n            if self.cuda_devices == \"auto\":\n                raise ImportError(\n                    \"The 'pynvml' library is not installed. It is required to automatically\"\n                    \" assign CUDA devices to the `LLM`s. Please, install it and try again.\"\n                ) from ie\n\n            if self.cuda_devices:\n                self._logger.warning(  # type: ignore\n                    \"The 'pynvml' library is not installed. It is recommended to install it\"\n                    \" to check if the CUDA devices assigned to the LLM are available.\"\n                )\n\n        self._assign_cuda_devices()\n\n    def set_device_placement_info(\n        self,\n        llm_identifier: str,\n        device_llm_placement_map: \"DictProxy[str, Any]\",\n        device_llm_placement_lock: \"Lock\",\n    ) -&gt; None:\n        \"\"\"Sets the value of `_device_llm_placement_map` to be used to assign CUDA devices\n        to the LLM.\n\n        Args:\n            llm_identifier: the identifier of the LLM to be used as key in the device\n                placement information.\n            device_llm_placement_map: a dictionary with the device placement information for\n                each LLM. It should have two keys. The first key is \"lock\" and its value is\n                a lock object to be used to synchronize the access to the device placement\n                information. The second key is \"value\" and its value is a dictionary with the\n                device placement information for each LLM.\n            device_llm_placement_lock: a lock object to be used to synchronize the access to\n                `_device_llm_placement_map`.\n        \"\"\"\n        self._llm_identifier = llm_identifier\n        self._device_llm_placement_map = device_llm_placement_map\n        self._device_llm_placement_lock = device_llm_placement_lock\n\n    def _assign_cuda_devices(self) -&gt; None:\n        \"\"\"Assigns CUDA devices to the LLM based on the device placement information provided\n        in `_device_llm_placement_map`. If the `cuda_devices` attribute is set to \"auto\", it\n        will be set to the first available CUDA device that is not going to be used by any\n        other LLM. If the `cuda_devices` attribute is set to a list of devices, it will be\n        checked if the devices are available to be used by the LLM. If not, a warning will be\n        logged.\"\"\"\n\n        if self._device_llm_placement_map is not None:\n            with self._device_llm_placement_lock:  # type: ignore\n                if self.cuda_devices == \"auto\":\n                    self.cuda_devices = [\n                        self._get_cuda_device(self._device_llm_placement_map)\n                    ]\n                else:\n                    self._check_cuda_devices(self._device_llm_placement_map)\n\n                self._device_llm_placement_map[self._llm_identifier] = self.cuda_devices  # type: ignore\n\n        # `_device_llm_placement_map` was not provided and user didn't set the `cuda_devices`\n        # attribute. In this case, the `cuda_devices` attribute will be set to an empty list.\n        if self.cuda_devices == \"auto\":\n            self.cuda_devices = []\n\n        self._set_cuda_visible_devices()\n\n    def _check_cuda_devices(self, device_map: Dict[str, List[int]]) -&gt; None:\n        \"\"\"Checks if the CUDA devices assigned to the LLM are also assigned to other LLMs.\n\n        Args:\n            device_map: a dictionary with the device placement information for each LLM.\n        \"\"\"\n        for device in self.cuda_devices:\n            for llm, devices in device_map.items():\n                if device in devices:\n                    self._logger.warning(\n                        f\"LLM with identifier '{llm}' is also going to use CUDA device \"\n                        f\"'{device}'. This may lead to performance issues or running out\"\n                        \" of memory depending on the device capabilities and the loaded\"\n                        \" models.\"\n                    )\n\n    def _get_cuda_device(self, device_map: Dict[str, List[int]]) -&gt; int:\n        \"\"\"Returns the first available CUDA device to be used by the LLM that is not going\n        to be used by any other LLM.\n\n        Args:\n            device_map: a dictionary with the device placement information for each LLM.\n\n        Returns:\n            The first available CUDA device to be used by the LLM.\n\n        Raises:\n            RuntimeError: if there is no available CUDA device to be used by the LLM.\n        \"\"\"\n        for device in self._available_cuda_devices:\n            if all(device not in devices for devices in device_map.values()):\n                return device\n\n        raise RuntimeError(\n            \"Couldn't find an available CUDA device automatically to be used by the LLM\"\n            f\" '{self._llm_identifier}'. For forcing the use of a specific device, set the\"\n            \" `cuda_devices` attribute to a list with the desired device(s).\"\n        )\n\n    def _set_cuda_visible_devices(self) -&gt; None:\n        \"\"\"Sets the `CUDA_VISIBLE_DEVICES` environment variable to the list of CUDA devices\n        to be used by the LLM.\n        \"\"\"\n        if not self.cuda_devices:\n            return\n\n        if self._can_check_cuda_devices and not all(\n            device in self._available_cuda_devices for device in self.cuda_devices\n        ):\n            raise RuntimeError(\n                f\"Invalid CUDA devices for LLM '{self._llm_identifier}': {self.cuda_devices}.\"\n                f\" The available devices are: {self._available_cuda_devices}. Please, review\"\n                \" the 'cuda_devices' attribute and try again.\"\n            )\n\n        cuda_devices = \",\".join([str(device) for device in self.cuda_devices])\n        self._logger.info(\n            f\"\ud83c\udfae LLM '{self._llm_identifier}' is going to use the following CUDA devices:\"\n            f\" {self.cuda_devices}.\"\n        )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_devices\n</code></pre>"},{"location":"reference/distilabel/llms/mixins/#distilabel.llms.mixins.CudaDevicePlacementMixin.load","title":"<code>load()</code>","text":"<p>Assign CUDA devices to the LLM based on the device placement information provided in <code>_device_llm_placement_map</code>.</p> Source code in <code>src/distilabel/llms/mixins.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Assign CUDA devices to the LLM based on the device placement information provided\n    in `_device_llm_placement_map`.\"\"\"\n\n    try:\n        import pynvml\n\n        pynvml.nvmlInit()\n        device_count = pynvml.nvmlDeviceGetCount()\n        self._available_cuda_devices = list(range(device_count))\n        self._can_check_cuda_devices = True\n    except ImportError as ie:\n        if self.cuda_devices == \"auto\":\n            raise ImportError(\n                \"The 'pynvml' library is not installed. It is required to automatically\"\n                \" assign CUDA devices to the `LLM`s. Please, install it and try again.\"\n            ) from ie\n\n        if self.cuda_devices:\n            self._logger.warning(  # type: ignore\n                \"The 'pynvml' library is not installed. It is recommended to install it\"\n                \" to check if the CUDA devices assigned to the LLM are available.\"\n            )\n\n    self._assign_cuda_devices()\n</code></pre>"},{"location":"reference/distilabel/llms/mixins/#distilabel.llms.mixins.CudaDevicePlacementMixin.set_device_placement_info","title":"<code>set_device_placement_info(llm_identifier, device_llm_placement_map, device_llm_placement_lock)</code>","text":"<p>Sets the value of <code>_device_llm_placement_map</code> to be used to assign CUDA devices to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>llm_identifier</code> <code>str</code> <p>the identifier of the LLM to be used as key in the device placement information.</p> required <code>device_llm_placement_map</code> <code>DictProxy[str, Any]</code> <p>a dictionary with the device placement information for each LLM. It should have two keys. The first key is \"lock\" and its value is a lock object to be used to synchronize the access to the device placement information. The second key is \"value\" and its value is a dictionary with the device placement information for each LLM.</p> required <code>device_llm_placement_lock</code> <code>Lock</code> <p>a lock object to be used to synchronize the access to <code>_device_llm_placement_map</code>.</p> required Source code in <code>src/distilabel/llms/mixins.py</code> <pre><code>def set_device_placement_info(\n    self,\n    llm_identifier: str,\n    device_llm_placement_map: \"DictProxy[str, Any]\",\n    device_llm_placement_lock: \"Lock\",\n) -&gt; None:\n    \"\"\"Sets the value of `_device_llm_placement_map` to be used to assign CUDA devices\n    to the LLM.\n\n    Args:\n        llm_identifier: the identifier of the LLM to be used as key in the device\n            placement information.\n        device_llm_placement_map: a dictionary with the device placement information for\n            each LLM. It should have two keys. The first key is \"lock\" and its value is\n            a lock object to be used to synchronize the access to the device placement\n            information. The second key is \"value\" and its value is a dictionary with the\n            device placement information for each LLM.\n        device_llm_placement_lock: a lock object to be used to synchronize the access to\n            `_device_llm_placement_map`.\n    \"\"\"\n    self._llm_identifier = llm_identifier\n    self._device_llm_placement_map = device_llm_placement_map\n    self._device_llm_placement_lock = device_llm_placement_lock\n</code></pre>"},{"location":"reference/distilabel/llms/ollama/","title":"Ollama","text":""},{"location":"reference/distilabel/llms/ollama/#distilabel.llms.ollama.OllamaLLM","title":"<code>OllamaLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>Ollama LLM implementation running the Async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"notus\".</p> <code>host</code> <code>Optional[RuntimeParameter[str]]</code> <p>the Ollama server host.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the timeout for the LLM. Defaults to <code>120</code>.</p> <code>_aclient</code> <code>Optional[AsyncClient]</code> <p>the <code>AsyncClient</code> to use for the Ollama API. It is meant to be used internally. Set in the <code>load</code> method.</p> Runtime parameters <ul> <li><code>host</code>: the Ollama server host.</li> <li><code>timeout</code>: the client timeout for the Ollama API. Defaults to <code>120</code>.</li> </ul> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>class OllamaLLM(AsyncLLM):\n    \"\"\"Ollama LLM implementation running the Async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"notus\".\n        host: the Ollama server host.\n        timeout: the timeout for the LLM. Defaults to `120`.\n        _aclient: the `AsyncClient` to use for the Ollama API. It is meant to be used internally.\n            Set in the `load` method.\n\n    Runtime parameters:\n        - `host`: the Ollama server host.\n        - `timeout`: the client timeout for the Ollama API. Defaults to `120`.\n    \"\"\"\n\n    model: str\n    host: Optional[RuntimeParameter[str]] = Field(\n        default=None, description=\"The host of the Ollama API.\"\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120, description=\"The timeout for the Ollama API.\"\n    )\n    follow_redirects: bool = True\n\n    _aclient: Optional[\"AsyncClient\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncClient` to use Ollama async API.\"\"\"\n        super().load()\n\n        try:\n            from ollama import AsyncClient\n\n            self._aclient = AsyncClient(\n                host=self.host,\n                timeout=self.timeout,\n                follow_redirects=self.follow_redirects,\n            )\n        except ImportError as e:\n            raise ImportError(\n                \"Ollama Python client is not installed. Please install it using\"\n                \" `pip install ollama`.\"\n            ) from e\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        format: Literal[\"\", \"json\"] = \"\",\n        # TODO: include relevant options from `Options` in `agenerate` method.\n        options: Union[Options, None] = None,\n        keep_alive: Union[bool, None] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Generates a response asynchronously, using the [Ollama Async API definition](https://github.com/ollama/ollama-python).\n\n        Args:\n            input: the input to use for the generation.\n            num_generations: the number of generations to produce. Defaults to `1`.\n            format: the format to use for the generation. Defaults to `\"\"`.\n            options: the options to use for the generation. Defaults to `None`.\n            keep_alive: whether to keep the connection alive. Defaults to `None`.\n\n        Returns:\n            A list of strings as completion for the given input.\n        \"\"\"\n        generations = []\n        # TODO: remove this for-loop and override the `generate` method\n        for _ in range(num_generations):\n            completion = await self._aclient.chat(  # type: ignore\n                model=self.model,\n                messages=input,  # type: ignore\n                stream=False,\n                format=format,\n                options=options,\n                keep_alive=keep_alive,\n            )\n            # TODO: improve error handling\n            generations.append(completion[\"message\"][\"content\"])\n\n        return generations\n</code></pre>"},{"location":"reference/distilabel/llms/ollama/#distilabel.llms.ollama.OllamaLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/ollama/#distilabel.llms.ollama.OllamaLLM.agenerate","title":"<code>agenerate(input, num_generations=1, format='', options=None, keep_alive=None)</code>  <code>async</code>","text":"<p>Generates a response asynchronously, using the Ollama Async API definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>the input to use for the generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to produce. Defaults to <code>1</code>.</p> <code>1</code> <code>format</code> <code>Literal['', 'json']</code> <p>the format to use for the generation. Defaults to <code>\"\"</code>.</p> <code>''</code> <code>options</code> <code>Union[Options, None]</code> <p>the options to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>keep_alive</code> <code>Union[bool, None]</code> <p>whether to keep the connection alive. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings as completion for the given input.</p> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    format: Literal[\"\", \"json\"] = \"\",\n    # TODO: include relevant options from `Options` in `agenerate` method.\n    options: Union[Options, None] = None,\n    keep_alive: Union[bool, None] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Generates a response asynchronously, using the [Ollama Async API definition](https://github.com/ollama/ollama-python).\n\n    Args:\n        input: the input to use for the generation.\n        num_generations: the number of generations to produce. Defaults to `1`.\n        format: the format to use for the generation. Defaults to `\"\"`.\n        options: the options to use for the generation. Defaults to `None`.\n        keep_alive: whether to keep the connection alive. Defaults to `None`.\n\n    Returns:\n        A list of strings as completion for the given input.\n    \"\"\"\n    generations = []\n    # TODO: remove this for-loop and override the `generate` method\n    for _ in range(num_generations):\n        completion = await self._aclient.chat(  # type: ignore\n            model=self.model,\n            messages=input,  # type: ignore\n            stream=False,\n            format=format,\n            options=options,\n            keep_alive=keep_alive,\n        )\n        # TODO: improve error handling\n        generations.append(completion[\"message\"][\"content\"])\n\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/ollama/#distilabel.llms.ollama.OllamaLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncClient</code> to use Ollama async API.</p> Source code in <code>src/distilabel/llms/ollama.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncClient` to use Ollama async API.\"\"\"\n    super().load()\n\n    try:\n        from ollama import AsyncClient\n\n        self._aclient = AsyncClient(\n            host=self.host,\n            timeout=self.timeout,\n            follow_redirects=self.follow_redirects,\n        )\n    except ImportError as e:\n        raise ImportError(\n            \"Ollama Python client is not installed. Please install it using\"\n            \" `pip install ollama`.\"\n        ) from e\n</code></pre>"},{"location":"reference/distilabel/llms/openai/","title":"Openai","text":""},{"location":"reference/distilabel/llms/openai/#distilabel.llms.openai.OpenAILLM","title":"<code>OpenAILLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>OpenAI LLM implementation running the async API client.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gpt-3.5-turbo\", \"gpt-4\", etc. Supported models can be found here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the OpenAI API requests. Defaults to <code>None</code>, which means that the value set for the environment variable <code>OPENAI_BASE_URL</code> will be used, or \"https://api.openai.com/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the OpenAI API. Defaults to <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>max_retries</code> <code>RuntimeParameter[int]</code> <p>the maximum number of times to retry the request to the API before failing. Defaults to <code>6</code>.</p> <code>timeout</code> <code>RuntimeParameter[int]</code> <p>the maximum time in seconds to wait for a response from the API. Defaults to <code>120</code>.</p> <code>structured_output</code> <code>RuntimeParameter[int]</code> <p>a dictionary containing the structured output configuration configuration using <code>instructor</code>. You can take a look at the dictionary structure in <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> Runtime parameters <ul> <li><code>base_url</code>: the base URL to use for the OpenAI API requests. Defaults to <code>None</code>.</li> <li><code>api_key</code>: the API key to authenticate the requests to the OpenAI API. Defaults     to <code>None</code>.</li> <li><code>max_retries</code>: the maximum number of times to retry the request to the API before     failing. Defaults to <code>6</code>.</li> <li><code>timeout</code>: the maximum time in seconds to wait for a response from the API. Defaults     to <code>120</code>.</li> </ul> Icon <p><code>:simple-openai:</code></p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>class OpenAILLM(AsyncLLM):\n    \"\"\"OpenAI LLM implementation running the async API client.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gpt-3.5-turbo\", \"gpt-4\", etc.\n            Supported models can be found [here](https://platform.openai.com/docs/guides/text-generation).\n        base_url: the base URL to use for the OpenAI API requests. Defaults to `None`, which\n            means that the value set for the environment variable `OPENAI_BASE_URL` will\n            be used, or \"https://api.openai.com/v1\" if not set.\n        api_key: the API key to authenticate the requests to the OpenAI API. Defaults to\n            `None` which means that the value set for the environment variable `OPENAI_API_KEY`\n            will be used, or `None` if not set.\n        max_retries: the maximum number of times to retry the request to the API before\n            failing. Defaults to `6`.\n        timeout: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n        structured_output: a dictionary containing the structured output configuration configuration\n            using `instructor`. You can take a look at the dictionary structure in\n            `InstructorStructuredOutputType` from `distilabel.steps.tasks.structured_outputs.instructor`.\n\n    Runtime parameters:\n        - `base_url`: the base URL to use for the OpenAI API requests. Defaults to `None`.\n        - `api_key`: the API key to authenticate the requests to the OpenAI API. Defaults\n            to `None`.\n        - `max_retries`: the maximum number of times to retry the request to the API before\n            failing. Defaults to `6`.\n        - `timeout`: the maximum time in seconds to wait for a response from the API. Defaults\n            to `120`.\n\n    Icon:\n        `:simple-openai:`\n    \"\"\"\n\n    model: str\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"OPENAI_BASE_URL\", \"https://api.openai.com/v1\"\n        ),\n        description=\"The base URL to use for the OpenAI API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_OPENAI_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the OpenAI API.\",\n    )\n    max_retries: RuntimeParameter[int] = Field(\n        default=6,\n        description=\"The maximum number of times to retry the request to the API before\"\n        \" failing.\",\n    )\n    timeout: RuntimeParameter[int] = Field(\n        default=120,\n        description=\"The maximum time in seconds to wait for a response from the API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_OPENAI_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[\"AsyncOpenAI\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `AsyncOpenAI` client to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from openai import AsyncOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        if self.api_key is None:\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n                f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n            )\n\n        self._aclient = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=self.max_retries,  # type: ignore\n            timeout=self.timeout,\n        )\n\n        if self.structured_output:\n            result = self._prepare_structured_output(\n                structured_output=self.structured_output,\n                client=self._aclient,\n                framework=\"openai\",\n            )\n            self._aclient = result.get(\"client\")\n            if structured_output := result.get(\"structured_output\"):\n                self.structured_output = structured_output\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        stop: Optional[Union[str, List[str]]] = None,\n        response_format: str = \"text\",\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the OpenAI async\n        client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            stop: a string or a list of strings to use as a stop sequence for the generation.\n                Defaults to `None`.\n            response_format: the format of the response to return. Must be one of\n                \"text\" or \"json\". Read the documentation [here](https://platform.openai.com/docs/guides/text-generation/json-mode)\n                for more information on how to use the JSON model from OpenAI. Defaults to `text`.\n\n        Note:\n            If response_format\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        if response_format == \"json\":\n            response_format = \"json_object\"\n        elif response_format != \"text\":\n            raise ValueError(\n                f\"Invalid response format '{response_format}'. Must be either 'text' or 'json'.\"\n            )\n\n        kwargs = {\n            \"messages\": input,  # type: ignore\n            \"model\": self.model,\n            \"max_tokens\": max_new_tokens,\n            \"n\": num_generations,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"stop\": stop,\n            \"timeout\": 50,\n            \"response_format\": {\"type\": response_format},\n        }\n        if self.structured_output:\n            kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n        generations = []\n        completion = await self._aclient.chat.completions.create(**kwargs)\n\n        if self.structured_output:\n            generations.append(completion.model_dump_json())\n            return generations\n\n        for choice in completion.choices:\n            if (content := choice.message.content) is None:\n                self._logger.warning(  # type: ignore\n                    f\"Received no response using OpenAI client (model: '{self.model}').\"\n                    f\" Finish reason was: {choice.finish_reason}\"\n                )\n            generations.append(content)\n        return generations\n</code></pre>"},{"location":"reference/distilabel/llms/openai/#distilabel.llms.openai.OpenAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/openai/#distilabel.llms.openai.OpenAILLM.agenerate","title":"<code>agenerate(input, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, stop=None, response_format='text')</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the OpenAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>a string or a list of strings to use as a stop sequence for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>response_format</code> <code>str</code> <p>the format of the response to return. Must be one of \"text\" or \"json\". Read the documentation here for more information on how to use the JSON model from OpenAI. Defaults to <code>text</code>.</p> <code>'text'</code> Note <p>If response_format</p> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stop: Optional[Union[str, List[str]]] = None,\n    response_format: str = \"text\",\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the OpenAI async\n    client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        stop: a string or a list of strings to use as a stop sequence for the generation.\n            Defaults to `None`.\n        response_format: the format of the response to return. Must be one of\n            \"text\" or \"json\". Read the documentation [here](https://platform.openai.com/docs/guides/text-generation/json-mode)\n            for more information on how to use the JSON model from OpenAI. Defaults to `text`.\n\n    Note:\n        If response_format\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    if response_format == \"json\":\n        response_format = \"json_object\"\n    elif response_format != \"text\":\n        raise ValueError(\n            f\"Invalid response format '{response_format}'. Must be either 'text' or 'json'.\"\n        )\n\n    kwargs = {\n        \"messages\": input,  # type: ignore\n        \"model\": self.model,\n        \"max_tokens\": max_new_tokens,\n        \"n\": num_generations,\n        \"frequency_penalty\": frequency_penalty,\n        \"presence_penalty\": presence_penalty,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"stop\": stop,\n        \"timeout\": 50,\n        \"response_format\": {\"type\": response_format},\n    }\n    if self.structured_output:\n        kwargs = self._prepare_kwargs(kwargs, self.structured_output)\n\n    generations = []\n    completion = await self._aclient.chat.completions.create(**kwargs)\n\n    if self.structured_output:\n        generations.append(completion.model_dump_json())\n        return generations\n\n    for choice in completion.choices:\n        if (content := choice.message.content) is None:\n            self._logger.warning(  # type: ignore\n                f\"Received no response using OpenAI client (model: '{self.model}').\"\n                f\" Finish reason was: {choice.finish_reason}\"\n            )\n        generations.append(content)\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/openai/#distilabel.llms.openai.OpenAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>AsyncOpenAI</code> client to benefit from async requests.</p> Source code in <code>src/distilabel/llms/openai.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `AsyncOpenAI` client to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from openai import AsyncOpenAI\n    except ImportError as ie:\n        raise ImportError(\n            \"OpenAI Python client is not installed. Please install it using\"\n            \" `pip install openai`.\"\n        ) from ie\n\n    if self.api_key is None:\n        raise ValueError(\n            f\"To use `{self.__class__.__name__}` an API key must be provided via `api_key`\"\n            f\" attribute or runtime parameter, or set the environment variable `{self._api_key_env_var}`.\"\n        )\n\n    self._aclient = AsyncOpenAI(\n        base_url=self.base_url,\n        api_key=self.api_key.get_secret_value(),\n        max_retries=self.max_retries,  # type: ignore\n        timeout=self.timeout,\n    )\n\n    if self.structured_output:\n        result = self._prepare_structured_output(\n            structured_output=self.structured_output,\n            client=self._aclient,\n            framework=\"openai\",\n        )\n        self._aclient = result.get(\"client\")\n        if structured_output := result.get(\"structured_output\"):\n            self.structured_output = structured_output\n</code></pre>"},{"location":"reference/distilabel/llms/together/","title":"Together","text":""},{"location":"reference/distilabel/llms/together/#distilabel.llms.together.TogetherLLM","title":"<code>TogetherLLM</code>","text":"<p>               Bases: <code>OpenAILLM</code></p> <p>TogetherLLM LLM implementation running the async API client of OpenAI.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the model name to use for the LLM e.g. \"mistralai/Mixtral-8x7B-Instruct-v0.1\". Supported models can be found here.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Together API can be set with <code>TOGETHER_BASE_URL</code>. Defaults to <code>None</code> which means that the value set for the environment variable <code>TOGETHER_BASE_URL</code> will be used, or \"https://api.together.xyz/v1\" if not set.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Together API. Defaults to <code>None</code> which means that the value set for the environment variable <code>TOGETHER_API_KEY</code> will be used, or <code>None</code> if not set.</p> <code>_api_key_env_var</code> <code>str</code> <p>the name of the environment variable to use for the API key. It is meant to be used internally.</p> Source code in <code>src/distilabel/llms/together.py</code> <pre><code>class TogetherLLM(OpenAILLM):\n    \"\"\"TogetherLLM LLM implementation running the async API client of OpenAI.\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"mistralai/Mixtral-8x7B-Instruct-v0.1\".\n            Supported models can be found [here](https://api.together.xyz/models).\n        base_url: the base URL to use for the Together API can be set with `TOGETHER_BASE_URL`.\n            Defaults to `None` which means that the value set for the environment variable\n            `TOGETHER_BASE_URL` will be used, or \"https://api.together.xyz/v1\" if not set.\n        api_key: the API key to authenticate the requests to the Together API. Defaults to `None`\n            which means that the value set for the environment variable `TOGETHER_API_KEY` will be\n            used, or `None` if not set.\n        _api_key_env_var: the name of the environment variable to use for the API key. It\n            is meant to be used internally.\n    \"\"\"\n\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\n            \"TOGETHER_BASE_URL\", \"https://api.together.xyz/v1\"\n        ),\n        description=\"The base URL to use for the Together API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_TOGETHER_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Together API.\",\n    )\n\n    _api_key_env_var: str = PrivateAttr(_TOGETHER_API_KEY_ENV_VAR_NAME)\n</code></pre>"},{"location":"reference/distilabel/llms/typing/","title":"Typing","text":""},{"location":"reference/distilabel/llms/vertexai/","title":"Vertexai","text":""},{"location":"reference/distilabel/llms/vertexai/#distilabel.llms.vertexai.VertexAILLM","title":"<code>VertexAILLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>VertexAI LLM implementation running the async API clients for Gemini.</p> <ul> <li>Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini</li> </ul> <p>To use the <code>VertexAILLM</code> is necessary to have configured the Google Cloud authentication using one of these methods:</p> <ul> <li>Setting <code>GOOGLE_CLOUD_CREDENTIALS</code> environment variable</li> <li>Using <code>gcloud auth application-default login</code> command</li> <li>Using <code>vertexai.init</code> function from the <code>google-cloud-aiplatform</code> library</li> </ul> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model name to use for the LLM e.g. \"gemini-1.0-pro\". Supported models.</p> <code>_aclient</code> <code>Optional[GenerativeModel]</code> <p>the <code>GenerativeModel</code> to use for the Vertex AI Gemini API. It is meant to be used internally. Set in the <code>load</code> method.</p> Icon <p><code>:simple-googlecloud:</code></p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>class VertexAILLM(AsyncLLM):\n    \"\"\"VertexAI LLM implementation running the async API clients for Gemini.\n\n    - Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n\n    To use the `VertexAILLM` is necessary to have configured the Google Cloud authentication\n    using one of these methods:\n\n    - Setting `GOOGLE_CLOUD_CREDENTIALS` environment variable\n    - Using `gcloud auth application-default login` command\n    - Using `vertexai.init` function from the `google-cloud-aiplatform` library\n\n    Attributes:\n        model: the model name to use for the LLM e.g. \"gemini-1.0-pro\". [Supported models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models).\n        _aclient: the `GenerativeModel` to use for the Vertex AI Gemini API. It is meant\n            to be used internally. Set in the `load` method.\n\n    Icon:\n        `:simple-googlecloud:`\n    \"\"\"\n\n    model: str\n\n    _aclient: Optional[\"GenerativeModel\"] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `GenerativeModel` class which has access to `generate_content_async` to benefit from async requests.\"\"\"\n        super().load()\n\n        try:\n            from vertexai.generative_models import GenerationConfig, GenerativeModel\n\n            self._generation_config_class = GenerationConfig\n        except ImportError as e:\n            raise ImportError(\n                \"vertexai is not installed. Please install it using\"\n                \" `pip install google-cloud-aiplatform`.\"\n            ) from e\n\n        if _is_gemini_model(self.model):\n            self._aclient = GenerativeModel(model_name=self.model)\n        else:\n            raise NotImplementedError(\n                \"`VertexAILLM` is only implemented for `gemini` models that allow for `ChatType` data.\"\n            )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def _chattype_to_content(self, input: \"StandardInput\") -&gt; List[\"Content\"]:\n        \"\"\"Converts a chat type to a list of content items expected by the API.\n\n        Args:\n            input: the chat type to be converted.\n\n        Returns:\n            List[str]: a list of content items expected by the API.\n        \"\"\"\n        from vertexai.generative_models import Content, Part\n\n        contents = []\n        for message in input:\n            if message[\"role\"] not in [\"user\", \"model\"]:\n                raise ValueError(\n                    \"`VertexAILLM only supports the roles 'user' or 'model'.\"\n                )\n            contents.append(\n                Content(\n                    role=message[\"role\"], parts=[Part.from_text(message[\"content\"])]\n                )\n            )\n        return contents\n\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: StandardInput,\n        num_generations: int = 1,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        max_output_tokens: Optional[int] = None,\n        stop_sequences: Optional[List[str]] = None,\n        safety_settings: Optional[Dict[str, Any]] = None,\n        tools: Optional[List[Dict[str, Any]]] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates `num_generations` responses for the given input using the [VertexAI async client definition](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini).\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            temperature: Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to `None`.\n            top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to `None`.\n            top_k: If specified, top-k sampling will be used. Defaults to `None`.\n            max_output_tokens: The maximum number of output tokens to generate per message. Defaults to `None`.\n            stop_sequences: A list of stop sequences. Defaults to `None`.\n            safety_settings: Safety configuration for returned content from the API. Defaults to `None`.\n            tools: A potential list of tools that can be used by the API. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        from vertexai.generative_models import GenerationConfig\n\n        contents = self._chattype_to_content(input)\n        generations = []\n        # TODO: remove this for-loop and override `generate`\n        for _ in range(num_generations):\n            content = await self._aclient.generate_content_async(  # type: ignore\n                contents=contents,\n                generation_config=GenerationConfig(\n                    candidate_count=1,  # only one candidate allowed per call\n                    temperature=temperature,\n                    top_k=top_k,\n                    top_p=top_p,\n                    max_output_tokens=max_output_tokens,\n                    stop_sequences=stop_sequences,\n                ),\n                safety_settings=safety_settings,\n                tools=tools,\n                stream=False,\n            )\n\n            text = None\n            try:\n                text = content.candidates[0].text\n            except ValueError:\n                self._logger.warning(\n                    f\"Received no response using VertexAI client (model: '{self.model}').\"\n                    f\" Finish reason was: '{content.candidates[0].finish_reason}'.\"\n                )\n            generations.append(text)\n\n        return generations\n</code></pre>"},{"location":"reference/distilabel/llms/vertexai/#distilabel.llms.vertexai.VertexAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/vertexai/#distilabel.llms.vertexai.VertexAILLM.agenerate","title":"<code>agenerate(input, num_generations=1, temperature=None, top_p=None, top_k=None, max_output_tokens=None, stop_sequences=None, safety_settings=None, tools=None)</code>  <code>async</code>","text":"<p>Generates <code>num_generations</code> responses for the given input using the VertexAI async client definition.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StandardInput</code> <p>a single input in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>temperature</code> <code>Optional[float]</code> <p>Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to <code>None</code>.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to <code>None</code>.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>If specified, top-k sampling will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_output_tokens</code> <code>Optional[int]</code> <p>The maximum number of output tokens to generate per message. Defaults to <code>None</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>A list of stop sequences. Defaults to <code>None</code>.</p> <code>None</code> <code>safety_settings</code> <code>Optional[Dict[str, Any]]</code> <p>Safety configuration for returned content from the API. Defaults to <code>None</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Dict[str, Any]]]</code> <p>A potential list of tools that can be used by the API. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: StandardInput,\n    num_generations: int = 1,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    safety_settings: Optional[Dict[str, Any]] = None,\n    tools: Optional[List[Dict[str, Any]]] = None,\n) -&gt; GenerateOutput:\n    \"\"\"Generates `num_generations` responses for the given input using the [VertexAI async client definition](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini).\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]. Defaults to `None`.\n        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]. Defaults to `None`.\n        top_k: If specified, top-k sampling will be used. Defaults to `None`.\n        max_output_tokens: The maximum number of output tokens to generate per message. Defaults to `None`.\n        stop_sequences: A list of stop sequences. Defaults to `None`.\n        safety_settings: Safety configuration for returned content from the API. Defaults to `None`.\n        tools: A potential list of tools that can be used by the API. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    from vertexai.generative_models import GenerationConfig\n\n    contents = self._chattype_to_content(input)\n    generations = []\n    # TODO: remove this for-loop and override `generate`\n    for _ in range(num_generations):\n        content = await self._aclient.generate_content_async(  # type: ignore\n            contents=contents,\n            generation_config=GenerationConfig(\n                candidate_count=1,  # only one candidate allowed per call\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                max_output_tokens=max_output_tokens,\n                stop_sequences=stop_sequences,\n            ),\n            safety_settings=safety_settings,\n            tools=tools,\n            stream=False,\n        )\n\n        text = None\n        try:\n            text = content.candidates[0].text\n        except ValueError:\n            self._logger.warning(\n                f\"Received no response using VertexAI client (model: '{self.model}').\"\n                f\" Finish reason was: '{content.candidates[0].finish_reason}'.\"\n            )\n        generations.append(text)\n\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llms/vertexai/#distilabel.llms.vertexai.VertexAILLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>GenerativeModel</code> class which has access to <code>generate_content_async</code> to benefit from async requests.</p> Source code in <code>src/distilabel/llms/vertexai.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `GenerativeModel` class which has access to `generate_content_async` to benefit from async requests.\"\"\"\n    super().load()\n\n    try:\n        from vertexai.generative_models import GenerationConfig, GenerativeModel\n\n        self._generation_config_class = GenerationConfig\n    except ImportError as e:\n        raise ImportError(\n            \"vertexai is not installed. Please install it using\"\n            \" `pip install google-cloud-aiplatform`.\"\n        ) from e\n\n    if _is_gemini_model(self.model):\n        self._aclient = GenerativeModel(model_name=self.model)\n    else:\n        raise NotImplementedError(\n            \"`VertexAILLM` is only implemented for `gemini` models that allow for `ChatType` data.\"\n        )\n</code></pre>"},{"location":"reference/distilabel/llms/vllm/","title":"Vllm","text":""},{"location":"reference/distilabel/llms/vllm/#distilabel.llms.vllm.vLLM","title":"<code>vLLM</code>","text":"<p>               Bases: <code>LLM</code>, <code>CudaDevicePlacementMixin</code></p> <p><code>vLLM</code> library LLM implementation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model Hugging Face Hub repo id or a path to a directory containing the model weights and configuration files.</p> <code>dtype</code> <code>str</code> <p>the data type to use for the model. Defaults to <code>auto</code>.</p> <code>trust_remote_code</code> <code>bool</code> <p>whether to trust the remote code when loading the model. Defaults to <code>False</code>.</p> <code>quantization</code> <code>Optional[str]</code> <p>the quantization mode to use for the model. Defaults to <code>None</code>.</p> <code>revision</code> <code>Optional[str]</code> <p>the revision of the model to load. Defaults to <code>None</code>.</p> <code>tokenizer</code> <code>Optional[str]</code> <p>the tokenizer Hugging Face Hub repo id or a path to a directory containing the tokenizer files. If not provided, the tokenizer will be loaded from the model directory. Defaults to <code>None</code>.</p> <code>tokenizer_mode</code> <code>Literal['auto', 'slow']</code> <p>the mode to use for the tokenizer. Defaults to <code>auto</code>.</p> <code>tokenizer_revision</code> <code>Optional[str]</code> <p>the revision of the tokenizer to load. Defaults to <code>None</code>.</p> <code>skip_tokenizer_init</code> <code>bool</code> <p>whether to skip the initialization of the tokenizer. Defaults to <code>False</code>.</p> <code>chat_template</code> <code>Optional[str]</code> <p>a chat template that will be used to build the prompts before sending them to the model. If not provided, the chat template defined in the tokenizer config will be used. If not provided and the tokenizer doesn't have a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> <code>structured_output</code> <code>Optional[str]</code> <p>a dictionary containing the structured output configuration or if more fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> <code>seed</code> <code>int</code> <p>the seed to use for the random number generator. Defaults to <code>0</code>.</p> <code>extra_kwargs</code> <code>Optional[RuntimeParameter[Dict[str, Any]]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>LLM</code> class of <code>vllm</code> library. Defaults to <code>{}</code>.</p> <code>_model</code> <code>Optional[LLM]</code> <p>the <code>vLLM</code> model instance. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> <code>_tokenizer</code> <code>Optional[PreTrainedTokenizer]</code> <p>the tokenizer instance used to format the prompt before passing it to the <code>LLM</code>. This attribute is meant to be used internally and should not be accessed directly. It will be set in the <code>load</code> method.</p> References <ul> <li>https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py</li> </ul> Runtime parameters <ul> <li><code>extra_kwargs</code>: additional dictionary of keyword arguments that will be passed to     the <code>LLM</code> class of <code>vllm</code> library.</li> </ul> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>class vLLM(LLM, CudaDevicePlacementMixin):\n    \"\"\"`vLLM` library LLM implementation.\n\n    Attributes:\n        model: the model Hugging Face Hub repo id or a path to a directory containing the\n            model weights and configuration files.\n        dtype: the data type to use for the model. Defaults to `auto`.\n        trust_remote_code: whether to trust the remote code when loading the model. Defaults\n            to `False`.\n        quantization: the quantization mode to use for the model. Defaults to `None`.\n        revision: the revision of the model to load. Defaults to `None`.\n        tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing\n            the tokenizer files. If not provided, the tokenizer will be loaded from the\n            model directory. Defaults to `None`.\n        tokenizer_mode: the mode to use for the tokenizer. Defaults to `auto`.\n        tokenizer_revision: the revision of the tokenizer to load. Defaults to `None`.\n        skip_tokenizer_init: whether to skip the initialization of the tokenizer. Defaults\n            to `False`.\n        chat_template: a chat template that will be used to build the prompts before\n            sending them to the model. If not provided, the chat template defined in the\n            tokenizer config will be used. If not provided and the tokenizer doesn't have\n            a chat template, then ChatML template will be used. Defaults to `None`.\n        structured_output: a dictionary containing the structured output configuration or if more\n            fine-grained control is needed, an instance of `OutlinesStructuredOutput`. Defaults to None.\n        seed: the seed to use for the random number generator. Defaults to `0`.\n        extra_kwargs: additional dictionary of keyword arguments that will be passed to the\n            `LLM` class of `vllm` library. Defaults to `{}`.\n        _model: the `vLLM` model instance. This attribute is meant to be used internally\n            and should not be accessed directly. It will be set in the `load` method.\n        _tokenizer: the tokenizer instance used to format the prompt before passing it to\n            the `LLM`. This attribute is meant to be used internally and should not be\n            accessed directly. It will be set in the `load` method.\n\n    References:\n        - https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\n\n    Runtime parameters:\n        - `extra_kwargs`: additional dictionary of keyword arguments that will be passed to\n            the `LLM` class of `vllm` library.\n    \"\"\"\n\n    model: str\n    dtype: str = \"auto\"\n    trust_remote_code: bool = False\n    quantization: Optional[str] = None\n    revision: Optional[str] = None\n\n    tokenizer: Optional[str] = None\n    tokenizer_mode: Literal[\"auto\", \"slow\"] = \"auto\"\n    tokenizer_revision: Optional[str] = None\n    skip_tokenizer_init: bool = False\n    chat_template: Optional[str] = None\n\n    seed: int = 0\n\n    extra_kwargs: Optional[RuntimeParameter[Dict[str, Any]]] = Field(\n        default_factory=dict,\n        description=\"Additional dictionary of keyword arguments that will be passed to the\"\n        \" `vLLM` class of `vllm` library. See all the supported arguments at: \"\n        \"https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\",\n    )\n\n    _model: Optional[\"_vLLM\"] = PrivateAttr(...)\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(...)\n    _logits_processor: Optional[Callable] = PrivateAttr(default=None)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `vLLM` model using either the path or the Hugging Face Hub repository id.\n        Additionally, this method also sets the `chat_template` for the tokenizer, so as to properly\n        parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the\n        default value is ChatML format, unless explicitly provided.\n        \"\"\"\n        super().load()\n\n        CudaDevicePlacementMixin.load(self)\n\n        try:\n            from vllm import LLM as _vLLM\n            from vllm import SamplingParams as _SamplingParams\n\n            global SamplingParams\n            SamplingParams = _SamplingParams\n        except ImportError as ie:\n            raise ImportError(\n                \"vLLM is not installed. Please install it using `pip install vllm`.\"\n            ) from ie\n\n        self._model = _vLLM(\n            self.model,\n            dtype=self.dtype,\n            trust_remote_code=self.trust_remote_code,\n            quantization=self.quantization,\n            revision=self.revision,\n            tokenizer=self.tokenizer,\n            tokenizer_mode=self.tokenizer_mode,\n            tokenizer_revision=self.tokenizer_revision,\n            skip_tokenizer_init=self.skip_tokenizer_init,\n            seed=self.seed,\n            **self.extra_kwargs,\n        )\n\n        self._tokenizer = self._model.get_tokenizer()  # type: ignore\n        if self.chat_template is not None:\n            self._tokenizer.chat_template = self.chat_template  # type: ignore\n        elif (\n            self._tokenizer.chat_template is None  # type: ignore\n            and self._tokenizer.default_chat_template is None  # type: ignore\n        ):\n            self._tokenizer.chat_template = CHATML_TEMPLATE\n\n        if self.structured_output:\n            self._logits_processor = self._prepare_structured_output(\n                self.structured_output\n            )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def prepare_input(self, input: \"StandardInput\") -&gt; str:\n        \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n        as an OpenAI conversation, and adding the generation prompt.\n        \"\"\"\n        return self._tokenizer.apply_chat_template(  # type: ignore\n            input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,  # type: ignore\n        )\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        extra_sampling_params: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for each input using the text generation\n        pipeline.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            top_k: the top-k value to use for the generation. Defaults to `0`.\n            extra_sampling_params: dictionary with additional arguments to be passed to\n                the `SamplingParams` class from `vllm`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        prepared_inputs = [self.prepare_input(input) for input in inputs]\n\n        if extra_sampling_params is None:\n            extra_sampling_params = {}\n\n        sampling_params = SamplingParams(  # type: ignore\n            n=num_generations,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            max_tokens=max_new_tokens,\n            logits_processors=(\n                [self._logits_processor] if self._logits_processor else None\n            ),\n            **extra_sampling_params,\n        )\n\n        batch_outputs = self._model.generate(  # type: ignore\n            prepared_inputs,\n            sampling_params,\n            use_tqdm=False,  # type: ignore\n        )\n        return [\n            [output.text for output in outputs.outputs] for outputs in batch_outputs\n        ]\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Callable, None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(structured_output, \"vllm\", self._model)\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"reference/distilabel/llms/vllm/#distilabel.llms.vllm.vLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/vllm/#distilabel.llms.vllm.vLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, extra_sampling_params=None)</code>","text":"<p>Generates <code>num_generations</code> responses for each input using the text generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to use for the generation. Defaults to <code>0</code>.</p> <code>-1</code> <code>extra_sampling_params</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary with additional arguments to be passed to the <code>SamplingParams</code> class from <code>vllm</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = -1,\n    extra_sampling_params: Optional[Dict[str, Any]] = None,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for each input using the text generation\n    pipeline.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        top_k: the top-k value to use for the generation. Defaults to `0`.\n        extra_sampling_params: dictionary with additional arguments to be passed to\n            the `SamplingParams` class from `vllm`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    prepared_inputs = [self.prepare_input(input) for input in inputs]\n\n    if extra_sampling_params is None:\n        extra_sampling_params = {}\n\n    sampling_params = SamplingParams(  # type: ignore\n        n=num_generations,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        max_tokens=max_new_tokens,\n        logits_processors=(\n            [self._logits_processor] if self._logits_processor else None\n        ),\n        **extra_sampling_params,\n    )\n\n    batch_outputs = self._model.generate(  # type: ignore\n        prepared_inputs,\n        sampling_params,\n        use_tqdm=False,  # type: ignore\n    )\n    return [\n        [output.text for output in outputs.outputs] for outputs in batch_outputs\n    ]\n</code></pre>"},{"location":"reference/distilabel/llms/vllm/#distilabel.llms.vllm.vLLM.load","title":"<code>load()</code>","text":"<p>Loads the <code>vLLM</code> model using either the path or the Hugging Face Hub repository id. Additionally, this method also sets the <code>chat_template</code> for the tokenizer, so as to properly parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the default value is ChatML format, unless explicitly provided.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `vLLM` model using either the path or the Hugging Face Hub repository id.\n    Additionally, this method also sets the `chat_template` for the tokenizer, so as to properly\n    parse the list of OpenAI formatted inputs using the expected format by the model, otherwise, the\n    default value is ChatML format, unless explicitly provided.\n    \"\"\"\n    super().load()\n\n    CudaDevicePlacementMixin.load(self)\n\n    try:\n        from vllm import LLM as _vLLM\n        from vllm import SamplingParams as _SamplingParams\n\n        global SamplingParams\n        SamplingParams = _SamplingParams\n    except ImportError as ie:\n        raise ImportError(\n            \"vLLM is not installed. Please install it using `pip install vllm`.\"\n        ) from ie\n\n    self._model = _vLLM(\n        self.model,\n        dtype=self.dtype,\n        trust_remote_code=self.trust_remote_code,\n        quantization=self.quantization,\n        revision=self.revision,\n        tokenizer=self.tokenizer,\n        tokenizer_mode=self.tokenizer_mode,\n        tokenizer_revision=self.tokenizer_revision,\n        skip_tokenizer_init=self.skip_tokenizer_init,\n        seed=self.seed,\n        **self.extra_kwargs,\n    )\n\n    self._tokenizer = self._model.get_tokenizer()  # type: ignore\n    if self.chat_template is not None:\n        self._tokenizer.chat_template = self.chat_template  # type: ignore\n    elif (\n        self._tokenizer.chat_template is None  # type: ignore\n        and self._tokenizer.default_chat_template is None  # type: ignore\n    ):\n        self._tokenizer.chat_template = CHATML_TEMPLATE\n\n    if self.structured_output:\n        self._logits_processor = self._prepare_structured_output(\n            self.structured_output\n        )\n</code></pre>"},{"location":"reference/distilabel/llms/vllm/#distilabel.llms.vllm.vLLM.prepare_input","title":"<code>prepare_input(input)</code>","text":"<p>Prepares the input by applying the chat template to the input, which is formatted as an OpenAI conversation, and adding the generation prompt.</p> Source code in <code>src/distilabel/llms/vllm.py</code> <pre><code>def prepare_input(self, input: \"StandardInput\") -&gt; str:\n    \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n    as an OpenAI conversation, and adding the generation prompt.\n    \"\"\"\n    return self._tokenizer.apply_chat_template(  # type: ignore\n        input,  # type: ignore\n        tokenize=False,\n        add_generation_prompt=True,  # type: ignore\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/","title":"Index","text":""},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.InferenceEndpointsLLM","title":"<code>InferenceEndpointsLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>InferenceEndpoints LLM implementation running the async API client.</p> <p>This LLM will internally use <code>huggingface_hub.AsyncInferenceClient</code> or <code>openai.AsyncOpenAI</code> depending on the <code>use_openai_client</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>Optional[str]</code> <p>the model ID to use for the LLM as available in the Hugging Face Hub, which will be used to resolve the base URL for the serverless Inference Endpoints API requests. Defaults to <code>None</code>.</p> <code>endpoint_name</code> <code>Optional[RuntimeParameter[str]]</code> <p>the name of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> <code>endpoint_namespace</code> <code>Optional[RuntimeParameter[str]]</code> <p>the namespace of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Inference Endpoints API requests.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Inference Endpoints API.</p> <code>tokenizer_id</code> <code>Optional[str]</code> <p>the tokenizer ID to use for the LLM as available in the Hugging Face Hub. Defaults to <code>None</code>, but defining one is recommended to properly format the prompt.</p> <code>model_display_name</code> <code>Optional[str]</code> <p>the model display name to use for the LLM. Defaults to <code>None</code>.</p> <code>use_openai_client</code> <code>bool</code> <p>whether to use the OpenAI client instead of the Hugging Face client.</p> Icon <p><code>:hugging:</code></p> <p>Examples:</p> <pre><code>Free serverless Inference API:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n\nDedicated Inference Endpoints:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    endpoint_namespace=\"&lt;USER|ORG&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n\nDedicated Inference Endpoints or TGI:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    base_url=\"&lt;BASE_URL&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n</code></pre> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>class InferenceEndpointsLLM(AsyncLLM):\n    \"\"\"InferenceEndpoints LLM implementation running the async API client.\n\n    This LLM will internally use `huggingface_hub.AsyncInferenceClient` or `openai.AsyncOpenAI`\n    depending on the `use_openai_client` attribute.\n\n    Attributes:\n        model_id: the model ID to use for the LLM as available in the Hugging Face Hub, which\n            will be used to resolve the base URL for the serverless Inference Endpoints API requests.\n            Defaults to `None`.\n        endpoint_name: the name of the Inference Endpoint to use for the LLM. Defaults to `None`.\n        endpoint_namespace: the namespace of the Inference Endpoint to use for the LLM. Defaults to `None`.\n        base_url: the base URL to use for the Inference Endpoints API requests.\n        api_key: the API key to authenticate the requests to the Inference Endpoints API.\n        tokenizer_id: the tokenizer ID to use for the LLM as available in the Hugging Face Hub.\n            Defaults to `None`, but defining one is recommended to properly format the prompt.\n        model_display_name: the model display name to use for the LLM. Defaults to `None`.\n        use_openai_client: whether to use the OpenAI client instead of the Hugging Face client.\n\n    Icon:\n        `:hugging:`\n\n    Examples:\n\n        Free serverless Inference API:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n\n        Dedicated Inference Endpoints:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n            api_key=\"&lt;HF_API_KEY&gt;\",\n            endpoint_namespace=\"&lt;USER|ORG&gt;\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n\n        Dedicated Inference Endpoints or TGI:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            api_key=\"&lt;HF_API_KEY&gt;\",\n            base_url=\"&lt;BASE_URL&gt;\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n    \"\"\"\n\n    model_id: Optional[str] = None\n\n    endpoint_name: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The name of the Inference Endpoint to use for the LLM.\",\n    )\n    endpoint_namespace: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The namespace of the Inference Endpoint to use for the LLM.\",\n    )\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The base URL to use for the Inference Endpoints API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default=os.getenv(_INFERENCE_ENDPOINTS_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Inference Endpoints API.\",\n    )\n\n    tokenizer_id: Optional[str] = None\n    model_display_name: Optional[str] = None\n    use_openai_client: bool = False\n\n    grammar: Optional[RuntimeParameter[Grammar]] = Field(\n        default=None,\n        description=\"The grammar to use across all the generations.\",\n    )\n\n    _model_name: Optional[str] = PrivateAttr(default=None)\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(default=None)\n    _api_key_env_var: str = PrivateAttr(_INFERENCE_ENDPOINTS_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[Union[\"AsyncInferenceClient\", \"AsyncOpenAI\"]] = PrivateAttr(...)\n\n    @model_validator(mode=\"after\")  # type: ignore\n    def only_one_of_model_id_endpoint_name_or_base_url_provided(\n        self,\n    ) -&gt; \"InferenceEndpointsLLM\":\n        \"\"\"Validates that only one of `model_id` or `endpoint_name` is provided; and if `base_url` is also\n        provided, a warning will be shown informing the user that the provided `base_url` will be ignored in\n        favour of the dynamically calculated one..\"\"\"\n\n        if self.base_url and (self.model_id or self.endpoint_name):\n            self._logger.warning(  # type: ignore\n                f\"Since the `base_url={self.base_url}` is available and either one of `model_id` or `endpoint_name`\"\n                \" is also provided, the `base_url` will either be ignored or overwritten with the one generated\"\n                \" from either of those args, for serverless or dedicated inference endpoints, respectively.\"\n            )\n\n        if self.base_url and not (self.model_id or self.endpoint_name):\n            return self\n\n        if self.model_id and not self.endpoint_name:\n            return self\n\n        if self.endpoint_name and not self.model_id:\n            return self\n\n        raise ValidationError(\n            \"Only one of `model_id` or `endpoint_name` must be provided. If `base_url` is provided too,\"\n            \" it will be overwritten instead. Found `model_id`={self.model_id}, `endpoint_name`={self.endpoint_name},\"\n            f\" and `base_url`={self.base_url}.\"\n        )\n\n    def load(self) -&gt; None:  # noqa: C901\n        \"\"\"Loads the either the `AsyncInferenceClient` or the `AsyncOpenAI` client to benefit\n        from async requests, running the Hugging Face Inference Endpoint underneath via the\n        `/v1/chat/completions` endpoint, exposed for the models running on TGI using the\n        `text-generation` task.\n\n        Raises:\n            ImportError: if the `openai` Python client is not installed.\n            ImportError: if the `huggingface-hub` Python client is not installed.\n            ValueError: if the model is not currently deployed or is not running the TGI framework.\n            ImportError: if the `transformers` Python client is not installed.\n        \"\"\"\n        super().load()\n\n        try:\n            from huggingface_hub import (\n                AsyncInferenceClient,\n                InferenceClient,\n                constants,\n                get_inference_endpoint,\n            )\n        except ImportError as ie:\n            raise ImportError(\n                \"Hugging Face Hub Python client is not installed. Please install it using\"\n                \" `pip install huggingface-hub`.\"\n            ) from ie\n\n        if self.api_key is None:\n            if not Path(constants.HF_TOKEN_PATH).exists():\n                raise ValueError(\n                    f\"To use `{self.__class__.__name__}` an API key must be provided via\"\n                    \" `api_key` attribute or runtime parameter, set the environment variable\"\n                    f\" `{self._api_key_env_var}` or use the `huggingface-hub` CLI to login\"\n                    \" with `huggingface-cli login`.\"\n                )\n            self.api_key = SecretStr(open(constants.HF_TOKEN_PATH).read().strip())\n\n        if self.model_id is not None:\n            client = InferenceClient()\n            status = client.get_model_status(self.model_id)\n\n            if (\n                status.state not in {\"Loadable\", \"Loaded\"}\n                and status.framework != \"text-generation-inference\"\n            ):\n                raise ValueError(\n                    f\"Model {self.model_id} is not currently deployed or is not running the TGI framework\"\n                )\n\n            self.base_url = client._resolve_url(\n                model=self.model_id, task=\"text-generation\"\n            )\n\n        if self.endpoint_name is not None:\n            client = get_inference_endpoint(\n                name=self.endpoint_name,\n                namespace=self.endpoint_namespace,\n                token=self.api_key.get_secret_value(),\n            )\n            if client.status in [\"paused\", \"scaledToZero\"]:\n                client.resume().wait(timeout=300)\n            elif client.status in [\"initializing\"]:\n                client.wait(timeout=300)\n\n            self.base_url = client.url\n            self._model_name = client.repository\n\n        if self.use_openai_client:\n            try:\n                from openai import AsyncOpenAI\n            except ImportError as ie:\n                raise ImportError(\n                    \"OpenAI Python client is not installed. Please install it using\"\n                    \" `pip install openai`.\"\n                ) from ie\n\n            self._aclient = AsyncOpenAI(\n                base_url=self.base_url,\n                api_key=self.api_key.get_secret_value(),\n                max_retries=6,\n            )\n        else:\n            self._aclient = AsyncInferenceClient(\n                model=self.base_url,\n                token=self.api_key.get_secret_value(),\n            )\n\n        if self.tokenizer_id:\n            try:\n                from transformers import AutoTokenizer\n            except ImportError as ie:\n                raise ImportError(\n                    \"Transformers Python client is not installed. Please install it using\"\n                    \" `pip install transformers`.\"\n                ) from ie\n\n            self._tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n\n    @property\n    @override\n    def model_name(self) -&gt; Union[str, None]:  # type: ignore\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return (\n            self.model_display_name\n            or self._model_name\n            or self.model_id\n            or self.endpoint_name\n            or self.base_url\n        )\n\n    async def _openai_agenerate(\n        self,\n        input: \"StandardInput\",\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: Optional[float] = None,\n        stop: Optional[Union[str, List[str]]] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates completions for the given input using the OpenAI async client.\"\"\"\n        completion = await self._aclient.chat.completions.create(  # type: ignore\n            messages=input,  # type: ignore\n            model=\"tgi\",\n            max_tokens=max_new_tokens,\n            n=1,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop,\n            timeout=50,\n        )\n        if completion.choices[0].message.content is None:\n            self._logger.warning(  # type: ignore\n                f\"\u26a0\ufe0f Received no response using OpenAI client (model: '{self.model_name}').\"\n                f\" Finish reason was: {completion.choices[0].finish_reason}\"\n            )\n        return [completion.choices[0].message.content]\n\n    # TODO: add `num_generations` parameter once either TGI or `AsyncInferenceClient` allows `n` parameter\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: \"FormattedInput\",\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        repetition_penalty: Optional[float] = None,\n        temperature: float = 1.0,\n        do_sample: bool = False,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        typical_p: Optional[float] = None,\n        stop_sequences: Optional[Union[str, List[str]]] = None,\n        return_full_text: bool = False,\n        seed: Optional[int] = None,\n        watermark: bool = False,\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Generates completions for the given input using the OpenAI async client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`. Only applies if `use_openai_client=True`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`. Only applies if `use_openai_client=True`.\n            repetition_penalty: the repetition penalty to use for the generation. Defaults\n                to `None`. Only applies if `use_openai_client=False`.\n            temperature: the temperature to use for the generation. Defaults to `1.0`.\n            do_sample: whether to use sampling for the generation. Defaults to `False`.\n                Only applies if `use_openai_client=False`.\n            top_k: the top-k value to use for the generation. Defaults to `0.8`, since neither\n                `0.0` nor `1.0` are valid values in TGI.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            typical_p: the typical-p value to use for the generation. Defaults to `0.5`.\n            stop_sequences: either a single string or a list of strings containing the sequences\n                to stop the generation at. Defaults to `None`, but will be set to the\n                `tokenizer.eos_token` if available.\n            return_full_text: whether to return the full text of the completion or just the\n                generated text. Defaults to `False`, meaning that only the generated text will be\n                returned.\n            seed: the seed to use for the generation. Defaults to `None`.\n            watermark: whether to add the watermark to the generated text. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        if stop_sequences is not None:\n            if isinstance(stop_sequences, str):\n                stop_sequences = [stop_sequences]\n            if len(stop_sequences) &gt; 4:\n                warnings.warn(\n                    \"Only up to 4 stop sequences are allowed, so keeping the first 4 items only.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                stop_sequences = stop_sequences[:4]\n\n        grammar = None\n        if isinstance(input, tuple):\n            input, grammar = input\n\n        if self.use_openai_client:\n            return await self._openai_agenerate(\n                input=input,\n                max_new_tokens=max_new_tokens,\n                frequency_penalty=frequency_penalty,\n                presence_penalty=presence_penalty,\n                temperature=temperature,\n                top_p=top_p,\n                stop=stop_sequences,\n            )\n\n        if self._tokenizer is not None:\n            prompt = self._tokenizer.apply_chat_template(  # type: ignore\n                conversation=input,  # type: ignore\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        else:\n            # TODO: should we apply a default chat template here instead? e.g. ChatML\n            prompt = \"\\n\".join([message[\"content\"] for message in input])\n\n        try:\n            completion = await self._aclient.text_generation(  # type: ignore\n                prompt=prompt,  # type: ignore\n                max_new_tokens=max_new_tokens,\n                do_sample=do_sample,\n                typical_p=typical_p,\n                repetition_penalty=repetition_penalty,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                stop_sequences=stop_sequences,\n                return_full_text=return_full_text,\n                watermark=watermark,\n                # NOTE: `self.grammar` applies to all the generations, while `grammar` is intended\n                # to be different per each input, and those are not intended to be used together\n                grammar=grammar or self.grammar,  # type: ignore\n                # NOTE: here to ensure that the cache is not used and a different response is\n                # generated every time\n                seed=seed or random.randint(0, 2147483647),\n            )\n            return [completion]\n        except Exception as e:\n            self._logger.warning(  # type: ignore\n                f\"\u26a0\ufe0f Received no response using Inference Client (model: '{self.model_name}').\"\n                f\" Finish reason was: {e}\"\n            )\n            return [None]\n\n    # TODO: remove this function once `AsyncInferenceClient` allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"FormattedInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.InferenceEndpointsLLM.model_name","title":"<code>model_name: Union[str, None]</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.InferenceEndpointsLLM.agenerate","title":"<code>agenerate(input, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, repetition_penalty=None, temperature=1.0, do_sample=False, top_k=None, top_p=None, typical_p=None, stop_sequences=None, return_full_text=False, seed=None, watermark=False)</code>  <code>async</code>","text":"<p>Generates completions for the given input using the OpenAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>FormattedInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>. Only applies if <code>use_openai_client=True</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>. Only applies if <code>use_openai_client=True</code>.</p> <code>0.0</code> <code>repetition_penalty</code> <code>Optional[float]</code> <p>the repetition penalty to use for the generation. Defaults to <code>None</code>. Only applies if <code>use_openai_client=False</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>do_sample</code> <code>bool</code> <p>whether to use sampling for the generation. Defaults to <code>False</code>. Only applies if <code>use_openai_client=False</code>.</p> <code>False</code> <code>top_k</code> <code>Optional[int]</code> <p>the top-k value to use for the generation. Defaults to <code>0.8</code>, since neither <code>0.0</code> nor <code>1.0</code> are valid values in TGI.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>None</code> <code>typical_p</code> <code>Optional[float]</code> <p>the typical-p value to use for the generation. Defaults to <code>0.5</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[Union[str, List[str]]]</code> <p>either a single string or a list of strings containing the sequences to stop the generation at. Defaults to <code>None</code>, but will be set to the <code>tokenizer.eos_token</code> if available.</p> <code>None</code> <code>return_full_text</code> <code>bool</code> <p>whether to return the full text of the completion or just the generated text. Defaults to <code>False</code>, meaning that only the generated text will be returned.</p> <code>False</code> <code>seed</code> <code>Optional[int]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>watermark</code> <code>bool</code> <p>whether to add the watermark to the generated text. Defaults to <code>None</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: \"FormattedInput\",\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    repetition_penalty: Optional[float] = None,\n    temperature: float = 1.0,\n    do_sample: bool = False,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    typical_p: Optional[float] = None,\n    stop_sequences: Optional[Union[str, List[str]]] = None,\n    return_full_text: bool = False,\n    seed: Optional[int] = None,\n    watermark: bool = False,\n) -&gt; \"GenerateOutput\":\n    \"\"\"Generates completions for the given input using the OpenAI async client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`. Only applies if `use_openai_client=True`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`. Only applies if `use_openai_client=True`.\n        repetition_penalty: the repetition penalty to use for the generation. Defaults\n            to `None`. Only applies if `use_openai_client=False`.\n        temperature: the temperature to use for the generation. Defaults to `1.0`.\n        do_sample: whether to use sampling for the generation. Defaults to `False`.\n            Only applies if `use_openai_client=False`.\n        top_k: the top-k value to use for the generation. Defaults to `0.8`, since neither\n            `0.0` nor `1.0` are valid values in TGI.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        typical_p: the typical-p value to use for the generation. Defaults to `0.5`.\n        stop_sequences: either a single string or a list of strings containing the sequences\n            to stop the generation at. Defaults to `None`, but will be set to the\n            `tokenizer.eos_token` if available.\n        return_full_text: whether to return the full text of the completion or just the\n            generated text. Defaults to `False`, meaning that only the generated text will be\n            returned.\n        seed: the seed to use for the generation. Defaults to `None`.\n        watermark: whether to add the watermark to the generated text. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    if stop_sequences is not None:\n        if isinstance(stop_sequences, str):\n            stop_sequences = [stop_sequences]\n        if len(stop_sequences) &gt; 4:\n            warnings.warn(\n                \"Only up to 4 stop sequences are allowed, so keeping the first 4 items only.\",\n                UserWarning,\n                stacklevel=2,\n            )\n            stop_sequences = stop_sequences[:4]\n\n    grammar = None\n    if isinstance(input, tuple):\n        input, grammar = input\n\n    if self.use_openai_client:\n        return await self._openai_agenerate(\n            input=input,\n            max_new_tokens=max_new_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop_sequences,\n        )\n\n    if self._tokenizer is not None:\n        prompt = self._tokenizer.apply_chat_template(  # type: ignore\n            conversation=input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n    else:\n        # TODO: should we apply a default chat template here instead? e.g. ChatML\n        prompt = \"\\n\".join([message[\"content\"] for message in input])\n\n    try:\n        completion = await self._aclient.text_generation(  # type: ignore\n            prompt=prompt,  # type: ignore\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            typical_p=typical_p,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop_sequences=stop_sequences,\n            return_full_text=return_full_text,\n            watermark=watermark,\n            # NOTE: `self.grammar` applies to all the generations, while `grammar` is intended\n            # to be different per each input, and those are not intended to be used together\n            grammar=grammar or self.grammar,  # type: ignore\n            # NOTE: here to ensure that the cache is not used and a different response is\n            # generated every time\n            seed=seed or random.randint(0, 2147483647),\n        )\n        return [completion]\n    except Exception as e:\n        self._logger.warning(  # type: ignore\n            f\"\u26a0\ufe0f Received no response using Inference Client (model: '{self.model_name}').\"\n            f\" Finish reason was: {e}\"\n        )\n        return [None]\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.InferenceEndpointsLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"FormattedInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.InferenceEndpointsLLM.load","title":"<code>load()</code>","text":"<p>Loads the either the <code>AsyncInferenceClient</code> or the <code>AsyncOpenAI</code> client to benefit from async requests, running the Hugging Face Inference Endpoint underneath via the <code>/v1/chat/completions</code> endpoint, exposed for the models running on TGI using the <code>text-generation</code> task.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>if the <code>openai</code> Python client is not installed.</p> <code>ImportError</code> <p>if the <code>huggingface-hub</code> Python client is not installed.</p> <code>ValueError</code> <p>if the model is not currently deployed or is not running the TGI framework.</p> <code>ImportError</code> <p>if the <code>transformers</code> Python client is not installed.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>def load(self) -&gt; None:  # noqa: C901\n    \"\"\"Loads the either the `AsyncInferenceClient` or the `AsyncOpenAI` client to benefit\n    from async requests, running the Hugging Face Inference Endpoint underneath via the\n    `/v1/chat/completions` endpoint, exposed for the models running on TGI using the\n    `text-generation` task.\n\n    Raises:\n        ImportError: if the `openai` Python client is not installed.\n        ImportError: if the `huggingface-hub` Python client is not installed.\n        ValueError: if the model is not currently deployed or is not running the TGI framework.\n        ImportError: if the `transformers` Python client is not installed.\n    \"\"\"\n    super().load()\n\n    try:\n        from huggingface_hub import (\n            AsyncInferenceClient,\n            InferenceClient,\n            constants,\n            get_inference_endpoint,\n        )\n    except ImportError as ie:\n        raise ImportError(\n            \"Hugging Face Hub Python client is not installed. Please install it using\"\n            \" `pip install huggingface-hub`.\"\n        ) from ie\n\n    if self.api_key is None:\n        if not Path(constants.HF_TOKEN_PATH).exists():\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via\"\n                \" `api_key` attribute or runtime parameter, set the environment variable\"\n                f\" `{self._api_key_env_var}` or use the `huggingface-hub` CLI to login\"\n                \" with `huggingface-cli login`.\"\n            )\n        self.api_key = SecretStr(open(constants.HF_TOKEN_PATH).read().strip())\n\n    if self.model_id is not None:\n        client = InferenceClient()\n        status = client.get_model_status(self.model_id)\n\n        if (\n            status.state not in {\"Loadable\", \"Loaded\"}\n            and status.framework != \"text-generation-inference\"\n        ):\n            raise ValueError(\n                f\"Model {self.model_id} is not currently deployed or is not running the TGI framework\"\n            )\n\n        self.base_url = client._resolve_url(\n            model=self.model_id, task=\"text-generation\"\n        )\n\n    if self.endpoint_name is not None:\n        client = get_inference_endpoint(\n            name=self.endpoint_name,\n            namespace=self.endpoint_namespace,\n            token=self.api_key.get_secret_value(),\n        )\n        if client.status in [\"paused\", \"scaledToZero\"]:\n            client.resume().wait(timeout=300)\n        elif client.status in [\"initializing\"]:\n            client.wait(timeout=300)\n\n        self.base_url = client.url\n        self._model_name = client.repository\n\n    if self.use_openai_client:\n        try:\n            from openai import AsyncOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        self._aclient = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=6,\n        )\n    else:\n        self._aclient = AsyncInferenceClient(\n            model=self.base_url,\n            token=self.api_key.get_secret_value(),\n        )\n\n    if self.tokenizer_id:\n        try:\n            from transformers import AutoTokenizer\n        except ImportError as ie:\n            raise ImportError(\n                \"Transformers Python client is not installed. Please install it using\"\n                \" `pip install transformers`.\"\n            ) from ie\n\n        self._tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.InferenceEndpointsLLM.only_one_of_model_id_endpoint_name_or_base_url_provided","title":"<code>only_one_of_model_id_endpoint_name_or_base_url_provided()</code>","text":"<p>Validates that only one of <code>model_id</code> or <code>endpoint_name</code> is provided; and if <code>base_url</code> is also provided, a warning will be shown informing the user that the provided <code>base_url</code> will be ignored in favour of the dynamically calculated one..</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@model_validator(mode=\"after\")  # type: ignore\ndef only_one_of_model_id_endpoint_name_or_base_url_provided(\n    self,\n) -&gt; \"InferenceEndpointsLLM\":\n    \"\"\"Validates that only one of `model_id` or `endpoint_name` is provided; and if `base_url` is also\n    provided, a warning will be shown informing the user that the provided `base_url` will be ignored in\n    favour of the dynamically calculated one..\"\"\"\n\n    if self.base_url and (self.model_id or self.endpoint_name):\n        self._logger.warning(  # type: ignore\n            f\"Since the `base_url={self.base_url}` is available and either one of `model_id` or `endpoint_name`\"\n            \" is also provided, the `base_url` will either be ignored or overwritten with the one generated\"\n            \" from either of those args, for serverless or dedicated inference endpoints, respectively.\"\n        )\n\n    if self.base_url and not (self.model_id or self.endpoint_name):\n        return self\n\n    if self.model_id and not self.endpoint_name:\n        return self\n\n    if self.endpoint_name and not self.model_id:\n        return self\n\n    raise ValidationError(\n        \"Only one of `model_id` or `endpoint_name` must be provided. If `base_url` is provided too,\"\n        \" it will be overwritten instead. Found `model_id`={self.model_id}, `endpoint_name`={self.endpoint_name},\"\n        f\" and `base_url`={self.base_url}.\"\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.TransformersLLM","title":"<code>TransformersLLM</code>","text":"<p>               Bases: <code>LLM</code>, <code>CudaDevicePlacementMixin</code></p> <p>Hugging Face <code>transformers</code> library LLM implementation using the text generation pipeline.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model Hugging Face Hub repo id or a path to a directory containing the model weights and configuration files.</p> <code>revision</code> <code>str</code> <p>if <code>model</code> refers to a Hugging Face Hub repository, then the revision (e.g. a branch name or a commit id) to use. Defaults to <code>\"main\"</code>.</p> <code>torch_dtype</code> <code>str</code> <p>the torch dtype to use for the model e.g. \"float16\", \"float32\", etc. Defaults to <code>\"auto\"</code>.</p> <code>trust_remote_code</code> <code>bool</code> <p>whether to trust or not remote (code in the Hugging Face Hub repository) code to load the model. Defaults to <code>False</code>.</p> <code>model_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>from_pretrained</code> method of the model.</p> <code>tokenizer</code> <code>Optional[str]</code> <p>the tokenizer Hugging Face Hub repo id or a path to a directory containing the tokenizer config files. If not provided, the one associated to the <code>model</code> will be used. Defaults to <code>None</code>.</p> <code>use_fast</code> <code>bool</code> <p>whether to use a fast tokenizer or not. Defaults to <code>True</code>.</p> <code>chat_template</code> <code>Optional[str]</code> <p>a chat template that will be used to build the prompts before sending them to the model. If not provided, the chat template defined in the tokenizer config will be used. If not provided and the tokenizer doesn't have a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> <code>device</code> <code>Optional[Union[str, int]]</code> <p>the name or index of the device where the model will be loaded. Defaults to <code>None</code>.</p> <code>device_map</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>a dictionary mapping each layer of the model to a device, or a mode like <code>\"sequential\"</code> or <code>\"auto\"</code>. Defaults to <code>None</code>.</p> <code>token</code> <code>Optional[str]</code> <p>the Hugging Face Hub token that will be used to authenticate to the Hugging Face Hub. If not provided, the <code>HF_TOKEN</code> environment or <code>huggingface_hub</code> package local configuration will be used. Defaults to <code>None</code>.</p> Icon <p><code>:hugging:</code></p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>class TransformersLLM(LLM, CudaDevicePlacementMixin):\n    \"\"\"Hugging Face `transformers` library LLM implementation using the text generation\n    pipeline.\n\n    Attributes:\n        model: the model Hugging Face Hub repo id or a path to a directory containing the\n            model weights and configuration files.\n        revision: if `model` refers to a Hugging Face Hub repository, then the revision\n            (e.g. a branch name or a commit id) to use. Defaults to `\"main\"`.\n        torch_dtype: the torch dtype to use for the model e.g. \"float16\", \"float32\", etc.\n            Defaults to `\"auto\"`.\n        trust_remote_code: whether to trust or not remote (code in the Hugging Face Hub\n            repository) code to load the model. Defaults to `False`.\n        model_kwargs: additional dictionary of keyword arguments that will be passed to\n            the `from_pretrained` method of the model.\n        tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing\n            the tokenizer config files. If not provided, the one associated to the `model`\n            will be used. Defaults to `None`.\n        use_fast: whether to use a fast tokenizer or not. Defaults to `True`.\n        chat_template: a chat template that will be used to build the prompts before\n            sending them to the model. If not provided, the chat template defined in the\n            tokenizer config will be used. If not provided and the tokenizer doesn't have\n            a chat template, then ChatML template will be used. Defaults to `None`.\n        device: the name or index of the device where the model will be loaded. Defaults\n            to `None`.\n        device_map: a dictionary mapping each layer of the model to a device, or a mode\n            like `\"sequential\"` or `\"auto\"`. Defaults to `None`.\n        token: the Hugging Face Hub token that will be used to authenticate to the Hugging\n            Face Hub. If not provided, the `HF_TOKEN` environment or `huggingface_hub` package\n            local configuration will be used. Defaults to `None`.\n\n    Icon:\n        `:hugging:`\n    \"\"\"\n\n    model: str\n    revision: str = \"main\"\n    torch_dtype: str = \"auto\"\n    trust_remote_code: bool = False\n    model_kwargs: Optional[Dict[str, Any]] = None\n    tokenizer: Optional[str] = None\n    use_fast: bool = True\n    chat_template: Optional[str] = None\n    device: Optional[Union[str, int]] = None\n    device_map: Optional[Union[str, Dict[str, Any]]] = None\n    token: Optional[str] = None\n\n    _pipeline: Optional[\"Pipeline\"] = PrivateAttr(...)\n    _prefix_allowed_tokens_fn: Union[Callable, None] = PrivateAttr(default=None)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the model and tokenizer and creates the text generation pipeline. In addition,\n        it will configure the tokenizer chat template.\"\"\"\n        if self.device == \"cuda\":\n            CudaDevicePlacementMixin.load(self)\n\n        try:\n            from transformers import pipeline\n        except ImportError as ie:\n            raise ImportError(\n                \"Transformers is not installed. Please install it using `pip install transformers`.\"\n            ) from ie\n\n        self._pipeline = pipeline(\n            \"text-generation\",\n            model=self.model,\n            revision=self.revision,\n            torch_dtype=self.torch_dtype,\n            trust_remote_code=self.trust_remote_code,\n            model_kwargs=self.model_kwargs or {},\n            tokenizer=self.tokenizer or self.model,\n            use_fast=self.use_fast,\n            device=self.device,\n            device_map=self.device_map,\n            token=self.token or os.getenv(\"HF_TOKEN\"),\n            return_full_text=False,\n        )\n\n        if self.chat_template is not None:\n            self._pipeline.tokenizer.chat_template = self.chat_template  # type: ignore\n        elif (\n            self._pipeline.tokenizer.chat_template is None  # type: ignore\n            and self._pipeline.tokenizer.default_chat_template is None  # type: ignore\n        ):\n            self._pipeline.tokenizer.chat_template = CHATML_TEMPLATE  # type: ignore\n\n        if self.structured_output:\n            self._prefix_allowed_tokens_fn = self._prepare_structured_output(\n                self.structured_output\n            )\n\n        super().load()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def prepare_input(self, input: \"StandardInput\") -&gt; str:\n        \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n        as an OpenAI conversation, and adding the generation prompt.\n        \"\"\"\n        return self._pipeline.tokenizer.apply_chat_template(  # type: ignore\n            input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        temperature: float = 0.1,\n        repetition_penalty: float = 1.1,\n        top_p: float = 1.0,\n        top_k: int = 0,\n        do_sample: bool = True,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for each input using the text generation\n        pipeline.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            repetition_penalty: the repetition penalty to use for the generation. Defaults\n                to `1.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            top_k: the top-k value to use for the generation. Defaults to `0`.\n            do_sample: whether to use sampling or not. Defaults to `True`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        prepared_inputs = [self.prepare_input(input=input) for input in inputs]\n\n        outputs: List[List[Dict[str, str]]] = self._pipeline(  # type: ignore\n            prepared_inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            repetition_penalty=repetition_penalty,\n            top_p=top_p,\n            top_k=top_k,\n            do_sample=do_sample,\n            num_return_sequences=num_generations,\n            prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n        )\n        return [\n            [generation[\"generated_text\"] for generation in output]\n            for output in outputs\n        ]\n\n    def get_last_hidden_states(\n        self, inputs: List[\"StandardInput\"]\n    ) -&gt; List[\"HiddenState\"]:\n        \"\"\"Gets the last `hidden_states` of the model for the given inputs. It doesn't\n        execute the task head.\n\n        Args:\n            inputs: a list of inputs in chat format to generate the embeddings for.\n\n        Returns:\n            A list containing the last hidden state for each sequence using a NumPy array\n            with shape [num_tokens, hidden_size].\n        \"\"\"\n        model: \"PreTrainedModel\" = (\n            self._pipeline.model.model  # type: ignore\n            if hasattr(self._pipeline.model, \"model\")  # type: ignore\n            else next(self._pipeline.model.children())  # type: ignore\n        )\n        tokenizer: \"PreTrainedTokenizer\" = self._pipeline.tokenizer  # type: ignore\n        input_ids = tokenizer(\n            [self.prepare_input(input) for input in inputs],  # type: ignore\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(model.device)\n        last_hidden_states = model(**input_ids)[\"last_hidden_state\"]\n\n        return [\n            seq_last_hidden_state[attention_mask.bool(), :].detach().cpu().numpy()\n            for seq_last_hidden_state, attention_mask in zip(\n                last_hidden_states,\n                input_ids[\"attention_mask\"],  # type: ignore\n            )\n        ]\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Callable, None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(\n            structured_output, \"transformers\", self._pipeline\n        )\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.TransformersLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.TransformersLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, temperature=0.1, repetition_penalty=1.1, top_p=1.0, top_k=0, do_sample=True)</code>","text":"<p>Generates <code>num_generations</code> responses for each input using the text generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>0.1</code> <code>repetition_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>1.1</code>.</p> <code>1.1</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to use for the generation. Defaults to <code>0</code>.</p> <code>0</code> <code>do_sample</code> <code>bool</code> <p>whether to use sampling or not. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    temperature: float = 0.1,\n    repetition_penalty: float = 1.1,\n    top_p: float = 1.0,\n    top_k: int = 0,\n    do_sample: bool = True,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for each input using the text generation\n    pipeline.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        repetition_penalty: the repetition penalty to use for the generation. Defaults\n            to `1.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        top_k: the top-k value to use for the generation. Defaults to `0`.\n        do_sample: whether to use sampling or not. Defaults to `True`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    prepared_inputs = [self.prepare_input(input=input) for input in inputs]\n\n    outputs: List[List[Dict[str, str]]] = self._pipeline(  # type: ignore\n        prepared_inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        top_p=top_p,\n        top_k=top_k,\n        do_sample=do_sample,\n        num_return_sequences=num_generations,\n        prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n    )\n    return [\n        [generation[\"generated_text\"] for generation in output]\n        for output in outputs\n    ]\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.TransformersLLM.get_last_hidden_states","title":"<code>get_last_hidden_states(inputs)</code>","text":"<p>Gets the last <code>hidden_states</code> of the model for the given inputs. It doesn't execute the task head.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate the embeddings for.</p> required <p>Returns:</p> Type Description <code>List[HiddenState]</code> <p>A list containing the last hidden state for each sequence using a NumPy array</p> <code>List[HiddenState]</code> <p>with shape [num_tokens, hidden_size].</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def get_last_hidden_states(\n    self, inputs: List[\"StandardInput\"]\n) -&gt; List[\"HiddenState\"]:\n    \"\"\"Gets the last `hidden_states` of the model for the given inputs. It doesn't\n    execute the task head.\n\n    Args:\n        inputs: a list of inputs in chat format to generate the embeddings for.\n\n    Returns:\n        A list containing the last hidden state for each sequence using a NumPy array\n        with shape [num_tokens, hidden_size].\n    \"\"\"\n    model: \"PreTrainedModel\" = (\n        self._pipeline.model.model  # type: ignore\n        if hasattr(self._pipeline.model, \"model\")  # type: ignore\n        else next(self._pipeline.model.children())  # type: ignore\n    )\n    tokenizer: \"PreTrainedTokenizer\" = self._pipeline.tokenizer  # type: ignore\n    input_ids = tokenizer(\n        [self.prepare_input(input) for input in inputs],  # type: ignore\n        return_tensors=\"pt\",\n        padding=True,\n    ).to(model.device)\n    last_hidden_states = model(**input_ids)[\"last_hidden_state\"]\n\n    return [\n        seq_last_hidden_state[attention_mask.bool(), :].detach().cpu().numpy()\n        for seq_last_hidden_state, attention_mask in zip(\n            last_hidden_states,\n            input_ids[\"attention_mask\"],  # type: ignore\n        )\n    ]\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.TransformersLLM.load","title":"<code>load()</code>","text":"<p>Loads the model and tokenizer and creates the text generation pipeline. In addition, it will configure the tokenizer chat template.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the model and tokenizer and creates the text generation pipeline. In addition,\n    it will configure the tokenizer chat template.\"\"\"\n    if self.device == \"cuda\":\n        CudaDevicePlacementMixin.load(self)\n\n    try:\n        from transformers import pipeline\n    except ImportError as ie:\n        raise ImportError(\n            \"Transformers is not installed. Please install it using `pip install transformers`.\"\n        ) from ie\n\n    self._pipeline = pipeline(\n        \"text-generation\",\n        model=self.model,\n        revision=self.revision,\n        torch_dtype=self.torch_dtype,\n        trust_remote_code=self.trust_remote_code,\n        model_kwargs=self.model_kwargs or {},\n        tokenizer=self.tokenizer or self.model,\n        use_fast=self.use_fast,\n        device=self.device,\n        device_map=self.device_map,\n        token=self.token or os.getenv(\"HF_TOKEN\"),\n        return_full_text=False,\n    )\n\n    if self.chat_template is not None:\n        self._pipeline.tokenizer.chat_template = self.chat_template  # type: ignore\n    elif (\n        self._pipeline.tokenizer.chat_template is None  # type: ignore\n        and self._pipeline.tokenizer.default_chat_template is None  # type: ignore\n    ):\n        self._pipeline.tokenizer.chat_template = CHATML_TEMPLATE  # type: ignore\n\n    if self.structured_output:\n        self._prefix_allowed_tokens_fn = self._prepare_structured_output(\n            self.structured_output\n        )\n\n    super().load()\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/#distilabel.llms.huggingface.TransformersLLM.prepare_input","title":"<code>prepare_input(input)</code>","text":"<p>Prepares the input by applying the chat template to the input, which is formatted as an OpenAI conversation, and adding the generation prompt.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def prepare_input(self, input: \"StandardInput\") -&gt; str:\n    \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n    as an OpenAI conversation, and adding the generation prompt.\n    \"\"\"\n    return self._pipeline.tokenizer.apply_chat_template(  # type: ignore\n        input,  # type: ignore\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/inference_endpoints/","title":"Inference endpoints","text":""},{"location":"reference/distilabel/llms/huggingface/inference_endpoints/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM","title":"<code>InferenceEndpointsLLM</code>","text":"<p>               Bases: <code>AsyncLLM</code></p> <p>InferenceEndpoints LLM implementation running the async API client.</p> <p>This LLM will internally use <code>huggingface_hub.AsyncInferenceClient</code> or <code>openai.AsyncOpenAI</code> depending on the <code>use_openai_client</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>Optional[str]</code> <p>the model ID to use for the LLM as available in the Hugging Face Hub, which will be used to resolve the base URL for the serverless Inference Endpoints API requests. Defaults to <code>None</code>.</p> <code>endpoint_name</code> <code>Optional[RuntimeParameter[str]]</code> <p>the name of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> <code>endpoint_namespace</code> <code>Optional[RuntimeParameter[str]]</code> <p>the namespace of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> <code>base_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>the base URL to use for the Inference Endpoints API requests.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>the API key to authenticate the requests to the Inference Endpoints API.</p> <code>tokenizer_id</code> <code>Optional[str]</code> <p>the tokenizer ID to use for the LLM as available in the Hugging Face Hub. Defaults to <code>None</code>, but defining one is recommended to properly format the prompt.</p> <code>model_display_name</code> <code>Optional[str]</code> <p>the model display name to use for the LLM. Defaults to <code>None</code>.</p> <code>use_openai_client</code> <code>bool</code> <p>whether to use the OpenAI client instead of the Hugging Face client.</p> Icon <p><code>:hugging:</code></p> <p>Examples:</p> <pre><code>Free serverless Inference API:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n\nDedicated Inference Endpoints:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    endpoint_namespace=\"&lt;USER|ORG&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n\nDedicated Inference Endpoints or TGI:\n\n```python\nfrom distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    base_url=\"&lt;BASE_URL&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n```\n</code></pre> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>class InferenceEndpointsLLM(AsyncLLM):\n    \"\"\"InferenceEndpoints LLM implementation running the async API client.\n\n    This LLM will internally use `huggingface_hub.AsyncInferenceClient` or `openai.AsyncOpenAI`\n    depending on the `use_openai_client` attribute.\n\n    Attributes:\n        model_id: the model ID to use for the LLM as available in the Hugging Face Hub, which\n            will be used to resolve the base URL for the serverless Inference Endpoints API requests.\n            Defaults to `None`.\n        endpoint_name: the name of the Inference Endpoint to use for the LLM. Defaults to `None`.\n        endpoint_namespace: the namespace of the Inference Endpoint to use for the LLM. Defaults to `None`.\n        base_url: the base URL to use for the Inference Endpoints API requests.\n        api_key: the API key to authenticate the requests to the Inference Endpoints API.\n        tokenizer_id: the tokenizer ID to use for the LLM as available in the Hugging Face Hub.\n            Defaults to `None`, but defining one is recommended to properly format the prompt.\n        model_display_name: the model display name to use for the LLM. Defaults to `None`.\n        use_openai_client: whether to use the OpenAI client instead of the Hugging Face client.\n\n    Icon:\n        `:hugging:`\n\n    Examples:\n\n        Free serverless Inference API:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n\n        Dedicated Inference Endpoints:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n            api_key=\"&lt;HF_API_KEY&gt;\",\n            endpoint_namespace=\"&lt;USER|ORG&gt;\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n\n        Dedicated Inference Endpoints or TGI:\n\n        ```python\n        from distilabel.llms.huggingface import InferenceEndpointsLLM\n\n        llm = InferenceEndpointsLLM(\n            api_key=\"&lt;HF_API_KEY&gt;\",\n            base_url=\"&lt;BASE_URL&gt;\",\n        )\n\n        llm.load()\n\n        # Synchrounous request\n        output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n        # Asynchronous request\n        output = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n        ```\n    \"\"\"\n\n    model_id: Optional[str] = None\n\n    endpoint_name: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The name of the Inference Endpoint to use for the LLM.\",\n    )\n    endpoint_namespace: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The namespace of the Inference Endpoint to use for the LLM.\",\n    )\n    base_url: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The base URL to use for the Inference Endpoints API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default=os.getenv(_INFERENCE_ENDPOINTS_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Inference Endpoints API.\",\n    )\n\n    tokenizer_id: Optional[str] = None\n    model_display_name: Optional[str] = None\n    use_openai_client: bool = False\n\n    grammar: Optional[RuntimeParameter[Grammar]] = Field(\n        default=None,\n        description=\"The grammar to use across all the generations.\",\n    )\n\n    _model_name: Optional[str] = PrivateAttr(default=None)\n    _tokenizer: Optional[\"PreTrainedTokenizer\"] = PrivateAttr(default=None)\n    _api_key_env_var: str = PrivateAttr(_INFERENCE_ENDPOINTS_API_KEY_ENV_VAR_NAME)\n    _aclient: Optional[Union[\"AsyncInferenceClient\", \"AsyncOpenAI\"]] = PrivateAttr(...)\n\n    @model_validator(mode=\"after\")  # type: ignore\n    def only_one_of_model_id_endpoint_name_or_base_url_provided(\n        self,\n    ) -&gt; \"InferenceEndpointsLLM\":\n        \"\"\"Validates that only one of `model_id` or `endpoint_name` is provided; and if `base_url` is also\n        provided, a warning will be shown informing the user that the provided `base_url` will be ignored in\n        favour of the dynamically calculated one..\"\"\"\n\n        if self.base_url and (self.model_id or self.endpoint_name):\n            self._logger.warning(  # type: ignore\n                f\"Since the `base_url={self.base_url}` is available and either one of `model_id` or `endpoint_name`\"\n                \" is also provided, the `base_url` will either be ignored or overwritten with the one generated\"\n                \" from either of those args, for serverless or dedicated inference endpoints, respectively.\"\n            )\n\n        if self.base_url and not (self.model_id or self.endpoint_name):\n            return self\n\n        if self.model_id and not self.endpoint_name:\n            return self\n\n        if self.endpoint_name and not self.model_id:\n            return self\n\n        raise ValidationError(\n            \"Only one of `model_id` or `endpoint_name` must be provided. If `base_url` is provided too,\"\n            \" it will be overwritten instead. Found `model_id`={self.model_id}, `endpoint_name`={self.endpoint_name},\"\n            f\" and `base_url`={self.base_url}.\"\n        )\n\n    def load(self) -&gt; None:  # noqa: C901\n        \"\"\"Loads the either the `AsyncInferenceClient` or the `AsyncOpenAI` client to benefit\n        from async requests, running the Hugging Face Inference Endpoint underneath via the\n        `/v1/chat/completions` endpoint, exposed for the models running on TGI using the\n        `text-generation` task.\n\n        Raises:\n            ImportError: if the `openai` Python client is not installed.\n            ImportError: if the `huggingface-hub` Python client is not installed.\n            ValueError: if the model is not currently deployed or is not running the TGI framework.\n            ImportError: if the `transformers` Python client is not installed.\n        \"\"\"\n        super().load()\n\n        try:\n            from huggingface_hub import (\n                AsyncInferenceClient,\n                InferenceClient,\n                constants,\n                get_inference_endpoint,\n            )\n        except ImportError as ie:\n            raise ImportError(\n                \"Hugging Face Hub Python client is not installed. Please install it using\"\n                \" `pip install huggingface-hub`.\"\n            ) from ie\n\n        if self.api_key is None:\n            if not Path(constants.HF_TOKEN_PATH).exists():\n                raise ValueError(\n                    f\"To use `{self.__class__.__name__}` an API key must be provided via\"\n                    \" `api_key` attribute or runtime parameter, set the environment variable\"\n                    f\" `{self._api_key_env_var}` or use the `huggingface-hub` CLI to login\"\n                    \" with `huggingface-cli login`.\"\n                )\n            self.api_key = SecretStr(open(constants.HF_TOKEN_PATH).read().strip())\n\n        if self.model_id is not None:\n            client = InferenceClient()\n            status = client.get_model_status(self.model_id)\n\n            if (\n                status.state not in {\"Loadable\", \"Loaded\"}\n                and status.framework != \"text-generation-inference\"\n            ):\n                raise ValueError(\n                    f\"Model {self.model_id} is not currently deployed or is not running the TGI framework\"\n                )\n\n            self.base_url = client._resolve_url(\n                model=self.model_id, task=\"text-generation\"\n            )\n\n        if self.endpoint_name is not None:\n            client = get_inference_endpoint(\n                name=self.endpoint_name,\n                namespace=self.endpoint_namespace,\n                token=self.api_key.get_secret_value(),\n            )\n            if client.status in [\"paused\", \"scaledToZero\"]:\n                client.resume().wait(timeout=300)\n            elif client.status in [\"initializing\"]:\n                client.wait(timeout=300)\n\n            self.base_url = client.url\n            self._model_name = client.repository\n\n        if self.use_openai_client:\n            try:\n                from openai import AsyncOpenAI\n            except ImportError as ie:\n                raise ImportError(\n                    \"OpenAI Python client is not installed. Please install it using\"\n                    \" `pip install openai`.\"\n                ) from ie\n\n            self._aclient = AsyncOpenAI(\n                base_url=self.base_url,\n                api_key=self.api_key.get_secret_value(),\n                max_retries=6,\n            )\n        else:\n            self._aclient = AsyncInferenceClient(\n                model=self.base_url,\n                token=self.api_key.get_secret_value(),\n            )\n\n        if self.tokenizer_id:\n            try:\n                from transformers import AutoTokenizer\n            except ImportError as ie:\n                raise ImportError(\n                    \"Transformers Python client is not installed. Please install it using\"\n                    \" `pip install transformers`.\"\n                ) from ie\n\n            self._tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n\n    @property\n    @override\n    def model_name(self) -&gt; Union[str, None]:  # type: ignore\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return (\n            self.model_display_name\n            or self._model_name\n            or self.model_id\n            or self.endpoint_name\n            or self.base_url\n        )\n\n    async def _openai_agenerate(\n        self,\n        input: \"StandardInput\",\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: Optional[float] = None,\n        stop: Optional[Union[str, List[str]]] = None,\n    ) -&gt; GenerateOutput:\n        \"\"\"Generates completions for the given input using the OpenAI async client.\"\"\"\n        completion = await self._aclient.chat.completions.create(  # type: ignore\n            messages=input,  # type: ignore\n            model=\"tgi\",\n            max_tokens=max_new_tokens,\n            n=1,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop,\n            timeout=50,\n        )\n        if completion.choices[0].message.content is None:\n            self._logger.warning(  # type: ignore\n                f\"\u26a0\ufe0f Received no response using OpenAI client (model: '{self.model_name}').\"\n                f\" Finish reason was: {completion.choices[0].finish_reason}\"\n            )\n        return [completion.choices[0].message.content]\n\n    # TODO: add `num_generations` parameter once either TGI or `AsyncInferenceClient` allows `n` parameter\n    @validate_call\n    async def agenerate(  # type: ignore\n        self,\n        input: \"FormattedInput\",\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        repetition_penalty: Optional[float] = None,\n        temperature: float = 1.0,\n        do_sample: bool = False,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        typical_p: Optional[float] = None,\n        stop_sequences: Optional[Union[str, List[str]]] = None,\n        return_full_text: bool = False,\n        seed: Optional[int] = None,\n        watermark: bool = False,\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Generates completions for the given input using the OpenAI async client.\n\n        Args:\n            input: a single input in chat format to generate responses for.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            frequency_penalty: the repetition penalty to use for the generation. Defaults\n                to `0.0`. Only applies if `use_openai_client=True`.\n            presence_penalty: the presence penalty to use for the generation. Defaults to\n                `0.0`. Only applies if `use_openai_client=True`.\n            repetition_penalty: the repetition penalty to use for the generation. Defaults\n                to `None`. Only applies if `use_openai_client=False`.\n            temperature: the temperature to use for the generation. Defaults to `1.0`.\n            do_sample: whether to use sampling for the generation. Defaults to `False`.\n                Only applies if `use_openai_client=False`.\n            top_k: the top-k value to use for the generation. Defaults to `0.8`, since neither\n                `0.0` nor `1.0` are valid values in TGI.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            typical_p: the typical-p value to use for the generation. Defaults to `0.5`.\n            stop_sequences: either a single string or a list of strings containing the sequences\n                to stop the generation at. Defaults to `None`, but will be set to the\n                `tokenizer.eos_token` if available.\n            return_full_text: whether to return the full text of the completion or just the\n                generated text. Defaults to `False`, meaning that only the generated text will be\n                returned.\n            seed: the seed to use for the generation. Defaults to `None`.\n            watermark: whether to add the watermark to the generated text. Defaults to `None`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        if stop_sequences is not None:\n            if isinstance(stop_sequences, str):\n                stop_sequences = [stop_sequences]\n            if len(stop_sequences) &gt; 4:\n                warnings.warn(\n                    \"Only up to 4 stop sequences are allowed, so keeping the first 4 items only.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                stop_sequences = stop_sequences[:4]\n\n        grammar = None\n        if isinstance(input, tuple):\n            input, grammar = input\n\n        if self.use_openai_client:\n            return await self._openai_agenerate(\n                input=input,\n                max_new_tokens=max_new_tokens,\n                frequency_penalty=frequency_penalty,\n                presence_penalty=presence_penalty,\n                temperature=temperature,\n                top_p=top_p,\n                stop=stop_sequences,\n            )\n\n        if self._tokenizer is not None:\n            prompt = self._tokenizer.apply_chat_template(  # type: ignore\n                conversation=input,  # type: ignore\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        else:\n            # TODO: should we apply a default chat template here instead? e.g. ChatML\n            prompt = \"\\n\".join([message[\"content\"] for message in input])\n\n        try:\n            completion = await self._aclient.text_generation(  # type: ignore\n                prompt=prompt,  # type: ignore\n                max_new_tokens=max_new_tokens,\n                do_sample=do_sample,\n                typical_p=typical_p,\n                repetition_penalty=repetition_penalty,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                stop_sequences=stop_sequences,\n                return_full_text=return_full_text,\n                watermark=watermark,\n                # NOTE: `self.grammar` applies to all the generations, while `grammar` is intended\n                # to be different per each input, and those are not intended to be used together\n                grammar=grammar or self.grammar,  # type: ignore\n                # NOTE: here to ensure that the cache is not used and a different response is\n                # generated every time\n                seed=seed or random.randint(0, 2147483647),\n            )\n            return [completion]\n        except Exception as e:\n            self._logger.warning(  # type: ignore\n                f\"\u26a0\ufe0f Received no response using Inference Client (model: '{self.model_name}').\"\n                f\" Finish reason was: {e}\"\n            )\n            return [None]\n\n    # TODO: remove this function once `AsyncInferenceClient` allows `n` parameter\n    @override\n    def generate(\n        self,\n        inputs: List[\"FormattedInput\"],\n        num_generations: int = 1,\n        **kwargs: Any,\n    ) -&gt; List[\"GenerateOutput\"]:\n        \"\"\"Method to generate a list of responses asynchronously, returning the output\n        synchronously awaiting for the response of each input sent to `agenerate`.\n        \"\"\"\n\n        async def agenerate(\n            inputs: List[\"FormattedInput\"], **kwargs: Any\n        ) -&gt; \"GenerateOutput\":\n            \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n            tasks = [\n                asyncio.create_task(self.agenerate(input=input, **kwargs))\n                for input in inputs\n                for _ in range(num_generations)\n            ]\n            return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n        outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n        return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/inference_endpoints/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.model_name","title":"<code>model_name: Union[str, None]</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/huggingface/inference_endpoints/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.agenerate","title":"<code>agenerate(input, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, repetition_penalty=None, temperature=1.0, do_sample=False, top_k=None, top_p=None, typical_p=None, stop_sequences=None, return_full_text=False, seed=None, watermark=False)</code>  <code>async</code>","text":"<p>Generates completions for the given input using the OpenAI async client.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>FormattedInput</code> <p>a single input in chat format to generate responses for.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>0.0</code>. Only applies if <code>use_openai_client=True</code>.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to use for the generation. Defaults to <code>0.0</code>. Only applies if <code>use_openai_client=True</code>.</p> <code>0.0</code> <code>repetition_penalty</code> <code>Optional[float]</code> <p>the repetition penalty to use for the generation. Defaults to <code>None</code>. Only applies if <code>use_openai_client=False</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>do_sample</code> <code>bool</code> <p>whether to use sampling for the generation. Defaults to <code>False</code>. Only applies if <code>use_openai_client=False</code>.</p> <code>False</code> <code>top_k</code> <code>Optional[int]</code> <p>the top-k value to use for the generation. Defaults to <code>0.8</code>, since neither <code>0.0</code> nor <code>1.0</code> are valid values in TGI.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>None</code> <code>typical_p</code> <code>Optional[float]</code> <p>the typical-p value to use for the generation. Defaults to <code>0.5</code>.</p> <code>None</code> <code>stop_sequences</code> <code>Optional[Union[str, List[str]]]</code> <p>either a single string or a list of strings containing the sequences to stop the generation at. Defaults to <code>None</code>, but will be set to the <code>tokenizer.eos_token</code> if available.</p> <code>None</code> <code>return_full_text</code> <code>bool</code> <p>whether to return the full text of the completion or just the generated text. Defaults to <code>False</code>, meaning that only the generated text will be returned.</p> <code>False</code> <code>seed</code> <code>Optional[int]</code> <p>the seed to use for the generation. Defaults to <code>None</code>.</p> <code>None</code> <code>watermark</code> <code>bool</code> <p>whether to add the watermark to the generated text. Defaults to <code>None</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>GenerateOutput</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@validate_call\nasync def agenerate(  # type: ignore\n    self,\n    input: \"FormattedInput\",\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    repetition_penalty: Optional[float] = None,\n    temperature: float = 1.0,\n    do_sample: bool = False,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    typical_p: Optional[float] = None,\n    stop_sequences: Optional[Union[str, List[str]]] = None,\n    return_full_text: bool = False,\n    seed: Optional[int] = None,\n    watermark: bool = False,\n) -&gt; \"GenerateOutput\":\n    \"\"\"Generates completions for the given input using the OpenAI async client.\n\n    Args:\n        input: a single input in chat format to generate responses for.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        frequency_penalty: the repetition penalty to use for the generation. Defaults\n            to `0.0`. Only applies if `use_openai_client=True`.\n        presence_penalty: the presence penalty to use for the generation. Defaults to\n            `0.0`. Only applies if `use_openai_client=True`.\n        repetition_penalty: the repetition penalty to use for the generation. Defaults\n            to `None`. Only applies if `use_openai_client=False`.\n        temperature: the temperature to use for the generation. Defaults to `1.0`.\n        do_sample: whether to use sampling for the generation. Defaults to `False`.\n            Only applies if `use_openai_client=False`.\n        top_k: the top-k value to use for the generation. Defaults to `0.8`, since neither\n            `0.0` nor `1.0` are valid values in TGI.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        typical_p: the typical-p value to use for the generation. Defaults to `0.5`.\n        stop_sequences: either a single string or a list of strings containing the sequences\n            to stop the generation at. Defaults to `None`, but will be set to the\n            `tokenizer.eos_token` if available.\n        return_full_text: whether to return the full text of the completion or just the\n            generated text. Defaults to `False`, meaning that only the generated text will be\n            returned.\n        seed: the seed to use for the generation. Defaults to `None`.\n        watermark: whether to add the watermark to the generated text. Defaults to `None`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    if stop_sequences is not None:\n        if isinstance(stop_sequences, str):\n            stop_sequences = [stop_sequences]\n        if len(stop_sequences) &gt; 4:\n            warnings.warn(\n                \"Only up to 4 stop sequences are allowed, so keeping the first 4 items only.\",\n                UserWarning,\n                stacklevel=2,\n            )\n            stop_sequences = stop_sequences[:4]\n\n    grammar = None\n    if isinstance(input, tuple):\n        input, grammar = input\n\n    if self.use_openai_client:\n        return await self._openai_agenerate(\n            input=input,\n            max_new_tokens=max_new_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop_sequences,\n        )\n\n    if self._tokenizer is not None:\n        prompt = self._tokenizer.apply_chat_template(  # type: ignore\n            conversation=input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n    else:\n        # TODO: should we apply a default chat template here instead? e.g. ChatML\n        prompt = \"\\n\".join([message[\"content\"] for message in input])\n\n    try:\n        completion = await self._aclient.text_generation(  # type: ignore\n            prompt=prompt,  # type: ignore\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            typical_p=typical_p,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop_sequences=stop_sequences,\n            return_full_text=return_full_text,\n            watermark=watermark,\n            # NOTE: `self.grammar` applies to all the generations, while `grammar` is intended\n            # to be different per each input, and those are not intended to be used together\n            grammar=grammar or self.grammar,  # type: ignore\n            # NOTE: here to ensure that the cache is not used and a different response is\n            # generated every time\n            seed=seed or random.randint(0, 2147483647),\n        )\n        return [completion]\n    except Exception as e:\n        self._logger.warning(  # type: ignore\n            f\"\u26a0\ufe0f Received no response using Inference Client (model: '{self.model_name}').\"\n            f\" Finish reason was: {e}\"\n        )\n        return [None]\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/inference_endpoints/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.generate","title":"<code>generate(inputs, num_generations=1, **kwargs)</code>","text":"<p>Method to generate a list of responses asynchronously, returning the output synchronously awaiting for the response of each input sent to <code>agenerate</code>.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@override\ndef generate(\n    self,\n    inputs: List[\"FormattedInput\"],\n    num_generations: int = 1,\n    **kwargs: Any,\n) -&gt; List[\"GenerateOutput\"]:\n    \"\"\"Method to generate a list of responses asynchronously, returning the output\n    synchronously awaiting for the response of each input sent to `agenerate`.\n    \"\"\"\n\n    async def agenerate(\n        inputs: List[\"FormattedInput\"], **kwargs: Any\n    ) -&gt; \"GenerateOutput\":\n        \"\"\"Internal function to parallelize the asynchronous generation of responses.\"\"\"\n        tasks = [\n            asyncio.create_task(self.agenerate(input=input, **kwargs))\n            for input in inputs\n            for _ in range(num_generations)\n        ]\n        return [outputs[0] for outputs in await asyncio.gather(*tasks)]\n\n    outputs = self.event_loop.run_until_complete(agenerate(inputs, **kwargs))\n    return list(grouper(outputs, n=num_generations, incomplete=\"ignore\"))\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/inference_endpoints/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.load","title":"<code>load()</code>","text":"<p>Loads the either the <code>AsyncInferenceClient</code> or the <code>AsyncOpenAI</code> client to benefit from async requests, running the Hugging Face Inference Endpoint underneath via the <code>/v1/chat/completions</code> endpoint, exposed for the models running on TGI using the <code>text-generation</code> task.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>if the <code>openai</code> Python client is not installed.</p> <code>ImportError</code> <p>if the <code>huggingface-hub</code> Python client is not installed.</p> <code>ValueError</code> <p>if the model is not currently deployed or is not running the TGI framework.</p> <code>ImportError</code> <p>if the <code>transformers</code> Python client is not installed.</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>def load(self) -&gt; None:  # noqa: C901\n    \"\"\"Loads the either the `AsyncInferenceClient` or the `AsyncOpenAI` client to benefit\n    from async requests, running the Hugging Face Inference Endpoint underneath via the\n    `/v1/chat/completions` endpoint, exposed for the models running on TGI using the\n    `text-generation` task.\n\n    Raises:\n        ImportError: if the `openai` Python client is not installed.\n        ImportError: if the `huggingface-hub` Python client is not installed.\n        ValueError: if the model is not currently deployed or is not running the TGI framework.\n        ImportError: if the `transformers` Python client is not installed.\n    \"\"\"\n    super().load()\n\n    try:\n        from huggingface_hub import (\n            AsyncInferenceClient,\n            InferenceClient,\n            constants,\n            get_inference_endpoint,\n        )\n    except ImportError as ie:\n        raise ImportError(\n            \"Hugging Face Hub Python client is not installed. Please install it using\"\n            \" `pip install huggingface-hub`.\"\n        ) from ie\n\n    if self.api_key is None:\n        if not Path(constants.HF_TOKEN_PATH).exists():\n            raise ValueError(\n                f\"To use `{self.__class__.__name__}` an API key must be provided via\"\n                \" `api_key` attribute or runtime parameter, set the environment variable\"\n                f\" `{self._api_key_env_var}` or use the `huggingface-hub` CLI to login\"\n                \" with `huggingface-cli login`.\"\n            )\n        self.api_key = SecretStr(open(constants.HF_TOKEN_PATH).read().strip())\n\n    if self.model_id is not None:\n        client = InferenceClient()\n        status = client.get_model_status(self.model_id)\n\n        if (\n            status.state not in {\"Loadable\", \"Loaded\"}\n            and status.framework != \"text-generation-inference\"\n        ):\n            raise ValueError(\n                f\"Model {self.model_id} is not currently deployed or is not running the TGI framework\"\n            )\n\n        self.base_url = client._resolve_url(\n            model=self.model_id, task=\"text-generation\"\n        )\n\n    if self.endpoint_name is not None:\n        client = get_inference_endpoint(\n            name=self.endpoint_name,\n            namespace=self.endpoint_namespace,\n            token=self.api_key.get_secret_value(),\n        )\n        if client.status in [\"paused\", \"scaledToZero\"]:\n            client.resume().wait(timeout=300)\n        elif client.status in [\"initializing\"]:\n            client.wait(timeout=300)\n\n        self.base_url = client.url\n        self._model_name = client.repository\n\n    if self.use_openai_client:\n        try:\n            from openai import AsyncOpenAI\n        except ImportError as ie:\n            raise ImportError(\n                \"OpenAI Python client is not installed. Please install it using\"\n                \" `pip install openai`.\"\n            ) from ie\n\n        self._aclient = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            max_retries=6,\n        )\n    else:\n        self._aclient = AsyncInferenceClient(\n            model=self.base_url,\n            token=self.api_key.get_secret_value(),\n        )\n\n    if self.tokenizer_id:\n        try:\n            from transformers import AutoTokenizer\n        except ImportError as ie:\n            raise ImportError(\n                \"Transformers Python client is not installed. Please install it using\"\n                \" `pip install transformers`.\"\n            ) from ie\n\n        self._tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/inference_endpoints/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.only_one_of_model_id_endpoint_name_or_base_url_provided","title":"<code>only_one_of_model_id_endpoint_name_or_base_url_provided()</code>","text":"<p>Validates that only one of <code>model_id</code> or <code>endpoint_name</code> is provided; and if <code>base_url</code> is also provided, a warning will be shown informing the user that the provided <code>base_url</code> will be ignored in favour of the dynamically calculated one..</p> Source code in <code>src/distilabel/llms/huggingface/inference_endpoints.py</code> <pre><code>@model_validator(mode=\"after\")  # type: ignore\ndef only_one_of_model_id_endpoint_name_or_base_url_provided(\n    self,\n) -&gt; \"InferenceEndpointsLLM\":\n    \"\"\"Validates that only one of `model_id` or `endpoint_name` is provided; and if `base_url` is also\n    provided, a warning will be shown informing the user that the provided `base_url` will be ignored in\n    favour of the dynamically calculated one..\"\"\"\n\n    if self.base_url and (self.model_id or self.endpoint_name):\n        self._logger.warning(  # type: ignore\n            f\"Since the `base_url={self.base_url}` is available and either one of `model_id` or `endpoint_name`\"\n            \" is also provided, the `base_url` will either be ignored or overwritten with the one generated\"\n            \" from either of those args, for serverless or dedicated inference endpoints, respectively.\"\n        )\n\n    if self.base_url and not (self.model_id or self.endpoint_name):\n        return self\n\n    if self.model_id and not self.endpoint_name:\n        return self\n\n    if self.endpoint_name and not self.model_id:\n        return self\n\n    raise ValidationError(\n        \"Only one of `model_id` or `endpoint_name` must be provided. If `base_url` is provided too,\"\n        \" it will be overwritten instead. Found `model_id`={self.model_id}, `endpoint_name`={self.endpoint_name},\"\n        f\" and `base_url`={self.base_url}.\"\n    )\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/transformers/","title":"Transformers","text":""},{"location":"reference/distilabel/llms/huggingface/transformers/#distilabel.llms.huggingface.transformers.TransformersLLM","title":"<code>TransformersLLM</code>","text":"<p>               Bases: <code>LLM</code>, <code>CudaDevicePlacementMixin</code></p> <p>Hugging Face <code>transformers</code> library LLM implementation using the text generation pipeline.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>the model Hugging Face Hub repo id or a path to a directory containing the model weights and configuration files.</p> <code>revision</code> <code>str</code> <p>if <code>model</code> refers to a Hugging Face Hub repository, then the revision (e.g. a branch name or a commit id) to use. Defaults to <code>\"main\"</code>.</p> <code>torch_dtype</code> <code>str</code> <p>the torch dtype to use for the model e.g. \"float16\", \"float32\", etc. Defaults to <code>\"auto\"</code>.</p> <code>trust_remote_code</code> <code>bool</code> <p>whether to trust or not remote (code in the Hugging Face Hub repository) code to load the model. Defaults to <code>False</code>.</p> <code>model_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>additional dictionary of keyword arguments that will be passed to the <code>from_pretrained</code> method of the model.</p> <code>tokenizer</code> <code>Optional[str]</code> <p>the tokenizer Hugging Face Hub repo id or a path to a directory containing the tokenizer config files. If not provided, the one associated to the <code>model</code> will be used. Defaults to <code>None</code>.</p> <code>use_fast</code> <code>bool</code> <p>whether to use a fast tokenizer or not. Defaults to <code>True</code>.</p> <code>chat_template</code> <code>Optional[str]</code> <p>a chat template that will be used to build the prompts before sending them to the model. If not provided, the chat template defined in the tokenizer config will be used. If not provided and the tokenizer doesn't have a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> <code>device</code> <code>Optional[Union[str, int]]</code> <p>the name or index of the device where the model will be loaded. Defaults to <code>None</code>.</p> <code>device_map</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>a dictionary mapping each layer of the model to a device, or a mode like <code>\"sequential\"</code> or <code>\"auto\"</code>. Defaults to <code>None</code>.</p> <code>token</code> <code>Optional[str]</code> <p>the Hugging Face Hub token that will be used to authenticate to the Hugging Face Hub. If not provided, the <code>HF_TOKEN</code> environment or <code>huggingface_hub</code> package local configuration will be used. Defaults to <code>None</code>.</p> Icon <p><code>:hugging:</code></p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>class TransformersLLM(LLM, CudaDevicePlacementMixin):\n    \"\"\"Hugging Face `transformers` library LLM implementation using the text generation\n    pipeline.\n\n    Attributes:\n        model: the model Hugging Face Hub repo id or a path to a directory containing the\n            model weights and configuration files.\n        revision: if `model` refers to a Hugging Face Hub repository, then the revision\n            (e.g. a branch name or a commit id) to use. Defaults to `\"main\"`.\n        torch_dtype: the torch dtype to use for the model e.g. \"float16\", \"float32\", etc.\n            Defaults to `\"auto\"`.\n        trust_remote_code: whether to trust or not remote (code in the Hugging Face Hub\n            repository) code to load the model. Defaults to `False`.\n        model_kwargs: additional dictionary of keyword arguments that will be passed to\n            the `from_pretrained` method of the model.\n        tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing\n            the tokenizer config files. If not provided, the one associated to the `model`\n            will be used. Defaults to `None`.\n        use_fast: whether to use a fast tokenizer or not. Defaults to `True`.\n        chat_template: a chat template that will be used to build the prompts before\n            sending them to the model. If not provided, the chat template defined in the\n            tokenizer config will be used. If not provided and the tokenizer doesn't have\n            a chat template, then ChatML template will be used. Defaults to `None`.\n        device: the name or index of the device where the model will be loaded. Defaults\n            to `None`.\n        device_map: a dictionary mapping each layer of the model to a device, or a mode\n            like `\"sequential\"` or `\"auto\"`. Defaults to `None`.\n        token: the Hugging Face Hub token that will be used to authenticate to the Hugging\n            Face Hub. If not provided, the `HF_TOKEN` environment or `huggingface_hub` package\n            local configuration will be used. Defaults to `None`.\n\n    Icon:\n        `:hugging:`\n    \"\"\"\n\n    model: str\n    revision: str = \"main\"\n    torch_dtype: str = \"auto\"\n    trust_remote_code: bool = False\n    model_kwargs: Optional[Dict[str, Any]] = None\n    tokenizer: Optional[str] = None\n    use_fast: bool = True\n    chat_template: Optional[str] = None\n    device: Optional[Union[str, int]] = None\n    device_map: Optional[Union[str, Dict[str, Any]]] = None\n    token: Optional[str] = None\n\n    _pipeline: Optional[\"Pipeline\"] = PrivateAttr(...)\n    _prefix_allowed_tokens_fn: Union[Callable, None] = PrivateAttr(default=None)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the model and tokenizer and creates the text generation pipeline. In addition,\n        it will configure the tokenizer chat template.\"\"\"\n        if self.device == \"cuda\":\n            CudaDevicePlacementMixin.load(self)\n\n        try:\n            from transformers import pipeline\n        except ImportError as ie:\n            raise ImportError(\n                \"Transformers is not installed. Please install it using `pip install transformers`.\"\n            ) from ie\n\n        self._pipeline = pipeline(\n            \"text-generation\",\n            model=self.model,\n            revision=self.revision,\n            torch_dtype=self.torch_dtype,\n            trust_remote_code=self.trust_remote_code,\n            model_kwargs=self.model_kwargs or {},\n            tokenizer=self.tokenizer or self.model,\n            use_fast=self.use_fast,\n            device=self.device,\n            device_map=self.device_map,\n            token=self.token or os.getenv(\"HF_TOKEN\"),\n            return_full_text=False,\n        )\n\n        if self.chat_template is not None:\n            self._pipeline.tokenizer.chat_template = self.chat_template  # type: ignore\n        elif (\n            self._pipeline.tokenizer.chat_template is None  # type: ignore\n            and self._pipeline.tokenizer.default_chat_template is None  # type: ignore\n        ):\n            self._pipeline.tokenizer.chat_template = CHATML_TEMPLATE  # type: ignore\n\n        if self.structured_output:\n            self._prefix_allowed_tokens_fn = self._prepare_structured_output(\n                self.structured_output\n            )\n\n        super().load()\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name used for the LLM.\"\"\"\n        return self.model\n\n    def prepare_input(self, input: \"StandardInput\") -&gt; str:\n        \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n        as an OpenAI conversation, and adding the generation prompt.\n        \"\"\"\n        return self._pipeline.tokenizer.apply_chat_template(  # type: ignore\n            input,  # type: ignore\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    @validate_call\n    def generate(  # type: ignore\n        self,\n        inputs: List[StandardInput],\n        num_generations: int = 1,\n        max_new_tokens: int = 128,\n        temperature: float = 0.1,\n        repetition_penalty: float = 1.1,\n        top_p: float = 1.0,\n        top_k: int = 0,\n        do_sample: bool = True,\n    ) -&gt; List[GenerateOutput]:\n        \"\"\"Generates `num_generations` responses for each input using the text generation\n        pipeline.\n\n        Args:\n            inputs: a list of inputs in chat format to generate responses for.\n            num_generations: the number of generations to create per input. Defaults to\n                `1`.\n            max_new_tokens: the maximum number of new tokens that the model will generate.\n                Defaults to `128`.\n            temperature: the temperature to use for the generation. Defaults to `0.1`.\n            repetition_penalty: the repetition penalty to use for the generation. Defaults\n                to `1.1`.\n            top_p: the top-p value to use for the generation. Defaults to `1.0`.\n            top_k: the top-k value to use for the generation. Defaults to `0`.\n            do_sample: whether to use sampling or not. Defaults to `True`.\n\n        Returns:\n            A list of lists of strings containing the generated responses for each input.\n        \"\"\"\n        prepared_inputs = [self.prepare_input(input=input) for input in inputs]\n\n        outputs: List[List[Dict[str, str]]] = self._pipeline(  # type: ignore\n            prepared_inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            repetition_penalty=repetition_penalty,\n            top_p=top_p,\n            top_k=top_k,\n            do_sample=do_sample,\n            num_return_sequences=num_generations,\n            prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n        )\n        return [\n            [generation[\"generated_text\"] for generation in output]\n            for output in outputs\n        ]\n\n    def get_last_hidden_states(\n        self, inputs: List[\"StandardInput\"]\n    ) -&gt; List[\"HiddenState\"]:\n        \"\"\"Gets the last `hidden_states` of the model for the given inputs. It doesn't\n        execute the task head.\n\n        Args:\n            inputs: a list of inputs in chat format to generate the embeddings for.\n\n        Returns:\n            A list containing the last hidden state for each sequence using a NumPy array\n            with shape [num_tokens, hidden_size].\n        \"\"\"\n        model: \"PreTrainedModel\" = (\n            self._pipeline.model.model  # type: ignore\n            if hasattr(self._pipeline.model, \"model\")  # type: ignore\n            else next(self._pipeline.model.children())  # type: ignore\n        )\n        tokenizer: \"PreTrainedTokenizer\" = self._pipeline.tokenizer  # type: ignore\n        input_ids = tokenizer(\n            [self.prepare_input(input) for input in inputs],  # type: ignore\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(model.device)\n        last_hidden_states = model(**input_ids)[\"last_hidden_state\"]\n\n        return [\n            seq_last_hidden_state[attention_mask.bool(), :].detach().cpu().numpy()\n            for seq_last_hidden_state, attention_mask in zip(\n                last_hidden_states,\n                input_ids[\"attention_mask\"],  # type: ignore\n            )\n        ]\n\n    def _prepare_structured_output(\n        self, structured_output: Optional[\"StructuredOutputType\"] = None\n    ) -&gt; Union[Callable, None]:\n        \"\"\"Creates the appropriate function to filter tokens to generate structured outputs.\n\n        Args:\n            structured_output: the configuration dict to prepare the structured output.\n\n        Returns:\n            The callable that will be used to guide the generation of the model.\n        \"\"\"\n        from distilabel.steps.tasks.structured_outputs.outlines import (\n            prepare_guided_output,\n        )\n\n        result = prepare_guided_output(\n            structured_output, \"transformers\", self._pipeline\n        )\n        if schema := result.get(\"schema\"):\n            self.structured_output[\"schema\"] = schema\n        return result[\"processor\"]\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/transformers/#distilabel.llms.huggingface.transformers.TransformersLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name used for the LLM.</p>"},{"location":"reference/distilabel/llms/huggingface/transformers/#distilabel.llms.huggingface.transformers.TransformersLLM.generate","title":"<code>generate(inputs, num_generations=1, max_new_tokens=128, temperature=0.1, repetition_penalty=1.1, top_p=1.0, top_k=0, do_sample=True)</code>","text":"<p>Generates <code>num_generations</code> responses for each input using the text generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate responses for.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to create per input. Defaults to <code>1</code>.</p> <code>1</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of new tokens that the model will generate. Defaults to <code>128</code>.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the generation. Defaults to <code>0.1</code>.</p> <code>0.1</code> <code>repetition_penalty</code> <code>float</code> <p>the repetition penalty to use for the generation. Defaults to <code>1.1</code>.</p> <code>1.1</code> <code>top_p</code> <code>float</code> <p>the top-p value to use for the generation. Defaults to <code>1.0</code>.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to use for the generation. Defaults to <code>0</code>.</p> <code>0</code> <code>do_sample</code> <code>bool</code> <p>whether to use sampling or not. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[GenerateOutput]</code> <p>A list of lists of strings containing the generated responses for each input.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>@validate_call\ndef generate(  # type: ignore\n    self,\n    inputs: List[StandardInput],\n    num_generations: int = 1,\n    max_new_tokens: int = 128,\n    temperature: float = 0.1,\n    repetition_penalty: float = 1.1,\n    top_p: float = 1.0,\n    top_k: int = 0,\n    do_sample: bool = True,\n) -&gt; List[GenerateOutput]:\n    \"\"\"Generates `num_generations` responses for each input using the text generation\n    pipeline.\n\n    Args:\n        inputs: a list of inputs in chat format to generate responses for.\n        num_generations: the number of generations to create per input. Defaults to\n            `1`.\n        max_new_tokens: the maximum number of new tokens that the model will generate.\n            Defaults to `128`.\n        temperature: the temperature to use for the generation. Defaults to `0.1`.\n        repetition_penalty: the repetition penalty to use for the generation. Defaults\n            to `1.1`.\n        top_p: the top-p value to use for the generation. Defaults to `1.0`.\n        top_k: the top-k value to use for the generation. Defaults to `0`.\n        do_sample: whether to use sampling or not. Defaults to `True`.\n\n    Returns:\n        A list of lists of strings containing the generated responses for each input.\n    \"\"\"\n    prepared_inputs = [self.prepare_input(input=input) for input in inputs]\n\n    outputs: List[List[Dict[str, str]]] = self._pipeline(  # type: ignore\n        prepared_inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        top_p=top_p,\n        top_k=top_k,\n        do_sample=do_sample,\n        num_return_sequences=num_generations,\n        prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n    )\n    return [\n        [generation[\"generated_text\"] for generation in output]\n        for output in outputs\n    ]\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/transformers/#distilabel.llms.huggingface.transformers.TransformersLLM.get_last_hidden_states","title":"<code>get_last_hidden_states(inputs)</code>","text":"<p>Gets the last <code>hidden_states</code> of the model for the given inputs. It doesn't execute the task head.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[StandardInput]</code> <p>a list of inputs in chat format to generate the embeddings for.</p> required <p>Returns:</p> Type Description <code>List[HiddenState]</code> <p>A list containing the last hidden state for each sequence using a NumPy array</p> <code>List[HiddenState]</code> <p>with shape [num_tokens, hidden_size].</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def get_last_hidden_states(\n    self, inputs: List[\"StandardInput\"]\n) -&gt; List[\"HiddenState\"]:\n    \"\"\"Gets the last `hidden_states` of the model for the given inputs. It doesn't\n    execute the task head.\n\n    Args:\n        inputs: a list of inputs in chat format to generate the embeddings for.\n\n    Returns:\n        A list containing the last hidden state for each sequence using a NumPy array\n        with shape [num_tokens, hidden_size].\n    \"\"\"\n    model: \"PreTrainedModel\" = (\n        self._pipeline.model.model  # type: ignore\n        if hasattr(self._pipeline.model, \"model\")  # type: ignore\n        else next(self._pipeline.model.children())  # type: ignore\n    )\n    tokenizer: \"PreTrainedTokenizer\" = self._pipeline.tokenizer  # type: ignore\n    input_ids = tokenizer(\n        [self.prepare_input(input) for input in inputs],  # type: ignore\n        return_tensors=\"pt\",\n        padding=True,\n    ).to(model.device)\n    last_hidden_states = model(**input_ids)[\"last_hidden_state\"]\n\n    return [\n        seq_last_hidden_state[attention_mask.bool(), :].detach().cpu().numpy()\n        for seq_last_hidden_state, attention_mask in zip(\n            last_hidden_states,\n            input_ids[\"attention_mask\"],  # type: ignore\n        )\n    ]\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/transformers/#distilabel.llms.huggingface.transformers.TransformersLLM.load","title":"<code>load()</code>","text":"<p>Loads the model and tokenizer and creates the text generation pipeline. In addition, it will configure the tokenizer chat template.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the model and tokenizer and creates the text generation pipeline. In addition,\n    it will configure the tokenizer chat template.\"\"\"\n    if self.device == \"cuda\":\n        CudaDevicePlacementMixin.load(self)\n\n    try:\n        from transformers import pipeline\n    except ImportError as ie:\n        raise ImportError(\n            \"Transformers is not installed. Please install it using `pip install transformers`.\"\n        ) from ie\n\n    self._pipeline = pipeline(\n        \"text-generation\",\n        model=self.model,\n        revision=self.revision,\n        torch_dtype=self.torch_dtype,\n        trust_remote_code=self.trust_remote_code,\n        model_kwargs=self.model_kwargs or {},\n        tokenizer=self.tokenizer or self.model,\n        use_fast=self.use_fast,\n        device=self.device,\n        device_map=self.device_map,\n        token=self.token or os.getenv(\"HF_TOKEN\"),\n        return_full_text=False,\n    )\n\n    if self.chat_template is not None:\n        self._pipeline.tokenizer.chat_template = self.chat_template  # type: ignore\n    elif (\n        self._pipeline.tokenizer.chat_template is None  # type: ignore\n        and self._pipeline.tokenizer.default_chat_template is None  # type: ignore\n    ):\n        self._pipeline.tokenizer.chat_template = CHATML_TEMPLATE  # type: ignore\n\n    if self.structured_output:\n        self._prefix_allowed_tokens_fn = self._prepare_structured_output(\n            self.structured_output\n        )\n\n    super().load()\n</code></pre>"},{"location":"reference/distilabel/llms/huggingface/transformers/#distilabel.llms.huggingface.transformers.TransformersLLM.prepare_input","title":"<code>prepare_input(input)</code>","text":"<p>Prepares the input by applying the chat template to the input, which is formatted as an OpenAI conversation, and adding the generation prompt.</p> Source code in <code>src/distilabel/llms/huggingface/transformers.py</code> <pre><code>def prepare_input(self, input: \"StandardInput\") -&gt; str:\n    \"\"\"Prepares the input by applying the chat template to the input, which is formatted\n    as an OpenAI conversation, and adding the generation prompt.\n    \"\"\"\n    return self._pipeline.tokenizer.apply_chat_template(  # type: ignore\n        input,  # type: ignore\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n</code></pre>"},{"location":"reference/distilabel/mixins/","title":"Index","text":""},{"location":"reference/distilabel/mixins/runtime_parameters/","title":"Runtime parameters","text":""},{"location":"reference/distilabel/mixins/runtime_parameters/#distilabel.mixins.runtime_parameters.RuntimeParameter","title":"<code>RuntimeParameter = Annotated[Union[_T, None], Field(default=None), _RUNTIME_PARAMETER_ANNOTATION]</code>  <code>module-attribute</code>","text":"<p>Used to mark the attributes of a <code>Step</code> as a runtime parameter.</p>"},{"location":"reference/distilabel/mixins/runtime_parameters/#distilabel.mixins.runtime_parameters.RuntimeParametersMixin","title":"<code>RuntimeParametersMixin</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Mixin for classes that have <code>RuntimeParameter</code>s attributes.</p> <p>Attributes:</p> Name Type Description <code>_runtime_parameters</code> <code>Dict[str, Any]</code> <p>A dictionary containing the values of the runtime parameters of the class. This attribute is meant to be used internally and should not be accessed directly.</p> Source code in <code>src/distilabel/mixins/runtime_parameters.py</code> <pre><code>class RuntimeParametersMixin(BaseModel):\n    \"\"\"Mixin for classes that have `RuntimeParameter`s attributes.\n\n    Attributes:\n        _runtime_parameters: A dictionary containing the values of the runtime parameters\n            of the class. This attribute is meant to be used internally and should not be\n            accessed directly.\n    \"\"\"\n\n    _runtime_parameters: Dict[str, Any] = PrivateAttr(default_factory=dict)\n\n    @property\n    def runtime_parameters_names(self) -&gt; RuntimeParametersNames:\n        \"\"\"Returns a dictionary containing the name of the runtime parameters of the class\n        as keys and whether the parameter is required or not as values.\n\n        Returns:\n            A dictionary containing the name of the runtime parameters of the class as keys\n            and whether the parameter is required or not as values.\n        \"\"\"\n\n        runtime_parameters = {}\n\n        for name, field_info in self.model_fields.items():  # type: ignore\n            is_runtime_param, is_optional = _is_runtime_parameter(field_info)\n            if is_runtime_param:\n                runtime_parameters[name] = is_optional\n                continue\n\n            attr = getattr(self, name)\n            if isinstance(attr, RuntimeParametersMixin):\n                runtime_parameters[name] = attr.runtime_parameters_names\n\n        return runtime_parameters\n\n    def get_runtime_parameters_info(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Gets the information of the runtime parameters of the class such as the name and\n        the description. This function is meant to include the information of the runtime\n        parameters in the serialized data of the class.\n\n        Returns:\n            A list containing the information for each runtime parameter of the class.\n        \"\"\"\n        runtime_parameters_info = []\n        for name, field_info in self.model_fields.items():  # type: ignore\n            if name not in self.runtime_parameters_names:\n                continue\n\n            attr = getattr(self, name)\n            if isinstance(attr, RuntimeParametersMixin):\n                runtime_parameters_info.append(\n                    {\n                        \"name\": name,\n                        \"runtime_parameters_info\": attr.get_runtime_parameters_info(),\n                    }\n                )\n                continue\n\n            info = {\"name\": name, \"optional\": self.runtime_parameters_names[name]}\n            if field_info.description is not None:\n                info[\"description\"] = field_info.description\n            runtime_parameters_info.append(info)\n        return runtime_parameters_info\n\n    def set_runtime_parameters(self, runtime_parameters: Dict[str, Any]) -&gt; None:\n        \"\"\"Sets the runtime parameters of the class using the provided values. If the attr\n        to be set is a `RuntimeParametersMixin`, it will call `set_runtime_parameters` on\n        the attr.\n\n        Args:\n            runtime_parameters: A dictionary containing the values of the runtime parameters\n                to set.\n        \"\"\"\n        runtime_parameters_names = list(self.runtime_parameters_names.keys())\n        for name, value in runtime_parameters.items():\n            if name not in self.runtime_parameters_names:\n                # Check done just to ensure the unit tests for the mixin run\n                if getattr(self, \"pipeline\", None):\n                    closest = difflib.get_close_matches(\n                        name, runtime_parameters_names, cutoff=0.5\n                    )\n                    msg = (\n                        f\"\u26a0\ufe0f  Runtime parameter '{name}' unknown in step '{self.name}'.\"  # type: ignore\n                    )\n                    if closest:\n                        msg += f\" Did you mean any of: {closest}\"\n                    else:\n                        msg += f\" Available runtime parameters for the step: {runtime_parameters_names}.\"\n                    self.pipeline._logger.warning(msg)  # type: ignore\n                continue\n\n            attr = getattr(self, name)\n            if isinstance(attr, RuntimeParametersMixin):\n                attr.set_runtime_parameters(value)\n                self._runtime_parameters[name] = value\n                continue\n\n            # Handle settings values for `_SecretField`\n            field_info = self.model_fields[name]\n            inner_type = _extract_runtime_parameter_inner_type(field_info.annotation)\n            if inspect.isclass(inner_type) and issubclass(inner_type, _SecretField):\n                value = inner_type(value)\n\n            setattr(self, name, value)\n            self._runtime_parameters[name] = value\n</code></pre>"},{"location":"reference/distilabel/mixins/runtime_parameters/#distilabel.mixins.runtime_parameters.RuntimeParametersMixin.runtime_parameters_names","title":"<code>runtime_parameters_names: RuntimeParametersNames</code>  <code>property</code>","text":"<p>Returns a dictionary containing the name of the runtime parameters of the class as keys and whether the parameter is required or not as values.</p> <p>Returns:</p> Type Description <code>RuntimeParametersNames</code> <p>A dictionary containing the name of the runtime parameters of the class as keys</p> <code>RuntimeParametersNames</code> <p>and whether the parameter is required or not as values.</p>"},{"location":"reference/distilabel/mixins/runtime_parameters/#distilabel.mixins.runtime_parameters.RuntimeParametersMixin.get_runtime_parameters_info","title":"<code>get_runtime_parameters_info()</code>","text":"<p>Gets the information of the runtime parameters of the class such as the name and the description. This function is meant to include the information of the runtime parameters in the serialized data of the class.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list containing the information for each runtime parameter of the class.</p> Source code in <code>src/distilabel/mixins/runtime_parameters.py</code> <pre><code>def get_runtime_parameters_info(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Gets the information of the runtime parameters of the class such as the name and\n    the description. This function is meant to include the information of the runtime\n    parameters in the serialized data of the class.\n\n    Returns:\n        A list containing the information for each runtime parameter of the class.\n    \"\"\"\n    runtime_parameters_info = []\n    for name, field_info in self.model_fields.items():  # type: ignore\n        if name not in self.runtime_parameters_names:\n            continue\n\n        attr = getattr(self, name)\n        if isinstance(attr, RuntimeParametersMixin):\n            runtime_parameters_info.append(\n                {\n                    \"name\": name,\n                    \"runtime_parameters_info\": attr.get_runtime_parameters_info(),\n                }\n            )\n            continue\n\n        info = {\"name\": name, \"optional\": self.runtime_parameters_names[name]}\n        if field_info.description is not None:\n            info[\"description\"] = field_info.description\n        runtime_parameters_info.append(info)\n    return runtime_parameters_info\n</code></pre>"},{"location":"reference/distilabel/mixins/runtime_parameters/#distilabel.mixins.runtime_parameters.RuntimeParametersMixin.set_runtime_parameters","title":"<code>set_runtime_parameters(runtime_parameters)</code>","text":"<p>Sets the runtime parameters of the class using the provided values. If the attr to be set is a <code>RuntimeParametersMixin</code>, it will call <code>set_runtime_parameters</code> on the attr.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_parameters</code> <code>Dict[str, Any]</code> <p>A dictionary containing the values of the runtime parameters to set.</p> required Source code in <code>src/distilabel/mixins/runtime_parameters.py</code> <pre><code>def set_runtime_parameters(self, runtime_parameters: Dict[str, Any]) -&gt; None:\n    \"\"\"Sets the runtime parameters of the class using the provided values. If the attr\n    to be set is a `RuntimeParametersMixin`, it will call `set_runtime_parameters` on\n    the attr.\n\n    Args:\n        runtime_parameters: A dictionary containing the values of the runtime parameters\n            to set.\n    \"\"\"\n    runtime_parameters_names = list(self.runtime_parameters_names.keys())\n    for name, value in runtime_parameters.items():\n        if name not in self.runtime_parameters_names:\n            # Check done just to ensure the unit tests for the mixin run\n            if getattr(self, \"pipeline\", None):\n                closest = difflib.get_close_matches(\n                    name, runtime_parameters_names, cutoff=0.5\n                )\n                msg = (\n                    f\"\u26a0\ufe0f  Runtime parameter '{name}' unknown in step '{self.name}'.\"  # type: ignore\n                )\n                if closest:\n                    msg += f\" Did you mean any of: {closest}\"\n                else:\n                    msg += f\" Available runtime parameters for the step: {runtime_parameters_names}.\"\n                self.pipeline._logger.warning(msg)  # type: ignore\n            continue\n\n        attr = getattr(self, name)\n        if isinstance(attr, RuntimeParametersMixin):\n            attr.set_runtime_parameters(value)\n            self._runtime_parameters[name] = value\n            continue\n\n        # Handle settings values for `_SecretField`\n        field_info = self.model_fields[name]\n        inner_type = _extract_runtime_parameter_inner_type(field_info.annotation)\n        if inspect.isclass(inner_type) and issubclass(inner_type, _SecretField):\n            value = inner_type(value)\n\n        setattr(self, name, value)\n        self._runtime_parameters[name] = value\n</code></pre>"},{"location":"reference/distilabel/pipeline/","title":"Index","text":""},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Local pipeline implementation using <code>multiprocessing</code>.</p> Source code in <code>src/distilabel/pipeline/local.py</code> <pre><code>class Pipeline(BasePipeline):\n    \"\"\"Local pipeline implementation using `multiprocessing`.\"\"\"\n\n    def run(\n        self,\n        parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        use_cache: bool = True,\n        storage_parameters: Optional[Dict[str, Any]] = None,\n        use_fs_to_pass_data: bool = False,\n    ) -&gt; \"Distiset\":\n        \"\"\"Runs the pipeline.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n                the runtime parameters for the step as the value. Defaults to `None`.\n            use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n                `True`.\n            storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n                that will be used to store the data of the `_Batch`es passed between the\n                steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n                `GlobalStep` it will be always used). It must have at least the \"path\" key,\n                and it can contain additional keys depending on the protocol. By default,\n                it will use the local file system and a directory in the cache directory.\n                Defaults to `None`.\n            use_fs_to_pass_data: Whether to use the file system to pass the data of\n                the `_Batch`es between the steps. Even if this parameter is `False`, the\n                `Batch`es received by `GlobalStep`s will always use the file system to\n                pass the data. Defaults to `False`.\n\n        Returns:\n            The `Distiset` created by the pipeline.\n\n        Raises:\n            RuntimeError: If the pipeline fails to load all the steps.\n        \"\"\"\n        log_queue = mp.Queue()\n\n        self._set_logging_parameters(\n            {\"log_queue\": log_queue, \"filename\": self._cache_location[\"log_file\"]}\n        )\n\n        if distiset := super().run(\n            parameters, use_cache, storage_parameters, use_fs_to_pass_data\n        ):\n            return distiset\n\n        num_processes = len(self.dag)\n        ctx = mp.get_context()  # type: ignore\n        with ctx.Manager() as manager, ctx.Pool(\n            num_processes,\n            initializer=_init_worker,\n            initargs=(log_queue,),\n        ) as pool:\n            self.output_queue: \"Queue[Any]\" = manager.Queue()\n            self.shared_info = self._create_shared_info_dict(manager)\n            self._handle_keyboard_interrupt(manager=manager, pool=pool)\n\n            # Run the steps using the pool of processes\n            self._run_steps_in_loop(pool, manager, self.output_queue, self.shared_info)\n\n            # Wait for all the steps to be loaded correctly\n            if not self._all_steps_loaded():\n                self._write_buffer.close()  # type: ignore\n                self._batch_manager = None\n                stop_logging()\n                raise RuntimeError(\n                    \"Failed to load all the steps. Could not run pipeline.\"\n                ) from _SUBPROCESS_EXCEPTION\n\n            # Send the \"first\" batches to the steps so the batches starts flowing through\n            # the input queues and output queue\n            self._request_initial_batches()\n\n            # Start a loop to receive the output batches from the steps\n            self._run_output_queue_loop_in_thread()\n\n            # Send `None` to steps `input_queue`s just in case some step is still waiting\n            self._notify_steps_to_stop()\n\n        # `Pool.__exit__` has already called `terminate`, `join` the pool to make sure\n        # all the processes have finished\n        pool.join()\n        manager.join()\n\n        self._write_buffer.close()  # type: ignore\n        distiset = create_distiset(\n            self._cache_location[\"data\"],\n            pipeline_path=self._cache_location[\"pipeline\"],\n            log_filename_path=self._cache_location[\"log_file\"],\n            enable_metadata=self._enable_metadata,\n        )\n        stop_logging()\n        return distiset\n\n    def _run_output_queue_loop_in_thread(self) -&gt; None:\n        \"\"\"Runs the output queue loop in a separate thread to receive the output batches\n        from the steps. This is done to avoid the signal handler to block the loop, which\n        would prevent the pipeline from stopping correctly.\"\"\"\n        thread = threading.Thread(target=self._output_queue_loop)\n        thread.start()\n        thread.join()\n\n    def _notify_steps_to_stop(self) -&gt; None:\n        \"\"\"Notifies the steps to stop their infinite running loop by sending `None` to\n        their input queues.\"\"\"\n        for step_name in self.dag:\n            if input_queue := self.dag.get_step(step_name).get(INPUT_QUEUE_ATTR_NAME):\n                input_queue.put(None)\n\n    def _output_queue_loop(self) -&gt; None:\n        \"\"\"Loop to receive the output batches from the steps and manage the flow of the\n        batches through the pipeline.\"\"\"\n        while self._batch_manager.can_generate() and not _STOP_CALLED:  # type: ignore\n            self._logger.debug(\"Waiting for output batch from step...\")\n            if (batch := self.output_queue.get()) is None:\n                self._logger.debug(\"Received `None` from output queue. Breaking loop.\")\n                break\n\n            self._logger.debug(\n                f\"Received batch with seq_no {batch.seq_no} from step '{batch.step_name}'\"\n                f\" from output queue: {batch}\"\n            )\n\n            if batch.data_path:\n                self._logger.debug(\n                    f\"Reading {batch.seq_no} batch data from '{batch.step_name}': '{batch.data_path}'\"\n                )\n                batch.read_batch_data_from_fs()\n\n            if batch.step_name in self.dag.leaf_steps:\n                self._write_buffer.add_batch(batch)  # type: ignore\n\n            # If `_STOP_CALLED` was set to `True` while waiting for the output queue, then\n            # we need to handle the stop of the pipeline and break the loop to avoid\n            # propagating the batches through the pipeline and making the stop process\n            # slower.\n            if _STOP_CALLED:\n                self._handle_batch_on_stop(batch)\n                break\n\n            self._manage_batch_flow(batch)\n\n        if _STOP_CALLED:\n            self._handle_stop()\n\n    def _manage_batch_flow(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Checks if the step that generated the batch has more data in its buffer to\n        generate a new batch. If there's data, then a new batch is sent to the step. If\n        the step has no data in its buffer, then the predecessors generator steps are\n        requested to send a new batch.\n\n        Args:\n            batch: The batch that was processed.\n        \"\"\"\n        assert self._batch_manager, \"Batch manager is not set\"\n\n        # Make sure to send the `LAST_BATCH_SENT_FLAG` to the predecessors of the convergence\n        # step if the batch is the last one, so they stop their processing loop even if\n        # they haven't received the last batch because of the routing function.\n        if self._is_convergence_step(batch.step_name) and batch.last_batch:\n            for step_name in self.dag.get_step_predecessors(batch.step_name):\n                self._send_last_batch_flag_to_step(step_name)\n\n        route_to, routed = self._get_successors(batch)\n\n        # Keep track of the steps that the batch was routed to\n        if routed:\n            batch.batch_routed_to = route_to\n\n        self._register_batch(batch)\n\n        step = self._get_step_from_batch(batch)\n\n        # Add the batch to the successors input buffers\n        for successor in route_to:\n            # Copy batch to avoid modifying the same reference in the batch manager\n            batch_to_add = batch.copy() if len(route_to) &gt; 1 else batch\n\n            self._batch_manager.add_batch(successor, batch_to_add)\n\n            # Check if the step is a generator and if there are successors that need data\n            # from this step. This usually happens when the generator `batch_size` is smaller\n            # than the `input_batch_size` of the successor steps.\n            if (\n                step.is_generator\n                and step.name in self._batch_manager.step_empty_buffers(successor)\n            ):\n                last_batch_sent = self._batch_manager.get_last_batch_sent(step.name)\n                self._send_batch_to_step(last_batch_sent.next_batch())  # type: ignore\n\n            # If successor step has enough data in its buffer to create a new batch, then\n            # send the batch to the step.\n            if new_batch := self._batch_manager.get_batch(successor):\n                self._send_batch_to_step(new_batch)\n\n        if not step.is_generator:\n            # Step (\"this\", the one from which the batch was received) has enough data on its\n            # buffers to create a new batch\n            if new_batch := self._batch_manager.get_batch(step.name):  # type: ignore\n                self._send_batch_to_step(new_batch)\n            else:\n                self._request_more_batches_if_needed(step)\n\n        self._cache()\n\n    def _register_batch(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Registers a batch in the batch manager.\n\n        Args:\n            batch: The batch to register.\n        \"\"\"\n        self._batch_manager.register_batch(batch)  # type: ignore\n        self._logger.debug(\n            f\"Batch {batch.seq_no} from step '{batch.step_name}' registered in batch\"\n            \" manager\"\n        )\n\n    def _get_successors(self, batch: \"_Batch\") -&gt; Tuple[List[str], bool]:\n        \"\"\"Gets the successors and the successors to which the batch has to be routed.\n\n        Args:\n            batch: The batch to which the successors will be determined.\n\n        Returns:\n            The successors to route the batch to and whether the batch was routed using\n            a routing function.\n        \"\"\"\n        node = self.dag.get_step(batch.step_name)\n        step: \"Step\" = node[STEP_ATTR_NAME]\n        successors = list(self.dag.get_step_successors(step.name))  # type: ignore\n        route_to = successors\n\n        # Check if the step has a routing function to send the batch to specific steps\n        if routing_batch_function := node.get(ROUTING_BATCH_FUNCTION_ATTR_NAME):\n            route_to = routing_batch_function(batch, successors)\n            successors_str = \", \".join(f\"'{successor}'\" for successor in route_to)\n            self._logger.info(\n                f\"\ud83d\ude8f Using '{step.name}' routing function to send batch {batch.seq_no} to steps: {successors_str}\"\n            )\n\n        return route_to, route_to != successors\n\n    def _get_step_from_batch(self, batch: \"_Batch\") -&gt; \"Step\":\n        \"\"\"Gets the `Step` instance from a batch.\n\n        Args:\n            batch: The batch to get the step from.\n\n        Returns:\n            The `Step` instance.\n        \"\"\"\n        return self.dag.get_step(batch.step_name)[STEP_ATTR_NAME]\n\n    def _request_more_batches_if_needed(self, step: \"Step\") -&gt; None:\n        \"\"\"Request more batches to the predecessors steps of `step` if needed.\n\n        Args:\n            step: The step of which it has to be checked if more batches are needed from\n                its predecessors.\n        \"\"\"\n        empty_buffers = self._batch_manager.step_empty_buffers(step.name)  # type: ignore\n        for previous_step_name in empty_buffers:\n            if previous_step_name not in self.dag.root_steps:\n                continue\n\n            last_batch = self._batch_manager.get_last_batch_sent(previous_step_name)  # type: ignore\n            if last_batch is None:\n                continue\n\n            self._logger.debug(\n                f\"Step '{step.name}' input buffer for step '{previous_step_name}' is\"\n                \" empty. Requesting new batch...\"\n            )\n            self._send_batch_to_step(last_batch.next_batch())\n\n    def _handle_stop(self) -&gt; None:\n        \"\"\"Handles the stop of the pipeline execution, which will stop the steps from\n        processing more batches and wait for the output queue to be empty, to not lose\n        any data that was already processed by the steps before the stop was called.\"\"\"\n        self._logger.debug(\"Handling stop of the pipeline execution...\")\n\n        # Add the remaining batches in the input queues back to the batch manager\n        for step_name in self.dag:\n            node = self.dag.get_step(step_name)\n            step: \"_Step\" = node[STEP_ATTR_NAME]\n            if step.is_generator:\n                continue\n            if input_queue := node.get(INPUT_QUEUE_ATTR_NAME):\n                while not input_queue.empty():\n                    batch = input_queue.get()\n                    if batch is None:\n                        continue\n                    self._batch_manager.add_batch(  # type: ignore\n                        to_step=step_name, batch=batch, prepend=True\n                    )\n                    self._logger.debug(\n                        f\"Adding batch back to the batch manager: {batch}\"\n                    )\n                input_queue.put(None)\n\n        # Wait for the input queue to be empty, which means that all the steps finished\n        # processing the batches that were sent before the stop flag.\n        for step_name in self.dag:\n            self._wait_step_input_queue_empty(step_name)\n\n        # Consume the output queue until it's empty to not lose any data that was already\n        # processed by the steps before stop was called.\n        while not self.output_queue.empty():\n            batch = self.output_queue.get()\n            if batch is None:\n                continue\n\n            if batch.step_name in self.dag.leaf_steps:\n                self._write_buffer.add_batch(batch)  # type: ignore\n\n            self._handle_batch_on_stop(batch)\n\n        self._cache()\n\n    def _handle_batch_on_stop(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Handles a batch that was received from the output queue when the pipeline was\n        stopped. It will add and register the batch in the batch manager.\n\n        Args:\n            batch: The batch to handle.\n        \"\"\"\n        self._batch_manager.register_batch(batch)  # type: ignore\n        step: \"Step\" = self.dag.get_step(batch.step_name)[STEP_ATTR_NAME]\n        for successor in self.dag.get_step_successors(step.name):  # type: ignore\n            self._batch_manager.add_batch(successor, batch)  # type: ignore\n\n    def _wait_step_input_queue_empty(self, step_name: str) -&gt; Union[\"Queue[Any]\", None]:\n        \"\"\"Waits for the input queue of a step to be empty.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            The input queue of the step if it's not loaded or finished, `None` otherwise.\n        \"\"\"\n        if self._check_step_not_loaded_or_finished(step_name):\n            return None\n\n        if input_queue := self.dag.get_step(step_name).get(INPUT_QUEUE_ATTR_NAME):\n            while input_queue.qsize() != 0:\n                pass\n            return input_queue\n\n    def _create_shared_info_dict(self, manager: \"SyncManager\") -&gt; \"DictProxy[str, Any]\":\n        \"\"\"Creates the shared information dictionary to be used by the processes.\n\n        Args:\n            manager: The manager to create the shared information.\n\n        Returns:\n            The shared information dictionary.\n        \"\"\"\n        # TODO: not very important, but we could use a different lock for each matter\n        return manager.dict(\n            **{\n                _STEPS_LOADED_KEY: manager.list(),\n                _STEPS_LOADED_LOCK_KEY: manager.Lock(),\n                _CUDA_LLM_DEVICE_PLACEMENT_KEY: manager.dict(**{}),\n                _CUDA_LLM_DEVICE_PLACEMENT_LOCK_KEY: manager.Lock(),\n            }\n        )\n\n    def _all_steps_loaded(self) -&gt; bool:\n        \"\"\"Waits for all the steps to load.\n\n        Returns:\n            `True` if all the steps have been loaded correctly, `False` otherwise.\n        \"\"\"\n\n        def _update_all_steps_loaded(steps_loaded: List[str]) -&gt; None:\n            with _STEPS_LOADED_LOCK:\n                _STEPS_LOADED.update(steps_loaded)\n\n        self._logger.info(\"\u23f3 Waiting for all the steps to load...\")\n        previous_message = None\n        while not _STOP_CALLED:\n            with self.shared_info[_STEPS_LOADED_LOCK_KEY]:\n                steps_loaded = self.shared_info[_STEPS_LOADED_KEY]\n                num_steps_loaded = (\n                    len(steps_loaded)\n                    if steps_loaded != [_STEPS_LOADED_ERROR_CODE]\n                    else 0\n                )\n                self._logger.debug(f\"Steps loaded: {steps_loaded}\")\n\n                message = f\"\u23f3 Steps loaded: {num_steps_loaded}/{len(self.dag)}\"\n                if num_steps_loaded &gt; 0 and message != previous_message:\n                    self._logger.info(message)\n                    previous_message = message\n\n                if num_steps_loaded == len(self.dag):\n                    self._logger.info(\"\u2705 All the steps have been loaded!\")\n                    _update_all_steps_loaded(steps_loaded)\n                    return True\n\n                if steps_loaded == [_STEPS_LOADED_ERROR_CODE]:\n                    self._logger.error(\"\u274c Failed to load all the steps\")\n                    _update_all_steps_loaded(steps_loaded)\n                    return False\n\n            time.sleep(2.5)\n\n        return not _STOP_CALLED\n\n    def _request_initial_batches(self) -&gt; None:\n        \"\"\"Requests the initial batches to the generator steps.\"\"\"\n        assert self._batch_manager, \"Batch manager is not set\"\n\n        for step in self._batch_manager._steps.values():\n            if batch := step.get_batch():\n                self._logger.debug(\n                    f\"Sending initial batch to '{step.step_name}' step: {batch}\"\n                )\n                self._send_batch_to_step(batch)\n\n        for step_name in self.dag.root_steps:\n            seq_no = 0\n            if last_batch := self._batch_manager.get_last_batch(step_name):\n                seq_no = last_batch.seq_no + 1\n            batch = _Batch(seq_no=seq_no, step_name=step_name, last_batch=self._dry_run)\n            self._logger.debug(\n                f\"Requesting initial batch to '{step_name}' generator step: {batch}\"\n            )\n            self._send_batch_to_step(batch)\n\n    def _send_batch_to_step(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Sends a batch to the input queue of a step.\n\n        Args:\n            batch: The batch to send.\n        \"\"\"\n        super()._send_batch_to_step(batch)\n        input_queue = self.dag.get_step(batch.step_name)[INPUT_QUEUE_ATTR_NAME]\n        input_queue.put(batch)\n\n    def _is_convergence_step(self, step_name: str) -&gt; None:\n        \"\"\"Checks if a step is a convergence step.\n\n        Args:\n            step_name: The name of the step.\n        \"\"\"\n        return self.dag.get_step(step_name).get(CONVERGENCE_STEP_ATTR_NAME)\n\n    def _send_last_batch_flag_to_step(self, step_name: str) -&gt; None:\n        \"\"\"Sends the `LAST_BATCH_SENT_FLAG` to a step to stop processing batches.\n\n        Args:\n            step_name: The name of the step.\n        \"\"\"\n        batch = self._batch_manager.get_last_batch_sent(step_name)  # type: ignore\n        if batch and batch.last_batch:\n            return\n\n        self._logger.debug(\n            f\"Sending `LAST_BATCH_SENT_FLAG` to '{step_name}' step to stop processing\"\n            \" batches...\"\n        )\n        input_queue = self.dag.get_step(step_name)[INPUT_QUEUE_ATTR_NAME]\n        input_queue.put(LAST_BATCH_SENT_FLAG)\n        self._batch_manager.set_last_batch_flag_sent_to(step_name)  # type: ignore\n\n    def _run_steps_in_loop(\n        self,\n        pool: \"Pool\",\n        manager: \"SyncManager\",\n        output_queue: \"Queue[_Batch]\",\n        shared_info: \"DictProxy[str, Any]\",\n    ) -&gt; None:\n        \"\"\"Using the `pool`, runs the steps in the DAG in an infinite loop waiting for\n        input batches and sending the output batches to the `output_queue`.\n\n        Each `Step` is wrapped in a `_ProcessWrapper`, which will handle the lifecycle of\n        the `Step` and the communication with the `input_queue` and `output_queue`. The\n        `_ProcessWrapper.run` method is the target function of the process.\n\n        Args:\n            pool: The pool of processes.\n            manager: The manager to create the queues.\n            output_queue: The queue to send the output batches.\n            shared_info: The shared information between the processes.\n        \"\"\"\n        for step_name in self.dag:\n            step: \"Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n            input_queue = manager.Queue()\n            self.dag.set_step_attr(step.name, INPUT_QUEUE_ATTR_NAME, input_queue)  # type: ignore\n\n            # Set `pipeline` to `None` as in some Python environments the pipeline is not\n            # picklable and it will raise an error when trying to send the step to the process.\n            # `TypeError: cannot pickle 'code' object`\n            step.pipeline = None\n\n            process_wrapper = _ProcessWrapper(\n                step=step,\n                input_queue=input_queue,\n                output_queue=output_queue,\n                shared_info=shared_info,\n                dry_run=self._dry_run,\n            )\n\n            pool.apply_async(\n                process_wrapper.run,\n                callback=self._finished_callback,\n                error_callback=self._error_callback,\n            )  # type: ignore\n\n    def _error_callback(self, e: BaseException) -&gt; None:\n        \"\"\"Error callback that will be called when an error occurs in a `Step` process.\n\n        Args:\n            e: The exception raised by the process.\n        \"\"\"\n        global _SUBPROCESS_EXCEPTION\n\n        # First we check that the exception is a `_ProcessWrapperException`, otherwise, we\n        # print it out and stop the pipeline, since some errors may be unhandled\n        if not isinstance(e, _ProcessWrapperException):\n            self._logger.error(f\"\u274c Failed with an unhandled exception: {e}\")\n            self._stop()\n            return\n\n        if e.is_load_error:\n            self._logger.error(f\"\u274c Failed to load step '{e.step.name}': {e.message}\")\n            with self.shared_info[_STEPS_LOADED_LOCK_KEY]:\n                self.shared_info[_STEPS_LOADED_KEY] = [_STEPS_LOADED_ERROR_CODE]\n            _SUBPROCESS_EXCEPTION = e.subprocess_exception\n            _SUBPROCESS_EXCEPTION.__traceback__ = tblib.Traceback.from_string(  # type: ignore\n                e.formatted_traceback\n            ).as_traceback()\n            return\n\n        # If the step is global, is not in the last trophic level and has no successors,\n        # then we can ignore the error and continue executing the pipeline\n        step_name: str = e.step.name  # type: ignore\n        if (\n            e.step.is_global\n            and not self.dag.step_in_last_trophic_level(step_name)\n            and list(self.dag.get_step_successors(step_name)) == []\n        ):\n            self._logger.error(\n                f\"\u270b An error occurred when running global step '{step_name}' with no\"\n                \" successors and not in the last trophic level. Pipeline execution can\"\n                f\" continue. Error will be ignored.\"\n            )\n            self._logger.error(f\"Subprocess traceback:\\n\\n{e.formatted_traceback}\")\n            return\n\n        # Global step with successors failed\n        self._logger.error(f\"An error occurred in global step '{step_name}'\")\n        self._logger.error(f\"Subprocess traceback:\\n\\n{e.formatted_traceback}\")\n        self._cache()\n        self._stop()\n\n    def _finished_callback(self, step_name: str) -&gt; None:\n        \"\"\"Callback that will be called when a `Step` process finishes.\n\n        Args:\n            step_name: The name of the step that finished.\n        \"\"\"\n        with _STEPS_FINISHED_LOCK:\n            _STEPS_FINISHED.add(step_name)\n\n    def _check_step_not_loaded_or_finished(self, step_name: str) -&gt; bool:\n        \"\"\"Checks if a step is not loaded or already finished.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            `True` if the step is not loaded or already finished, `False` otherwise.\n        \"\"\"\n        with _STEPS_LOADED_LOCK:\n            if step_name not in _STEPS_LOADED:\n                return True\n\n        with _STEPS_FINISHED_LOCK:\n            if step_name in _STEPS_FINISHED:\n                return True\n\n        return False\n\n    def _stop(\n        self, manager: Optional[\"SyncManager\"] = None, pool: Optional[\"Pool\"] = None\n    ) -&gt; None:\n        \"\"\"Stops the pipeline execution. It will first send `None` to the input queues\n        of all the steps and then wait until the output queue is empty i.e. all the steps\n        finished processing the batches that were sent before the stop flag. Then it will\n        send `None` to the output queue to notify the pipeline to stop.\"\"\"\n\n        global _STOP_CALLED\n\n        with _STOP_CALLED_LOCK:\n            if _STOP_CALLED:\n                global _STOP_CALLS\n                _STOP_CALLS += 1\n                if _STOP_CALLS == 1:\n                    self._logger.warning(\n                        \"\ud83d\uded1 Press again to force the pipeline to stop.\"\n                    )\n                elif _STOP_CALLS &gt; 1:\n                    self._logger.warning(\"\ud83d\uded1 Forcing pipeline interruption.\")\n\n                    if pool:\n                        pool.terminate()\n                        pool.join()\n\n                    if manager:\n                        manager.shutdown()\n                        manager.join()\n\n                    stop_logging()\n\n                    sys.exit(1)\n\n                return\n            _STOP_CALLED = True\n\n        self._logger.debug(f\"Steps loaded before calling `stop`: {_STEPS_LOADED}\")\n        self._logger.info(\n            \"\ud83d\uded1 Stopping pipeline. Waiting for steps to finish processing batches...\"\n        )\n        self._logger.debug(\"Sending `None` to the output queue to notify stop...\")\n        self.output_queue.put(None)\n\n    def _handle_keyboard_interrupt(\n        self, manager: Optional[\"SyncManager\"] = None, pool: Optional[\"Pool\"] = None\n    ) -&gt; None:\n        \"\"\"Handles KeyboardInterrupt signal sent during the Pipeline.run method.\n\n        It will try to call self._stop (if the pipeline didn't started yet, it won't\n        have any effect), and if the pool is already started, will close it before exiting\n        the program.\n        \"\"\"\n\n        def signal_handler(signumber: int, frame: Any) -&gt; None:\n            self._stop(manager=manager, pool=pool)\n\n        signal.signal(signal.SIGINT, signal_handler)\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline.run","title":"<code>run(parameters=None, use_cache=True, storage_parameters=None, use_fs_to_pass_data=False)</code>","text":"<p>Runs the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a dictionary with the runtime parameters for the step as the value. Defaults to <code>None</code>.</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use the cache from previous pipeline runs. Defaults to <code>True</code>.</p> <code>True</code> <code>storage_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary with the storage parameters (<code>fsspec</code> and path) that will be used to store the data of the <code>_Batch</code>es passed between the steps if <code>use_fs_to_pass_data</code> is <code>True</code> (for the batches received by a <code>GlobalStep</code> it will be always used). It must have at least the \"path\" key, and it can contain additional keys depending on the protocol. By default, it will use the local file system and a directory in the cache directory. Defaults to <code>None</code>.</p> <code>None</code> <code>use_fs_to_pass_data</code> <code>bool</code> <p>Whether to use the file system to pass the data of the <code>_Batch</code>es between the steps. Even if this parameter is <code>False</code>, the <code>Batch</code>es received by <code>GlobalStep</code>s will always use the file system to pass the data. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distiset</code> <p>The <code>Distiset</code> created by the pipeline.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the pipeline fails to load all the steps.</p> Source code in <code>src/distilabel/pipeline/local.py</code> <pre><code>def run(\n    self,\n    parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    use_cache: bool = True,\n    storage_parameters: Optional[Dict[str, Any]] = None,\n    use_fs_to_pass_data: bool = False,\n) -&gt; \"Distiset\":\n    \"\"\"Runs the pipeline.\n\n    Args:\n        parameters: A dictionary with the step name as the key and a dictionary with\n            the runtime parameters for the step as the value. Defaults to `None`.\n        use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n            `True`.\n        storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n            that will be used to store the data of the `_Batch`es passed between the\n            steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n            `GlobalStep` it will be always used). It must have at least the \"path\" key,\n            and it can contain additional keys depending on the protocol. By default,\n            it will use the local file system and a directory in the cache directory.\n            Defaults to `None`.\n        use_fs_to_pass_data: Whether to use the file system to pass the data of\n            the `_Batch`es between the steps. Even if this parameter is `False`, the\n            `Batch`es received by `GlobalStep`s will always use the file system to\n            pass the data. Defaults to `False`.\n\n    Returns:\n        The `Distiset` created by the pipeline.\n\n    Raises:\n        RuntimeError: If the pipeline fails to load all the steps.\n    \"\"\"\n    log_queue = mp.Queue()\n\n    self._set_logging_parameters(\n        {\"log_queue\": log_queue, \"filename\": self._cache_location[\"log_file\"]}\n    )\n\n    if distiset := super().run(\n        parameters, use_cache, storage_parameters, use_fs_to_pass_data\n    ):\n        return distiset\n\n    num_processes = len(self.dag)\n    ctx = mp.get_context()  # type: ignore\n    with ctx.Manager() as manager, ctx.Pool(\n        num_processes,\n        initializer=_init_worker,\n        initargs=(log_queue,),\n    ) as pool:\n        self.output_queue: \"Queue[Any]\" = manager.Queue()\n        self.shared_info = self._create_shared_info_dict(manager)\n        self._handle_keyboard_interrupt(manager=manager, pool=pool)\n\n        # Run the steps using the pool of processes\n        self._run_steps_in_loop(pool, manager, self.output_queue, self.shared_info)\n\n        # Wait for all the steps to be loaded correctly\n        if not self._all_steps_loaded():\n            self._write_buffer.close()  # type: ignore\n            self._batch_manager = None\n            stop_logging()\n            raise RuntimeError(\n                \"Failed to load all the steps. Could not run pipeline.\"\n            ) from _SUBPROCESS_EXCEPTION\n\n        # Send the \"first\" batches to the steps so the batches starts flowing through\n        # the input queues and output queue\n        self._request_initial_batches()\n\n        # Start a loop to receive the output batches from the steps\n        self._run_output_queue_loop_in_thread()\n\n        # Send `None` to steps `input_queue`s just in case some step is still waiting\n        self._notify_steps_to_stop()\n\n    # `Pool.__exit__` has already called `terminate`, `join` the pool to make sure\n    # all the processes have finished\n    pool.join()\n    manager.join()\n\n    self._write_buffer.close()  # type: ignore\n    distiset = create_distiset(\n        self._cache_location[\"data\"],\n        pipeline_path=self._cache_location[\"pipeline\"],\n        log_filename_path=self._cache_location[\"log_file\"],\n        enable_metadata=self._enable_metadata,\n    )\n    stop_logging()\n    return distiset\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.sample_n_steps","title":"<code>sample_n_steps(n)</code>","text":"<p>A simple function that creates a routing batch function that samples <code>n</code> steps from the list of all the downstream steps.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of steps to sample from the list of all the downstream steps.</p> required <p>Returns:</p> Type Description <code>RoutingBatchFunction</code> <p>A <code>RoutingBatchFunction</code> instance that can be used with the <code>&gt;&gt;</code> operators and with</p> <code>RoutingBatchFunction</code> <p>the <code>Pipeline.connect</code> method when defining the pipeline.</p> <p>Example:</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline, sample_n_steps\nfrom distilabel.steps import LoadHubDataset, CombineColumns\n\n\nrandom_routing_batch = sample_n_steps(2)\n\n\nwith Pipeline(name=\"routing-batch-function\") as pipeline:\n    load_data = LoadHubDataset()\n\n    generations = []\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        generations.append(task)\n\n    combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n    load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n</code></pre> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def sample_n_steps(n: int) -&gt; RoutingBatchFunction:\n    \"\"\"A simple function that creates a routing batch function that samples `n` steps from\n    the list of all the downstream steps.\n\n    Args:\n        n: The number of steps to sample from the list of all the downstream steps.\n\n    Returns:\n        A `RoutingBatchFunction` instance that can be used with the `&gt;&gt;` operators and with\n        the `Pipeline.connect` method when defining the pipeline.\n\n    Example:\n\n    ```python\n    from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\n    from distilabel.pipeline import Pipeline, sample_n_steps\n    from distilabel.steps import LoadHubDataset, CombineColumns\n\n\n    random_routing_batch = sample_n_steps(2)\n\n\n    with Pipeline(name=\"routing-batch-function\") as pipeline:\n        load_data = LoadHubDataset()\n\n        generations = []\n        for llm in (\n            OpenAILLM(model=\"gpt-4-0125-preview\"),\n            MistralLLM(model=\"mistral-large-2402\"),\n            VertexAILLM(model=\"gemini-1.5-pro\"),\n        ):\n            task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n            generations.append(task)\n\n        combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n        load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n    ```\n    \"\"\"\n\n    @routing_batch_function(\n        description=f\"Sample {n} steps from the list of downstream steps.\"\n    )\n    def sample_n(steps: List[str]) -&gt; List[str]:\n        return random.sample(steps, n)\n\n    return sample_n\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/","title":"dag","text":""},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG","title":"<code>DAG</code>","text":"<p>               Bases: <code>_Serializable</code></p> <p>A Directed Acyclic Graph (DAG) to represent the pipeline.</p> <p>Attributes:</p> Name Type Description <code>G</code> <p>The graph representing the pipeline.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>class DAG(_Serializable):\n    \"\"\"A Directed Acyclic Graph (DAG) to represent the pipeline.\n\n    Attributes:\n        G: The graph representing the pipeline.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self.G = nx.DiGraph()\n\n    def __iter__(self) -&gt; Generator[str, None, None]:\n        yield from self.G\n\n    def __len__(self) -&gt; int:\n        return len(self.G)\n\n    def add_step(self, step: \"_Step\") -&gt; None:\n        \"\"\"Add a step to the DAG.\n\n        Args:\n            step: The step to add to the DAG.\n\n        Raises:\n            ValueError: If a step with the same name already exists in the DAG.\n        \"\"\"\n        name = step.name\n        if name in self.G:\n            raise ValueError(f\"Step with name '{name}' already exists\")\n        self.G.add_node(name, step=step)\n\n    def get_step(self, name: str) -&gt; Dict[str, Any]:\n        \"\"\"Get a step from the DAG.\n\n        Args:\n            name: The name of the step to get.\n\n        Returns:\n            The step with the given name.\n\n        Raises:\n            ValueError: If the step with the given name does not exist.\n        \"\"\"\n        if name not in self.G:\n            raise ValueError(f\"Step with name '{name}' does not exist\")\n        return self.G.nodes[name]\n\n    def set_step_attr(self, name: str, attr: str, value: Any) -&gt; None:\n        \"\"\"Set an attribute of a step in the DAG.\n\n        Args:\n            name: The name of the step.\n            attr: The attribute to set.\n            value: The value to set.\n\n        Raises:\n            ValueError: If the step with the given name does not exist.\n        \"\"\"\n        if name not in self.G:\n            raise ValueError(f\"Step with name '{name}' does not exist\")\n        self.G.nodes[name][attr] = value\n\n    def add_edge(self, from_step: str, to_step: str) -&gt; None:\n        \"\"\"Add an edge between two steps in the DAG.\n\n        Args:\n            from_step: The name of the step from which the edge starts.\n            to_step: The name of the step to which the edge ends.\n\n        Raises:\n            ValueError: If the edge cannot be added.\n        \"\"\"\n        if from_step not in self.G:\n            raise ValueError(f\"Step with name '{from_step}' does not exist\")\n\n        if to_step not in self.G:\n            raise ValueError(f\"Step with name '{to_step}' does not exist\")\n\n        if to_step in self.G[from_step]:\n            raise ValueError(\n                f\"There is already a edge from '{to_step}' to '{from_step}'\"\n            )\n\n        if to_step in nx.ancestors(self.G, from_step):\n            raise ValueError(\n                f\"Cannot add edge from '{from_step}' to '{to_step}' as it would create a cycle.\"\n            )\n\n        self.G.add_edge(from_step, to_step)\n\n    @cached_property\n    def root_steps(self) -&gt; Set[str]:\n        \"\"\"The steps that don't have any predecessors i.e. generator steps.\n\n        Returns:\n            A list with the names of the steps that don't have any predecessors.\n        \"\"\"\n        return {node for node, degree in self.G.in_degree() if degree == 0}\n\n    @cached_property\n    def leaf_steps(self) -&gt; Set[str]:\n        \"\"\"The steps that don't have any successors.\n\n        Returns:\n            A list with the names of the steps that don't have any successors.\n        \"\"\"\n        return {node for node, degree in self.G.out_degree() if degree == 0}\n\n    @cached_property\n    def trophic_levels(self) -&gt; Dict[str, int]:\n        \"\"\"The trophic level of each step in the DAG.\n\n        Returns:\n            A dictionary with the trophic level of each step.\n        \"\"\"\n        return {step: int(level) for step, level in nx.trophic_levels(self.G).items()}\n\n    def get_step_predecessors(self, step_name: str) -&gt; Iterable[str]:\n        \"\"\"Gets the predecessors of a step.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            An iterable with the names of the steps that are predecessors of the given step.\n        \"\"\"\n        if step_name not in self.G:\n            raise ValueError(f\"Step '{step_name}' does not exist\")\n        return self.G.predecessors(step_name)\n\n    def get_step_successors(self, step_name: str) -&gt; Iterable[str]:\n        \"\"\"Gets the successors of a step.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            An iterable with the names of the steps that are successors of the given step.\n        \"\"\"\n\n        if step_name not in self.G:\n            raise ValueError(f\"Step '{step_name}' does not exist\")\n        return self.G.successors(step_name)\n\n    def iter_based_on_trophic_levels(self) -&gt; Iterable[List[str]]:\n        \"\"\"Iterate over steps names in the DAG based on their trophic levels. This is similar\n        to a topological sort, but we also know which steps are at the same level and\n        can be run in parallel.\n\n        Yields:\n            A list containing the names of the steps that can be run in parallel.\n        \"\"\"\n        v = defaultdict(list)\n        for step, trophic_level in self.trophic_levels.items():\n            v[trophic_level].append(step)\n\n        for trophic_level in sorted(v.keys()):\n            yield v[trophic_level]\n\n    def get_step_trophic_level(self, step_name: str) -&gt; int:\n        \"\"\"Gets the trophic level of a step.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            The trophic level of the step.\n        \"\"\"\n        return int(self.trophic_levels[step_name])\n\n    def is_step_in_trophic_level(self, step_name: str, trophic_level: int) -&gt; bool:\n        \"\"\"Checks if a step is in a given trophic level.\n\n        Args:\n            step_name: The name of the step.\n            trophic_level: The trophic level.\n\n        Returns:\n            True if the step is in the given trophic level, False otherwise.\n        \"\"\"\n        return self.get_step_trophic_level(step_name) == trophic_level\n\n    def step_in_last_trophic_level(self, step_name: str) -&gt; bool:\n        \"\"\"Checks if a step is in the last trophic level.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            True if the step is in the last trophic level, False otherwise.\n        \"\"\"\n        return self.is_step_in_trophic_level(\n            step_name, max(self.trophic_levels.values())\n        )\n\n    def validate(self) -&gt; None:\n        \"\"\"Validates that the `Step`s included in the pipeline are correctly connected and\n        have the correct inputs and outputs.\n\n        Raises:\n            ValueError: If the pipeline is not valid.\n        \"\"\"\n\n        steps_receiving_routed_batches = []\n\n        for trophic_level, steps in enumerate(\n            self.iter_based_on_trophic_levels(), start=1\n        ):\n            for step_name in steps:\n                node = self.get_step(step_name)\n                step: \"_Step\" = node[STEP_ATTR_NAME]\n\n                # Check if the step `process` function has `StepInput` argument\n                self._validate_step_process_arguments(step)\n\n                # Check if the required runtime parameters are provided\n                self._validate_step_process_runtime_parameters(step)\n\n                # Validate step mappings\n                step.verify_inputs_mappings()\n                step.verify_outputs_mappings()\n\n                # Validate that the steps in the first trophic level are `GeneratorStep`s\n                if trophic_level == 1:\n                    if not isinstance(step, GeneratorStep):\n                        raise ValueError(\n                            f\"Step '{step_name}' cannot be a root step because it is not\"\n                            \" a `GeneratorStep`. It should have a previous step in the pipeline.\"\n                        )\n                    self._validate_generator_step_process_signature(step)\n                else:\n                    self._step_inputs_are_available(step)\n\n                    # Validate routing batch function (if any)\n                    predecessors = list(self.get_step_predecessors(step.name))  # type: ignore\n                    self._validate_convergence_step(\n                        step,\n                        predecessors,\n                        steps_receiving_routed_batches,  # type: ignore\n                    )\n                    receives_routed_batches = self._validate_routing_batch_function(\n                        step, predecessors\n                    )\n                    if receives_routed_batches:\n                        steps_receiving_routed_batches.append(step.name)\n\n    def _step_inputs_are_available(self, step: \"_Step\") -&gt; None:\n        \"\"\"Validates that the `Step.inputs` will be available when the step gets to be\n        executed in the pipeline i.e. the step will receive list of dictionaries containing\n        its inputs as keys.\n\n        Args:\n            step: The step.\n        \"\"\"\n        inputs_available_for_step = [\n            output\n            for step_name in nx.ancestors(self.G, step.name)\n            for output in self.get_step(step_name)[STEP_ATTR_NAME].get_outputs()  # type: ignore\n        ]\n        step_inputs = step.get_inputs()\n        if not all(input in inputs_available_for_step for input in step_inputs):\n            raise ValueError(\n                f\"Step '{step.name}' requires inputs {step_inputs}, but only the inputs\"\n                f\"={inputs_available_for_step} are available, which means that the inputs\"\n                f\"={list(set(step_inputs) - set(inputs_available_for_step))} are missing or not\"\n                \" available when the step gets to be executed in the pipeline.\"\n                f\" Please make sure previous steps to '{step.name}' are generating\"\n                \" the required inputs.\"\n            )\n\n    def _validate_step_process_arguments(self, step: \"_Step\") -&gt; None:\n        \"\"\"Validates the arguments of the `Step.process` method, checking there is an\n        argument with type hint `StepInput` and that all the required runtime parameters\n        are provided.\n\n        Args:\n            step: The step to validate.\n\n        Raises:\n            ValueError: If the arguments of the `process` method of the step are not valid.\n        \"\"\"\n\n        step_input_parameter = step.get_process_step_input()\n        self._validate_process_step_input_parameter(step.name, step_input_parameter)  # type: ignore\n\n    def _validate_convergence_step(\n        self,\n        step: \"Step\",\n        predecessors: List[str],\n        steps_receiving_routed_batches: List[str],\n    ) -&gt; None:\n        \"\"\"Checks if the `step` is a convergence step (receiving batches from steps to\n        which the batches were routed). If so, it validates that all the predecessors of\n        the steps receives routed batches from the same step, and that the `input_batch_size`\n        of the `step` is equal or lower to the `input_batch_size` of the previous steps.\n\n        Args:\n            step: The step to validate.\n            predecessors: The predecessors of the step.\n            steps_receiving_routed_batches: The steps that are receiving routed batches\n                from other steps in the pipeline.\n        \"\"\"\n        if not any(\n            predecessor in steps_receiving_routed_batches\n            for predecessor in predecessors\n        ):\n            return\n\n        # Mark the step as a convergence step\n        self.set_step_attr(step.name, CONVERGENCE_STEP_ATTR_NAME, True)  # type: ignore\n\n        # Check if all the predecessors of the step are receiving routed batches from the\n        # same step\n        previous_steps_predecessors = [\n            list(self.get_step_predecessors(predecessor))\n            for predecessor in predecessors\n        ]\n        if not all(\n            prev_step_predecessors == previous_steps_predecessors[0]\n            for prev_step_predecessors in previous_steps_predecessors\n        ):\n            raise ValueError(\n                f\"Convergence step '{step.name}' should receive batches from steps receiving\"\n                \" routed batches from the same previous step and `routing_batch_function`.\"\n            )\n\n        # Check if the `input_batch_size` of the step is equal or lower than the\n        for predecessor in predecessors:\n            prev_step: \"Step\" = self.get_step(predecessor)[STEP_ATTR_NAME]\n            if step.input_batch_size &gt; prev_step.input_batch_size:  # type: ignore\n                raise ValueError(\n                    \"A convergence step should have an `input_batch_size` equal or lower\"\n                    \" than the `input_batch_size` of the connected previous steps.\"\n                    f\" Convergence step '{step.name}' has an `input_batch_size` of {step.input_batch_size}\"\n                    f\" and the previous step '{prev_step.name}' has an `input_batch_size`\"\n                    f\" of {prev_step.input_batch_size}.\"\n                )\n\n    def _validate_routing_batch_function(\n        self, step: \"_Step\", predecessors: List[str]\n    ) -&gt; bool:\n        \"\"\"Checks if the `step` is going to receive routed batches (i.e. `routing_batch_function`\n        chooses which batches from upstream step goes to the downstream step). If so, then it\n        validates that the step has only one predecessor and that its `input_batch_size` is\n        equal or lower than the `input_batch_size` or `batch_size` of the previous step.\n        These are requirements to keep batches synchronized when executing the pipeline.\n\n        Args:\n            step: The step to validate.\n            predecessors: The predecessors of the step.\n\n        Returns:\n            `True` if the `step` is going to receive routed batches, `False` otherwise.\n\n        Raises:\n            ValueError: If the `step` is going to receive routed batches and it has multiple\n                predecessors or its `input_batch_size` is higher than the previous step\n                `input_batch_size` or `batch_size`.\n        \"\"\"\n        routing_batch_function = None\n        for predecessor in predecessors:\n            node = self.get_step(predecessor)\n            routing_batch_function = node.get(ROUTING_BATCH_FUNCTION_ATTR_NAME)\n            if routing_batch_function is not None and len(predecessors) &gt; 1:\n                raise ValueError(\n                    f\"Step '{step.name}' cannot have multiple predecessors when the batches\"\n                    \" of one are being routed with a `routing_batch_function`.\"\n                )\n\n        if routing_batch_function is None:\n            return False\n\n        # If the step receives routed batches, then check its `input_batch_size` is lower\n        # or equal to the `input_batch_size` or `batch_size` of the previous step from which\n        # the batches are being routed.\n        predecessor_step: \"_Step\" = self.get_step(predecessors[0])[STEP_ATTR_NAME]  # type: ignore\n        batch_size = (\n            predecessor_step.batch_size  # type: ignore\n            if predecessor_step.is_generator\n            else predecessor_step.input_batch_size  # type: ignore\n        )\n        if step.input_batch_size &gt; batch_size:  # type: ignore\n            raise ValueError(\n                f\"Step '{step.name}' should have an `input_batch_size` equal or lower\"\n                f\" than the `input_batch_size` or `batch_size` of the previous step.\"\n                f\" This is because the batches are being routed with a `routing_batch_function`\"\n                f\" from step '{predecessor_step.name}' to step '{step.name}'.\"\n            )\n\n        if batch_size % step.input_batch_size != 0:  # type: ignore\n            raise ValueError(\n                f\"Step '{step.name}' should have an `input_batch_size` that is a multiple\"\n                f\" of the `input_batch_size` or `batch_size` of the previous step.\"\n                f\" This is because the batches are being routed with a `routing_batch_function`\"\n                f\" from step '{predecessor_step.name}' to step '{step.name}'.\"\n            )\n\n        return True\n\n    def _validate_process_step_input_parameter(\n        self,\n        step_name: str,\n        step_input_parameter: Union[inspect.Parameter, None] = None,\n    ) -&gt; None:\n        \"\"\"Validates that the `Step.process` method has a parameter with type hint `StepInput`\n\n        Args:\n            step_name: The name of the step.\n            step_input_parameter: The parameter with type hint `StepInput` of the `process`\n                method of the step.\n\n        Raises:\n            ValueError: If the `step_input_parameter` is not valid.\n        \"\"\"\n\n        predecessors = {\n            step_name: self.get_step(step_name)[STEP_ATTR_NAME]\n            for step_name in self.G.predecessors(step_name)\n        }\n        num_predecessors = len(predecessors)\n\n        if num_predecessors == 0:\n            return\n\n        if step_input_parameter is None:\n            if num_predecessors &gt; 1:\n                prev_steps = \", \".join([f\"'{step_name}'\" for step_name in predecessors])\n                raise ValueError(\n                    f\"Step '{step_name}' should have a `*args` parameter with type hint\"\n                    f\" `StepInput` to receive outputs from previous steps: {prev_steps}.\"\n                )\n\n            prev_step_name = next(iter(predecessors))\n            raise ValueError(\n                f\"Step '{step_name}' should have a parameter with type hint `StepInput`\"\n                f\" to receive the output from the previous step: '{prev_step_name}'.\"\n            )\n\n        if (\n            num_predecessors &gt; 1\n            and step_input_parameter.kind != inspect.Parameter.VAR_POSITIONAL\n        ):\n            raise ValueError(\n                f\"Step '{step_name}' should have a `*args` parameter with type hint `StepInput`\"\n                f\" to receive outputs from previous steps.\"\n            )\n\n    def _validate_step_process_runtime_parameters(  # noqa: C901\n        self, step: \"_Step\"\n    ) -&gt; None:\n        \"\"\"Validates that the required runtime parameters of the step are provided. A\n        runtime parameter is considered required if it doesn't have a default value. The\n        name of the runtime parameters are separated by dots to represent nested parameters.\n\n        Args:\n            step: The step to validate.\n\n        Raises:\n            ValueError: If not all the required runtime parameters haven't been provided\n                with a value.\n        \"\"\"\n\n        def _get_pipeline_aux_code(step_name: str, param_name: str) -&gt; str:\n            parts = param_name.split(\".\")\n            result = f'pipeline.run(parameters={{\"{step_name}\":'\n            nested_dict = \"...\"\n            for part in reversed(parts):\n                nested_dict = f' {{\"{part}\": {nested_dict}}}'\n            result += nested_dict + \"})\"\n            return result\n\n        def _get_attribute_default(\n            step: \"_Step\", composed_param_name: str\n        ) -&gt; Union[Any, None]:\n            parts = composed_param_name.split(\".\")\n            attr = step\n            for part in parts:\n                if isinstance(attr, dict):\n                    attr = attr.get(part, None)\n                elif isinstance(attr, object):\n                    attr = getattr(attr, part)\n            return attr\n\n        def _check_required_parameter(\n            param_name: str,\n            composed_param_name: str,\n            is_optional_or_nested: Union[bool, \"RuntimeParametersNames\"],\n            runtime_parameters: Dict[str, Any],\n            runtime_parameters_names: \"RuntimeParametersNames\",\n        ) -&gt; None:\n            if isinstance(is_optional_or_nested, dict):\n                runtime_parameters_names = runtime_parameters_names[param_name]  # type: ignore\n                for subparam, value in runtime_parameters_names.items():\n                    _check_required_parameter(\n                        param_name=subparam,\n                        composed_param_name=f\"{composed_param_name}.{subparam}\",\n                        is_optional_or_nested=value,\n                        # NOTE: `runtime_parameters` get is for the specific case of `LLM` in `Task`\n                        runtime_parameters=runtime_parameters.get(\n                            param_name, runtime_parameters\n                        ),\n                        runtime_parameters_names=runtime_parameters_names,\n                    )\n                return\n\n            if (\n                not is_optional_or_nested\n                and param_name not in runtime_parameters\n                and _get_attribute_default(\n                    step=step, composed_param_name=composed_param_name\n                )\n                is None\n            ):\n                aux_code = _get_pipeline_aux_code(step.name, composed_param_name)\n                raise ValueError(\n                    f\"Step '{step.name}' is missing required runtime parameter '{param_name}'.\"\n                    \" Please, provide a value for it when calling `Pipeline.run` method:\\n\\n\"\n                    f\"    {aux_code}\"\n                )\n\n        runtime_parameters_names = step.runtime_parameters_names\n        for param_name, value in runtime_parameters_names.items():\n            _check_required_parameter(\n                param_name=param_name,\n                composed_param_name=param_name,\n                is_optional_or_nested=value,\n                runtime_parameters=step._runtime_parameters,\n                runtime_parameters_names=runtime_parameters_names,\n            )\n\n    def _validate_generator_step_process_signature(self, step: \"GeneratorStep\") -&gt; None:\n        \"\"\"Validates that the `process` method of the `GeneratorStep` does not expect the\n        `inputs` arg within the method signature, and also the `offset` arg should always\n        be present.\n\n        Args:\n            step: The step to validate.\n\n        Raises:\n            ValueError: If the `process` method of the `GeneratorStep` expects the `inputs` arg.\n            ValueError: If the `process` method of the `GeneratorStep` does not expect the `offset` arg.\n        \"\"\"\n        if step.get_process_step_input() is not None:\n            raise ValueError(\n                f\"Generator step '{step.name}' should not have a parameter with type hint\"\n                \" `StepInput` within the `process` method signature.\"\n            )\n        if not any(\"offset\" == parameter.name for parameter in step.process_parameters):\n            raise ValueError(\n                f\"Generator step '{step.name}' should have an `offset` parameter within\"\n                \" the `process` method signature.\"\n            )\n\n    def _model_dump(self, obj: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"Dumps the content of the DAG to a dict.\n\n        References:\n        * [`adjacency_data` - NetworkX Documentation](https://networkx.org/documentation/stable/reference/readwrite/generated/networkx.readwrite.json_graph.adjacency_data.html#networkx.readwrite.json_graph.adjacency_data)\n\n        Args:\n            obj (Any): Unused, just kept to match the signature of the parent method.\n            kwargs (Any): Additional arguments that could be passed to the networkx function.\n\n        Returns:\n            Dict[str, Any]: Internal representation of the DAG from networkx in a serializable format.\n        \"\"\"\n        from networkx.readwrite import json_graph\n\n        adjacency_data = json_graph.adjacency_data(self.G, **kwargs)\n\n        data = {\"steps\": [], \"connections\": [], \"routing_batch_functions\": []}\n        for i, node in enumerate(adjacency_data[\"nodes\"]):\n            name = node[\"id\"]\n            data[\"steps\"].append({\"step\": node[STEP_ATTR_NAME].dump(), \"name\": name})\n            data[\"connections\"].append(\n                {\n                    \"from\": name,\n                    \"to\": [node[\"id\"] for node in adjacency_data[\"adjacency\"][i]],\n                }\n            )\n            if routing_batch_function := node.get(ROUTING_BATCH_FUNCTION_ATTR_NAME):\n                data[\"routing_batch_functions\"].append(routing_batch_function.dump())\n\n        return data\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"DAG\":\n        \"\"\"Generates the DAG from a dictionary with the steps serialized.\n\n        Args:\n            data (Dict[str, Any]): Dictionary with the serialized content (the content from self.dump()).\n\n        Returns:\n            DAG: Instance of the DAG from the serialized content.\n        \"\"\"\n\n        dag = cls()\n\n        for step in data[\"steps\"]:\n            cls_step: Type[\"_Step\"] = _get_module_attr(\n                **step[STEP_ATTR_NAME][TYPE_INFO_KEY]\n            )\n            dag.add_step(cls_step.from_dict(step[STEP_ATTR_NAME]))\n\n        for connection in data[\"connections\"]:\n            from_step = connection[\"from\"]\n            for to_step in connection[\"to\"]:\n                dag.add_edge(from_step, to_step)\n\n        for routing_batch_function in data.get(\"routing_batch_functions\", []):\n            step_name = routing_batch_function[\"step\"]\n            routing_batch_function = RoutingBatchFunction.from_dict(\n                routing_batch_function\n            )\n            dag.set_step_attr(\n                name=step_name,\n                attr=ROUTING_BATCH_FUNCTION_ATTR_NAME,\n                value=routing_batch_function,\n            )\n\n        return dag\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.leaf_steps","title":"<code>leaf_steps: Set[str]</code>  <code>cached</code> <code>property</code>","text":"<p>The steps that don't have any successors.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>A list with the names of the steps that don't have any successors.</p>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.root_steps","title":"<code>root_steps: Set[str]</code>  <code>cached</code> <code>property</code>","text":"<p>The steps that don't have any predecessors i.e. generator steps.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>A list with the names of the steps that don't have any predecessors.</p>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.trophic_levels","title":"<code>trophic_levels: Dict[str, int]</code>  <code>cached</code> <code>property</code>","text":"<p>The trophic level of each step in the DAG.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>A dictionary with the trophic level of each step.</p>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.add_edge","title":"<code>add_edge(from_step, to_step)</code>","text":"<p>Add an edge between two steps in the DAG.</p> <p>Parameters:</p> Name Type Description Default <code>from_step</code> <code>str</code> <p>The name of the step from which the edge starts.</p> required <code>to_step</code> <code>str</code> <p>The name of the step to which the edge ends.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the edge cannot be added.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def add_edge(self, from_step: str, to_step: str) -&gt; None:\n    \"\"\"Add an edge between two steps in the DAG.\n\n    Args:\n        from_step: The name of the step from which the edge starts.\n        to_step: The name of the step to which the edge ends.\n\n    Raises:\n        ValueError: If the edge cannot be added.\n    \"\"\"\n    if from_step not in self.G:\n        raise ValueError(f\"Step with name '{from_step}' does not exist\")\n\n    if to_step not in self.G:\n        raise ValueError(f\"Step with name '{to_step}' does not exist\")\n\n    if to_step in self.G[from_step]:\n        raise ValueError(\n            f\"There is already a edge from '{to_step}' to '{from_step}'\"\n        )\n\n    if to_step in nx.ancestors(self.G, from_step):\n        raise ValueError(\n            f\"Cannot add edge from '{from_step}' to '{to_step}' as it would create a cycle.\"\n        )\n\n    self.G.add_edge(from_step, to_step)\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.add_step","title":"<code>add_step(step)</code>","text":"<p>Add a step to the DAG.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>_Step</code> <p>The step to add to the DAG.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a step with the same name already exists in the DAG.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def add_step(self, step: \"_Step\") -&gt; None:\n    \"\"\"Add a step to the DAG.\n\n    Args:\n        step: The step to add to the DAG.\n\n    Raises:\n        ValueError: If a step with the same name already exists in the DAG.\n    \"\"\"\n    name = step.name\n    if name in self.G:\n        raise ValueError(f\"Step with name '{name}' already exists\")\n    self.G.add_node(name, step=step)\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Generates the DAG from a dictionary with the steps serialized.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary with the serialized content (the content from self.dump()).</p> required <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>Instance of the DAG from the serialized content.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"DAG\":\n    \"\"\"Generates the DAG from a dictionary with the steps serialized.\n\n    Args:\n        data (Dict[str, Any]): Dictionary with the serialized content (the content from self.dump()).\n\n    Returns:\n        DAG: Instance of the DAG from the serialized content.\n    \"\"\"\n\n    dag = cls()\n\n    for step in data[\"steps\"]:\n        cls_step: Type[\"_Step\"] = _get_module_attr(\n            **step[STEP_ATTR_NAME][TYPE_INFO_KEY]\n        )\n        dag.add_step(cls_step.from_dict(step[STEP_ATTR_NAME]))\n\n    for connection in data[\"connections\"]:\n        from_step = connection[\"from\"]\n        for to_step in connection[\"to\"]:\n            dag.add_edge(from_step, to_step)\n\n    for routing_batch_function in data.get(\"routing_batch_functions\", []):\n        step_name = routing_batch_function[\"step\"]\n        routing_batch_function = RoutingBatchFunction.from_dict(\n            routing_batch_function\n        )\n        dag.set_step_attr(\n            name=step_name,\n            attr=ROUTING_BATCH_FUNCTION_ATTR_NAME,\n            value=routing_batch_function,\n        )\n\n    return dag\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.get_step","title":"<code>get_step(name)</code>","text":"<p>Get a step from the DAG.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the step to get.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The step with the given name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the step with the given name does not exist.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def get_step(self, name: str) -&gt; Dict[str, Any]:\n    \"\"\"Get a step from the DAG.\n\n    Args:\n        name: The name of the step to get.\n\n    Returns:\n        The step with the given name.\n\n    Raises:\n        ValueError: If the step with the given name does not exist.\n    \"\"\"\n    if name not in self.G:\n        raise ValueError(f\"Step with name '{name}' does not exist\")\n    return self.G.nodes[name]\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.get_step_predecessors","title":"<code>get_step_predecessors(step_name)</code>","text":"<p>Gets the predecessors of a step.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>The name of the step.</p> required <p>Returns:</p> Type Description <code>Iterable[str]</code> <p>An iterable with the names of the steps that are predecessors of the given step.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def get_step_predecessors(self, step_name: str) -&gt; Iterable[str]:\n    \"\"\"Gets the predecessors of a step.\n\n    Args:\n        step_name: The name of the step.\n\n    Returns:\n        An iterable with the names of the steps that are predecessors of the given step.\n    \"\"\"\n    if step_name not in self.G:\n        raise ValueError(f\"Step '{step_name}' does not exist\")\n    return self.G.predecessors(step_name)\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.get_step_successors","title":"<code>get_step_successors(step_name)</code>","text":"<p>Gets the successors of a step.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>The name of the step.</p> required <p>Returns:</p> Type Description <code>Iterable[str]</code> <p>An iterable with the names of the steps that are successors of the given step.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def get_step_successors(self, step_name: str) -&gt; Iterable[str]:\n    \"\"\"Gets the successors of a step.\n\n    Args:\n        step_name: The name of the step.\n\n    Returns:\n        An iterable with the names of the steps that are successors of the given step.\n    \"\"\"\n\n    if step_name not in self.G:\n        raise ValueError(f\"Step '{step_name}' does not exist\")\n    return self.G.successors(step_name)\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.get_step_trophic_level","title":"<code>get_step_trophic_level(step_name)</code>","text":"<p>Gets the trophic level of a step.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>The name of the step.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The trophic level of the step.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def get_step_trophic_level(self, step_name: str) -&gt; int:\n    \"\"\"Gets the trophic level of a step.\n\n    Args:\n        step_name: The name of the step.\n\n    Returns:\n        The trophic level of the step.\n    \"\"\"\n    return int(self.trophic_levels[step_name])\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.is_step_in_trophic_level","title":"<code>is_step_in_trophic_level(step_name, trophic_level)</code>","text":"<p>Checks if a step is in a given trophic level.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>The name of the step.</p> required <code>trophic_level</code> <code>int</code> <p>The trophic level.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the step is in the given trophic level, False otherwise.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def is_step_in_trophic_level(self, step_name: str, trophic_level: int) -&gt; bool:\n    \"\"\"Checks if a step is in a given trophic level.\n\n    Args:\n        step_name: The name of the step.\n        trophic_level: The trophic level.\n\n    Returns:\n        True if the step is in the given trophic level, False otherwise.\n    \"\"\"\n    return self.get_step_trophic_level(step_name) == trophic_level\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.iter_based_on_trophic_levels","title":"<code>iter_based_on_trophic_levels()</code>","text":"<p>Iterate over steps names in the DAG based on their trophic levels. This is similar to a topological sort, but we also know which steps are at the same level and can be run in parallel.</p> <p>Yields:</p> Type Description <code>Iterable[List[str]]</code> <p>A list containing the names of the steps that can be run in parallel.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def iter_based_on_trophic_levels(self) -&gt; Iterable[List[str]]:\n    \"\"\"Iterate over steps names in the DAG based on their trophic levels. This is similar\n    to a topological sort, but we also know which steps are at the same level and\n    can be run in parallel.\n\n    Yields:\n        A list containing the names of the steps that can be run in parallel.\n    \"\"\"\n    v = defaultdict(list)\n    for step, trophic_level in self.trophic_levels.items():\n        v[trophic_level].append(step)\n\n    for trophic_level in sorted(v.keys()):\n        yield v[trophic_level]\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.set_step_attr","title":"<code>set_step_attr(name, attr, value)</code>","text":"<p>Set an attribute of a step in the DAG.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the step.</p> required <code>attr</code> <code>str</code> <p>The attribute to set.</p> required <code>value</code> <code>Any</code> <p>The value to set.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the step with the given name does not exist.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def set_step_attr(self, name: str, attr: str, value: Any) -&gt; None:\n    \"\"\"Set an attribute of a step in the DAG.\n\n    Args:\n        name: The name of the step.\n        attr: The attribute to set.\n        value: The value to set.\n\n    Raises:\n        ValueError: If the step with the given name does not exist.\n    \"\"\"\n    if name not in self.G:\n        raise ValueError(f\"Step with name '{name}' does not exist\")\n    self.G.nodes[name][attr] = value\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.step_in_last_trophic_level","title":"<code>step_in_last_trophic_level(step_name)</code>","text":"<p>Checks if a step is in the last trophic level.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>The name of the step.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the step is in the last trophic level, False otherwise.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def step_in_last_trophic_level(self, step_name: str) -&gt; bool:\n    \"\"\"Checks if a step is in the last trophic level.\n\n    Args:\n        step_name: The name of the step.\n\n    Returns:\n        True if the step is in the last trophic level, False otherwise.\n    \"\"\"\n    return self.is_step_in_trophic_level(\n        step_name, max(self.trophic_levels.values())\n    )\n</code></pre>"},{"location":"reference/distilabel/pipeline/_dag/#distilabel.pipeline._dag.DAG.validate","title":"<code>validate()</code>","text":"<p>Validates that the <code>Step</code>s included in the pipeline are correctly connected and have the correct inputs and outputs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not valid.</p> Source code in <code>src/distilabel/pipeline/_dag.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validates that the `Step`s included in the pipeline are correctly connected and\n    have the correct inputs and outputs.\n\n    Raises:\n        ValueError: If the pipeline is not valid.\n    \"\"\"\n\n    steps_receiving_routed_batches = []\n\n    for trophic_level, steps in enumerate(\n        self.iter_based_on_trophic_levels(), start=1\n    ):\n        for step_name in steps:\n            node = self.get_step(step_name)\n            step: \"_Step\" = node[STEP_ATTR_NAME]\n\n            # Check if the step `process` function has `StepInput` argument\n            self._validate_step_process_arguments(step)\n\n            # Check if the required runtime parameters are provided\n            self._validate_step_process_runtime_parameters(step)\n\n            # Validate step mappings\n            step.verify_inputs_mappings()\n            step.verify_outputs_mappings()\n\n            # Validate that the steps in the first trophic level are `GeneratorStep`s\n            if trophic_level == 1:\n                if not isinstance(step, GeneratorStep):\n                    raise ValueError(\n                        f\"Step '{step_name}' cannot be a root step because it is not\"\n                        \" a `GeneratorStep`. It should have a previous step in the pipeline.\"\n                    )\n                self._validate_generator_step_process_signature(step)\n            else:\n                self._step_inputs_are_available(step)\n\n                # Validate routing batch function (if any)\n                predecessors = list(self.get_step_predecessors(step.name))  # type: ignore\n                self._validate_convergence_step(\n                    step,\n                    predecessors,\n                    steps_receiving_routed_batches,  # type: ignore\n                )\n                receives_routed_batches = self._validate_routing_batch_function(\n                    step, predecessors\n                )\n                if receives_routed_batches:\n                    steps_receiving_routed_batches.append(step.name)\n</code></pre>"},{"location":"reference/distilabel/pipeline/base/","title":"Base","text":""},{"location":"reference/distilabel/pipeline/base/#distilabel.pipeline.base.BasePipeline","title":"<code>BasePipeline</code>","text":"<p>               Bases: <code>_Serializable</code></p> <p>Base class for a <code>distilabel</code> pipeline.</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>The name of the pipeline.</p> <code>description</code> <p>A description of the pipeline.</p> <code>dag</code> <p>The <code>DAG</code> instance that represents the pipeline.</p> <code>_cache_dir</code> <p>The directory where the pipeline will be cached.</p> <code>_logger</code> <p>The logger instance that will be used by the pipeline.</p> <code>_batch_manager</code> <code>Optional[_BatchManager]</code> <p>The batch manager that will manage the batches received from the steps while running the pipeline. It will be created when the pipeline is run, from scratch or from cache. Defaults to <code>None</code>.</p> <code>_write_buffer</code> <code>Optional[_WriteBuffer]</code> <p>The buffer that will store the data of the leaf steps of the pipeline while running, so the <code>Distiset</code> can be created at the end. It will be created when the pipeline is run. Defaults to <code>None</code>.</p> <code>_logging_parameters</code> <code>Dict[str, Any]</code> <p>A dictionary containing the parameters that will passed to <code>setup_logging</code> function to initialize the logging. Defaults to <code>{}</code>.</p> <code>_fs</code> <code>Optional[AbstractFileSystem]</code> <p>The <code>fsspec</code> filesystem to be used to store the data of the <code>_Batch</code>es passed between the steps. It will be set when the pipeline is run. Defaults to <code>None</code>.</p> <code>_storage_base_path</code> <code>Optional[str]</code> <p>The base path where the data of the <code>_Batch</code>es passed between the steps will be stored. It will be set then the pipeline is run. Defaults to <code>None</code>.</p> <code>_use_fs_to_pass_data</code> <code>bool</code> <p>Whether to use the file system to pass the data of the <code>_Batch</code>es between the steps. Even if this parameter is <code>False</code>, the <code>Batch</code>es received by <code>GlobalStep</code>s will always use the file system to pass the data. Defaults to <code>False</code>.</p> <code>_dry_run</code> <code>bool</code> <p>A flag to indicate if the pipeline is running in dry run mode. Defaults to <code>False</code>.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>class BasePipeline(_Serializable):\n    \"\"\"Base class for a `distilabel` pipeline.\n\n    Attributes:\n        name: The name of the pipeline.\n        description: A description of the pipeline.\n        dag: The `DAG` instance that represents the pipeline.\n        _cache_dir: The directory where the pipeline will be cached.\n        _logger: The logger instance that will be used by the pipeline.\n        _batch_manager: The batch manager that will manage the batches received from the\n            steps while running the pipeline. It will be created when the pipeline is run,\n            from scratch or from cache. Defaults to `None`.\n        _write_buffer: The buffer that will store the data of the leaf steps of the pipeline\n            while running, so the `Distiset` can be created at the end. It will be created\n            when the pipeline is run. Defaults to `None`.\n        _logging_parameters: A dictionary containing the parameters that will passed to\n            `setup_logging` function to initialize the logging. Defaults to `{}`.\n        _fs: The `fsspec` filesystem to be used to store the data of the `_Batch`es passed\n            between the steps. It will be set when the pipeline is run. Defaults to `None`.\n        _storage_base_path: The base path where the data of the `_Batch`es passed between\n            the steps will be stored. It will be set then the pipeline is run. Defaults\n            to `None`.\n        _use_fs_to_pass_data: Whether to use the file system to pass the data of the\n            `_Batch`es between the steps. Even if this parameter is `False`, the `Batch`es\n            received by `GlobalStep`s will always use the file system to pass the data.\n            Defaults to `False`.\n        _dry_run: A flag to indicate if the pipeline is running in dry run mode. Defaults\n            to `False`.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        description: Optional[str] = None,\n        cache_dir: Optional[Union[str, \"PathLike\"]] = None,\n        enable_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the `BasePipeline` instance.\n\n        Args:\n            name: The name of the pipeline.\n            description: A description of the pipeline. Defaults to `None`.\n            cache_dir: A directory where the pipeline will be cached. Defaults to `None`.\n            enable_metadata: Whether to include the distilabel metadata column for the pipeline\n                in the final `Distiset`. It contains metadata used by distilabel, for example\n                the raw outputs of the `LLM` without processing would be here, inside `raw_output_...`\n                field. Defaults to `False`.\n        \"\"\"\n        self.name = name\n        self.description = description\n        self._enable_metadata = enable_metadata\n        self.dag = DAG()\n\n        if cache_dir:\n            self._cache_dir = Path(cache_dir)\n        elif env_cache_dir := os.getenv(\"DISTILABEL_CACHE_DIR\"):\n            self._cache_dir = Path(env_cache_dir)\n        else:\n            self._cache_dir = BASE_CACHE_DIR\n\n        self._logger = logging.getLogger(\"distilabel.pipeline\")\n\n        self._batch_manager: Optional[\"_BatchManager\"] = None\n        self._write_buffer: Optional[\"_WriteBuffer\"] = None\n        self._logging_parameters: Dict[str, Any] = {\n            \"filename\": self._cache_location[\"log_file\"]\n        }\n\n        self._fs: Optional[fsspec.AbstractFileSystem] = None\n        self._storage_base_path: Optional[str] = None\n        self._use_fs_to_pass_data: bool = False\n        self._dry_run: bool = False\n\n    def __enter__(self) -&gt; Self:\n        \"\"\"Set the global pipeline instance when entering a pipeline context.\"\"\"\n        _GlobalPipelineManager.set_pipeline(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback) -&gt; None:\n        \"\"\"Unset the global pipeline instance when exiting a pipeline context.\"\"\"\n        _GlobalPipelineManager.set_pipeline(None)\n\n    def _create_signature(self) -&gt; str:\n        \"\"\"Makes a signature (hash) of a pipeline, using the step ids and the adjacency between them.\n\n        The main use is to find the pipeline in the cache folder.\n\n        Returns:\n            int: Signature of the pipeline.\n        \"\"\"\n        hasher = hashlib.sha1()\n\n        steps_info = []\n        pipeline_dump = self.dump()[\"pipeline\"]\n\n        for step in pipeline_dump[\"steps\"]:\n            step_info = step[\"name\"]\n            for argument, value in sorted(step[STEP_ATTR_NAME].items()):\n                if (argument == TYPE_INFO_KEY) or (value is None):\n                    continue\n\n                if isinstance(value, dict):\n                    # input_mappings/output_mappings\n                    step_info += \"-\".join(\n                        [f\"{str(k)}-{str(v)}\" for k, v in value.items()]\n                    )\n                elif isinstance(value, (list, tuple)):\n                    # runtime_parameters_info\n                    step_info += \"-\".join([str(v) for v in value])\n                elif isinstance(value, (int, str, float)):\n                    # batch_size/name\n                    step_info += str(value)\n                else:\n                    raise ValueError(\n                        f\"Field '{argument}' in step '{step['name']}' has type {type(value)}, explicitly cast the type to 'str'.\"\n                    )\n\n            steps_info.append(step_info)\n\n        connections_info = [\n            f\"{c['from']}-{'-'.join(c['to'])}\" for c in pipeline_dump[\"connections\"]\n        ]\n\n        routing_batch_functions_info = []\n        for function in pipeline_dump[\"routing_batch_functions\"]:\n            step = function[\"step\"]\n            routing_batch_function: \"RoutingBatchFunction\" = self.dag.get_step(step)[\n                ROUTING_BATCH_FUNCTION_ATTR_NAME\n            ]\n            if type_info := routing_batch_function._get_type_info():\n                step += f\"-{type_info}\"\n\n        hasher.update(\n            \",\".join(\n                steps_info + connections_info + routing_batch_functions_info\n            ).encode()\n        )\n\n        return hasher.hexdigest()\n\n    def _set_logging_parameters(self, parameters: Dict[str, Any]) -&gt; None:\n        \"\"\"Set the parameters that will be passed to the `setup_logging` function to\n        initialize the logging.\n\n        Args:\n            parameters: A dictionary with the parameters that will be passed to the\n                `setup_logging` function.\n        \"\"\"\n        self._logging_parameters = parameters\n\n    def run(\n        self,\n        parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        use_cache: bool = True,\n        storage_parameters: Optional[Dict[str, Any]] = None,\n        use_fs_to_pass_data: bool = False,\n    ) -&gt; \"Distiset\":  # type: ignore\n        \"\"\"Run the pipeline. It will set the runtime parameters for the steps and validate\n        the pipeline.\n\n        This method should be extended by the specific pipeline implementation,\n        adding the logic to run the pipeline.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n                the runtime parameters for the step as the value. Defaults to `None`.\n            use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n                `True`.\n            storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n                that will be used to store the data of the `_Batch`es passed between the\n                steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n                `GlobalStep` it will be always used). It must have at least the \"path\" key,\n                and it can contain additional keys depending on the protocol. By default,\n                it will use the local file system and a directory in the cache directory.\n                Defaults to `None`.\n            use_fs_to_pass_data: Whether to use the file system to pass the data of\n                the `_Batch`es between the steps. Even if this parameter is `False`, the\n                `Batch`es received by `GlobalStep`s will always use the file system to\n                pass the data. Defaults to `False`.\n\n        Returns:\n            The `Distiset` created by the pipeline.\n        \"\"\"\n\n        # Set the runtime parameters that will be used during the pipeline execution.\n        # They are used to generate the signature of the pipeline that is used to hit the\n        # cache when the pipeline is run, so it's important to do it first.\n        self._set_runtime_parameters(parameters or {})\n\n        setup_logging(\n            **{\n                **self._logging_parameters,\n                \"filename\": str(self._cache_location[\"log_file\"]),\n            }\n        )\n\n        # Validate the pipeline DAG to check that all the steps are chainable, there are\n        # no missing runtime parameters, batch sizes are correct, etc.\n        self.dag.validate()\n\n        # Load the `_BatchManager` from cache or create one from scratch\n        self._load_batch_manager(use_cache)\n\n        # Setup the filesystem that will be used to pass the data of the `_Batch`es\n        self._setup_fsspec(storage_parameters)\n        self._use_fs_to_pass_data = use_fs_to_pass_data\n\n        if self._dry_run:\n            self._logger.info(\"\ud83c\udf35 Dry run mode\")\n\n        # If the batch manager is not able to generate batches, that means that the loaded\n        # `_BatchManager` from cache didn't have any remaining batches to process i.e.\n        # the previous pipeline execution was completed successfully.\n        if not self._batch_manager.can_generate():  # type: ignore\n            self._logger.info(\n                \"\ud83d\udcbe Loaded batch manager from cache doesn't contain any remaining data.\"\n                \" Returning `Distiset` from cache data...\"\n            )\n            stop_logging()\n            return create_distiset(\n                self._cache_location[\"data\"],\n                pipeline_path=self._cache_location[\"pipeline\"],\n                log_filename_path=self._cache_location[\"log_file\"],\n                enable_metadata=self._enable_metadata,\n            )\n\n        self._setup_write_buffer()\n\n    def dry_run(\n        self,\n        parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        batch_size: int = 1,\n    ) -&gt; \"Distiset\":\n        \"\"\"Do a dry run to test the pipeline runs as expected.\n\n        Running a `Pipeline` in dry run mode will set all the `batch_size` of generator steps\n        to the specified `batch_size`, and run just with a single batch, effectively\n        running the whole pipeline with a single example. The cache will be set to `False`.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n                the runtime parameters for the step as the value. Defaults to `None`.\n            batch_size: The batch size of the unique batch generated by the generators\n                steps of the pipeline. Defaults to `1`.\n\n        Returns:\n            Will return the `Distiset` as the main run method would do.\n        \"\"\"\n        self._dry_run = True\n\n        for step_name in self.dag:\n            step = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n\n            if step.is_generator:\n                if not parameters:\n                    parameters = {}\n                parameters[step_name] = {\"batch_size\": batch_size}\n\n        distiset = self.run(parameters=parameters, use_cache=False)\n\n        self._dry_run = False\n        return distiset\n\n    def get_runtime_parameters_info(self) -&gt; Dict[str, List[Dict[str, Any]]]:\n        \"\"\"Get the runtime parameters for the steps in the pipeline.\n\n        Returns:\n            A dictionary with the step name as the key and a list of dictionaries with\n            the parameter name and the parameter info as the value.\n        \"\"\"\n        runtime_parameters = {}\n        for step_name in self.dag:\n            step: \"_Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n            runtime_parameters[step_name] = step.get_runtime_parameters_info()\n        return runtime_parameters\n\n    def _setup_fsspec(\n        self, storage_parameters: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Setups the `fsspec` filesystem to be used to store the data of the `_Batch`es\n        passed between the steps.\n\n        Args:\n            storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n                that will be used to store the data of the `_Batch`es passed between the\n                steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n                `GlobalStep` it will be always used). It must have at least the \"path\" key,\n                and it can contain additional keys depending on the protocol. By default,\n                it will use the local file system and a directory in the cache directory.\n                Defaults to `None`.\n        \"\"\"\n        if not storage_parameters:\n            self._fs = fsspec.filesystem(\"file\")\n            self._storage_base_path = (\n                f\"file://{self._cache_location['batch_input_data']}\"\n            )\n            return\n\n        if \"path\" not in storage_parameters:\n            raise ValueError(\n                \"The 'path' key must be present in the `storage_parameters` dictionary\"\n                \" if it's not `None`.\"\n            )\n\n        path = storage_parameters.pop(\"path\")\n        protocol = UPath(path).protocol\n\n        self._fs = fsspec.filesystem(protocol, **storage_parameters)\n        self._storage_base_path = path\n\n    def _add_step(self, step: \"_Step\") -&gt; None:\n        \"\"\"Add a step to the pipeline.\n\n        Args:\n            step: The step to be added to the pipeline.\n        \"\"\"\n        self.dag.add_step(step)\n\n    def _add_edge(self, from_step: str, to_step: str) -&gt; None:\n        \"\"\"Add an edge between two steps in the pipeline.\n\n        Args:\n            from_step: The name of the step that will generate the input for `to_step`.\n            to_step: The name of the step that will receive the input from `from_step`.\n        \"\"\"\n        self.dag.add_edge(from_step, to_step)\n\n        # Check if `from_step` has a `routing_batch_function`. If it does, then mark\n        # `to_step` as a step that will receive a routed batch.\n        node = self.dag.get_step(from_step)  # type: ignore\n        routing_batch_function = node.get(ROUTING_BATCH_FUNCTION_ATTR_NAME, None)\n        self.dag.set_step_attr(\n            name=to_step,\n            attr=RECEIVES_ROUTED_BATCHES_ATTR_NAME,\n            value=routing_batch_function is not None,\n        )\n\n    def _add_routing_batch_function(\n        self, step_name: str, routing_batch_function: \"RoutingBatchFunction\"\n    ) -&gt; None:\n        \"\"\"Add a routing batch function to a step.\n\n        Args:\n            step_name: The name of the step that will receive the routed batch.\n            routing_batch_function: The function that will route the batch to the step.\n        \"\"\"\n        self.dag.set_step_attr(\n            name=step_name,\n            attr=ROUTING_BATCH_FUNCTION_ATTR_NAME,\n            value=routing_batch_function,\n        )\n\n    def _set_runtime_parameters(self, parameters: Dict[str, Dict[str, Any]]) -&gt; None:\n        \"\"\"Set the runtime parameters for the steps in the pipeline.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n            the parameter name as the key and the parameter value as the value.\n        \"\"\"\n        step_names = set(self.dag.G)\n        for step_name, step_parameters in parameters.items():\n            if step_name not in step_names:\n                self._logger.warning(\n                    f\"\u2753 Step '{step_name}' provided in `Pipeline.run(parameters={{...}})` not found in the pipeline.\"\n                    f\" Available steps are: {step_names}.\"\n                )\n            else:\n                step: \"_Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n                step.set_runtime_parameters(step_parameters)\n\n    def _model_dump(self, obj: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"Dumps the DAG content to a dict.\n\n        Args:\n            obj (Any): Unused, just kept to match the signature of the parent method.\n            kwargs (Any): Unused, just kept to match the signature of the parent method.\n\n        Returns:\n            Dict[str, Any]: Internal representation of the DAG from networkx in a serializable format.\n        \"\"\"\n        return self.dag.dump()\n\n    def dump(self, **kwargs: Any) -&gt; Dict[str, Any]:\n        return {\n            \"distilabel\": {\"version\": __version__},\n            \"pipeline\": {\n                \"name\": self.name,\n                \"description\": self.description,\n                **super().dump(),\n            },\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; Self:\n        \"\"\"Create a Pipeline from a dict containing the serialized data.\n\n        Note:\n            It's intended for internal use.\n\n        Args:\n            data (Dict[str, Any]): Dictionary containing the serialized data from a Pipeline.\n\n        Returns:\n            BasePipeline: Pipeline recreated from the dictionary info.\n        \"\"\"\n        name = data[\"pipeline\"][\"name\"]\n        description = data[\"pipeline\"].get(\"description\")\n        with cls(name=name, description=description) as pipe:\n            pipe.dag = DAG.from_dict(data[\"pipeline\"])\n        return pipe\n\n    @property\n    def _cache_location(self) -&gt; _CacheLocation:\n        \"\"\"Dictionary containing the the object that will stored and the location,\n        whether it is a filename or a folder.\n\n        Returns:\n            Path: Filenames where the pipeline content will be serialized.\n        \"\"\"\n        folder = self._cache_dir / self.name / self._create_signature()\n        return {\n            \"pipeline\": folder / \"pipeline.yaml\",\n            \"batch_manager\": folder / \"batch_manager.json\",\n            \"data\": folder / \"data\",\n            \"batch_input_data\": folder / \"batch_input_data\",\n            \"log_file\": folder / \"pipeline.log\",\n        }\n\n    def _cache(self) -&gt; None:\n        \"\"\"Saves the `BasePipeline` using the `_cache_filename`.\"\"\"\n        if self._dry_run:\n            return\n\n        self.save(\n            path=self._cache_location[\"pipeline\"],\n            format=self._cache_location[\"pipeline\"].suffix.replace(\".\", \"\"),  # type: ignore\n        )\n        if self._batch_manager is not None:\n            self._batch_manager.cache(self._cache_location[\"batch_manager\"])\n        self._logger.debug(\"Pipeline and batch manager saved to cache.\")\n\n    def _load_batch_manager(self, use_cache: bool = True) -&gt; None:\n        \"\"\"Will try to load the `_BatchManager` from the cache dir if found. Otherwise,\n        it will create one from scratch.\n        \"\"\"\n        batch_manager_cache_loc = self._cache_location[\"batch_manager\"]\n        if use_cache and batch_manager_cache_loc.exists():\n            self._logger.info(\n                f\"\ud83d\udcbe Loading `_BatchManager` from cache: '{batch_manager_cache_loc}'\"\n            )\n            self._batch_manager = _BatchManager.load_from_cache(batch_manager_cache_loc)\n        else:\n            self._batch_manager = _BatchManager.from_dag(self.dag)\n\n    def _setup_write_buffer(self) -&gt; None:\n        \"\"\"Setups the `_WriteBuffer` that will store the data of the leaf steps of the\n        pipeline while running, so the `Distiset` can be created at the end.\n        \"\"\"\n        buffer_data_path = self._cache_location[\"data\"]\n        self._logger.info(f\"\ud83d\udcdd Pipeline data will be written to '{buffer_data_path}'\")\n        self._write_buffer = _WriteBuffer(buffer_data_path, self.dag.leaf_steps)\n\n    def _send_batch_to_step(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Sends a batch to the input queue of a step, writing the data of the batch\n        to the filesystem and setting `batch.data_path` with the path where the data\n        was written (if requiered i.e. the step is a global step or `use_fs_to_pass_data`)\n\n        This method should be extended by the specific pipeline implementation, adding\n        the logic to send the batch to the step.\n\n        Args:\n            batch: The batch to send.\n        \"\"\"\n        self._logger.debug(\n            f\"Setting batch {batch.seq_no} as last batch sent to '{batch.step_name}': {batch}\"\n        )\n        self._batch_manager.set_last_batch_sent(batch)  # type: ignore\n\n        step: \"_Step\" = self.dag.get_step(batch.step_name)[STEP_ATTR_NAME]\n        if not step.is_generator and (step.is_global or self._use_fs_to_pass_data):\n            base_path = UPath(self._storage_base_path) / step.name  # type: ignore\n            self._logger.debug(\n                f\"Writing {batch.seq_no} batch for '{batch.step_name}' step to filesystem: {base_path}\"\n            )\n            batch.write_batch_data_to_fs(self._fs, base_path)  # type: ignore\n\n        self._logger.debug(\n            f\"Sending batch {batch.seq_no} to step '{batch.step_name}': {batch}\"\n        )\n</code></pre>"},{"location":"reference/distilabel/pipeline/base/#distilabel.pipeline.base.BasePipeline.__enter__","title":"<code>__enter__()</code>","text":"<p>Set the global pipeline instance when entering a pipeline context.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Set the global pipeline instance when entering a pipeline context.\"\"\"\n    _GlobalPipelineManager.set_pipeline(self)\n    return self\n</code></pre>"},{"location":"reference/distilabel/pipeline/base/#distilabel.pipeline.base.BasePipeline.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Unset the global pipeline instance when exiting a pipeline context.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback) -&gt; None:\n    \"\"\"Unset the global pipeline instance when exiting a pipeline context.\"\"\"\n    _GlobalPipelineManager.set_pipeline(None)\n</code></pre>"},{"location":"reference/distilabel/pipeline/base/#distilabel.pipeline.base.BasePipeline.__init__","title":"<code>__init__(name, description=None, cache_dir=None, enable_metadata=False)</code>","text":"<p>Initialize the <code>BasePipeline</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pipeline.</p> required <code>description</code> <code>Optional[str]</code> <p>A description of the pipeline. Defaults to <code>None</code>.</p> <code>None</code> <code>cache_dir</code> <code>Optional[Union[str, PathLike]]</code> <p>A directory where the pipeline will be cached. Defaults to <code>None</code>.</p> <code>None</code> <code>enable_metadata</code> <code>bool</code> <p>Whether to include the distilabel metadata column for the pipeline in the final <code>Distiset</code>. It contains metadata used by distilabel, for example the raw outputs of the <code>LLM</code> without processing would be here, inside <code>raw_output_...</code> field. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: Optional[str] = None,\n    cache_dir: Optional[Union[str, \"PathLike\"]] = None,\n    enable_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the `BasePipeline` instance.\n\n    Args:\n        name: The name of the pipeline.\n        description: A description of the pipeline. Defaults to `None`.\n        cache_dir: A directory where the pipeline will be cached. Defaults to `None`.\n        enable_metadata: Whether to include the distilabel metadata column for the pipeline\n            in the final `Distiset`. It contains metadata used by distilabel, for example\n            the raw outputs of the `LLM` without processing would be here, inside `raw_output_...`\n            field. Defaults to `False`.\n    \"\"\"\n    self.name = name\n    self.description = description\n    self._enable_metadata = enable_metadata\n    self.dag = DAG()\n\n    if cache_dir:\n        self._cache_dir = Path(cache_dir)\n    elif env_cache_dir := os.getenv(\"DISTILABEL_CACHE_DIR\"):\n        self._cache_dir = Path(env_cache_dir)\n    else:\n        self._cache_dir = BASE_CACHE_DIR\n\n    self._logger = logging.getLogger(\"distilabel.pipeline\")\n\n    self._batch_manager: Optional[\"_BatchManager\"] = None\n    self._write_buffer: Optional[\"_WriteBuffer\"] = None\n    self._logging_parameters: Dict[str, Any] = {\n        \"filename\": self._cache_location[\"log_file\"]\n    }\n\n    self._fs: Optional[fsspec.AbstractFileSystem] = None\n    self._storage_base_path: Optional[str] = None\n    self._use_fs_to_pass_data: bool = False\n    self._dry_run: bool = False\n</code></pre>"},{"location":"reference/distilabel/pipeline/base/#distilabel.pipeline.base.BasePipeline.dry_run","title":"<code>dry_run(parameters=None, batch_size=1)</code>","text":"<p>Do a dry run to test the pipeline runs as expected.</p> <p>Running a <code>Pipeline</code> in dry run mode will set all the <code>batch_size</code> of generator steps to the specified <code>batch_size</code>, and run just with a single batch, effectively running the whole pipeline with a single example. The cache will be set to <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a dictionary with the runtime parameters for the step as the value. Defaults to <code>None</code>.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size of the unique batch generated by the generators steps of the pipeline. Defaults to <code>1</code>.</p> <code>1</code> <p>Returns:</p> Type Description <code>Distiset</code> <p>Will return the <code>Distiset</code> as the main run method would do.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def dry_run(\n    self,\n    parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    batch_size: int = 1,\n) -&gt; \"Distiset\":\n    \"\"\"Do a dry run to test the pipeline runs as expected.\n\n    Running a `Pipeline` in dry run mode will set all the `batch_size` of generator steps\n    to the specified `batch_size`, and run just with a single batch, effectively\n    running the whole pipeline with a single example. The cache will be set to `False`.\n\n    Args:\n        parameters: A dictionary with the step name as the key and a dictionary with\n            the runtime parameters for the step as the value. Defaults to `None`.\n        batch_size: The batch size of the unique batch generated by the generators\n            steps of the pipeline. Defaults to `1`.\n\n    Returns:\n        Will return the `Distiset` as the main run method would do.\n    \"\"\"\n    self._dry_run = True\n\n    for step_name in self.dag:\n        step = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n\n        if step.is_generator:\n            if not parameters:\n                parameters = {}\n            parameters[step_name] = {\"batch_size\": batch_size}\n\n    distiset = self.run(parameters=parameters, use_cache=False)\n\n    self._dry_run = False\n    return distiset\n</code></pre>"},{"location":"reference/distilabel/pipeline/base/#distilabel.pipeline.base.BasePipeline.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a Pipeline from a dict containing the serialized data.</p> Note <p>It's intended for internal use.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing the serialized data from a Pipeline.</p> required <p>Returns:</p> Name Type Description <code>BasePipeline</code> <code>Self</code> <p>Pipeline recreated from the dictionary info.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; Self:\n    \"\"\"Create a Pipeline from a dict containing the serialized data.\n\n    Note:\n        It's intended for internal use.\n\n    Args:\n        data (Dict[str, Any]): Dictionary containing the serialized data from a Pipeline.\n\n    Returns:\n        BasePipeline: Pipeline recreated from the dictionary info.\n    \"\"\"\n    name = data[\"pipeline\"][\"name\"]\n    description = data[\"pipeline\"].get(\"description\")\n    with cls(name=name, description=description) as pipe:\n        pipe.dag = DAG.from_dict(data[\"pipeline\"])\n    return pipe\n</code></pre>"},{"location":"reference/distilabel/pipeline/base/#distilabel.pipeline.base.BasePipeline.get_runtime_parameters_info","title":"<code>get_runtime_parameters_info()</code>","text":"<p>Get the runtime parameters for the steps in the pipeline.</p> <p>Returns:</p> Type Description <code>Dict[str, List[Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a list of dictionaries with</p> <code>Dict[str, List[Dict[str, Any]]]</code> <p>the parameter name and the parameter info as the value.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def get_runtime_parameters_info(self) -&gt; Dict[str, List[Dict[str, Any]]]:\n    \"\"\"Get the runtime parameters for the steps in the pipeline.\n\n    Returns:\n        A dictionary with the step name as the key and a list of dictionaries with\n        the parameter name and the parameter info as the value.\n    \"\"\"\n    runtime_parameters = {}\n    for step_name in self.dag:\n        step: \"_Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n        runtime_parameters[step_name] = step.get_runtime_parameters_info()\n    return runtime_parameters\n</code></pre>"},{"location":"reference/distilabel/pipeline/base/#distilabel.pipeline.base.BasePipeline.run","title":"<code>run(parameters=None, use_cache=True, storage_parameters=None, use_fs_to_pass_data=False)</code>","text":"<p>Run the pipeline. It will set the runtime parameters for the steps and validate the pipeline.</p> <p>This method should be extended by the specific pipeline implementation, adding the logic to run the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a dictionary with the runtime parameters for the step as the value. Defaults to <code>None</code>.</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use the cache from previous pipeline runs. Defaults to <code>True</code>.</p> <code>True</code> <code>storage_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary with the storage parameters (<code>fsspec</code> and path) that will be used to store the data of the <code>_Batch</code>es passed between the steps if <code>use_fs_to_pass_data</code> is <code>True</code> (for the batches received by a <code>GlobalStep</code> it will be always used). It must have at least the \"path\" key, and it can contain additional keys depending on the protocol. By default, it will use the local file system and a directory in the cache directory. Defaults to <code>None</code>.</p> <code>None</code> <code>use_fs_to_pass_data</code> <code>bool</code> <p>Whether to use the file system to pass the data of the <code>_Batch</code>es between the steps. Even if this parameter is <code>False</code>, the <code>Batch</code>es received by <code>GlobalStep</code>s will always use the file system to pass the data. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distiset</code> <p>The <code>Distiset</code> created by the pipeline.</p> Source code in <code>src/distilabel/pipeline/base.py</code> <pre><code>def run(\n    self,\n    parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    use_cache: bool = True,\n    storage_parameters: Optional[Dict[str, Any]] = None,\n    use_fs_to_pass_data: bool = False,\n) -&gt; \"Distiset\":  # type: ignore\n    \"\"\"Run the pipeline. It will set the runtime parameters for the steps and validate\n    the pipeline.\n\n    This method should be extended by the specific pipeline implementation,\n    adding the logic to run the pipeline.\n\n    Args:\n        parameters: A dictionary with the step name as the key and a dictionary with\n            the runtime parameters for the step as the value. Defaults to `None`.\n        use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n            `True`.\n        storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n            that will be used to store the data of the `_Batch`es passed between the\n            steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n            `GlobalStep` it will be always used). It must have at least the \"path\" key,\n            and it can contain additional keys depending on the protocol. By default,\n            it will use the local file system and a directory in the cache directory.\n            Defaults to `None`.\n        use_fs_to_pass_data: Whether to use the file system to pass the data of\n            the `_Batch`es between the steps. Even if this parameter is `False`, the\n            `Batch`es received by `GlobalStep`s will always use the file system to\n            pass the data. Defaults to `False`.\n\n    Returns:\n        The `Distiset` created by the pipeline.\n    \"\"\"\n\n    # Set the runtime parameters that will be used during the pipeline execution.\n    # They are used to generate the signature of the pipeline that is used to hit the\n    # cache when the pipeline is run, so it's important to do it first.\n    self._set_runtime_parameters(parameters or {})\n\n    setup_logging(\n        **{\n            **self._logging_parameters,\n            \"filename\": str(self._cache_location[\"log_file\"]),\n        }\n    )\n\n    # Validate the pipeline DAG to check that all the steps are chainable, there are\n    # no missing runtime parameters, batch sizes are correct, etc.\n    self.dag.validate()\n\n    # Load the `_BatchManager` from cache or create one from scratch\n    self._load_batch_manager(use_cache)\n\n    # Setup the filesystem that will be used to pass the data of the `_Batch`es\n    self._setup_fsspec(storage_parameters)\n    self._use_fs_to_pass_data = use_fs_to_pass_data\n\n    if self._dry_run:\n        self._logger.info(\"\ud83c\udf35 Dry run mode\")\n\n    # If the batch manager is not able to generate batches, that means that the loaded\n    # `_BatchManager` from cache didn't have any remaining batches to process i.e.\n    # the previous pipeline execution was completed successfully.\n    if not self._batch_manager.can_generate():  # type: ignore\n        self._logger.info(\n            \"\ud83d\udcbe Loaded batch manager from cache doesn't contain any remaining data.\"\n            \" Returning `Distiset` from cache data...\"\n        )\n        stop_logging()\n        return create_distiset(\n            self._cache_location[\"data\"],\n            pipeline_path=self._cache_location[\"pipeline\"],\n            log_filename_path=self._cache_location[\"log_file\"],\n            enable_metadata=self._enable_metadata,\n        )\n\n    self._setup_write_buffer()\n</code></pre>"},{"location":"reference/distilabel/pipeline/constants/","title":"Constants","text":""},{"location":"reference/distilabel/pipeline/local/","title":"Local","text":""},{"location":"reference/distilabel/pipeline/local/#distilabel.pipeline.local.Pipeline","title":"<code>Pipeline</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Local pipeline implementation using <code>multiprocessing</code>.</p> Source code in <code>src/distilabel/pipeline/local.py</code> <pre><code>class Pipeline(BasePipeline):\n    \"\"\"Local pipeline implementation using `multiprocessing`.\"\"\"\n\n    def run(\n        self,\n        parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        use_cache: bool = True,\n        storage_parameters: Optional[Dict[str, Any]] = None,\n        use_fs_to_pass_data: bool = False,\n    ) -&gt; \"Distiset\":\n        \"\"\"Runs the pipeline.\n\n        Args:\n            parameters: A dictionary with the step name as the key and a dictionary with\n                the runtime parameters for the step as the value. Defaults to `None`.\n            use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n                `True`.\n            storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n                that will be used to store the data of the `_Batch`es passed between the\n                steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n                `GlobalStep` it will be always used). It must have at least the \"path\" key,\n                and it can contain additional keys depending on the protocol. By default,\n                it will use the local file system and a directory in the cache directory.\n                Defaults to `None`.\n            use_fs_to_pass_data: Whether to use the file system to pass the data of\n                the `_Batch`es between the steps. Even if this parameter is `False`, the\n                `Batch`es received by `GlobalStep`s will always use the file system to\n                pass the data. Defaults to `False`.\n\n        Returns:\n            The `Distiset` created by the pipeline.\n\n        Raises:\n            RuntimeError: If the pipeline fails to load all the steps.\n        \"\"\"\n        log_queue = mp.Queue()\n\n        self._set_logging_parameters(\n            {\"log_queue\": log_queue, \"filename\": self._cache_location[\"log_file\"]}\n        )\n\n        if distiset := super().run(\n            parameters, use_cache, storage_parameters, use_fs_to_pass_data\n        ):\n            return distiset\n\n        num_processes = len(self.dag)\n        ctx = mp.get_context()  # type: ignore\n        with ctx.Manager() as manager, ctx.Pool(\n            num_processes,\n            initializer=_init_worker,\n            initargs=(log_queue,),\n        ) as pool:\n            self.output_queue: \"Queue[Any]\" = manager.Queue()\n            self.shared_info = self._create_shared_info_dict(manager)\n            self._handle_keyboard_interrupt(manager=manager, pool=pool)\n\n            # Run the steps using the pool of processes\n            self._run_steps_in_loop(pool, manager, self.output_queue, self.shared_info)\n\n            # Wait for all the steps to be loaded correctly\n            if not self._all_steps_loaded():\n                self._write_buffer.close()  # type: ignore\n                self._batch_manager = None\n                stop_logging()\n                raise RuntimeError(\n                    \"Failed to load all the steps. Could not run pipeline.\"\n                ) from _SUBPROCESS_EXCEPTION\n\n            # Send the \"first\" batches to the steps so the batches starts flowing through\n            # the input queues and output queue\n            self._request_initial_batches()\n\n            # Start a loop to receive the output batches from the steps\n            self._run_output_queue_loop_in_thread()\n\n            # Send `None` to steps `input_queue`s just in case some step is still waiting\n            self._notify_steps_to_stop()\n\n        # `Pool.__exit__` has already called `terminate`, `join` the pool to make sure\n        # all the processes have finished\n        pool.join()\n        manager.join()\n\n        self._write_buffer.close()  # type: ignore\n        distiset = create_distiset(\n            self._cache_location[\"data\"],\n            pipeline_path=self._cache_location[\"pipeline\"],\n            log_filename_path=self._cache_location[\"log_file\"],\n            enable_metadata=self._enable_metadata,\n        )\n        stop_logging()\n        return distiset\n\n    def _run_output_queue_loop_in_thread(self) -&gt; None:\n        \"\"\"Runs the output queue loop in a separate thread to receive the output batches\n        from the steps. This is done to avoid the signal handler to block the loop, which\n        would prevent the pipeline from stopping correctly.\"\"\"\n        thread = threading.Thread(target=self._output_queue_loop)\n        thread.start()\n        thread.join()\n\n    def _notify_steps_to_stop(self) -&gt; None:\n        \"\"\"Notifies the steps to stop their infinite running loop by sending `None` to\n        their input queues.\"\"\"\n        for step_name in self.dag:\n            if input_queue := self.dag.get_step(step_name).get(INPUT_QUEUE_ATTR_NAME):\n                input_queue.put(None)\n\n    def _output_queue_loop(self) -&gt; None:\n        \"\"\"Loop to receive the output batches from the steps and manage the flow of the\n        batches through the pipeline.\"\"\"\n        while self._batch_manager.can_generate() and not _STOP_CALLED:  # type: ignore\n            self._logger.debug(\"Waiting for output batch from step...\")\n            if (batch := self.output_queue.get()) is None:\n                self._logger.debug(\"Received `None` from output queue. Breaking loop.\")\n                break\n\n            self._logger.debug(\n                f\"Received batch with seq_no {batch.seq_no} from step '{batch.step_name}'\"\n                f\" from output queue: {batch}\"\n            )\n\n            if batch.data_path:\n                self._logger.debug(\n                    f\"Reading {batch.seq_no} batch data from '{batch.step_name}': '{batch.data_path}'\"\n                )\n                batch.read_batch_data_from_fs()\n\n            if batch.step_name in self.dag.leaf_steps:\n                self._write_buffer.add_batch(batch)  # type: ignore\n\n            # If `_STOP_CALLED` was set to `True` while waiting for the output queue, then\n            # we need to handle the stop of the pipeline and break the loop to avoid\n            # propagating the batches through the pipeline and making the stop process\n            # slower.\n            if _STOP_CALLED:\n                self._handle_batch_on_stop(batch)\n                break\n\n            self._manage_batch_flow(batch)\n\n        if _STOP_CALLED:\n            self._handle_stop()\n\n    def _manage_batch_flow(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Checks if the step that generated the batch has more data in its buffer to\n        generate a new batch. If there's data, then a new batch is sent to the step. If\n        the step has no data in its buffer, then the predecessors generator steps are\n        requested to send a new batch.\n\n        Args:\n            batch: The batch that was processed.\n        \"\"\"\n        assert self._batch_manager, \"Batch manager is not set\"\n\n        # Make sure to send the `LAST_BATCH_SENT_FLAG` to the predecessors of the convergence\n        # step if the batch is the last one, so they stop their processing loop even if\n        # they haven't received the last batch because of the routing function.\n        if self._is_convergence_step(batch.step_name) and batch.last_batch:\n            for step_name in self.dag.get_step_predecessors(batch.step_name):\n                self._send_last_batch_flag_to_step(step_name)\n\n        route_to, routed = self._get_successors(batch)\n\n        # Keep track of the steps that the batch was routed to\n        if routed:\n            batch.batch_routed_to = route_to\n\n        self._register_batch(batch)\n\n        step = self._get_step_from_batch(batch)\n\n        # Add the batch to the successors input buffers\n        for successor in route_to:\n            # Copy batch to avoid modifying the same reference in the batch manager\n            batch_to_add = batch.copy() if len(route_to) &gt; 1 else batch\n\n            self._batch_manager.add_batch(successor, batch_to_add)\n\n            # Check if the step is a generator and if there are successors that need data\n            # from this step. This usually happens when the generator `batch_size` is smaller\n            # than the `input_batch_size` of the successor steps.\n            if (\n                step.is_generator\n                and step.name in self._batch_manager.step_empty_buffers(successor)\n            ):\n                last_batch_sent = self._batch_manager.get_last_batch_sent(step.name)\n                self._send_batch_to_step(last_batch_sent.next_batch())  # type: ignore\n\n            # If successor step has enough data in its buffer to create a new batch, then\n            # send the batch to the step.\n            if new_batch := self._batch_manager.get_batch(successor):\n                self._send_batch_to_step(new_batch)\n\n        if not step.is_generator:\n            # Step (\"this\", the one from which the batch was received) has enough data on its\n            # buffers to create a new batch\n            if new_batch := self._batch_manager.get_batch(step.name):  # type: ignore\n                self._send_batch_to_step(new_batch)\n            else:\n                self._request_more_batches_if_needed(step)\n\n        self._cache()\n\n    def _register_batch(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Registers a batch in the batch manager.\n\n        Args:\n            batch: The batch to register.\n        \"\"\"\n        self._batch_manager.register_batch(batch)  # type: ignore\n        self._logger.debug(\n            f\"Batch {batch.seq_no} from step '{batch.step_name}' registered in batch\"\n            \" manager\"\n        )\n\n    def _get_successors(self, batch: \"_Batch\") -&gt; Tuple[List[str], bool]:\n        \"\"\"Gets the successors and the successors to which the batch has to be routed.\n\n        Args:\n            batch: The batch to which the successors will be determined.\n\n        Returns:\n            The successors to route the batch to and whether the batch was routed using\n            a routing function.\n        \"\"\"\n        node = self.dag.get_step(batch.step_name)\n        step: \"Step\" = node[STEP_ATTR_NAME]\n        successors = list(self.dag.get_step_successors(step.name))  # type: ignore\n        route_to = successors\n\n        # Check if the step has a routing function to send the batch to specific steps\n        if routing_batch_function := node.get(ROUTING_BATCH_FUNCTION_ATTR_NAME):\n            route_to = routing_batch_function(batch, successors)\n            successors_str = \", \".join(f\"'{successor}'\" for successor in route_to)\n            self._logger.info(\n                f\"\ud83d\ude8f Using '{step.name}' routing function to send batch {batch.seq_no} to steps: {successors_str}\"\n            )\n\n        return route_to, route_to != successors\n\n    def _get_step_from_batch(self, batch: \"_Batch\") -&gt; \"Step\":\n        \"\"\"Gets the `Step` instance from a batch.\n\n        Args:\n            batch: The batch to get the step from.\n\n        Returns:\n            The `Step` instance.\n        \"\"\"\n        return self.dag.get_step(batch.step_name)[STEP_ATTR_NAME]\n\n    def _request_more_batches_if_needed(self, step: \"Step\") -&gt; None:\n        \"\"\"Request more batches to the predecessors steps of `step` if needed.\n\n        Args:\n            step: The step of which it has to be checked if more batches are needed from\n                its predecessors.\n        \"\"\"\n        empty_buffers = self._batch_manager.step_empty_buffers(step.name)  # type: ignore\n        for previous_step_name in empty_buffers:\n            if previous_step_name not in self.dag.root_steps:\n                continue\n\n            last_batch = self._batch_manager.get_last_batch_sent(previous_step_name)  # type: ignore\n            if last_batch is None:\n                continue\n\n            self._logger.debug(\n                f\"Step '{step.name}' input buffer for step '{previous_step_name}' is\"\n                \" empty. Requesting new batch...\"\n            )\n            self._send_batch_to_step(last_batch.next_batch())\n\n    def _handle_stop(self) -&gt; None:\n        \"\"\"Handles the stop of the pipeline execution, which will stop the steps from\n        processing more batches and wait for the output queue to be empty, to not lose\n        any data that was already processed by the steps before the stop was called.\"\"\"\n        self._logger.debug(\"Handling stop of the pipeline execution...\")\n\n        # Add the remaining batches in the input queues back to the batch manager\n        for step_name in self.dag:\n            node = self.dag.get_step(step_name)\n            step: \"_Step\" = node[STEP_ATTR_NAME]\n            if step.is_generator:\n                continue\n            if input_queue := node.get(INPUT_QUEUE_ATTR_NAME):\n                while not input_queue.empty():\n                    batch = input_queue.get()\n                    if batch is None:\n                        continue\n                    self._batch_manager.add_batch(  # type: ignore\n                        to_step=step_name, batch=batch, prepend=True\n                    )\n                    self._logger.debug(\n                        f\"Adding batch back to the batch manager: {batch}\"\n                    )\n                input_queue.put(None)\n\n        # Wait for the input queue to be empty, which means that all the steps finished\n        # processing the batches that were sent before the stop flag.\n        for step_name in self.dag:\n            self._wait_step_input_queue_empty(step_name)\n\n        # Consume the output queue until it's empty to not lose any data that was already\n        # processed by the steps before stop was called.\n        while not self.output_queue.empty():\n            batch = self.output_queue.get()\n            if batch is None:\n                continue\n\n            if batch.step_name in self.dag.leaf_steps:\n                self._write_buffer.add_batch(batch)  # type: ignore\n\n            self._handle_batch_on_stop(batch)\n\n        self._cache()\n\n    def _handle_batch_on_stop(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Handles a batch that was received from the output queue when the pipeline was\n        stopped. It will add and register the batch in the batch manager.\n\n        Args:\n            batch: The batch to handle.\n        \"\"\"\n        self._batch_manager.register_batch(batch)  # type: ignore\n        step: \"Step\" = self.dag.get_step(batch.step_name)[STEP_ATTR_NAME]\n        for successor in self.dag.get_step_successors(step.name):  # type: ignore\n            self._batch_manager.add_batch(successor, batch)  # type: ignore\n\n    def _wait_step_input_queue_empty(self, step_name: str) -&gt; Union[\"Queue[Any]\", None]:\n        \"\"\"Waits for the input queue of a step to be empty.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            The input queue of the step if it's not loaded or finished, `None` otherwise.\n        \"\"\"\n        if self._check_step_not_loaded_or_finished(step_name):\n            return None\n\n        if input_queue := self.dag.get_step(step_name).get(INPUT_QUEUE_ATTR_NAME):\n            while input_queue.qsize() != 0:\n                pass\n            return input_queue\n\n    def _create_shared_info_dict(self, manager: \"SyncManager\") -&gt; \"DictProxy[str, Any]\":\n        \"\"\"Creates the shared information dictionary to be used by the processes.\n\n        Args:\n            manager: The manager to create the shared information.\n\n        Returns:\n            The shared information dictionary.\n        \"\"\"\n        # TODO: not very important, but we could use a different lock for each matter\n        return manager.dict(\n            **{\n                _STEPS_LOADED_KEY: manager.list(),\n                _STEPS_LOADED_LOCK_KEY: manager.Lock(),\n                _CUDA_LLM_DEVICE_PLACEMENT_KEY: manager.dict(**{}),\n                _CUDA_LLM_DEVICE_PLACEMENT_LOCK_KEY: manager.Lock(),\n            }\n        )\n\n    def _all_steps_loaded(self) -&gt; bool:\n        \"\"\"Waits for all the steps to load.\n\n        Returns:\n            `True` if all the steps have been loaded correctly, `False` otherwise.\n        \"\"\"\n\n        def _update_all_steps_loaded(steps_loaded: List[str]) -&gt; None:\n            with _STEPS_LOADED_LOCK:\n                _STEPS_LOADED.update(steps_loaded)\n\n        self._logger.info(\"\u23f3 Waiting for all the steps to load...\")\n        previous_message = None\n        while not _STOP_CALLED:\n            with self.shared_info[_STEPS_LOADED_LOCK_KEY]:\n                steps_loaded = self.shared_info[_STEPS_LOADED_KEY]\n                num_steps_loaded = (\n                    len(steps_loaded)\n                    if steps_loaded != [_STEPS_LOADED_ERROR_CODE]\n                    else 0\n                )\n                self._logger.debug(f\"Steps loaded: {steps_loaded}\")\n\n                message = f\"\u23f3 Steps loaded: {num_steps_loaded}/{len(self.dag)}\"\n                if num_steps_loaded &gt; 0 and message != previous_message:\n                    self._logger.info(message)\n                    previous_message = message\n\n                if num_steps_loaded == len(self.dag):\n                    self._logger.info(\"\u2705 All the steps have been loaded!\")\n                    _update_all_steps_loaded(steps_loaded)\n                    return True\n\n                if steps_loaded == [_STEPS_LOADED_ERROR_CODE]:\n                    self._logger.error(\"\u274c Failed to load all the steps\")\n                    _update_all_steps_loaded(steps_loaded)\n                    return False\n\n            time.sleep(2.5)\n\n        return not _STOP_CALLED\n\n    def _request_initial_batches(self) -&gt; None:\n        \"\"\"Requests the initial batches to the generator steps.\"\"\"\n        assert self._batch_manager, \"Batch manager is not set\"\n\n        for step in self._batch_manager._steps.values():\n            if batch := step.get_batch():\n                self._logger.debug(\n                    f\"Sending initial batch to '{step.step_name}' step: {batch}\"\n                )\n                self._send_batch_to_step(batch)\n\n        for step_name in self.dag.root_steps:\n            seq_no = 0\n            if last_batch := self._batch_manager.get_last_batch(step_name):\n                seq_no = last_batch.seq_no + 1\n            batch = _Batch(seq_no=seq_no, step_name=step_name, last_batch=self._dry_run)\n            self._logger.debug(\n                f\"Requesting initial batch to '{step_name}' generator step: {batch}\"\n            )\n            self._send_batch_to_step(batch)\n\n    def _send_batch_to_step(self, batch: \"_Batch\") -&gt; None:\n        \"\"\"Sends a batch to the input queue of a step.\n\n        Args:\n            batch: The batch to send.\n        \"\"\"\n        super()._send_batch_to_step(batch)\n        input_queue = self.dag.get_step(batch.step_name)[INPUT_QUEUE_ATTR_NAME]\n        input_queue.put(batch)\n\n    def _is_convergence_step(self, step_name: str) -&gt; None:\n        \"\"\"Checks if a step is a convergence step.\n\n        Args:\n            step_name: The name of the step.\n        \"\"\"\n        return self.dag.get_step(step_name).get(CONVERGENCE_STEP_ATTR_NAME)\n\n    def _send_last_batch_flag_to_step(self, step_name: str) -&gt; None:\n        \"\"\"Sends the `LAST_BATCH_SENT_FLAG` to a step to stop processing batches.\n\n        Args:\n            step_name: The name of the step.\n        \"\"\"\n        batch = self._batch_manager.get_last_batch_sent(step_name)  # type: ignore\n        if batch and batch.last_batch:\n            return\n\n        self._logger.debug(\n            f\"Sending `LAST_BATCH_SENT_FLAG` to '{step_name}' step to stop processing\"\n            \" batches...\"\n        )\n        input_queue = self.dag.get_step(step_name)[INPUT_QUEUE_ATTR_NAME]\n        input_queue.put(LAST_BATCH_SENT_FLAG)\n        self._batch_manager.set_last_batch_flag_sent_to(step_name)  # type: ignore\n\n    def _run_steps_in_loop(\n        self,\n        pool: \"Pool\",\n        manager: \"SyncManager\",\n        output_queue: \"Queue[_Batch]\",\n        shared_info: \"DictProxy[str, Any]\",\n    ) -&gt; None:\n        \"\"\"Using the `pool`, runs the steps in the DAG in an infinite loop waiting for\n        input batches and sending the output batches to the `output_queue`.\n\n        Each `Step` is wrapped in a `_ProcessWrapper`, which will handle the lifecycle of\n        the `Step` and the communication with the `input_queue` and `output_queue`. The\n        `_ProcessWrapper.run` method is the target function of the process.\n\n        Args:\n            pool: The pool of processes.\n            manager: The manager to create the queues.\n            output_queue: The queue to send the output batches.\n            shared_info: The shared information between the processes.\n        \"\"\"\n        for step_name in self.dag:\n            step: \"Step\" = self.dag.get_step(step_name)[STEP_ATTR_NAME]\n            input_queue = manager.Queue()\n            self.dag.set_step_attr(step.name, INPUT_QUEUE_ATTR_NAME, input_queue)  # type: ignore\n\n            # Set `pipeline` to `None` as in some Python environments the pipeline is not\n            # picklable and it will raise an error when trying to send the step to the process.\n            # `TypeError: cannot pickle 'code' object`\n            step.pipeline = None\n\n            process_wrapper = _ProcessWrapper(\n                step=step,\n                input_queue=input_queue,\n                output_queue=output_queue,\n                shared_info=shared_info,\n                dry_run=self._dry_run,\n            )\n\n            pool.apply_async(\n                process_wrapper.run,\n                callback=self._finished_callback,\n                error_callback=self._error_callback,\n            )  # type: ignore\n\n    def _error_callback(self, e: BaseException) -&gt; None:\n        \"\"\"Error callback that will be called when an error occurs in a `Step` process.\n\n        Args:\n            e: The exception raised by the process.\n        \"\"\"\n        global _SUBPROCESS_EXCEPTION\n\n        # First we check that the exception is a `_ProcessWrapperException`, otherwise, we\n        # print it out and stop the pipeline, since some errors may be unhandled\n        if not isinstance(e, _ProcessWrapperException):\n            self._logger.error(f\"\u274c Failed with an unhandled exception: {e}\")\n            self._stop()\n            return\n\n        if e.is_load_error:\n            self._logger.error(f\"\u274c Failed to load step '{e.step.name}': {e.message}\")\n            with self.shared_info[_STEPS_LOADED_LOCK_KEY]:\n                self.shared_info[_STEPS_LOADED_KEY] = [_STEPS_LOADED_ERROR_CODE]\n            _SUBPROCESS_EXCEPTION = e.subprocess_exception\n            _SUBPROCESS_EXCEPTION.__traceback__ = tblib.Traceback.from_string(  # type: ignore\n                e.formatted_traceback\n            ).as_traceback()\n            return\n\n        # If the step is global, is not in the last trophic level and has no successors,\n        # then we can ignore the error and continue executing the pipeline\n        step_name: str = e.step.name  # type: ignore\n        if (\n            e.step.is_global\n            and not self.dag.step_in_last_trophic_level(step_name)\n            and list(self.dag.get_step_successors(step_name)) == []\n        ):\n            self._logger.error(\n                f\"\u270b An error occurred when running global step '{step_name}' with no\"\n                \" successors and not in the last trophic level. Pipeline execution can\"\n                f\" continue. Error will be ignored.\"\n            )\n            self._logger.error(f\"Subprocess traceback:\\n\\n{e.formatted_traceback}\")\n            return\n\n        # Global step with successors failed\n        self._logger.error(f\"An error occurred in global step '{step_name}'\")\n        self._logger.error(f\"Subprocess traceback:\\n\\n{e.formatted_traceback}\")\n        self._cache()\n        self._stop()\n\n    def _finished_callback(self, step_name: str) -&gt; None:\n        \"\"\"Callback that will be called when a `Step` process finishes.\n\n        Args:\n            step_name: The name of the step that finished.\n        \"\"\"\n        with _STEPS_FINISHED_LOCK:\n            _STEPS_FINISHED.add(step_name)\n\n    def _check_step_not_loaded_or_finished(self, step_name: str) -&gt; bool:\n        \"\"\"Checks if a step is not loaded or already finished.\n\n        Args:\n            step_name: The name of the step.\n\n        Returns:\n            `True` if the step is not loaded or already finished, `False` otherwise.\n        \"\"\"\n        with _STEPS_LOADED_LOCK:\n            if step_name not in _STEPS_LOADED:\n                return True\n\n        with _STEPS_FINISHED_LOCK:\n            if step_name in _STEPS_FINISHED:\n                return True\n\n        return False\n\n    def _stop(\n        self, manager: Optional[\"SyncManager\"] = None, pool: Optional[\"Pool\"] = None\n    ) -&gt; None:\n        \"\"\"Stops the pipeline execution. It will first send `None` to the input queues\n        of all the steps and then wait until the output queue is empty i.e. all the steps\n        finished processing the batches that were sent before the stop flag. Then it will\n        send `None` to the output queue to notify the pipeline to stop.\"\"\"\n\n        global _STOP_CALLED\n\n        with _STOP_CALLED_LOCK:\n            if _STOP_CALLED:\n                global _STOP_CALLS\n                _STOP_CALLS += 1\n                if _STOP_CALLS == 1:\n                    self._logger.warning(\n                        \"\ud83d\uded1 Press again to force the pipeline to stop.\"\n                    )\n                elif _STOP_CALLS &gt; 1:\n                    self._logger.warning(\"\ud83d\uded1 Forcing pipeline interruption.\")\n\n                    if pool:\n                        pool.terminate()\n                        pool.join()\n\n                    if manager:\n                        manager.shutdown()\n                        manager.join()\n\n                    stop_logging()\n\n                    sys.exit(1)\n\n                return\n            _STOP_CALLED = True\n\n        self._logger.debug(f\"Steps loaded before calling `stop`: {_STEPS_LOADED}\")\n        self._logger.info(\n            \"\ud83d\uded1 Stopping pipeline. Waiting for steps to finish processing batches...\"\n        )\n        self._logger.debug(\"Sending `None` to the output queue to notify stop...\")\n        self.output_queue.put(None)\n\n    def _handle_keyboard_interrupt(\n        self, manager: Optional[\"SyncManager\"] = None, pool: Optional[\"Pool\"] = None\n    ) -&gt; None:\n        \"\"\"Handles KeyboardInterrupt signal sent during the Pipeline.run method.\n\n        It will try to call self._stop (if the pipeline didn't started yet, it won't\n        have any effect), and if the pool is already started, will close it before exiting\n        the program.\n        \"\"\"\n\n        def signal_handler(signumber: int, frame: Any) -&gt; None:\n            self._stop(manager=manager, pool=pool)\n\n        signal.signal(signal.SIGINT, signal_handler)\n</code></pre>"},{"location":"reference/distilabel/pipeline/local/#distilabel.pipeline.local.Pipeline.run","title":"<code>run(parameters=None, use_cache=True, storage_parameters=None, use_fs_to_pass_data=False)</code>","text":"<p>Runs the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>A dictionary with the step name as the key and a dictionary with the runtime parameters for the step as the value. Defaults to <code>None</code>.</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use the cache from previous pipeline runs. Defaults to <code>True</code>.</p> <code>True</code> <code>storage_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary with the storage parameters (<code>fsspec</code> and path) that will be used to store the data of the <code>_Batch</code>es passed between the steps if <code>use_fs_to_pass_data</code> is <code>True</code> (for the batches received by a <code>GlobalStep</code> it will be always used). It must have at least the \"path\" key, and it can contain additional keys depending on the protocol. By default, it will use the local file system and a directory in the cache directory. Defaults to <code>None</code>.</p> <code>None</code> <code>use_fs_to_pass_data</code> <code>bool</code> <p>Whether to use the file system to pass the data of the <code>_Batch</code>es between the steps. Even if this parameter is <code>False</code>, the <code>Batch</code>es received by <code>GlobalStep</code>s will always use the file system to pass the data. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distiset</code> <p>The <code>Distiset</code> created by the pipeline.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the pipeline fails to load all the steps.</p> Source code in <code>src/distilabel/pipeline/local.py</code> <pre><code>def run(\n    self,\n    parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    use_cache: bool = True,\n    storage_parameters: Optional[Dict[str, Any]] = None,\n    use_fs_to_pass_data: bool = False,\n) -&gt; \"Distiset\":\n    \"\"\"Runs the pipeline.\n\n    Args:\n        parameters: A dictionary with the step name as the key and a dictionary with\n            the runtime parameters for the step as the value. Defaults to `None`.\n        use_cache: Whether to use the cache from previous pipeline runs. Defaults to\n            `True`.\n        storage_parameters: A dictionary with the storage parameters (`fsspec` and path)\n            that will be used to store the data of the `_Batch`es passed between the\n            steps if `use_fs_to_pass_data` is `True` (for the batches received by a\n            `GlobalStep` it will be always used). It must have at least the \"path\" key,\n            and it can contain additional keys depending on the protocol. By default,\n            it will use the local file system and a directory in the cache directory.\n            Defaults to `None`.\n        use_fs_to_pass_data: Whether to use the file system to pass the data of\n            the `_Batch`es between the steps. Even if this parameter is `False`, the\n            `Batch`es received by `GlobalStep`s will always use the file system to\n            pass the data. Defaults to `False`.\n\n    Returns:\n        The `Distiset` created by the pipeline.\n\n    Raises:\n        RuntimeError: If the pipeline fails to load all the steps.\n    \"\"\"\n    log_queue = mp.Queue()\n\n    self._set_logging_parameters(\n        {\"log_queue\": log_queue, \"filename\": self._cache_location[\"log_file\"]}\n    )\n\n    if distiset := super().run(\n        parameters, use_cache, storage_parameters, use_fs_to_pass_data\n    ):\n        return distiset\n\n    num_processes = len(self.dag)\n    ctx = mp.get_context()  # type: ignore\n    with ctx.Manager() as manager, ctx.Pool(\n        num_processes,\n        initializer=_init_worker,\n        initargs=(log_queue,),\n    ) as pool:\n        self.output_queue: \"Queue[Any]\" = manager.Queue()\n        self.shared_info = self._create_shared_info_dict(manager)\n        self._handle_keyboard_interrupt(manager=manager, pool=pool)\n\n        # Run the steps using the pool of processes\n        self._run_steps_in_loop(pool, manager, self.output_queue, self.shared_info)\n\n        # Wait for all the steps to be loaded correctly\n        if not self._all_steps_loaded():\n            self._write_buffer.close()  # type: ignore\n            self._batch_manager = None\n            stop_logging()\n            raise RuntimeError(\n                \"Failed to load all the steps. Could not run pipeline.\"\n            ) from _SUBPROCESS_EXCEPTION\n\n        # Send the \"first\" batches to the steps so the batches starts flowing through\n        # the input queues and output queue\n        self._request_initial_batches()\n\n        # Start a loop to receive the output batches from the steps\n        self._run_output_queue_loop_in_thread()\n\n        # Send `None` to steps `input_queue`s just in case some step is still waiting\n        self._notify_steps_to_stop()\n\n    # `Pool.__exit__` has already called `terminate`, `join` the pool to make sure\n    # all the processes have finished\n    pool.join()\n    manager.join()\n\n    self._write_buffer.close()  # type: ignore\n    distiset = create_distiset(\n        self._cache_location[\"data\"],\n        pipeline_path=self._cache_location[\"pipeline\"],\n        log_filename_path=self._cache_location[\"log_file\"],\n        enable_metadata=self._enable_metadata,\n    )\n    stop_logging()\n    return distiset\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/","title":"Routing batch function","text":""},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunc","title":"<code>RoutingBatchFunc = Callable[[List[str]], List[str]]</code>  <code>module-attribute</code>","text":"<p>Type alias for a routing batch function. It takes a list of all the downstream steps and returns a list with the names of the steps that should receive the batch.</p>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction","title":"<code>RoutingBatchFunction</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>_Serializable</code></p> <p>A thin wrapper around a routing batch function that can be used to route batches from one upstream step to specific downstream steps.</p> <p>Attributes:</p> Name Type Description <code>routing_function</code> <code>RoutingBatchFunc</code> <p>The routing function that takes a list of all the downstream steps and returns a list with the names of the steps that should receive the batch.</p> <code>_step</code> <code>Union[_Step, None]</code> <p>The upstream step that is connected to the routing batch function.</p> <code>_routed_batch_registry</code> <code>Dict[str, Dict[int, List[str]]]</code> <p>A dictionary that keeps track of the batches that have been routed to specific downstream steps.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>class RoutingBatchFunction(BaseModel, _Serializable):\n    \"\"\"A thin wrapper around a routing batch function that can be used to route batches\n    from one upstream step to specific downstream steps.\n\n    Attributes:\n        routing_function: The routing function that takes a list of all the downstream steps\n            and returns a list with the names of the steps that should receive the batch.\n        _step: The upstream step that is connected to the routing batch function.\n        _routed_batch_registry: A dictionary that keeps track of the batches that have been\n            routed to specific downstream steps.\n    \"\"\"\n\n    routing_function: RoutingBatchFunc\n    description: Optional[str] = None\n\n    _step: Union[\"_Step\", None] = PrivateAttr(default=None)\n    _routed_batch_registry: Dict[str, Dict[int, List[str]]] = PrivateAttr(\n        default_factory=dict\n    )\n    _factory_function_module: Union[str, None] = PrivateAttr(default=None)\n    _factory_function_name: Union[str, None] = PrivateAttr(default=None)\n    _factory_function_kwargs: Union[Dict[str, Any], None] = PrivateAttr(default=None)\n\n    def route_batch(self, batch: \"_Batch\", steps: List[str]) -&gt; List[str]:\n        \"\"\"Returns a list of selected downstream steps from `steps` to which the `batch`\n        should be routed.\n\n        Args:\n            batch: The batch that should be routed.\n            steps: A list of all the downstream steps that can receive the batch.\n\n        Returns:\n            A list with the names of the steps that should receive the batch.\n        \"\"\"\n        routed_steps = self.routing_function(steps)\n        self._register_routed_batch(batch, routed_steps)\n        return routed_steps\n\n    def set_factory_function(\n        self,\n        factory_function_module: str,\n        factory_function_name: str,\n        factory_function_kwargs: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Sets the factory function that was used to create the `routing_batch_function`.\n\n        Args:\n            factory_function_module: The module name where the factory function is defined.\n            factory_function_name: The name of the factory function that was used to create\n                the `routing_batch_function`.\n            factory_function_kwargs: The keyword arguments that were used when calling the\n                factory function.\n        \"\"\"\n        self._factory_function_module = factory_function_module\n        self._factory_function_name = factory_function_name\n        self._factory_function_kwargs = factory_function_kwargs\n\n    def __call__(self, batch: \"_Batch\", steps: List[str]) -&gt; List[str]:\n        \"\"\"Returns a list of selected downstream steps from `steps` to which the `batch`\n        should be routed.\n\n        Args:\n            batch: The batch that should be routed.\n            steps: A list of all the downstream steps that can receive the batch.\n\n        Returns:\n            A list with the names of the steps that should receive the batch.\n        \"\"\"\n        return self.route_batch(batch, steps)\n\n    def _register_routed_batch(self, batch: \"_Batch\", routed_steps: List[str]) -&gt; None:\n        \"\"\"Registers a batch that has been routed to specific downstream steps.\n\n        Args:\n            batch: The batch that has been routed.\n            routed_steps: The list of downstream steps that have been selected to receive\n                the batch.\n        \"\"\"\n        upstream_step = batch.step_name\n        batch_seq_no = batch.seq_no\n        self._routed_batch_registry.setdefault(upstream_step, {}).setdefault(\n            batch_seq_no, routed_steps\n        )\n\n    def __rshift__(\n        self, other: List[\"DownstreamConnectableSteps\"]\n    ) -&gt; List[\"DownstreamConnectableSteps\"]:\n        \"\"\"Connects a list of dowstream steps to the upstream step of the routing batch\n        function.\n\n        Args:\n            other: A list of downstream steps that should be connected to the upstream step\n                of the routing batch function.\n\n        Returns:\n            The list of downstream steps that have been connected to the upstream step of the\n            routing batch function.\n        \"\"\"\n        if not isinstance(other, list):\n            raise ValueError(\n                f\"Can only set a `routing_batch_function` for a list of steps. Got: {other}.\"\n                \" Please, review the right-hand side of the `routing_batch_function &gt;&gt; other`\"\n                \" expression. It should be\"\n                \" `upstream_step &gt;&gt; routing_batch_function &gt;&gt; [downstream_step_1, dowstream_step_2, ...]`.\"\n            )\n\n        if not self._step:\n            raise ValueError(\n                \"Routing batch function doesn't have an upstream step. Cannot connect downstream\"\n                \" steps before connecting the upstream step. Connect this routing batch\"\n                \" function to an upstream step using the `&gt;&gt;` operator. For example:\"\n                \" `upstream_step &gt;&gt; routing_batch_function &gt;&gt; [downstream_step_1, downstream_step_2, ...]`.\"\n            )\n\n        for step in other:\n            self._step.connect(step)\n        return other\n\n    def dump(self, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"Dumps the routing batch function to a dictionary, and the information of the\n        factory function used to create this routing batch function.\n\n        Args:\n            **kwargs: Additional keyword arguments that should be included in the dump.\n\n        Returns:\n            A dictionary with the routing batch function information and the factory function\n            information.\n        \"\"\"\n        dump_info: Dict[str, Any] = {\"step\": self._step.name}  # type: ignore\n\n        if self.description:\n            dump_info[\"description\"] = self.description\n\n        if type_info := self._get_type_info():\n            dump_info[TYPE_INFO_KEY] = type_info\n\n        return dump_info\n\n    def _get_type_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the information of the factory function used to create the routing batch\n        function.\n\n        Returns:\n            A dictionary with the factory function information.\n        \"\"\"\n\n        type_info = {}\n\n        if self._factory_function_module:\n            type_info[\"module\"] = self._factory_function_module\n\n        if self._factory_function_name:\n            type_info[\"name\"] = self._factory_function_name\n\n        if self._factory_function_kwargs:\n            type_info[\"kwargs\"] = self._factory_function_kwargs\n\n        return type_info\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; Self:\n        \"\"\"Loads a routing batch function from a dictionary. It must contain the information\n        of the factory function used to create the routing batch function.\n\n        Args:\n            data: A dictionary with the routing batch function information and the factory\n                function information.\n        \"\"\"\n        type_info = data.get(TYPE_INFO_KEY)\n        if not type_info:\n            step = data.get(\"step\")\n            raise ValueError(\n                f\"The routing batch function for step '{step}' was created without a factory\"\n                \" function, and it cannot be reconstructed.\"\n            )\n\n        module = type_info.get(\"module\")\n        name = type_info.get(\"name\")\n        kwargs = type_info.get(\"kwargs\")\n\n        if not module or not name or not kwargs:\n            raise ValueError(\n                \"The routing batch function was created with a factory function, but the\"\n                \" information is incomplete. Cannot reconstruct the routing batch function.\"\n            )\n\n        routing_batch_function = _get_module_attr(module=module, name=name)(**kwargs)\n        routing_batch_function.description = data.get(\"description\")\n        routing_batch_function.set_factory_function(\n            factory_function_module=module,\n            factory_function_name=name,\n            factory_function_kwargs=kwargs,\n        )\n\n        return routing_batch_function\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.__call__","title":"<code>__call__(batch, steps)</code>","text":"<p>Returns a list of selected downstream steps from <code>steps</code> to which the <code>batch</code> should be routed.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>_Batch</code> <p>The batch that should be routed.</p> required <code>steps</code> <code>List[str]</code> <p>A list of all the downstream steps that can receive the batch.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list with the names of the steps that should receive the batch.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def __call__(self, batch: \"_Batch\", steps: List[str]) -&gt; List[str]:\n    \"\"\"Returns a list of selected downstream steps from `steps` to which the `batch`\n    should be routed.\n\n    Args:\n        batch: The batch that should be routed.\n        steps: A list of all the downstream steps that can receive the batch.\n\n    Returns:\n        A list with the names of the steps that should receive the batch.\n    \"\"\"\n    return self.route_batch(batch, steps)\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.__rshift__","title":"<code>__rshift__(other)</code>","text":"<p>Connects a list of dowstream steps to the upstream step of the routing batch function.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>List[DownstreamConnectableSteps]</code> <p>A list of downstream steps that should be connected to the upstream step of the routing batch function.</p> required <p>Returns:</p> Type Description <code>List[DownstreamConnectableSteps]</code> <p>The list of downstream steps that have been connected to the upstream step of the</p> <code>List[DownstreamConnectableSteps]</code> <p>routing batch function.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def __rshift__(\n    self, other: List[\"DownstreamConnectableSteps\"]\n) -&gt; List[\"DownstreamConnectableSteps\"]:\n    \"\"\"Connects a list of dowstream steps to the upstream step of the routing batch\n    function.\n\n    Args:\n        other: A list of downstream steps that should be connected to the upstream step\n            of the routing batch function.\n\n    Returns:\n        The list of downstream steps that have been connected to the upstream step of the\n        routing batch function.\n    \"\"\"\n    if not isinstance(other, list):\n        raise ValueError(\n            f\"Can only set a `routing_batch_function` for a list of steps. Got: {other}.\"\n            \" Please, review the right-hand side of the `routing_batch_function &gt;&gt; other`\"\n            \" expression. It should be\"\n            \" `upstream_step &gt;&gt; routing_batch_function &gt;&gt; [downstream_step_1, dowstream_step_2, ...]`.\"\n        )\n\n    if not self._step:\n        raise ValueError(\n            \"Routing batch function doesn't have an upstream step. Cannot connect downstream\"\n            \" steps before connecting the upstream step. Connect this routing batch\"\n            \" function to an upstream step using the `&gt;&gt;` operator. For example:\"\n            \" `upstream_step &gt;&gt; routing_batch_function &gt;&gt; [downstream_step_1, downstream_step_2, ...]`.\"\n        )\n\n    for step in other:\n        self._step.connect(step)\n    return other\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.dump","title":"<code>dump(**kwargs)</code>","text":"<p>Dumps the routing batch function to a dictionary, and the information of the factory function used to create this routing batch function.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that should be included in the dump.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary with the routing batch function information and the factory function</p> <code>Dict[str, Any]</code> <p>information.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def dump(self, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"Dumps the routing batch function to a dictionary, and the information of the\n    factory function used to create this routing batch function.\n\n    Args:\n        **kwargs: Additional keyword arguments that should be included in the dump.\n\n    Returns:\n        A dictionary with the routing batch function information and the factory function\n        information.\n    \"\"\"\n    dump_info: Dict[str, Any] = {\"step\": self._step.name}  # type: ignore\n\n    if self.description:\n        dump_info[\"description\"] = self.description\n\n    if type_info := self._get_type_info():\n        dump_info[TYPE_INFO_KEY] = type_info\n\n    return dump_info\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Loads a routing batch function from a dictionary. It must contain the information of the factory function used to create the routing batch function.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>A dictionary with the routing batch function information and the factory function information.</p> required Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; Self:\n    \"\"\"Loads a routing batch function from a dictionary. It must contain the information\n    of the factory function used to create the routing batch function.\n\n    Args:\n        data: A dictionary with the routing batch function information and the factory\n            function information.\n    \"\"\"\n    type_info = data.get(TYPE_INFO_KEY)\n    if not type_info:\n        step = data.get(\"step\")\n        raise ValueError(\n            f\"The routing batch function for step '{step}' was created without a factory\"\n            \" function, and it cannot be reconstructed.\"\n        )\n\n    module = type_info.get(\"module\")\n    name = type_info.get(\"name\")\n    kwargs = type_info.get(\"kwargs\")\n\n    if not module or not name or not kwargs:\n        raise ValueError(\n            \"The routing batch function was created with a factory function, but the\"\n            \" information is incomplete. Cannot reconstruct the routing batch function.\"\n        )\n\n    routing_batch_function = _get_module_attr(module=module, name=name)(**kwargs)\n    routing_batch_function.description = data.get(\"description\")\n    routing_batch_function.set_factory_function(\n        factory_function_module=module,\n        factory_function_name=name,\n        factory_function_kwargs=kwargs,\n    )\n\n    return routing_batch_function\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.route_batch","title":"<code>route_batch(batch, steps)</code>","text":"<p>Returns a list of selected downstream steps from <code>steps</code> to which the <code>batch</code> should be routed.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>_Batch</code> <p>The batch that should be routed.</p> required <code>steps</code> <code>List[str]</code> <p>A list of all the downstream steps that can receive the batch.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list with the names of the steps that should receive the batch.</p> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def route_batch(self, batch: \"_Batch\", steps: List[str]) -&gt; List[str]:\n    \"\"\"Returns a list of selected downstream steps from `steps` to which the `batch`\n    should be routed.\n\n    Args:\n        batch: The batch that should be routed.\n        steps: A list of all the downstream steps that can receive the batch.\n\n    Returns:\n        A list with the names of the steps that should receive the batch.\n    \"\"\"\n    routed_steps = self.routing_function(steps)\n    self._register_routed_batch(batch, routed_steps)\n    return routed_steps\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.RoutingBatchFunction.set_factory_function","title":"<code>set_factory_function(factory_function_module, factory_function_name, factory_function_kwargs)</code>","text":"<p>Sets the factory function that was used to create the <code>routing_batch_function</code>.</p> <p>Parameters:</p> Name Type Description Default <code>factory_function_module</code> <code>str</code> <p>The module name where the factory function is defined.</p> required <code>factory_function_name</code> <code>str</code> <p>The name of the factory function that was used to create the <code>routing_batch_function</code>.</p> required <code>factory_function_kwargs</code> <code>Dict[str, Any]</code> <p>The keyword arguments that were used when calling the factory function.</p> required Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def set_factory_function(\n    self,\n    factory_function_module: str,\n    factory_function_name: str,\n    factory_function_kwargs: Dict[str, Any],\n) -&gt; None:\n    \"\"\"Sets the factory function that was used to create the `routing_batch_function`.\n\n    Args:\n        factory_function_module: The module name where the factory function is defined.\n        factory_function_name: The name of the factory function that was used to create\n            the `routing_batch_function`.\n        factory_function_kwargs: The keyword arguments that were used when calling the\n            factory function.\n    \"\"\"\n    self._factory_function_module = factory_function_module\n    self._factory_function_name = factory_function_name\n    self._factory_function_kwargs = factory_function_kwargs\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.routing_batch_function","title":"<code>routing_batch_function(description=None)</code>","text":"<p>Creates a routing batch function that can be used to route batches from one upstream step to specific downstream steps.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>Optional[str]</code> <p>An optional description for the routing batch function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[RoutingBatchFunc], RoutingBatchFunction]</code> <p>A <code>RoutingBatchFunction</code> instance that can be used with the <code>&gt;&gt;</code> operators and with</p> <code>Callable[[RoutingBatchFunc], RoutingBatchFunction]</code> <p>the <code>Pipeline.connect</code> method when defining the pipeline.</p> <p>Example:</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline, routing_batch_function\nfrom distilabel.steps import LoadHubDataset, CombineColumns\n\n\n@routing_batch_function\ndef random_routing_batch(steps: List[str]) -&gt; List[str]:\n    return random.sample(steps, 2)\n\n\nwith Pipeline(name=\"routing-batch-function\") as pipeline:\n    load_data = LoadHubDataset()\n\n    generations = []\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        generations.append(task)\n\n    combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n    load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n</code></pre> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def routing_batch_function(\n    description: Optional[str] = None,\n) -&gt; Callable[[RoutingBatchFunc], RoutingBatchFunction]:\n    \"\"\"Creates a routing batch function that can be used to route batches from one upstream\n    step to specific downstream steps.\n\n    Args:\n        description: An optional description for the routing batch function.\n\n    Returns:\n        A `RoutingBatchFunction` instance that can be used with the `&gt;&gt;` operators and with\n        the `Pipeline.connect` method when defining the pipeline.\n\n    Example:\n\n    ```python\n    from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\n    from distilabel.pipeline import Pipeline, routing_batch_function\n    from distilabel.steps import LoadHubDataset, CombineColumns\n\n\n    @routing_batch_function\n    def random_routing_batch(steps: List[str]) -&gt; List[str]:\n        return random.sample(steps, 2)\n\n\n    with Pipeline(name=\"routing-batch-function\") as pipeline:\n        load_data = LoadHubDataset()\n\n        generations = []\n        for llm in (\n            OpenAILLM(model=\"gpt-4-0125-preview\"),\n            MistralLLM(model=\"mistral-large-2402\"),\n            VertexAILLM(model=\"gemini-1.5-pro\"),\n        ):\n            task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n            generations.append(task)\n\n        combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n        load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n    ```\n    \"\"\"\n\n    def decorator(func: RoutingBatchFunc) -&gt; RoutingBatchFunction:\n        factory_function_name, factory_function_module, factory_function_kwargs = (\n            None,\n            None,\n            None,\n        )\n\n        # Check if `routing_batch_function` was created using a factory function from an installed package\n        stack = inspect.stack()\n        if len(stack) &gt; 2:\n            factory_function_frame_info = stack[1]\n\n            # Function factory path\n            if factory_function_frame_info.function != \"&lt;module&gt;\":\n                factory_function_name = factory_function_frame_info.function\n                factory_function_module = inspect.getmodule(\n                    factory_function_frame_info.frame\n                ).__name__  # type: ignore\n\n                # Function factory kwargs\n                factory_function_kwargs = factory_function_frame_info.frame.f_locals\n\n        routing_batch_function = RoutingBatchFunction(\n            routing_function=func,\n            description=description,\n        )\n\n        if (\n            factory_function_module\n            and factory_function_name\n            and factory_function_kwargs\n        ):\n            routing_batch_function.set_factory_function(\n                factory_function_module=factory_function_module,\n                factory_function_name=factory_function_name,\n                factory_function_kwargs=factory_function_kwargs,\n            )\n\n        return routing_batch_function\n\n    return decorator\n</code></pre>"},{"location":"reference/distilabel/pipeline/routing_batch_function/#distilabel.pipeline.routing_batch_function.sample_n_steps","title":"<code>sample_n_steps(n)</code>","text":"<p>A simple function that creates a routing batch function that samples <code>n</code> steps from the list of all the downstream steps.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of steps to sample from the list of all the downstream steps.</p> required <p>Returns:</p> Type Description <code>RoutingBatchFunction</code> <p>A <code>RoutingBatchFunction</code> instance that can be used with the <code>&gt;&gt;</code> operators and with</p> <code>RoutingBatchFunction</code> <p>the <code>Pipeline.connect</code> method when defining the pipeline.</p> <p>Example:</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline, sample_n_steps\nfrom distilabel.steps import LoadHubDataset, CombineColumns\n\n\nrandom_routing_batch = sample_n_steps(2)\n\n\nwith Pipeline(name=\"routing-batch-function\") as pipeline:\n    load_data = LoadHubDataset()\n\n    generations = []\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        generations.append(task)\n\n    combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n    load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n</code></pre> Source code in <code>src/distilabel/pipeline/routing_batch_function.py</code> <pre><code>def sample_n_steps(n: int) -&gt; RoutingBatchFunction:\n    \"\"\"A simple function that creates a routing batch function that samples `n` steps from\n    the list of all the downstream steps.\n\n    Args:\n        n: The number of steps to sample from the list of all the downstream steps.\n\n    Returns:\n        A `RoutingBatchFunction` instance that can be used with the `&gt;&gt;` operators and with\n        the `Pipeline.connect` method when defining the pipeline.\n\n    Example:\n\n    ```python\n    from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\n    from distilabel.pipeline import Pipeline, sample_n_steps\n    from distilabel.steps import LoadHubDataset, CombineColumns\n\n\n    random_routing_batch = sample_n_steps(2)\n\n\n    with Pipeline(name=\"routing-batch-function\") as pipeline:\n        load_data = LoadHubDataset()\n\n        generations = []\n        for llm in (\n            OpenAILLM(model=\"gpt-4-0125-preview\"),\n            MistralLLM(model=\"mistral-large-2402\"),\n            VertexAILLM(model=\"gemini-1.5-pro\"),\n        ):\n            task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n            generations.append(task)\n\n        combine_columns = CombineColumns(columns=[\"generation\", \"model_name\"])\n\n        load_data &gt;&gt; random_routing_batch &gt;&gt; generations &gt;&gt; combine_columns\n    ```\n    \"\"\"\n\n    @routing_batch_function(\n        description=f\"Sample {n} steps from the list of downstream steps.\"\n    )\n    def sample_n(steps: List[str]) -&gt; List[str]:\n        return random.sample(steps, n)\n\n    return sample_n\n</code></pre>"},{"location":"reference/distilabel/pipeline/typing/","title":"Typing","text":""},{"location":"reference/distilabel/pipeline/typing/#distilabel.pipeline.typing.DownstreamConnectable","title":"<code>DownstreamConnectable = Union['Step', 'GlobalStep']</code>  <code>module-attribute</code>","text":"<p>Alias for the <code>Step</code> types that can be connected as downstream steps.</p>"},{"location":"reference/distilabel/pipeline/typing/#distilabel.pipeline.typing.DownstreamConnectableSteps","title":"<code>DownstreamConnectableSteps = TypeVar('DownstreamConnectableSteps', bound=DownstreamConnectable, covariant=True)</code>  <code>module-attribute</code>","text":"<p>Type for the <code>Step</code> types that can be connected as downstream steps.</p>"},{"location":"reference/distilabel/pipeline/typing/#distilabel.pipeline.typing.UpstreamConnectableSteps","title":"<code>UpstreamConnectableSteps = TypeVar('UpstreamConnectableSteps', bound=Union['Step', 'GlobalStep', 'GeneratorStep'])</code>  <code>module-attribute</code>","text":"<p>Type for the <code>Step</code> types that can be connected as upstream steps.</p>"},{"location":"reference/distilabel/pipeline/utils/","title":"Utils","text":""},{"location":"reference/distilabel/pipeline/utils/#distilabel.pipeline.utils.combine_dicts","title":"<code>combine_dicts(*inputs, merge_keys, output_merge_keys=None)</code>","text":"<p>Combines multiple list of dictionaries into a single list of dictionaries on the specified <code>merge_keys</code>. If <code>output_merge_keys</code> are provided, then it will also rename <code>merge_keys</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>list of dictionaries to combine.</p> <code>()</code> <code>merge_keys</code> <code>List[str]</code> <p>list of keys to merge on.</p> required <code>output_merge_keys</code> <code>Optional[List[str]]</code> <p>list of keys to rename the merge keys to. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>StepInput</code> <p>A list of dictionaries where the values of the <code>merge_keys</code> are combined into a</p> <code>StepInput</code> <p>list and renamed to <code>output_merge_keys</code>.</p> Source code in <code>src/distilabel/pipeline/utils.py</code> <pre><code>def combine_dicts(\n    *inputs: StepInput,\n    merge_keys: List[str],\n    output_merge_keys: Optional[List[str]] = None,\n) -&gt; StepInput:\n    \"\"\"Combines multiple list of dictionaries into a single list of dictionaries on the\n    specified `merge_keys`. If `output_merge_keys` are provided, then it will also rename\n    `merge_keys`.\n\n    Args:\n        inputs: list of dictionaries to combine.\n        merge_keys: list of keys to merge on.\n        output_merge_keys: list of keys to rename the merge keys to. Defaults to `None`.\n\n    Returns:\n        A list of dictionaries where the values of the `merge_keys` are combined into a\n        list and renamed to `output_merge_keys`.\n    \"\"\"\n    if output_merge_keys is not None and len(output_merge_keys) != len(merge_keys):\n        raise ValueError(\n            \"The length of output_merge_keys must be the same as the length of merge_keys\"\n        )\n    if output_merge_keys is None:\n        output_merge_keys = [f\"merged_{key}\" for key in merge_keys]\n    merge_keys_dict = dict(zip(merge_keys, output_merge_keys))\n\n    result = []\n    # Use zip to iterate over lists based on their index\n    for dicts_at_index in zip(*inputs):\n        combined_dict = {}\n        # Iterate over dicts at the same index\n        for d in dicts_at_index:\n            # Iterate over key-value pairs in each dict\n            for key, value in d.items():\n                # If the key is in the merge_keys, append the value to the existing list\n                if key in merge_keys_dict.keys():\n                    combined_dict.setdefault(merge_keys_dict[key], []).append(value)\n                # If the key is not in the merge_keys, create a new key-value pair\n                else:\n                    combined_dict[key] = value\n        result.append(combined_dict)\n    return result\n</code></pre>"},{"location":"reference/distilabel/steps/","title":"Index","text":""},{"location":"reference/distilabel/steps/#distilabel.steps.GeneratorStepOutput","title":"<code>GeneratorStepOutput = Iterator[Tuple[List[Dict[str, Any]], bool]]</code>  <code>module-attribute</code>","text":"<p>GeneratorStepOutput is an alias of the typing <code>Iterator[Tuple[List[Dict[str, Any]], bool]]</code></p>"},{"location":"reference/distilabel/steps/#distilabel.steps.StepInput","title":"<code>StepInput = Annotated[List[Dict[str, Any]], _STEP_INPUT_ANNOTATION]</code>  <code>module-attribute</code>","text":"<p>StepInput is just an <code>Annotated</code> alias of the typing <code>List[Dict[str, Any]]</code> with extra metadata that allows <code>distilabel</code> to perform validations over the <code>process</code> step method defined in each <code>Step</code></p>"},{"location":"reference/distilabel/steps/#distilabel.steps.StepOutput","title":"<code>StepOutput = Iterator[List[Dict[str, Any]]]</code>  <code>module-attribute</code>","text":"<p>StepOutput is an alias of the typing <code>Iterator[List[Dict[str, Any]]]</code></p>"},{"location":"reference/distilabel/steps/#distilabel.steps.CombineColumns","title":"<code>CombineColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Combines columns from a list of <code>StepInput</code>.</p> <p><code>CombineColumns</code> is a <code>Step</code> that implements the <code>process</code> method that calls the <code>combine_dicts</code> function to handle and combine a list of <code>StepInput</code>. Also <code>CombineColumns</code> provides two attributes <code>columns</code> and <code>output_columns</code> to specify the columns to merge and the output columns which will override the default value for the properties <code>inputs</code> and <code>outputs</code>, respectively.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[str]</code> <p>List of strings with the names of the columns to merge.</p> <code>output_columns</code> <code>Optional[List[str]]</code> <p>Optional list of strings with the names of the output columns.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to merge.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> and <code>output_columns</code> attributes): The columns     that were merged.</li> </ul> Source code in <code>src/distilabel/steps/combine.py</code> <pre><code>class CombineColumns(Step):\n    \"\"\"Combines columns from a list of `StepInput`.\n\n    `CombineColumns` is a `Step` that implements the `process` method that calls the `combine_dicts`\n    function to handle and combine a list of `StepInput`. Also `CombineColumns` provides two attributes\n    `columns` and `output_columns` to specify the columns to merge and the output columns\n    which will override the default value for the properties `inputs` and `outputs`, respectively.\n\n    Attributes:\n        columns: List of strings with the names of the columns to merge.\n        output_columns: Optional list of strings with the names of the output columns.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to merge.\n\n    Output columns:\n        - dynamic (determined by `columns` and `output_columns` attributes): The columns\n            that were merged.\n    \"\"\"\n\n    columns: List[str]\n    output_columns: Optional[List[str]] = None\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the column names in `output_columns` or\n        `merged_{column}` for each column in `columns`.\"\"\"\n        return (\n            self.output_columns\n            if self.output_columns is not None\n            else [f\"merged_{column}\" for column in self.columns]\n        )\n\n    @override\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"The `process` method calls the `combine_dicts` function to handle and combine a list of `StepInput`.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with the combined `StepInput` using the `combine_dicts` function.\n        \"\"\"\n        yield combine_dicts(\n            *inputs,\n            merge_keys=self.inputs,\n            output_merge_keys=self.outputs,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.CombineColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the column names in <code>columns</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.CombineColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the column names in <code>output_columns</code> or <code>merged_{column}</code> for each column in <code>columns</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.CombineColumns.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method calls the <code>combine_dicts</code> function to handle and combine a list of <code>StepInput</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with the combined <code>StepInput</code> using the <code>combine_dicts</code> function.</p> Source code in <code>src/distilabel/steps/combine.py</code> <pre><code>@override\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"The `process` method calls the `combine_dicts` function to handle and combine a list of `StepInput`.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with the combined `StepInput` using the `combine_dicts` function.\n    \"\"\"\n    yield combine_dicts(\n        *inputs,\n        merge_keys=self.inputs,\n        output_merge_keys=self.outputs,\n    )\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.ConversationTemplate","title":"<code>ConversationTemplate</code>","text":"<p>               Bases: <code>Step</code></p> <p>Generate a conversation template from an instruction and a response.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to be used in the conversation.</li> <li>response (<code>str</code>): The response to be used in the conversation.</li> </ul> Output columns <ul> <li>conversation (<code>ChatType</code>): The conversation template.</li> </ul> Categories <ul> <li>format</li> <li>chat</li> <li>template</li> </ul> Source code in <code>src/distilabel/steps/formatting/conversation.py</code> <pre><code>class ConversationTemplate(Step):\n    \"\"\"Generate a conversation template from an instruction and a response.\n\n    Input columns:\n        - instruction (`str`): The instruction to be used in the conversation.\n        - response (`str`): The response to be used in the conversation.\n\n    Output columns:\n        - conversation (`ChatType`): The conversation template.\n\n    Categories:\n        - format\n        - chat\n        - template\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The instruction and response.\"\"\"\n        return [\"instruction\", \"response\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The conversation template.\"\"\"\n        return [\"conversation\"]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Generate a conversation template from an instruction and a response.\n\n        Args:\n            inputs: The input data.\n\n        Yields:\n            The input data with the conversation template.\n        \"\"\"\n        for input in inputs:\n            input[\"conversation\"] = [\n                {\"role\": \"user\", \"content\": input[\"instruction\"]},\n                {\"role\": \"assistant\", \"content\": input[\"response\"]},\n            ]\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.ConversationTemplate.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The instruction and response.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.ConversationTemplate.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The conversation template.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.ConversationTemplate.process","title":"<code>process(inputs)</code>","text":"<p>Generate a conversation template from an instruction and a response.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>The input data.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>The input data with the conversation template.</p> Source code in <code>src/distilabel/steps/formatting/conversation.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Generate a conversation template from an instruction and a response.\n\n    Args:\n        inputs: The input data.\n\n    Yields:\n        The input data with the conversation template.\n    \"\"\"\n    for input in inputs:\n        input[\"conversation\"] = [\n            {\"role\": \"user\", \"content\": input[\"instruction\"]},\n            {\"role\": \"assistant\", \"content\": input[\"response\"]},\n        ]\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.DeitaFiltering","title":"<code>DeitaFiltering</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Filter dataset rows using DEITA filtering strategy.</p> <p>Filter the dataset based on the DEITA score and the cosine distance between the embeddings. It's an implementation of the filtering step from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>data_budget</code> <code>RuntimeParameter[int]</code> <p>The desired size of the dataset after filtering.</p> <code>diversity_threshold</code> <code>RuntimeParameter[float]</code> <p>If a row has a cosine distance with respect to it's nearest neighbor greater than this value, it will be included in the filtered dataset. Defaults to <code>0.9</code>.</p> <code>normalize_embeddings</code> <code>RuntimeParameter[bool]</code> <p>Whether to normalize the embeddings before computing the cosine distance. Defaults to <code>True</code>.</p> Runtime parameters <ul> <li><code>data_budget</code>: The desired size of the dataset after filtering.</li> <li><code>diversity_threshold</code>: If a row has a cosine distance with respect to it's nearest     neighbor greater than this value, it will be included in the filtered dataset.</li> </ul> Input columns <ul> <li>evol_instruction_score (<code>float</code>): The score of the instruction generated by     <code>ComplexityScorer</code> step.</li> <li>evol_response_score (<code>float</code>): The score of the response generated by     <code>QualityScorer</code> step.</li> <li>embedding (<code>List[float]</code>): The embedding generated for the conversation of the     instruction-response pair using <code>GenerateEmbeddings</code> step.</li> </ul> Output columns <ul> <li>deita_score (<code>float</code>): The DEITA score for the instruction-response pair.</li> <li>deita_score_computed_with (<code>List[str]</code>): The scores used to compute the DEITA     score.</li> <li>nearest_neighbor_distance (<code>float</code>): The cosine distance between the embeddings     of the instruction-response pair.</li> </ul> Categories <ul> <li>filtering</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/deita.py</code> <pre><code>class DeitaFiltering(GlobalStep):\n    \"\"\"Filter dataset rows using DEITA filtering strategy.\n\n    Filter the dataset based on the DEITA score and the cosine distance between the embeddings.\n    It's an implementation of the filtering step from the paper 'What Makes Good Data\n    for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        data_budget: The desired size of the dataset after filtering.\n        diversity_threshold: If a row has a cosine distance with respect to it's nearest\n            neighbor greater than this value, it will be included in the filtered dataset.\n            Defaults to `0.9`.\n        normalize_embeddings: Whether to normalize the embeddings before computing the cosine\n            distance. Defaults to `True`.\n\n    Runtime parameters:\n        - `data_budget`: The desired size of the dataset after filtering.\n        - `diversity_threshold`: If a row has a cosine distance with respect to it's nearest\n            neighbor greater than this value, it will be included in the filtered dataset.\n\n    Input columns:\n        - evol_instruction_score (`float`): The score of the instruction generated by\n            `ComplexityScorer` step.\n        - evol_response_score (`float`): The score of the response generated by\n            `QualityScorer` step.\n        - embedding (`List[float]`): The embedding generated for the conversation of the\n            instruction-response pair using `GenerateEmbeddings` step.\n\n    Output columns:\n        - deita_score (`float`): The DEITA score for the instruction-response pair.\n        - deita_score_computed_with (`List[str]`): The scores used to compute the DEITA\n            score.\n        - nearest_neighbor_distance (`float`): The cosine distance between the embeddings\n            of the instruction-response pair.\n\n    Categories:\n        - filtering\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    data_budget: RuntimeParameter[int] = Field(\n        default=None, description=\"The desired size of the dataset after filtering.\"\n    )\n    diversity_threshold: RuntimeParameter[float] = Field(\n        default=0.9,\n        description=\"If a row has a cosine distance with respect to it's nearest neighbor\"\n        \" greater than this value, it will be included in the filtered dataset.\",\n    )\n    normalize_embeddings: RuntimeParameter[bool] = Field(\n        default=True,\n        description=\"Whether to normalize the embeddings before computing the cosine distance.\",\n    )\n    distance_metric: RuntimeParameter[Literal[\"cosine\", \"manhattan\"]] = Field(\n        default=\"cosine\",\n        description=\"The distance metric to use. Currently only 'cosine' is supported.\",\n    )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        return [\"evol_instruction_score\", \"evol_response_score\", \"embedding\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return [\"deita_score\", \"nearest_neighbor_distance\", \"deita_score_computed_with\"]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Filter the dataset based on the DEITA score and the cosine distance between the\n        embeddings.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The filtered dataset.\n        \"\"\"\n        inputs = self._compute_deita_score(inputs)\n        inputs = self._compute_nearest_neighbor(inputs)\n        inputs.sort(key=lambda x: x[\"deita_score\"], reverse=True)\n\n        selected_rows = []\n        for input in inputs:\n            if len(selected_rows) &gt;= self.data_budget:  # type: ignore\n                break\n            if input[\"nearest_neighbor_distance\"] &gt;= self.diversity_threshold:\n                selected_rows.append(input)\n        yield selected_rows\n\n    def _compute_deita_score(self, inputs: StepInput) -&gt; StepInput:\n        \"\"\"Computes the DEITA score for each instruction-response pair. The DEITA score is\n        the product of the instruction score and the response score.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The input data with the DEITA score computed.\n        \"\"\"\n        for input_ in inputs:\n            evol_instruction_score = input_.get(\"evol_instruction_score\")\n            evol_response_score = input_.get(\"evol_response_score\")\n\n            if evol_instruction_score and evol_response_score:\n                deita_score = evol_instruction_score * evol_response_score\n                score_computed_with = [\"evol_instruction_score\", \"evol_response_score\"]\n            elif evol_instruction_score:\n                self._logger.warning(\n                    \"Response score is missing for the instruction-response pair. Using\"\n                    \" instruction score as DEITA score.\"\n                )\n                deita_score = evol_instruction_score\n                score_computed_with = [\"evol_instruction_score\"]\n            elif evol_response_score:\n                self._logger.warning(\n                    \"Instruction score is missing for the instruction-response pair. Using\"\n                    \" response score as DEITA score.\"\n                )\n                deita_score = evol_response_score\n                score_computed_with = [\"evol_response_score\"]\n            else:\n                self._logger.warning(\n                    \"Instruction and response scores are missing for the instruction-response\"\n                    \" pair. Setting DEITA score to 0.\"\n                )\n                deita_score = 0\n                score_computed_with = []\n\n            input_.update(\n                {\n                    \"deita_score\": deita_score,\n                    \"deita_score_computed_with\": score_computed_with,\n                }\n            )\n        return inputs\n\n    def _compute_nearest_neighbor(self, inputs: StepInput) -&gt; StepInput:\n        \"\"\"Computes the cosine distance between the embeddings of the instruction-response\n        pairs and the nearest neighbor.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The input data with the cosine distance computed.\n        \"\"\"\n        embeddings = np.array([input[\"embedding\"] for input in inputs])\n        if self.normalize_embeddings:\n            embeddings = self._normalize_embeddings(embeddings)\n        self._logger.info(\"\ud83d\udccf Computing nearest neighbor distance...\")\n\n        if self.distance_metric == \"cosine\":\n            self._logger.info(\"\ud83d\udccf Using cosine distance.\")\n            distances = self._cosine_distance(embeddings)\n        else:\n            self._logger.info(\"\ud83d\udccf Using manhattan distance.\")\n            distances = self._manhattan_distance(embeddings)\n\n        for distance, input in zip(distances, inputs):\n            input[\"nearest_neighbor_distance\"] = distance\n        return inputs\n\n    def _normalize_embeddings(self, embeddings: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize the embeddings.\n\n        Args:\n            embeddings: The embeddings to normalize.\n\n        Returns:\n            The normalized embeddings.\n        \"\"\"\n        self._logger.info(\"\u2696\ufe0f Normalizing embeddings...\")\n        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n        return embeddings / norms\n\n    def _cosine_distance(self, embeddings: np.array) -&gt; np.array:  # type: ignore\n        \"\"\"Computes the cosine distance between the embeddings.\n\n        Args:\n            embeddings: The embeddings.\n\n        Returns:\n            The cosine distance between the embeddings.\n        \"\"\"\n        cosine_similarity = np.dot(embeddings, embeddings.T)\n        cosine_distance = 1 - cosine_similarity\n        # Ignore self-distance\n        np.fill_diagonal(cosine_distance, np.inf)\n        return np.min(cosine_distance, axis=1)\n\n    def _manhattan_distance(self, embeddings: np.array) -&gt; np.array:  # type: ignore\n        \"\"\"Computes the manhattan distance between the embeddings.\n\n        Args:\n            embeddings: The embeddings.\n\n        Returns:\n            The manhattan distance between the embeddings.\n        \"\"\"\n        manhattan_distance = np.abs(embeddings[:, None] - embeddings).sum(-1)\n        # Ignore self-distance\n        np.fill_diagonal(manhattan_distance, np.inf)\n        return np.min(manhattan_distance, axis=1)\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.DeitaFiltering.process","title":"<code>process(inputs)</code>","text":"<p>Filter the dataset based on the DEITA score and the cosine distance between the embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>The input data.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>The filtered dataset.</p> Source code in <code>src/distilabel/steps/deita.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Filter the dataset based on the DEITA score and the cosine distance between the\n    embeddings.\n\n    Args:\n        inputs: The input data.\n\n    Returns:\n        The filtered dataset.\n    \"\"\"\n    inputs = self._compute_deita_score(inputs)\n    inputs = self._compute_nearest_neighbor(inputs)\n    inputs.sort(key=lambda x: x[\"deita_score\"], reverse=True)\n\n    selected_rows = []\n    for input in inputs:\n        if len(selected_rows) &gt;= self.data_budget:  # type: ignore\n            break\n        if input[\"nearest_neighbor_distance\"] &gt;= self.diversity_threshold:\n            selected_rows.append(input)\n    yield selected_rows\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.ExpandColumns","title":"<code>ExpandColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Expand columns that contain lists into multiple rows.</p> <p><code>ExpandColumns</code> is a <code>Step</code> that takes a list of columns and expands them into multiple rows. The new rows will have the same data as the original row, except for the expanded column, which will contain a single item from the original list.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>Union[Dict[str, str], List[str]]</code> <p>A dictionary that maps the column to be expanded to the new column name or a list of columns to be expanded. If a list is provided, the new column name will be the same as the column name.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to be expanded into     multiple rows.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> attribute):  The expanded columns.</li> </ul> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>class ExpandColumns(Step):\n    \"\"\"Expand columns that contain lists into multiple rows.\n\n    `ExpandColumns` is a `Step` that takes a list of columns and expands them into multiple\n    rows. The new rows will have the same data as the original row, except for the expanded\n    column, which will contain a single item from the original list.\n\n    Attributes:\n        columns: A dictionary that maps the column to be expanded to the new column name\n            or a list of columns to be expanded. If a list is provided, the new column name\n            will be the same as the column name.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to be expanded into\n            multiple rows.\n\n    Output columns:\n        - dynamic (determined by `columns` attribute):  The expanded columns.\n    \"\"\"\n\n    columns: Union[Dict[str, str], List[str]]\n\n    @field_validator(\"columns\")\n    @classmethod\n    def always_dict(cls, value: Union[Dict[str, str], List[str]]) -&gt; Dict[str, str]:\n        \"\"\"Ensure that the columns are always a dictionary.\n\n        Args:\n            value: The columns to be expanded.\n\n        Returns:\n            The columns to be expanded as a dictionary.\n        \"\"\"\n        if isinstance(value, list):\n            return {col: col for col in value}\n\n        return value\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The columns to be expanded.\"\"\"\n        return list(self.columns.keys())\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The expanded columns.\"\"\"\n        return [\n            new_column if new_column else expand_column\n            for expand_column, new_column in self.columns.items()\n        ]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Expand the columns in the input data.\n\n        Args:\n            inputs: The input data.\n\n        Yields:\n            The expanded rows.\n        \"\"\"\n        yield [row for input in inputs for row in self._expand_columns(input)]\n\n    def _expand_columns(self, input: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Expand the columns in the input data.\n\n        Args:\n            input: The input data.\n\n        Returns:\n            The expanded rows.\n        \"\"\"\n        expanded_rows = []\n        for expand_column, new_column in self.columns.items():  # type: ignore\n            data = input.get(expand_column)\n            rows = []\n            for item, expanded in zip_longest(*[data, expanded_rows], fillvalue=input):\n                rows.append({**expanded, new_column: item})\n            expanded_rows = rows\n        return expanded_rows\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.ExpandColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The columns to be expanded.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.ExpandColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The expanded columns.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.ExpandColumns.always_dict","title":"<code>always_dict(value)</code>  <code>classmethod</code>","text":"<p>Ensure that the columns are always a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[Dict[str, str], List[str]]</code> <p>The columns to be expanded.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>The columns to be expanded as a dictionary.</p> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>@field_validator(\"columns\")\n@classmethod\ndef always_dict(cls, value: Union[Dict[str, str], List[str]]) -&gt; Dict[str, str]:\n    \"\"\"Ensure that the columns are always a dictionary.\n\n    Args:\n        value: The columns to be expanded.\n\n    Returns:\n        The columns to be expanded as a dictionary.\n    \"\"\"\n    if isinstance(value, list):\n        return {col: col for col in value}\n\n    return value\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.ExpandColumns.process","title":"<code>process(inputs)</code>","text":"<p>Expand the columns in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>The input data.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>The expanded rows.</p> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Expand the columns in the input data.\n\n    Args:\n        inputs: The input data.\n\n    Yields:\n        The expanded rows.\n    \"\"\"\n    yield [row for input in inputs for row in self._expand_columns(input)]\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationDPO","title":"<code>FormatChatGenerationDPO</code>","text":"<p>               Bases: <code>Step</code></p> <p>Format the output of a combination of a <code>ChatGeneration</code> + a preference task such as <code>UltraFeedback</code>, for Direct Preference Optimization (DPO) following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p> <p><code>FormatChatGenerationDPO</code> is a <code>Step</code> that formats the output of the combination of a <code>ChatGeneration</code> task with a preference <code>Task</code> i.e. a task generating <code>ratings</code>, so that those are used to rank the existing generations and provide the <code>chosen</code> and <code>rejected</code> generations based on the <code>ratings</code>.</p> Note <p>The <code>messages</code> column should contain at least one message from the user, the <code>generations</code> column should contain at least two generations, the <code>ratings</code> column should contain the same number of ratings as generations.</p> Input columns <ul> <li>messages (<code>List[Dict[str, str]]</code>): The conversation messages.</li> <li>generations (<code>List[str]</code>): The generations produced by the <code>LLM</code>.</li> <li>generation_models (<code>List[str]</code>, optional): The model names used to generate the <code>generations</code>,     only available if the <code>model_name</code> from the <code>ChatGeneration</code> task/s is combined into a single     column named this way, otherwise, it will be ignored.</li> <li>ratings (<code>List[float]</code>): The ratings for each of the <code>generations</code>, produced by a preference     task such as <code>UltraFeedback</code>.</li> </ul> Output columns <ul> <li>prompt (<code>str</code>): The user message used to generate the <code>generations</code> with the <code>LLM</code>.</li> <li>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</li> <li>chosen (<code>List[Dict[str, str]]</code>): The <code>chosen</code> generation based on the <code>ratings</code>.</li> <li>chosen_model (<code>str</code>, optional): The model name used to generate the <code>chosen</code> generation,     if the <code>generation_models</code> are available.</li> <li>chosen_rating (<code>float</code>): The rating of the <code>chosen</code> generation.</li> <li>rejected (<code>List[Dict[str, str]]</code>): The <code>rejected</code> generation based on the <code>ratings</code>.</li> <li>rejected_model (<code>str</code>, optional): The model name used to generate the <code>rejected</code> generation,     if the <code>generation_models</code> are available.</li> <li>rejected_rating (<code>float</code>): The rating of the <code>rejected</code> generation.</li> </ul> Categories <ul> <li>format</li> <li>chat-generation</li> <li>preference</li> <li>messages</li> <li>generations</li> </ul> Source code in <code>src/distilabel/steps/formatting/dpo.py</code> <pre><code>class FormatChatGenerationDPO(Step):\n    \"\"\"Format the output of a combination of a `ChatGeneration` + a preference task such as\n    `UltraFeedback`, for Direct Preference Optimization (DPO) following the standard formatting\n    from frameworks such as `axolotl` or `alignment-handbook`.\n\n    `FormatChatGenerationDPO` is a `Step` that formats the output of the combination of a `ChatGeneration`\n    task with a preference `Task` i.e. a task generating `ratings`, so that those are used to rank the\n    existing generations and provide the `chosen` and `rejected` generations based on the `ratings`.\n\n    Note:\n        The `messages` column should contain at least one message from the user, the `generations`\n        column should contain at least two generations, the `ratings` column should contain the same\n        number of ratings as generations.\n\n    Input columns:\n        - messages (`List[Dict[str, str]]`): The conversation messages.\n        - generations (`List[str]`): The generations produced by the `LLM`.\n        - generation_models (`List[str]`, optional): The model names used to generate the `generations`,\n            only available if the `model_name` from the `ChatGeneration` task/s is combined into a single\n            column named this way, otherwise, it will be ignored.\n        - ratings (`List[float]`): The ratings for each of the `generations`, produced by a preference\n            task such as `UltraFeedback`.\n\n    Output columns:\n        - prompt (`str`): The user message used to generate the `generations` with the `LLM`.\n        - prompt_id (`str`): The `SHA256` hash of the `prompt`.\n        - chosen (`List[Dict[str, str]]`): The `chosen` generation based on the `ratings`.\n        - chosen_model (`str`, optional): The model name used to generate the `chosen` generation,\n            if the `generation_models` are available.\n        - chosen_rating (`float`): The rating of the `chosen` generation.\n        - rejected (`List[Dict[str, str]]`): The `rejected` generation based on the `ratings`.\n        - rejected_model (`str`, optional): The model name used to generate the `rejected` generation,\n            if the `generation_models` are available.\n        - rejected_rating (`float`): The rating of the `rejected` generation.\n\n    Categories:\n        - format\n        - chat-generation\n        - preference\n        - messages\n        - generations\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of inputs required by the `Step`, which in this case are: `messages`, `generations`,\n        and `ratings`.\"\"\"\n        return [\"messages\", \"generations\", \"ratings\"]\n\n    @property\n    def optional_inputs(self) -&gt; List[str]:\n        \"\"\"List of optional inputs, which are not required by the `Step` but used if available,\n        which in this case is: `generation_models`.\"\"\"\n        return [\"generation_models\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of outputs generated by the `Step`, which are: `prompt`, `prompt_id`, `chosen`,\n        `chosen_model`, `chosen_rating`, `rejected`, `rejected_model`, `rejected_rating`. Both\n        the `chosen_model` and `rejected_model` being optional and only used if `generation_models`\n        is available.\n\n        Reference:\n            - Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n        \"\"\"\n        return [\n            \"prompt\",\n            \"prompt_id\",\n            \"chosen\",\n            \"chosen_model\",\n            \"chosen_rating\",\n            \"rejected\",\n            \"rejected_model\",\n            \"rejected_rating\",\n        ]\n\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n        according to the DPO formatting standard.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with batches of formatted `StepInput` following the DPO standard.\n        \"\"\"\n        for input in inputs:\n            for item in input:\n                item[\"prompt\"] = next(\n                    (\n                        turn[\"content\"]\n                        for turn in item[\"messages\"]\n                        if turn[\"role\"] == \"user\"\n                    ),\n                    None,\n                )\n                item[\"prompt_id\"] = hashlib.sha256(\n                    item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n                ).hexdigest()\n\n                chosen_idx = max(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n                item[\"chosen\"] = item[\"messages\"] + [\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": item[\"generations\"][chosen_idx],\n                    }\n                ]\n                if \"generation_models\" in item:\n                    item[\"chosen_model\"] = item[\"generation_models\"][chosen_idx]\n                item[\"chosen_rating\"] = item[\"ratings\"][chosen_idx]\n\n                rejected_idx = min(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n                item[\"rejected\"] = item[\"messages\"] + [\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": item[\"generations\"][rejected_idx],\n                    }\n                ]\n                if \"generation_models\" in item:\n                    item[\"rejected_model\"] = item[\"generation_models\"][rejected_idx]\n                item[\"rejected_rating\"] = item[\"ratings\"][rejected_idx]\n\n            yield input\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationDPO.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of inputs required by the <code>Step</code>, which in this case are: <code>messages</code>, <code>generations</code>, and <code>ratings</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationDPO.optional_inputs","title":"<code>optional_inputs: List[str]</code>  <code>property</code>","text":"<p>List of optional inputs, which are not required by the <code>Step</code> but used if available, which in this case is: <code>generation_models</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationDPO.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of outputs generated by the <code>Step</code>, which are: <code>prompt</code>, <code>prompt_id</code>, <code>chosen</code>, <code>chosen_model</code>, <code>chosen_rating</code>, <code>rejected</code>, <code>rejected_model</code>, <code>rejected_rating</code>. Both the <code>chosen_model</code> and <code>rejected_model</code> being optional and only used if <code>generation_models</code> is available.</p> Reference <ul> <li>Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</li> </ul>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationDPO.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method formats the received <code>StepInput</code> or list of <code>StepInput</code> according to the DPO formatting standard.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with batches of formatted <code>StepInput</code> following the DPO standard.</p> Source code in <code>src/distilabel/steps/formatting/dpo.py</code> <pre><code>def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n    according to the DPO formatting standard.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with batches of formatted `StepInput` following the DPO standard.\n    \"\"\"\n    for input in inputs:\n        for item in input:\n            item[\"prompt\"] = next(\n                (\n                    turn[\"content\"]\n                    for turn in item[\"messages\"]\n                    if turn[\"role\"] == \"user\"\n                ),\n                None,\n            )\n            item[\"prompt_id\"] = hashlib.sha256(\n                item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            chosen_idx = max(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n            item[\"chosen\"] = item[\"messages\"] + [\n                {\n                    \"role\": \"assistant\",\n                    \"content\": item[\"generations\"][chosen_idx],\n                }\n            ]\n            if \"generation_models\" in item:\n                item[\"chosen_model\"] = item[\"generation_models\"][chosen_idx]\n            item[\"chosen_rating\"] = item[\"ratings\"][chosen_idx]\n\n            rejected_idx = min(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n            item[\"rejected\"] = item[\"messages\"] + [\n                {\n                    \"role\": \"assistant\",\n                    \"content\": item[\"generations\"][rejected_idx],\n                }\n            ]\n            if \"generation_models\" in item:\n                item[\"rejected_model\"] = item[\"generation_models\"][rejected_idx]\n            item[\"rejected_rating\"] = item[\"ratings\"][rejected_idx]\n\n        yield input\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationSFT","title":"<code>FormatChatGenerationSFT</code>","text":"<p>               Bases: <code>Step</code></p> <p>Format the output of a <code>ChatGeneration</code> task for Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p> <p><code>FormatChatGenerationSFT</code> is a <code>Step</code> that formats the output of a <code>ChatGeneration</code> task for Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>. The output of the <code>ChatGeneration</code> task is formatted into a chat-like conversation with the <code>instruction</code> as the user message and the <code>generation</code> as the assistant message. Optionally, if the <code>system_prompt</code> is available, it is included as the first message in the conversation.</p> Input columns <ul> <li>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the     <code>generation</code>, if available.</li> <li>instruction (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</li> <li>generation (<code>str</code>): The generation produced by the <code>LLM</code>.</li> </ul> Output columns <ul> <li>prompt (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</li> <li>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</li> <li>messages (<code>List[Dict[str, str]]</code>): The chat-like conversation with the <code>instruction</code> as     the user message and the <code>generation</code> as the assistant message.</li> </ul> Categories <ul> <li>format</li> <li>chat-generation</li> <li>instruction</li> <li>generation</li> </ul> Source code in <code>src/distilabel/steps/formatting/sft.py</code> <pre><code>class FormatChatGenerationSFT(Step):\n    \"\"\"Format the output of a `ChatGeneration` task for Supervised Fine-Tuning (SFT) following the\n    standard formatting from frameworks such as `axolotl` or `alignment-handbook`.\n\n    `FormatChatGenerationSFT` is a `Step` that formats the output of a `ChatGeneration` task for\n    Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as `axolotl`\n    or `alignment-handbook`. The output of the `ChatGeneration` task is formatted into a chat-like\n    conversation with the `instruction` as the user message and the `generation` as the assistant\n    message. Optionally, if the `system_prompt` is available, it is included as the first message\n    in the conversation.\n\n    Input columns:\n        - system_prompt (`str`, optional): The system prompt used within the `LLM` to generate the\n            `generation`, if available.\n        - instruction (`str`): The instruction used to generate the `generation` with the `LLM`.\n        - generation (`str`): The generation produced by the `LLM`.\n\n    Output columns:\n        - prompt (`str`): The instruction used to generate the `generation` with the `LLM`.\n        - prompt_id (`str`): The `SHA256` hash of the `prompt`.\n        - messages (`List[Dict[str, str]]`): The chat-like conversation with the `instruction` as\n            the user message and the `generation` as the assistant message.\n\n    Categories:\n        - format\n        - chat-generation\n        - instruction\n        - generation\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of inputs required by the `Step`, which in this case are: `instruction`, and `generation`.\"\"\"\n        return [\"messages\", \"generation\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of outputs generated by the `Step`, which are: `prompt`, `prompt_id`, `messages`.\n\n        Reference:\n            - Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n        \"\"\"\n        return [\"prompt\", \"prompt_id\", \"messages\"]\n\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n        according to the SFT formatting standard.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with batches of formatted `StepInput` following the SFT standard.\n        \"\"\"\n        for input in inputs:\n            for item in input:\n                item[\"prompt\"] = next(\n                    (\n                        turn[\"content\"]\n                        for turn in item[\"messages\"]\n                        if turn[\"role\"] == \"user\"\n                    ),\n                    None,\n                )\n\n                item[\"prompt_id\"] = hashlib.sha256(\n                    item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n                ).hexdigest()\n\n                item[\"messages\"] = item[\"messages\"] + [\n                    {\"role\": \"assistant\", \"content\": item[\"generation\"]},  # type: ignore\n                ]\n            yield input\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationSFT.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of inputs required by the <code>Step</code>, which in this case are: <code>instruction</code>, and <code>generation</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationSFT.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of outputs generated by the <code>Step</code>, which are: <code>prompt</code>, <code>prompt_id</code>, <code>messages</code>.</p> Reference <ul> <li>Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</li> </ul>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatChatGenerationSFT.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method formats the received <code>StepInput</code> or list of <code>StepInput</code> according to the SFT formatting standard.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with batches of formatted <code>StepInput</code> following the SFT standard.</p> Source code in <code>src/distilabel/steps/formatting/sft.py</code> <pre><code>def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n    according to the SFT formatting standard.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with batches of formatted `StepInput` following the SFT standard.\n    \"\"\"\n    for input in inputs:\n        for item in input:\n            item[\"prompt\"] = next(\n                (\n                    turn[\"content\"]\n                    for turn in item[\"messages\"]\n                    if turn[\"role\"] == \"user\"\n                ),\n                None,\n            )\n\n            item[\"prompt_id\"] = hashlib.sha256(\n                item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            item[\"messages\"] = item[\"messages\"] + [\n                {\"role\": \"assistant\", \"content\": item[\"generation\"]},  # type: ignore\n            ]\n        yield input\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationDPO","title":"<code>FormatTextGenerationDPO</code>","text":"<p>               Bases: <code>Step</code></p> <p>Format the output of your LLMs for Direct Preference Optimization (DPO).</p> <p><code>FormatTextGenerationDPO</code> is a <code>Step</code> that formats the output of the combination of a <code>TextGeneration</code> task with a preference <code>Task</code> i.e. a task generating <code>ratings</code>, so that those are used to rank the existing generations and provide the <code>chosen</code> and <code>rejected</code> generations based on the <code>ratings</code>. Use this step to transform the output of a combination of a <code>TextGeneration</code> + a preference task such as <code>UltraFeedback</code> following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p> Note <p>The <code>generations</code> column should contain at least two generations, the <code>ratings</code> column should contain the same number of ratings as generations.</p> Input columns <ul> <li>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the     <code>generations</code>, if available.</li> <li>instruction (<code>str</code>): The instruction used to generate the <code>generations</code> with the <code>LLM</code>.</li> <li>generations (<code>List[str]</code>): The generations produced by the <code>LLM</code>.</li> <li>generation_models (<code>List[str]</code>, optional): The model names used to generate the <code>generations</code>,     only available if the <code>model_name</code> from the <code>TextGeneration</code> task/s is combined into a single     column named this way, otherwise, it will be ignored.</li> <li>ratings (<code>List[float]</code>): The ratings for each of the <code>generations</code>, produced by a preference     task such as <code>UltraFeedback</code>.</li> </ul> Output columns <ul> <li>prompt (<code>str</code>): The instruction used to generate the <code>generations</code> with the <code>LLM</code>.</li> <li>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</li> <li>chosen (<code>List[Dict[str, str]]</code>): The <code>chosen</code> generation based on the <code>ratings</code>.</li> <li>chosen_model (<code>str</code>, optional): The model name used to generate the <code>chosen</code> generation,     if the <code>generation_models</code> are available.</li> <li>chosen_rating (<code>float</code>): The rating of the <code>chosen</code> generation.</li> <li>rejected (<code>List[Dict[str, str]]</code>): The <code>rejected</code> generation based on the <code>ratings</code>.</li> <li>rejected_model (<code>str</code>, optional): The model name used to generate the <code>rejected</code> generation,     if the <code>generation_models</code> are available.</li> <li>rejected_rating (<code>float</code>): The rating of the <code>rejected</code> generation.</li> </ul> Categories <ul> <li>format</li> <li>text-generation</li> <li>preference</li> <li>instruction</li> <li>generations</li> </ul> Source code in <code>src/distilabel/steps/formatting/dpo.py</code> <pre><code>class FormatTextGenerationDPO(Step):\n    \"\"\"Format the output of your LLMs for Direct Preference Optimization (DPO).\n\n    `FormatTextGenerationDPO` is a `Step` that formats the output of the combination of a `TextGeneration`\n    task with a preference `Task` i.e. a task generating `ratings`, so that those are used to rank the\n    existing generations and provide the `chosen` and `rejected` generations based on the `ratings`.\n    Use this step to transform the output of a combination of a `TextGeneration` + a preference task such as\n    `UltraFeedback` following the standard formatting from frameworks such as `axolotl` or `alignment-handbook`.\n\n    Note:\n        The `generations` column should contain at least two generations, the `ratings` column should\n        contain the same number of ratings as generations.\n\n    Input columns:\n        - system_prompt (`str`, optional): The system prompt used within the `LLM` to generate the\n            `generations`, if available.\n        - instruction (`str`): The instruction used to generate the `generations` with the `LLM`.\n        - generations (`List[str]`): The generations produced by the `LLM`.\n        - generation_models (`List[str]`, optional): The model names used to generate the `generations`,\n            only available if the `model_name` from the `TextGeneration` task/s is combined into a single\n            column named this way, otherwise, it will be ignored.\n        - ratings (`List[float]`): The ratings for each of the `generations`, produced by a preference\n            task such as `UltraFeedback`.\n\n    Output columns:\n        - prompt (`str`): The instruction used to generate the `generations` with the `LLM`.\n        - prompt_id (`str`): The `SHA256` hash of the `prompt`.\n        - chosen (`List[Dict[str, str]]`): The `chosen` generation based on the `ratings`.\n        - chosen_model (`str`, optional): The model name used to generate the `chosen` generation,\n            if the `generation_models` are available.\n        - chosen_rating (`float`): The rating of the `chosen` generation.\n        - rejected (`List[Dict[str, str]]`): The `rejected` generation based on the `ratings`.\n        - rejected_model (`str`, optional): The model name used to generate the `rejected` generation,\n            if the `generation_models` are available.\n        - rejected_rating (`float`): The rating of the `rejected` generation.\n\n    Categories:\n        - format\n        - text-generation\n        - preference\n        - instruction\n        - generations\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of inputs required by the `Step`, which in this case are: `instruction`, `generations`,\n        and `ratings`.\"\"\"\n        return [\"instruction\", \"generations\", \"ratings\"]\n\n    @property\n    def optional_inputs(self) -&gt; List[str]:\n        \"\"\"List of optional inputs, which are not required by the `Step` but used if available,\n        which in this case are: `system_prompt`, and `generation_models`.\"\"\"\n        return [\"system_prompt\", \"generation_models\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of outputs generated by the `Step`, which are: `prompt`, `prompt_id`, `chosen`,\n        `chosen_model`, `chosen_rating`, `rejected`, `rejected_model`, `rejected_rating`. Both\n        the `chosen_model` and `rejected_model` being optional and only used if `generation_models`\n        is available.\n\n        Reference:\n            - Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n        \"\"\"\n        return [\n            \"prompt\",\n            \"prompt_id\",\n            \"chosen\",\n            \"chosen_model\",\n            \"chosen_rating\",\n            \"rejected\",\n            \"rejected_model\",\n            \"rejected_rating\",\n        ]\n\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n        according to the DPO formatting standard.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with batches of formatted `StepInput` following the DPO standard.\n        \"\"\"\n        for input in inputs:\n            for item in input:\n                messages = [\n                    {\"role\": \"user\", \"content\": item[\"instruction\"]},  # type: ignore\n                ]\n                if (\n                    \"system_prompt\" in item\n                    and isinstance(item[\"system_prompt\"], str)  # type: ignore\n                    and len(item[\"system_prompt\"]) &gt; 0  # type: ignore\n                ):\n                    messages.insert(\n                        0,\n                        {\"role\": \"system\", \"content\": item[\"system_prompt\"]},  # type: ignore\n                    )\n\n                item[\"prompt\"] = item[\"instruction\"]\n                item[\"prompt_id\"] = hashlib.sha256(\n                    item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n                ).hexdigest()\n\n                chosen_idx = max(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n                item[\"chosen\"] = messages + [\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": item[\"generations\"][chosen_idx],\n                    }\n                ]\n                if \"generation_models\" in item:\n                    item[\"chosen_model\"] = item[\"generation_models\"][chosen_idx]\n                item[\"chosen_rating\"] = item[\"ratings\"][chosen_idx]\n\n                rejected_idx = min(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n                item[\"rejected\"] = messages + [\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": item[\"generations\"][rejected_idx],\n                    }\n                ]\n                if \"generation_models\" in item:\n                    item[\"rejected_model\"] = item[\"generation_models\"][rejected_idx]\n                item[\"rejected_rating\"] = item[\"ratings\"][rejected_idx]\n\n            yield input\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationDPO.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of inputs required by the <code>Step</code>, which in this case are: <code>instruction</code>, <code>generations</code>, and <code>ratings</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationDPO.optional_inputs","title":"<code>optional_inputs: List[str]</code>  <code>property</code>","text":"<p>List of optional inputs, which are not required by the <code>Step</code> but used if available, which in this case are: <code>system_prompt</code>, and <code>generation_models</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationDPO.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of outputs generated by the <code>Step</code>, which are: <code>prompt</code>, <code>prompt_id</code>, <code>chosen</code>, <code>chosen_model</code>, <code>chosen_rating</code>, <code>rejected</code>, <code>rejected_model</code>, <code>rejected_rating</code>. Both the <code>chosen_model</code> and <code>rejected_model</code> being optional and only used if <code>generation_models</code> is available.</p> Reference <ul> <li>Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</li> </ul>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationDPO.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method formats the received <code>StepInput</code> or list of <code>StepInput</code> according to the DPO formatting standard.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with batches of formatted <code>StepInput</code> following the DPO standard.</p> Source code in <code>src/distilabel/steps/formatting/dpo.py</code> <pre><code>def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n    according to the DPO formatting standard.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with batches of formatted `StepInput` following the DPO standard.\n    \"\"\"\n    for input in inputs:\n        for item in input:\n            messages = [\n                {\"role\": \"user\", \"content\": item[\"instruction\"]},  # type: ignore\n            ]\n            if (\n                \"system_prompt\" in item\n                and isinstance(item[\"system_prompt\"], str)  # type: ignore\n                and len(item[\"system_prompt\"]) &gt; 0  # type: ignore\n            ):\n                messages.insert(\n                    0,\n                    {\"role\": \"system\", \"content\": item[\"system_prompt\"]},  # type: ignore\n                )\n\n            item[\"prompt\"] = item[\"instruction\"]\n            item[\"prompt_id\"] = hashlib.sha256(\n                item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            chosen_idx = max(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n            item[\"chosen\"] = messages + [\n                {\n                    \"role\": \"assistant\",\n                    \"content\": item[\"generations\"][chosen_idx],\n                }\n            ]\n            if \"generation_models\" in item:\n                item[\"chosen_model\"] = item[\"generation_models\"][chosen_idx]\n            item[\"chosen_rating\"] = item[\"ratings\"][chosen_idx]\n\n            rejected_idx = min(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n            item[\"rejected\"] = messages + [\n                {\n                    \"role\": \"assistant\",\n                    \"content\": item[\"generations\"][rejected_idx],\n                }\n            ]\n            if \"generation_models\" in item:\n                item[\"rejected_model\"] = item[\"generation_models\"][rejected_idx]\n            item[\"rejected_rating\"] = item[\"ratings\"][rejected_idx]\n\n        yield input\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationSFT","title":"<code>FormatTextGenerationSFT</code>","text":"<p>               Bases: <code>Step</code></p> <p>Format the output of a <code>TextGeneration</code> task for Supervised Fine-Tuning (SFT).</p> <p><code>FormatTextGenerationSFT</code> is a <code>Step</code> that formats the output of a <code>TextGeneration</code> task for Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>. The output of the <code>TextGeneration</code> task is formatted into a chat-like conversation with the <code>instruction</code> as the user message and the <code>generation</code> as the assistant message. Optionally, if the <code>system_prompt</code> is available, it is included as the first message in the conversation.</p> Input columns <ul> <li>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the     <code>generation</code>, if available.</li> <li>instruction (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</li> <li>generation (<code>str</code>): The generation produced by the <code>LLM</code>.</li> </ul> Output columns <ul> <li>prompt (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</li> <li>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</li> <li>messages (<code>List[Dict[str, str]]</code>): The chat-like conversation with the <code>instruction</code> as     the user message and the <code>generation</code> as the assistant message.</li> </ul> Categories <ul> <li>format</li> <li>text-generation</li> <li>instruction</li> <li>generation</li> </ul> Source code in <code>src/distilabel/steps/formatting/sft.py</code> <pre><code>class FormatTextGenerationSFT(Step):\n    \"\"\"Format the output of a `TextGeneration` task for Supervised Fine-Tuning (SFT).\n\n    `FormatTextGenerationSFT` is a `Step` that formats the output of a `TextGeneration` task for\n    Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as `axolotl`\n    or `alignment-handbook`. The output of the `TextGeneration` task is formatted into a chat-like\n    conversation with the `instruction` as the user message and the `generation` as the assistant\n    message. Optionally, if the `system_prompt` is available, it is included as the first message\n    in the conversation.\n\n    Input columns:\n        - system_prompt (`str`, optional): The system prompt used within the `LLM` to generate the\n            `generation`, if available.\n        - instruction (`str`): The instruction used to generate the `generation` with the `LLM`.\n        - generation (`str`): The generation produced by the `LLM`.\n\n    Output columns:\n        - prompt (`str`): The instruction used to generate the `generation` with the `LLM`.\n        - prompt_id (`str`): The `SHA256` hash of the `prompt`.\n        - messages (`List[Dict[str, str]]`): The chat-like conversation with the `instruction` as\n            the user message and the `generation` as the assistant message.\n\n    Categories:\n        - format\n        - text-generation\n        - instruction\n        - generation\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of inputs required by the `Step`, which in this case are: `instruction`, and `generation`.\"\"\"\n        return [\"instruction\", \"generation\"]\n\n    @property\n    def optional_inputs(self) -&gt; List[str]:\n        \"\"\"List of optional inputs, which are not required by the `Step` but used if available,\n        which in this case is: `system_prompt`.\"\"\"\n        return [\"system_prompt\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of outputs generated by the `Step`, which are: `prompt`, `prompt_id`, `messages`.\n\n        Reference:\n            - Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n        \"\"\"\n        return [\"prompt\", \"prompt_id\", \"messages\"]\n\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n        according to the SFT formatting standard.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with batches of formatted `StepInput` following the SFT standard.\n        \"\"\"\n        for input in inputs:\n            for item in input:\n                item[\"prompt\"] = item[\"instruction\"]\n\n                item[\"prompt_id\"] = hashlib.sha256(\n                    item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n                ).hexdigest()\n\n                item[\"messages\"] = [\n                    {\"role\": \"user\", \"content\": item[\"instruction\"]},  # type: ignore\n                    {\"role\": \"assistant\", \"content\": item[\"generation\"]},  # type: ignore\n                ]\n                if (\n                    \"system_prompt\" in item\n                    and isinstance(item[\"system_prompt\"], str)  # type: ignore\n                    and len(item[\"system_prompt\"]) &gt; 0  # type: ignore\n                ):\n                    item[\"messages\"].insert(\n                        0,\n                        {\"role\": \"system\", \"content\": item[\"system_prompt\"]},  # type: ignore\n                    )\n\n            yield input\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationSFT.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of inputs required by the <code>Step</code>, which in this case are: <code>instruction</code>, and <code>generation</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationSFT.optional_inputs","title":"<code>optional_inputs: List[str]</code>  <code>property</code>","text":"<p>List of optional inputs, which are not required by the <code>Step</code> but used if available, which in this case is: <code>system_prompt</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationSFT.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of outputs generated by the <code>Step</code>, which are: <code>prompt</code>, <code>prompt_id</code>, <code>messages</code>.</p> Reference <ul> <li>Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</li> </ul>"},{"location":"reference/distilabel/steps/#distilabel.steps.FormatTextGenerationSFT.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method formats the received <code>StepInput</code> or list of <code>StepInput</code> according to the SFT formatting standard.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with batches of formatted <code>StepInput</code> following the SFT standard.</p> Source code in <code>src/distilabel/steps/formatting/sft.py</code> <pre><code>def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n    according to the SFT formatting standard.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with batches of formatted `StepInput` following the SFT standard.\n    \"\"\"\n    for input in inputs:\n        for item in input:\n            item[\"prompt\"] = item[\"instruction\"]\n\n            item[\"prompt_id\"] = hashlib.sha256(\n                item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            item[\"messages\"] = [\n                {\"role\": \"user\", \"content\": item[\"instruction\"]},  # type: ignore\n                {\"role\": \"assistant\", \"content\": item[\"generation\"]},  # type: ignore\n            ]\n            if (\n                \"system_prompt\" in item\n                and isinstance(item[\"system_prompt\"], str)  # type: ignore\n                and len(item[\"system_prompt\"]) &gt; 0  # type: ignore\n            ):\n                item[\"messages\"].insert(\n                    0,\n                    {\"role\": \"system\", \"content\": item[\"system_prompt\"]},  # type: ignore\n                )\n\n        yield input\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.GeneratorStep","title":"<code>GeneratorStep</code>","text":"<p>               Bases: <code>_Step</code>, <code>ABC</code></p> <p>A special kind of <code>Step</code> that is able to generate data i.e. it doesn't receive any input from the previous steps.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>RuntimeParameter[int]</code> <p>The number of rows that will contain the batches generated by the step. Defaults to <code>50</code>.</p> Runtime parameters <ul> <li><code>batch_size</code>: The number of rows that will contain the batches generated by     the step. Defaults to <code>50</code>.</li> </ul> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class GeneratorStep(_Step, ABC):\n    \"\"\"A special kind of `Step` that is able to generate data i.e. it doesn't receive\n    any input from the previous steps.\n\n    Attributes:\n        batch_size: The number of rows that will contain the batches generated by the\n            step. Defaults to `50`.\n\n    Runtime parameters:\n        - `batch_size`: The number of rows that will contain the batches generated by\n            the step. Defaults to `50`.\n    \"\"\"\n\n    batch_size: RuntimeParameter[int] = Field(\n        default=50,\n        description=\"The number of rows that will contain the batches generated by the\"\n        \" step.\",\n    )\n\n    @abstractmethod\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n        \"\"\"Method that defines the generation logic of the step. It should yield the\n        output rows and a boolean indicating if it's the last batch or not.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            The output rows and a boolean indicating if it's the last batch or not.\n        \"\"\"\n        pass\n\n    def process_applying_mappings(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n        \"\"\"Runs the `process` method of the step applying the `outputs_mappings` to the\n        output rows. This is the function that should be used to run the generation logic\n        of the step.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            The output rows and a boolean indicating if it's the last batch or not.\n        \"\"\"\n\n        # If the `Step` was built using the `@step` decorator, then we need to pass\n        # the runtime parameters as `kwargs`, so they can be used within the processing\n        # function\n        generator = (\n            self.process(offset=offset)\n            if not self._built_from_decorator\n            else self.process(offset=offset, **self._runtime_parameters)\n        )\n\n        for output_rows, last_batch in generator:\n            yield (\n                [\n                    {self.output_mappings.get(k, k): v for k, v in row.items()}\n                    for row in output_rows\n                ],\n                last_batch,\n            )\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.GeneratorStep.process","title":"<code>process(offset=0)</code>  <code>abstractmethod</code>","text":"<p>Method that defines the generation logic of the step. It should yield the output rows and a boolean indicating if it's the last batch or not.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>The output rows and a boolean indicating if it's the last batch or not.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>@abstractmethod\ndef process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n    \"\"\"Method that defines the generation logic of the step. It should yield the\n    output rows and a boolean indicating if it's the last batch or not.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        The output rows and a boolean indicating if it's the last batch or not.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.GeneratorStep.process_applying_mappings","title":"<code>process_applying_mappings(offset=0)</code>","text":"<p>Runs the <code>process</code> method of the step applying the <code>outputs_mappings</code> to the output rows. This is the function that should be used to run the generation logic of the step.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>The output rows and a boolean indicating if it's the last batch or not.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def process_applying_mappings(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n    \"\"\"Runs the `process` method of the step applying the `outputs_mappings` to the\n    output rows. This is the function that should be used to run the generation logic\n    of the step.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        The output rows and a boolean indicating if it's the last batch or not.\n    \"\"\"\n\n    # If the `Step` was built using the `@step` decorator, then we need to pass\n    # the runtime parameters as `kwargs`, so they can be used within the processing\n    # function\n    generator = (\n        self.process(offset=offset)\n        if not self._built_from_decorator\n        else self.process(offset=offset, **self._runtime_parameters)\n    )\n\n    for output_rows, last_batch in generator:\n        yield (\n            [\n                {self.output_mappings.get(k, k): v for k, v in row.items()}\n                for row in output_rows\n            ],\n            last_batch,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.GlobalStep","title":"<code>GlobalStep</code>","text":"<p>               Bases: <code>Step</code>, <code>ABC</code></p> <p>A special kind of <code>Step</code> which it's <code>process</code> method receives all the data processed by their previous steps at once, instead of receiving it in batches. This kind of steps are useful when the processing logic requires to have all the data at once, for example to train a model, to perform a global aggregation, etc.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class GlobalStep(Step, ABC):\n    \"\"\"A special kind of `Step` which it's `process` method receives all the data processed\n    by their previous steps at once, instead of receiving it in batches. This kind of steps\n    are useful when the processing logic requires to have all the data at once, for example\n    to train a model, to perform a global aggregation, etc.\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        return []\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return []\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.KeepColumns","title":"<code>KeepColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Keeps selected columns in the dataset.</p> <p><code>KeepColumns</code> is a <code>Step</code> that implements the <code>process</code> method that keeps only the columns specified in the <code>columns</code> attribute. Also <code>KeepColumns</code> provides an attribute <code>columns</code> to specify the columns to keep which will override the default value for the properties <code>inputs</code> and <code>outputs</code>.</p> Note <p>The order in which the columns are provided is important, as the output will be sorted using the provided order, which is useful before pushing either a <code>dataset.Dataset</code> via the <code>PushToHub</code> step or a <code>distilabel.Distiset</code> via the <code>Pipeline.run</code> output variable.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[str]</code> <p>List of strings with the names of the columns to keep.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to keep.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns that were kept.</li> </ul> Source code in <code>src/distilabel/steps/keep.py</code> <pre><code>class KeepColumns(Step):\n    \"\"\"Keeps selected columns in the dataset.\n\n    `KeepColumns` is a `Step` that implements the `process` method that keeps only the columns\n    specified in the `columns` attribute. Also `KeepColumns` provides an attribute `columns` to\n    specify the columns to keep which will override the default value for the properties `inputs`\n    and `outputs`.\n\n    Note:\n        The order in which the columns are provided is important, as the output will be sorted\n        using the provided order, which is useful before pushing either a `dataset.Dataset` via\n        the `PushToHub` step or a `distilabel.Distiset` via the `Pipeline.run` output variable.\n\n    Attributes:\n        columns: List of strings with the names of the columns to keep.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to keep.\n\n    Output columns:\n        - dynamic (determined by `columns` attribute): The columns that were kept.\n    \"\"\"\n\n    columns: List[str]\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @override\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"The `process` method keeps only the columns specified in the `columns` attribute.\n\n        Args:\n            *inputs: A list of dictionaries with the input data.\n\n        Yields:\n            A list of dictionaries with the output data.\n        \"\"\"\n        for input in inputs:\n            outputs = []\n            for item in input:\n                outputs.append({col: item[col] for col in self.columns})\n            yield outputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.KeepColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the column names in <code>columns</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.KeepColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the column names in <code>columns</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.KeepColumns.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method keeps only the columns specified in the <code>columns</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of dictionaries with the input data.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of dictionaries with the output data.</p> Source code in <code>src/distilabel/steps/keep.py</code> <pre><code>@override\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"The `process` method keeps only the columns specified in the `columns` attribute.\n\n    Args:\n        *inputs: A list of dictionaries with the input data.\n\n    Yields:\n        A list of dictionaries with the output data.\n    \"\"\"\n    for input in inputs:\n        outputs = []\n        for item in input:\n            outputs.append({col: item[col] for col in self.columns})\n        yield outputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromDicts","title":"<code>LoadDataFromDicts</code>","text":"<p>               Bases: <code>GeneratorStep</code></p> <p>Loads a dataset from a list of dictionaries.</p> <p><code>GeneratorStep</code> that loads a dataset from a list of dictionaries and yields it in batches.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>List[Dict[str, Any]]</code> <p>The list of dictionaries to load the data from.</p> Runtime parameters <ul> <li><code>batch_size</code>: The batch size to use when processing the data.</li> </ul> Output columns <ul> <li>dynamic (based on the keys found on the first dictionary of the list): The columns     of the dataset.</li> </ul> Categories <ul> <li>load</li> </ul> Source code in <code>src/distilabel/steps/generators/data.py</code> <pre><code>class LoadDataFromDicts(GeneratorStep):\n    \"\"\"Loads a dataset from a list of dictionaries.\n\n    `GeneratorStep` that loads a dataset from a list of dictionaries and yields it in\n    batches.\n\n    Attributes:\n        data: The list of dictionaries to load the data from.\n\n    Runtime parameters:\n        - `batch_size`: The batch size to use when processing the data.\n\n    Output columns:\n        - dynamic (based on the keys found on the first dictionary of the list): The columns\n            of the dataset.\n\n    Categories:\n        - load\n    \"\"\"\n\n    data: List[Dict[str, Any]]\n\n    @override\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n        \"\"\"Yields batches from a list of dictionaries.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to `0`.\n\n        Yields:\n            A list of Python dictionaries as read from the inputs (propagated in batches)\n            and a flag indicating whether the yield batch is the last one.\n        \"\"\"\n        if offset:\n            self.data = self.data[offset:]\n\n        while self.data:\n            batch = self.data[: self.batch_size]\n            self.data = self.data[self.batch_size :]\n            yield (\n                batch,\n                True if len(self.data) == 0 else False,\n            )\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"Returns a list of strings with the names of the columns that the step will generate.\"\"\"\n        return list(self.data[0].keys())\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromDicts.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>Returns a list of strings with the names of the columns that the step will generate.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromDicts.process","title":"<code>process(offset=0)</code>","text":"<p>Yields batches from a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to <code>0</code>.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>A list of Python dictionaries as read from the inputs (propagated in batches)</p> <code>GeneratorStepOutput</code> <p>and a flag indicating whether the yield batch is the last one.</p> Source code in <code>src/distilabel/steps/generators/data.py</code> <pre><code>@override\ndef process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n    \"\"\"Yields batches from a list of dictionaries.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to `0`.\n\n    Yields:\n        A list of Python dictionaries as read from the inputs (propagated in batches)\n        and a flag indicating whether the yield batch is the last one.\n    \"\"\"\n    if offset:\n        self.data = self.data[offset:]\n\n    while self.data:\n        batch = self.data[: self.batch_size]\n        self.data = self.data[self.batch_size :]\n        yield (\n            batch,\n            True if len(self.data) == 0 else False,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromDisk","title":"<code>LoadDataFromDisk</code>","text":"<p>               Bases: <code>LoadDataFromHub</code></p> <p>Load a dataset that was previously saved to disk.</p> <p>If you previously saved your dataset using the <code>save_to_disk</code> method, or <code>Distiset.save_to_disk</code> you can load it again to build a new pipeline using this class.</p> <p>Attributes:</p> Name Type Description <code>dataset_path</code> <code>RuntimeParameter[Union[str, Path]]</code> <p>The path to the dataset or distiset.</p> <code>split</code> <code>Optional[RuntimeParameter[str]]</code> <p>The split of the dataset to load (typically will be <code>train</code>, <code>test</code> or <code>validation</code>).</p> <code>config</code> <code>RuntimeParameter[str]</code> <p>The configuration of the dataset to load. This is optional and only needed if the dataset has multiple configurations.</p> Runtime parameters <ul> <li><code>batch_size</code>: The batch size to use when processing the data.</li> <li><code>dataset_path</code>: The path to the dataset or distiset.</li> <li><code>is_distiset</code>: Whether the dataset to load is a <code>Distiset</code> or not. Defaults to False.</li> <li><code>split</code>: The split of the dataset to load. Defaults to 'train'.</li> <li><code>config</code>: The configuration of the dataset to load. This is optional and only     needed if the dataset has multiple configurations.</li> <li><code>num_examples</code>: The number of examples to load from the dataset.     By default will load all examples.</li> <li><code>storage_options</code>: Key/value pairs to be passed on to the file-system backend, if any.     Defaults to <code>None</code>.</li> </ul> Output columns <ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the     datasets loaded from the Hugging Face Hub.</li> </ul> Categories <ul> <li>load</li> </ul> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>class LoadDataFromDisk(LoadDataFromHub):\n    \"\"\"Load a dataset that was previously saved to disk.\n\n    If you previously saved your dataset using the `save_to_disk` method, or\n    `Distiset.save_to_disk` you can load it again to build a new pipeline using this class.\n\n    Attributes:\n        dataset_path: The path to the dataset or distiset.\n        split: The split of the dataset to load (typically will be `train`, `test` or `validation`).\n        config: The configuration of the dataset to load. This is optional and only needed\n            if the dataset has multiple configurations.\n\n    Runtime parameters:\n        - `batch_size`: The batch size to use when processing the data.\n        - `dataset_path`: The path to the dataset or distiset.\n        - `is_distiset`: Whether the dataset to load is a `Distiset` or not. Defaults to False.\n        - `split`: The split of the dataset to load. Defaults to 'train'.\n        - `config`: The configuration of the dataset to load. This is optional and only\n            needed if the dataset has multiple configurations.\n        - `num_examples`: The number of examples to load from the dataset.\n            By default will load all examples.\n        - `storage_options`: Key/value pairs to be passed on to the file-system backend, if any.\n            Defaults to `None`.\n\n    Output columns:\n        - dynamic (`all`): The columns that will be generated by this step, based on the\n            datasets loaded from the Hugging Face Hub.\n\n    Categories:\n        - load\n    \"\"\"\n\n    dataset_path: RuntimeParameter[Union[str, Path]] = Field(\n        default=None,\n        description=\"_summary_\",\n    )\n    config: RuntimeParameter[str] = Field(\n        default=None,\n        description=\"The configuration of the dataset to load. This is optional and only\"\n        \" needed if the dataset has multiple configurations.\",\n    )\n    is_distiset: Optional[RuntimeParameter[bool]] = Field(\n        default=False,\n        description=\"Whether the dataset to load is a `Distiset` or not. Defaults to False.\",\n    )\n    keep_in_memory: Optional[RuntimeParameter[bool]] = Field(\n        default=None,\n        description=\"Whether to copy the dataset in-memory, see `datasets.Dataset.load_from_disk` \"\n        \" for more information. Defaults to `None`.\",\n    )\n    split: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The split of the dataset to load. By default will load the whole Dataset/Distiset.\",\n    )\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset from the file/s in disk.\"\"\"\n        super(GeneratorStep, self).load()\n        if self.is_distiset:\n            ds = Distiset.load_from_disk(\n                self.dataset_path,\n                keep_in_memory=self.keep_in_memory,\n                storage_options=self.storage_options,\n            )\n            if self.config:\n                ds = ds[self.config]\n\n        else:\n            ds = load_from_disk(\n                self.dataset_path,\n                keep_in_memory=self.keep_in_memory,\n                storage_options=self.storage_options,\n            )\n\n        if self.split:\n            ds = ds[self.split]\n\n        self._dataset = ds\n\n        if self.num_examples:\n            self._dataset = self._dataset.select(range(self.num_examples))\n        else:\n            self.num_examples = len(self._dataset)\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The columns that will be generated by this step, based on the datasets from a file\n        in disk.\n\n        Returns:\n            The columns that will be generated by this step.\n        \"\"\"\n        # We assume there are Dataset/IterableDataset, not it's ...Dict counterparts\n        if self._dataset is Ellipsis:\n            raise ValueError(\n                \"Dataset not loaded yet, you must call `load` method first.\"\n            )\n\n        return self._dataset.column_names\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromDisk.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The columns that will be generated by this step, based on the datasets from a file in disk.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The columns that will be generated by this step.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromDisk.load","title":"<code>load()</code>","text":"<p>Load the dataset from the file/s in disk.</p> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset from the file/s in disk.\"\"\"\n    super(GeneratorStep, self).load()\n    if self.is_distiset:\n        ds = Distiset.load_from_disk(\n            self.dataset_path,\n            keep_in_memory=self.keep_in_memory,\n            storage_options=self.storage_options,\n        )\n        if self.config:\n            ds = ds[self.config]\n\n    else:\n        ds = load_from_disk(\n            self.dataset_path,\n            keep_in_memory=self.keep_in_memory,\n            storage_options=self.storage_options,\n        )\n\n    if self.split:\n        ds = ds[self.split]\n\n    self._dataset = ds\n\n    if self.num_examples:\n        self._dataset = self._dataset.select(range(self.num_examples))\n    else:\n        self.num_examples = len(self._dataset)\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromFileSystem","title":"<code>LoadDataFromFileSystem</code>","text":"<p>               Bases: <code>LoadDataFromHub</code></p> <p>Loads a dataset from a file in your filesystem.</p> <p><code>GeneratorStep</code> that creates a dataset from a file in the filesystem, uses Hugging Face <code>datasets</code> library. Take a look at Hugging Face Datasets for more information of the supported file types.</p> <p>Attributes:</p> Name Type Description <code>data_files</code> <code>RuntimeParameter[Union[str, Path]]</code> <p>The path to the file, or directory containing the files that conform the dataset.</p> <code>split</code> <code>RuntimeParameter[Union[str, Path]]</code> <p>The split of the dataset to load (typically will be <code>train</code>, <code>test</code> or <code>validation</code>).</p> Runtime parameters <ul> <li><code>batch_size</code>: The batch size to use when processing the data.</li> <li><code>data_files</code>: The path to the file, or directory containing the files that conform     the dataset.</li> <li><code>split</code>: The split of the dataset to load. Defaults to 'train'.</li> <li><code>streaming</code>: Whether to load the dataset in streaming mode or not. Defaults to     <code>False</code>.</li> <li><code>num_examples</code>: The number of examples to load from the dataset.     By default will load all examples.</li> <li><code>storage_options</code>: Key/value pairs to be passed on to the file-system backend, if any.     Defaults to <code>None</code>.</li> <li><code>filetype</code>: The expected filetype. If not provided, it will be inferred from the file extension.     For more than one file, it will be inferred from the first file.</li> </ul> Output columns <ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the     datasets loaded from the Hugging Face Hub.</li> </ul> Categories <ul> <li>load</li> </ul> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>class LoadDataFromFileSystem(LoadDataFromHub):\n    \"\"\"Loads a dataset from a file in your filesystem.\n\n    `GeneratorStep` that creates a dataset from a file in the filesystem, uses Hugging Face `datasets`\n    library. Take a look at [Hugging Face Datasets](https://huggingface.co/docs/datasets/loading)\n    for more information of the supported file types.\n\n    Attributes:\n        data_files: The path to the file, or directory containing the files that conform\n            the dataset.\n        split: The split of the dataset to load (typically will be `train`, `test` or `validation`).\n\n    Runtime parameters:\n        - `batch_size`: The batch size to use when processing the data.\n        - `data_files`: The path to the file, or directory containing the files that conform\n            the dataset.\n        - `split`: The split of the dataset to load. Defaults to 'train'.\n        - `streaming`: Whether to load the dataset in streaming mode or not. Defaults to\n            `False`.\n        - `num_examples`: The number of examples to load from the dataset.\n            By default will load all examples.\n        - `storage_options`: Key/value pairs to be passed on to the file-system backend, if any.\n            Defaults to `None`.\n        - `filetype`: The expected filetype. If not provided, it will be inferred from the file extension.\n            For more than one file, it will be inferred from the first file.\n\n    Output columns:\n        - dynamic (`all`): The columns that will be generated by this step, based on the\n            datasets loaded from the Hugging Face Hub.\n\n    Categories:\n        - load\n    \"\"\"\n\n    data_files: RuntimeParameter[Union[str, Path]] = Field(\n        default=None,\n        description=\"The data files, or directory containing the data files, to generate the dataset from.\",\n    )\n    filetype: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The expected filetype. If not provided, it will be inferred from the file extension.\",\n    )\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset from the file/s in disk.\"\"\"\n        super(GeneratorStep, self).load()\n\n        data_path = UPath(self.data_files, storage_options=self.storage_options)\n\n        (data_files, self.filetype) = self._prepare_data_files(data_path)\n\n        self._dataset = load_dataset(\n            self.filetype,\n            data_files=data_files,\n            split=self.split,\n            streaming=self.streaming,\n            storage_options=self.storage_options,\n        )\n\n        if not self.streaming and self.num_examples:\n            self._dataset = self._dataset.select(range(self.num_examples))\n        if not self.num_examples:\n            if self.streaming:\n                # There's no better way to get the number of examples in a streaming dataset,\n                # load it again for the moment.\n                self.num_examples = len(\n                    load_dataset(\n                        self.filetype, data_files=self.data_files, split=self.split\n                    )\n                )\n            else:\n                self.num_examples = len(self._dataset)\n\n    @staticmethod\n    def _prepare_data_files(\n        data_path: UPath,\n    ) -&gt; Tuple[Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]], str]:\n        \"\"\"Prepare the loading process by setting the `data_files` attribute.\n\n        Args:\n            data_path: The path to the data files, or directory containing the data files.\n\n        Returns:\n            Tuple with the data files and the filetype.\n        \"\"\"\n\n        def get_filetype(data_path: UPath) -&gt; str:\n            filetype = data_path.suffix.lstrip(\".\")\n            if filetype == \"jsonl\":\n                filetype = \"json\"\n            return filetype\n\n        if data_path.is_file():\n            filetype = get_filetype(data_path)\n            data_files = str(data_path)\n        elif data_path.is_dir():\n            file_sequence = []\n            file_map = defaultdict(list)\n            for file_or_folder in data_path.iterdir():\n                if file_or_folder.is_file():\n                    file_sequence.append(str(file_or_folder))\n                elif file_or_folder.is_dir():\n                    for file in file_or_folder.iterdir():\n                        file_sequence.append(str(file))\n                        file_map[str(file_or_folder)].append(str(file))\n\n            data_files = file_sequence or file_map\n            # Try to obtain the filetype from any of the files, assuming all files have the same type.\n            if file_sequence:\n                filetype = get_filetype(UPath(file_sequence[0]))\n            else:\n                filetype = get_filetype(UPath(file_map[list(file_map.keys())[0]][0]))\n        return data_files, filetype\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The columns that will be generated by this step, based on the datasets from a file\n        in disk.\n\n        Returns:\n            The columns that will be generated by this step.\n        \"\"\"\n        # We assume there are Dataset/IterableDataset, not it's ...Dict counterparts\n        if self._dataset is Ellipsis:\n            raise ValueError(\n                \"Dataset not loaded yet, you must call `load` method first.\"\n            )\n\n        return self._dataset.column_names\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromFileSystem.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The columns that will be generated by this step, based on the datasets from a file in disk.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The columns that will be generated by this step.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromFileSystem.load","title":"<code>load()</code>","text":"<p>Load the dataset from the file/s in disk.</p> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset from the file/s in disk.\"\"\"\n    super(GeneratorStep, self).load()\n\n    data_path = UPath(self.data_files, storage_options=self.storage_options)\n\n    (data_files, self.filetype) = self._prepare_data_files(data_path)\n\n    self._dataset = load_dataset(\n        self.filetype,\n        data_files=data_files,\n        split=self.split,\n        streaming=self.streaming,\n        storage_options=self.storage_options,\n    )\n\n    if not self.streaming and self.num_examples:\n        self._dataset = self._dataset.select(range(self.num_examples))\n    if not self.num_examples:\n        if self.streaming:\n            # There's no better way to get the number of examples in a streaming dataset,\n            # load it again for the moment.\n            self.num_examples = len(\n                load_dataset(\n                    self.filetype, data_files=self.data_files, split=self.split\n                )\n            )\n        else:\n            self.num_examples = len(self._dataset)\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromHub","title":"<code>LoadDataFromHub</code>","text":"<p>               Bases: <code>GeneratorStep</code></p> <p>Loads a dataset from the Hugging Face Hub.</p> <p><code>GeneratorStep</code> that loads a dataset from the Hugging Face Hub using the <code>datasets</code> library.</p> <p>Attributes:</p> Name Type Description <code>repo_id</code> <code>RuntimeParameter[str]</code> <p>The Hugging Face Hub repository ID of the dataset to load.</p> <code>split</code> <code>RuntimeParameter[str]</code> <p>The split of the dataset to load.</p> <code>config</code> <code>Optional[RuntimeParameter[str]]</code> <p>The configuration of the dataset to load. This is optional and only needed if the dataset has multiple configurations.</p> Runtime parameters <ul> <li><code>batch_size</code>: The batch size to use when processing the data.</li> <li><code>repo_id</code>: The Hugging Face Hub repository ID of the dataset to load.</li> <li><code>split</code>: The split of the dataset to load. Defaults to 'train'.</li> <li><code>config</code>: The configuration of the dataset to load. This is optional and only     needed if the dataset has multiple configurations.</li> <li><code>streaming</code>: Whether to load the dataset in streaming mode or not. Defaults to     <code>False</code>.</li> <li><code>num_examples</code>: The number of examples to load from the dataset.     By default will load all examples.</li> <li><code>storage_options</code>: Key/value pairs to be passed on to the file-system backend, if any.     Defaults to <code>None</code>.</li> </ul> Output columns <ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the     datasets loaded from the Hugging Face Hub.</li> </ul> Categories <ul> <li>load</li> </ul> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>class LoadDataFromHub(GeneratorStep):\n    \"\"\"Loads a dataset from the Hugging Face Hub.\n\n    `GeneratorStep` that loads a dataset from the Hugging Face Hub using the `datasets`\n    library.\n\n    Attributes:\n        repo_id: The Hugging Face Hub repository ID of the dataset to load.\n        split: The split of the dataset to load.\n        config: The configuration of the dataset to load. This is optional and only needed\n            if the dataset has multiple configurations.\n\n    Runtime parameters:\n        - `batch_size`: The batch size to use when processing the data.\n        - `repo_id`: The Hugging Face Hub repository ID of the dataset to load.\n        - `split`: The split of the dataset to load. Defaults to 'train'.\n        - `config`: The configuration of the dataset to load. This is optional and only\n            needed if the dataset has multiple configurations.\n        - `streaming`: Whether to load the dataset in streaming mode or not. Defaults to\n            `False`.\n        - `num_examples`: The number of examples to load from the dataset.\n            By default will load all examples.\n        - `storage_options`: Key/value pairs to be passed on to the file-system backend, if any.\n            Defaults to `None`.\n\n    Output columns:\n        - dynamic (`all`): The columns that will be generated by this step, based on the\n            datasets loaded from the Hugging Face Hub.\n\n    Categories:\n        - load\n    \"\"\"\n\n    repo_id: RuntimeParameter[str] = Field(\n        default=None,\n        description=\"The Hugging Face Hub repository ID of the dataset to load.\",\n    )\n    split: RuntimeParameter[str] = Field(\n        default=\"train\",\n        description=\"The split of the dataset to load. Defaults to 'train'.\",\n    )\n    config: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The configuration of the dataset to load. This is optional and only\"\n        \" needed if the dataset has multiple configurations.\",\n    )\n    streaming: RuntimeParameter[bool] = Field(\n        default=False,\n        description=\"Whether to load the dataset in streaming mode or not. Defaults to False.\",\n    )\n    num_examples: Optional[RuntimeParameter[int]] = Field(\n        default=None,\n        description=\"The number of examples to load from the dataset. By default will load all examples.\",\n    )\n    storage_options: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=\"The storage options to use when loading the dataset.\",\n    )\n\n    _dataset: Union[IterableDataset, Dataset, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset from the Hugging Face Hub\"\"\"\n        super().load()\n\n        self._dataset = load_dataset(\n            self.repo_id,  # type: ignore\n            self.config,\n            split=self.split,\n            streaming=self.streaming,\n        )\n        num_examples = self._get_dataset_num_examples()\n        self.num_examples = (\n            min(self.num_examples, num_examples) if self.num_examples else num_examples\n        )\n\n        if not self.streaming:\n            self._dataset = self._dataset.select(range(self.num_examples))\n\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n        \"\"\"Yields batches from the loaded dataset from the Hugging Face Hub.\n\n        Args:\n            offset: The offset to start yielding the data from. Will be used during the caching\n                process to help skipping already processed data.\n\n        Yields:\n            A tuple containing a batch of rows and a boolean indicating if the batch is\n            the last one.\n        \"\"\"\n        num_returned_rows = 0\n        for batch_num, batch in enumerate(\n            self._dataset.iter(batch_size=self.batch_size)  # type: ignore\n        ):\n            if batch_num * self.batch_size &lt; offset:\n                continue\n            transformed_batch = self._transform_batch(batch)\n            batch_size = len(transformed_batch)\n            num_returned_rows += batch_size\n            yield transformed_batch, num_returned_rows &gt;= self.num_examples\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The columns that will be generated by this step, based on the datasets loaded\n        from the Hugging Face Hub.\n\n        Returns:\n            The columns that will be generated by this step.\n        \"\"\"\n        return self._get_dataset_columns()\n\n    def _transform_batch(self, batch: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Transform a batch of data from the Hugging Face Hub into a list of rows.\n\n        Args:\n            batch: The batch of data from the Hugging Face Hub.\n\n        Returns:\n            A list of rows, where each row is a dictionary of column names and values.\n        \"\"\"\n        length = len(next(iter(batch.values())))\n        rows = []\n        for i in range(length):\n            rows.append({col: values[i] for col, values in batch.items()})\n        return rows\n\n    def _get_dataset_num_examples(self) -&gt; int:\n        \"\"\"Get the number of examples in the dataset, based on the `split` and `config`\n        runtime parameters provided.\n\n        Returns:\n            The number of examples in the dataset.\n        \"\"\"\n        return (\n            self._dataset_info[self.config if self.config else \"default\"]\n            .splits[self.split]\n            .num_examples\n        )\n\n    def _get_dataset_columns(self) -&gt; List[str]:\n        \"\"\"Get the columns of the dataset, based on the `config` runtime parameter provided.\n\n        Returns:\n            The columns of the dataset.\n        \"\"\"\n        return list(\n            self._dataset_info[\n                self.config if self.config else \"default\"\n            ].features.keys()\n        )\n\n    @cached_property\n    def _dataset_info(self) -&gt; Dict[str, DatasetInfo]:\n        \"\"\"Calls the Datasets Server API from Hugging Face to obtain the dataset information.\n\n        Returns:\n            The dataset information.\n        \"\"\"\n        repo_id = self.repo_id\n        config = self.config\n\n        try:\n            return get_dataset_infos(repo_id)\n        except Exception as e:\n            # The previous could fail in case of a internet connection issues.\n            # Assuming the dataset is already loaded and we can get the info from the loaded dataset, otherwise it will fail anyway.\n            self._logger.warning(\n                f\"Failed to get dataset info from Hugging Face Hub, trying to get it loading the dataset. Error: {e}\"\n            )\n            ds = load_dataset(repo_id, config=self.config, split=self.split)\n            if config:\n                return ds[config].info\n            return ds.info\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromHub.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The columns that will be generated by this step, based on the datasets loaded from the Hugging Face Hub.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The columns that will be generated by this step.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromHub.load","title":"<code>load()</code>","text":"<p>Load the dataset from the Hugging Face Hub</p> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset from the Hugging Face Hub\"\"\"\n    super().load()\n\n    self._dataset = load_dataset(\n        self.repo_id,  # type: ignore\n        self.config,\n        split=self.split,\n        streaming=self.streaming,\n    )\n    num_examples = self._get_dataset_num_examples()\n    self.num_examples = (\n        min(self.num_examples, num_examples) if self.num_examples else num_examples\n    )\n\n    if not self.streaming:\n        self._dataset = self._dataset.select(range(self.num_examples))\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.LoadDataFromHub.process","title":"<code>process(offset=0)</code>","text":"<p>Yields batches from the loaded dataset from the Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start yielding the data from. Will be used during the caching process to help skipping already processed data.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>A tuple containing a batch of rows and a boolean indicating if the batch is</p> <code>GeneratorStepOutput</code> <p>the last one.</p> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n    \"\"\"Yields batches from the loaded dataset from the Hugging Face Hub.\n\n    Args:\n        offset: The offset to start yielding the data from. Will be used during the caching\n            process to help skipping already processed data.\n\n    Yields:\n        A tuple containing a batch of rows and a boolean indicating if the batch is\n        the last one.\n    \"\"\"\n    num_returned_rows = 0\n    for batch_num, batch in enumerate(\n        self._dataset.iter(batch_size=self.batch_size)  # type: ignore\n    ):\n        if batch_num * self.batch_size &lt; offset:\n            continue\n        transformed_batch = self._transform_batch(batch)\n        batch_size = len(transformed_batch)\n        num_returned_rows += batch_size\n        yield transformed_batch, num_returned_rows &gt;= self.num_examples\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.PreferenceToArgilla","title":"<code>PreferenceToArgilla</code>","text":"<p>               Bases: <code>Argilla</code></p> <p>Creates a preference dataset in Argilla.</p> <p>Step that creates a dataset in Argilla during the load phase, and then pushes the input batches into it as records. This dataset is a preference dataset, where there's one field for the instruction and one extra field per each generation within the same record, and then a rating question per each of the generation fields. The rating question asks the annotator to set a rating from 1 to 5 for each of the provided generations.</p> Note <p>This step is meant to be used in conjunction with the <code>UltraFeedback</code> step, or any other step generating both ratings and responses for a given set of instruction and generations for the given instruction. But alternatively, it can also be used with any other task or step generating only the <code>instruction</code> and <code>generations</code>, as the <code>ratings</code> and <code>rationales</code> are optional.</p> <p>Attributes:</p> Name Type Description <code>num_generations</code> <code>int</code> <p>The number of generations to include in the dataset.</p> <code>dataset_name</code> <code>int</code> <p>The name of the dataset in Argilla.</p> <code>dataset_workspace</code> <code>int</code> <p>The workspace where the dataset will be created in Argilla. Defaults to <code>None</code>, which means it will be created in the default workspace.</p> <code>api_url</code> <code>int</code> <p>The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_URL</code> environment variable.</p> <code>api_key</code> <code>int</code> <p>The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> Runtime parameters <ul> <li><code>api_url</code>: The base URL to use for the Argilla API requests.</li> <li><code>api_key</code>: The API key to authenticate the requests to the Argilla API.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the completion.</li> <li>generations (<code>List[str]</code>): The completion that was generated based on the input instruction.</li> <li>ratings (<code>List[str]</code>, optional): The ratings for the generations. If not provided, the     generated ratings won't be pushed to Argilla.</li> <li>rationales (<code>List[str]</code>, optional): The rationales for the ratings. If not provided, the     generated rationales won't be pushed to Argilla.</li> </ul> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>class PreferenceToArgilla(Argilla):\n    \"\"\"Creates a preference dataset in Argilla.\n\n    Step that creates a dataset in Argilla during the load phase, and then pushes the input\n    batches into it as records. This dataset is a preference dataset, where there's one field\n    for the instruction and one extra field per each generation within the same record, and then\n    a rating question per each of the generation fields. The rating question asks the annotator to\n    set a rating from 1 to 5 for each of the provided generations.\n\n    Note:\n        This step is meant to be used in conjunction with the `UltraFeedback` step, or any other step\n        generating both ratings and responses for a given set of instruction and generations for the\n        given instruction. But alternatively, it can also be used with any other task or step generating\n        only the `instruction` and `generations`, as the `ratings` and `rationales` are optional.\n\n    Attributes:\n        num_generations: The number of generations to include in the dataset.\n        dataset_name: The name of the dataset in Argilla.\n        dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to\n            `None`, which means it will be created in the default workspace.\n        api_url: The URL of the Argilla API. Defaults to `None`, which means it will be read from\n            the `ARGILLA_API_URL` environment variable.\n        api_key: The API key to authenticate with Argilla. Defaults to `None`, which means it will\n            be read from the `ARGILLA_API_KEY` environment variable.\n\n    Runtime parameters:\n        - `api_url`: The base URL to use for the Argilla API requests.\n        - `api_key`: The API key to authenticate the requests to the Argilla API.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the completion.\n        - generations (`List[str]`): The completion that was generated based on the input instruction.\n        - ratings (`List[str]`, optional): The ratings for the generations. If not provided, the\n            generated ratings won't be pushed to Argilla.\n        - rationales (`List[str]`, optional): The rationales for the ratings. If not provided, the\n            generated rationales won't be pushed to Argilla.\n    \"\"\"\n\n    num_generations: int\n\n    _id: str = PrivateAttr(default=\"id\")\n    _instruction: str = PrivateAttr(...)\n    _generations: str = PrivateAttr(...)\n    _ratings: str = PrivateAttr(...)\n    _rationales: str = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Sets the `_instruction` and `_generations` attributes based on the `inputs_mapping`, otherwise\n        uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n        the text-generation scenario. And then it pushes it to Argilla.\n        \"\"\"\n        super().load()\n\n        # Both `instruction` and `generations` will be used as the fields of the dataset\n        self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n        self._generations = self.input_mappings.get(\"generations\", \"generations\")\n        # Both `ratings` and `rationales` will be used as suggestions to the default questions of the dataset\n        self._ratings = self.input_mappings.get(\"ratings\", \"ratings\")\n        self._rationales = self.input_mappings.get(\"rationales\", \"rationales\")\n\n        if self._rg_dataset_exists():\n            _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n                name=self.dataset_name,\n                workspace=self.dataset_workspace,\n            )\n\n            for field in _rg_dataset.fields:\n                if (\n                    field.name\n                    not in [self._id, self._instruction]\n                    + [\n                        f\"{self._generations}-{idx}\"\n                        for idx in range(self.num_generations)\n                    ]\n                    and field.required\n                ):\n                    raise ValueError(\n                        f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                        f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`,\"\n                        f\" nor `{self._generations}`.\"\n                    )\n\n            self._rg_dataset = _rg_dataset\n        else:\n            _rg_dataset = rg.FeedbackDataset(  # type: ignore\n                fields=[\n                    rg.TextField(name=self._id, title=self._id),  # type: ignore\n                    rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                    *self._generation_fields(),  # type: ignore\n                ],\n                questions=self._rating_rationale_pairs(),  # type: ignore\n            )\n            self._rg_dataset = _rg_dataset.push_to_argilla(\n                name=self.dataset_name,  # type: ignore\n                workspace=self.dataset_workspace,\n            )\n\n    def _generation_fields(self) -&gt; List[\"TextField\"]:\n        \"\"\"Method to generate the fields for each of the generations.\"\"\"\n        return [\n            rg.TextField(  # type: ignore\n                name=f\"{self._generations}-{idx}\",\n                title=f\"{self._generations}-{idx}\",\n                required=True if idx == 0 else False,\n            )\n            for idx in range(self.num_generations)\n        ]\n\n    def _rating_rationale_pairs(\n        self,\n    ) -&gt; List[Union[\"RatingQuestion\", \"TextQuestion\"]]:\n        \"\"\"Method to generate the rating and rationale questions for each of the generations.\"\"\"\n        questions = []\n        for idx in range(self.num_generations):\n            questions.extend(\n                [\n                    rg.RatingQuestion(  # type: ignore\n                        name=f\"{self._generations}-{idx}-rating\",\n                        title=f\"Rate {self._generations}-{idx} given {self._instruction}.\",\n                        description=f\"Ignore this question if the corresponding `{self._generations}-{idx}` field is not available.\"\n                        if idx != 0\n                        else None,\n                        values=[1, 2, 3, 4, 5],\n                        required=True if idx == 0 else False,\n                    ),\n                    rg.TextQuestion(  # type: ignore\n                        name=f\"{self._generations}-{idx}-rationale\",\n                        title=f\"Specify the rationale for {self._generations}-{idx}'s rating.\",\n                        description=f\"Ignore this question if the corresponding `{self._generations}-{idx}` field is not available.\"\n                        if idx != 0\n                        else None,\n                        required=False,\n                    ),\n                ]\n            )\n        return questions\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the step are the `instruction` and the `generations`. Optionally, one could also\n        provide the `ratings` and the `rationales` for the generations.\"\"\"\n        return [\"instruction\", \"generations\"]\n\n    def _add_suggestions_if_any(\n        self, input: Dict[str, Any]\n    ) -&gt; List[\"SuggestionSchema\"]:\n        \"\"\"Method to generate the suggestions for the `FeedbackRecord` based on the input.\"\"\"\n        # Since the `suggestions` i.e. answers to the `questions` are optional, will default to {}\n        suggestions = []\n        # If `ratings` is in `input`, then add those as suggestions\n        if self._ratings in input:\n            suggestions.extend(\n                [\n                    {\n                        \"question_name\": f\"{self._generations}-{idx}-rating\",\n                        \"value\": rating,\n                    }\n                    for idx, rating in enumerate(input[self._ratings])\n                    if rating is not None\n                    and isinstance(rating, int)\n                    and rating in [1, 2, 3, 4, 5]\n                ],\n            )\n        # If `rationales` is in `input`, then add those as suggestions\n        if self._rationales in input:\n            suggestions.extend(\n                [\n                    {\n                        \"question_name\": f\"{self._generations}-{idx}-rationale\",\n                        \"value\": rationale,\n                    }\n                    for idx, rationale in enumerate(input[self._rationales])\n                    if rationale is not None and isinstance(rationale, str)\n                ],\n            )\n        return suggestions\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        records = []\n        for input in inputs:\n            # Generate the SHA-256 hash of the instruction to use it as the metadata\n            instruction_id = hashlib.sha256(\n                input[\"instruction\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            generations = {\n                f\"{self._generations}-{idx}\": generation\n                for idx, generation in enumerate(input[\"generations\"])  # type: ignore\n            }\n\n            records.append(  # type: ignore\n                rg.FeedbackRecord(  # type: ignore\n                    fields={\n                        \"id\": instruction_id,\n                        \"instruction\": input[\"instruction\"],  # type: ignore\n                        **generations,\n                    },\n                    suggestions=self._add_suggestions_if_any(input),  # type: ignore\n                )\n            )\n        self._rg_dataset.add_records(records)  # type: ignore\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.PreferenceToArgilla.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the step are the <code>instruction</code> and the <code>generations</code>. Optionally, one could also provide the <code>ratings</code> and the <code>rationales</code> for the generations.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.PreferenceToArgilla.load","title":"<code>load()</code>","text":"<p>Sets the <code>_instruction</code> and <code>_generations</code> attributes based on the <code>inputs_mapping</code>, otherwise uses the default values; and then uses those values to create a <code>FeedbackDataset</code> suited for the text-generation scenario. And then it pushes it to Argilla.</p> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Sets the `_instruction` and `_generations` attributes based on the `inputs_mapping`, otherwise\n    uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n    the text-generation scenario. And then it pushes it to Argilla.\n    \"\"\"\n    super().load()\n\n    # Both `instruction` and `generations` will be used as the fields of the dataset\n    self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n    self._generations = self.input_mappings.get(\"generations\", \"generations\")\n    # Both `ratings` and `rationales` will be used as suggestions to the default questions of the dataset\n    self._ratings = self.input_mappings.get(\"ratings\", \"ratings\")\n    self._rationales = self.input_mappings.get(\"rationales\", \"rationales\")\n\n    if self._rg_dataset_exists():\n        _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n            name=self.dataset_name,\n            workspace=self.dataset_workspace,\n        )\n\n        for field in _rg_dataset.fields:\n            if (\n                field.name\n                not in [self._id, self._instruction]\n                + [\n                    f\"{self._generations}-{idx}\"\n                    for idx in range(self.num_generations)\n                ]\n                and field.required\n            ):\n                raise ValueError(\n                    f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                    f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`,\"\n                    f\" nor `{self._generations}`.\"\n                )\n\n        self._rg_dataset = _rg_dataset\n    else:\n        _rg_dataset = rg.FeedbackDataset(  # type: ignore\n            fields=[\n                rg.TextField(name=self._id, title=self._id),  # type: ignore\n                rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                *self._generation_fields(),  # type: ignore\n            ],\n            questions=self._rating_rationale_pairs(),  # type: ignore\n        )\n        self._rg_dataset = _rg_dataset.push_to_argilla(\n            name=self.dataset_name,  # type: ignore\n            workspace=self.dataset_workspace,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.PreferenceToArgilla.process","title":"<code>process(inputs)</code>","text":"<p>Creates and pushes the records as FeedbackRecords to the Argilla dataset.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    records = []\n    for input in inputs:\n        # Generate the SHA-256 hash of the instruction to use it as the metadata\n        instruction_id = hashlib.sha256(\n            input[\"instruction\"].encode(\"utf-8\")  # type: ignore\n        ).hexdigest()\n\n        generations = {\n            f\"{self._generations}-{idx}\": generation\n            for idx, generation in enumerate(input[\"generations\"])  # type: ignore\n        }\n\n        records.append(  # type: ignore\n            rg.FeedbackRecord(  # type: ignore\n                fields={\n                    \"id\": instruction_id,\n                    \"instruction\": input[\"instruction\"],  # type: ignore\n                    **generations,\n                },\n                suggestions=self._add_suggestions_if_any(input),  # type: ignore\n            )\n        )\n    self._rg_dataset.add_records(records)  # type: ignore\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.PushToHub","title":"<code>PushToHub</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Push data to a Hugging Face Hub dataset.</p> <p>A <code>GlobalStep</code> which creates a <code>datasets.Dataset</code> with the input data and pushes it to the Hugging Face Hub.</p> <p>Attributes:</p> Name Type Description <code>repo_id</code> <code>RuntimeParameter[str]</code> <p>The Hugging Face Hub repository ID where the dataset will be uploaded.</p> <code>split</code> <code>RuntimeParameter[str]</code> <p>The split of the dataset that will be pushed. Defaults to <code>\"train\"</code>.</p> <code>private</code> <code>RuntimeParameter[bool]</code> <p>Whether the dataset to be pushed should be private or not. Defaults to <code>False</code>.</p> <code>token</code> <code>Optional[RuntimeParameter[str]]</code> <p>The token that will be used to authenticate in the Hub. If not provided, the token will be tried to be obtained from the environment variable <code>HF_TOKEN</code>. If not provided using one of the previous methods, then <code>huggingface_hub</code> library will try to use the token from the local Hugging Face CLI configuration. Defaults to <code>None</code>.</p> Runtime parameters <ul> <li><code>repo_id</code>: The Hugging Face Hub repository ID where the dataset will be uploaded.</li> <li><code>split</code>: The split of the dataset that will be pushed.</li> <li><code>private</code>: Whether the dataset to be pushed should be private or not.</li> <li><code>token</code>: The token that will be used to authenticate in the Hub.</li> </ul> Input columns <ul> <li>dynamic (<code>all</code>): all columns from the input will be used to create the dataset.</li> </ul> Categories <ul> <li>save</li> <li>dataset</li> <li>huggingface</li> </ul> Source code in <code>src/distilabel/steps/globals/huggingface.py</code> <pre><code>class PushToHub(GlobalStep):\n    \"\"\"Push data to a Hugging Face Hub dataset.\n\n    A `GlobalStep` which creates a `datasets.Dataset` with the input data and pushes\n    it to the Hugging Face Hub.\n\n    Attributes:\n        repo_id: The Hugging Face Hub repository ID where the dataset will be uploaded.\n        split: The split of the dataset that will be pushed. Defaults to `\"train\"`.\n        private: Whether the dataset to be pushed should be private or not. Defaults to\n            `False`.\n        token: The token that will be used to authenticate in the Hub. If not provided, the\n            token will be tried to be obtained from the environment variable `HF_TOKEN`.\n            If not provided using one of the previous methods, then `huggingface_hub` library\n            will try to use the token from the local Hugging Face CLI configuration. Defaults\n            to `None`.\n\n    Runtime parameters:\n        - `repo_id`: The Hugging Face Hub repository ID where the dataset will be uploaded.\n        - `split`: The split of the dataset that will be pushed.\n        - `private`: Whether the dataset to be pushed should be private or not.\n        - `token`: The token that will be used to authenticate in the Hub.\n\n    Input columns:\n        - dynamic (`all`): all columns from the input will be used to create the dataset.\n\n    Categories:\n        - save\n        - dataset\n        - huggingface\n    \"\"\"\n\n    repo_id: RuntimeParameter[str] = Field(\n        default=None,\n        description=\"The Hugging Face Hub repository ID where the dataset will be uploaded.\",\n    )\n    split: RuntimeParameter[str] = Field(\n        default=\"train\",\n        description=\"The split of the dataset that will be pushed. Defaults to 'train'.\",\n    )\n    private: RuntimeParameter[bool] = Field(\n        default=False,\n        description=\"Whether the dataset to be pushed should be private or not. Defaults\"\n        \" to `False`.\",\n    )\n    token: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The token that will be used to authenticate in the Hub. If not provided,\"\n        \" the token will be tried to be obtained from the environment variable `HF_TOKEN`.\"\n        \" If not provided using one of the previous methods, then `huggingface_hub` library\"\n        \" will try to use the token from the local Hugging Face CLI configuration. Defaults\"\n        \" to `None`\",\n    )\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Method that processes the input data, respecting the `datasets.Dataset` formatting,\n        and pushes it to the Hugging Face Hub based on the `RuntimeParameter`s attributes.\n\n        Args:\n            inputs: that input data within a single object (as it's a GlobalStep) that\n                will be transformed into a `datasets.Dataset`.\n\n        Yields:\n            Propagates the received inputs so that the `Distiset` can be generated if this is\n            the last step of the `Pipeline`, or if this is not a leaf step and has follow up\n            steps.\n        \"\"\"\n        dataset_dict = defaultdict(list)\n        for input in inputs:\n            for key, value in input.items():\n                dataset_dict[key].append(value)\n        dataset_dict = dict(dataset_dict)\n        dataset = Dataset.from_dict(dataset_dict)\n        dataset.push_to_hub(\n            self.repo_id,  # type: ignore\n            split=self.split,\n            private=self.private,\n            token=self.token or os.getenv(\"HF_TOKEN\"),\n        )\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.PushToHub.process","title":"<code>process(inputs)</code>","text":"<p>Method that processes the input data, respecting the <code>datasets.Dataset</code> formatting, and pushes it to the Hugging Face Hub based on the <code>RuntimeParameter</code>s attributes.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>that input data within a single object (as it's a GlobalStep) that will be transformed into a <code>datasets.Dataset</code>.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>Propagates the received inputs so that the <code>Distiset</code> can be generated if this is</p> <code>StepOutput</code> <p>the last step of the <code>Pipeline</code>, or if this is not a leaf step and has follow up</p> <code>StepOutput</code> <p>steps.</p> Source code in <code>src/distilabel/steps/globals/huggingface.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Method that processes the input data, respecting the `datasets.Dataset` formatting,\n    and pushes it to the Hugging Face Hub based on the `RuntimeParameter`s attributes.\n\n    Args:\n        inputs: that input data within a single object (as it's a GlobalStep) that\n            will be transformed into a `datasets.Dataset`.\n\n    Yields:\n        Propagates the received inputs so that the `Distiset` can be generated if this is\n        the last step of the `Pipeline`, or if this is not a leaf step and has follow up\n        steps.\n    \"\"\"\n    dataset_dict = defaultdict(list)\n    for input in inputs:\n        for key, value in input.items():\n            dataset_dict[key].append(value)\n    dataset_dict = dict(dataset_dict)\n    dataset = Dataset.from_dict(dataset_dict)\n    dataset.push_to_hub(\n        self.repo_id,  # type: ignore\n        split=self.split,\n        private=self.private,\n        token=self.token or os.getenv(\"HF_TOKEN\"),\n    )\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.Step","title":"<code>Step</code>","text":"<p>               Bases: <code>_Step</code>, <code>ABC</code></p> <p>Base class for the steps that can be included in a <code>Pipeline</code>.</p> <p>Attributes:</p> Name Type Description <code>input_batch_size</code> <code>RuntimeParameter[PositiveInt]</code> <p>The number of rows that will contain the batches processed by the step. Defaults to <code>50</code>.</p> Runtime parameters <ul> <li><code>input_batch_size</code>: The number of rows that will contain the batches processed     by the step. Defaults to <code>50</code>.</li> </ul> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class Step(_Step, ABC):\n    \"\"\"Base class for the steps that can be included in a `Pipeline`.\n\n    Attributes:\n        input_batch_size: The number of rows that will contain the batches processed by\n            the step. Defaults to `50`.\n\n    Runtime parameters:\n        - `input_batch_size`: The number of rows that will contain the batches processed\n            by the step. Defaults to `50`.\n    \"\"\"\n\n    input_batch_size: RuntimeParameter[PositiveInt] = Field(\n        default=DEFAULT_INPUT_BATCH_SIZE,\n        description=\"The number of rows that will contain the batches processed by the\"\n        \" step.\",\n    )\n\n    @abstractmethod\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"Method that defines the processing logic of the step. It should yield the\n        output rows.\n\n        Args:\n            *inputs: An argument used to receive the outputs of the previous steps. The\n                number of arguments depends on the number of previous steps. It doesn't\n                need to be an `*args` argument, it can be a regular argument annotated\n                with `StepInput` if the step has only one previous step.\n        \"\"\"\n        pass\n\n    def process_applying_mappings(self, *args: List[Dict[str, Any]]) -&gt; \"StepOutput\":\n        \"\"\"Runs the `process` method of the step applying the `input_mappings` to the input\n        rows and the `outputs_mappings` to the output rows. This is the function that\n        should be used to run the processing logic of the step.\n\n        Yields:\n            The output rows.\n        \"\"\"\n\n        inputs = self._apply_input_mappings(args) if self.input_mappings else args\n\n        # If the `Step` was built using the `@step` decorator, then we need to pass\n        # the runtime parameters as kwargs, so they can be used within the processing\n        # function\n        generator = (\n            self.process(*inputs)\n            if not self._built_from_decorator\n            else self.process(*inputs, **self._runtime_parameters)\n        )\n\n        for output_rows in generator:\n            yield [\n                {\n                    # Apply output mapping and revert input mapping\n                    self.output_mappings.get(k, None)\n                    or self.input_mappings.get(k, None)\n                    or k: v\n                    for k, v in row.items()\n                }\n                for row in output_rows\n            ]\n\n    def _revert_input_mappings(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Reverts the `input_mappings` of the step to the input row.\n\n        Args:\n            input: The input row.\n\n        Returns:\n            The input row with the `input_mappings` reverted.\n        \"\"\"\n        return {self.input_mappings.get(k, k): v for k, v in input.items()}\n\n    def _apply_input_mappings(\n        self, inputs: Tuple[List[Dict[str, Any]], ...]\n    ) -&gt; List[List[Dict[str, Any]]]:\n        \"\"\"Applies the `input_mappings` to the input rows.\n\n        Args:\n            inputs: The input rows.\n\n        Returns:\n            The input rows with the `input_mappings` applied.\n        \"\"\"\n        reverted_input_mappings = {v: k for k, v in self.input_mappings.items()}\n\n        return [\n            [\n                {reverted_input_mappings.get(k, k): v for k, v in row.items()}\n                for row in row_inputs\n            ]\n            for row_inputs in inputs\n        ]\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.Step.process","title":"<code>process(*inputs)</code>  <code>abstractmethod</code>","text":"<p>Method that defines the processing logic of the step. It should yield the output rows.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>An argument used to receive the outputs of the previous steps. The number of arguments depends on the number of previous steps. It doesn't need to be an <code>*args</code> argument, it can be a regular argument annotated with <code>StepInput</code> if the step has only one previous step.</p> <code>()</code> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>@abstractmethod\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"Method that defines the processing logic of the step. It should yield the\n    output rows.\n\n    Args:\n        *inputs: An argument used to receive the outputs of the previous steps. The\n            number of arguments depends on the number of previous steps. It doesn't\n            need to be an `*args` argument, it can be a regular argument annotated\n            with `StepInput` if the step has only one previous step.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.Step.process_applying_mappings","title":"<code>process_applying_mappings(*args)</code>","text":"<p>Runs the <code>process</code> method of the step applying the <code>input_mappings</code> to the input rows and the <code>outputs_mappings</code> to the output rows. This is the function that should be used to run the processing logic of the step.</p> <p>Yields:</p> Type Description <code>StepOutput</code> <p>The output rows.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def process_applying_mappings(self, *args: List[Dict[str, Any]]) -&gt; \"StepOutput\":\n    \"\"\"Runs the `process` method of the step applying the `input_mappings` to the input\n    rows and the `outputs_mappings` to the output rows. This is the function that\n    should be used to run the processing logic of the step.\n\n    Yields:\n        The output rows.\n    \"\"\"\n\n    inputs = self._apply_input_mappings(args) if self.input_mappings else args\n\n    # If the `Step` was built using the `@step` decorator, then we need to pass\n    # the runtime parameters as kwargs, so they can be used within the processing\n    # function\n    generator = (\n        self.process(*inputs)\n        if not self._built_from_decorator\n        else self.process(*inputs, **self._runtime_parameters)\n    )\n\n    for output_rows in generator:\n        yield [\n            {\n                # Apply output mapping and revert input mapping\n                self.output_mappings.get(k, None)\n                or self.input_mappings.get(k, None)\n                or k: v\n                for k, v in row.items()\n            }\n            for row in output_rows\n        ]\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.TextGenerationToArgilla","title":"<code>TextGenerationToArgilla</code>","text":"<p>               Bases: <code>Argilla</code></p> <p>Creates a text generation dataset in Argilla.</p> <p><code>Step</code> that creates a dataset in Argilla during the load phase, and then pushes the input batches into it as records. This dataset is a text-generation dataset, where there's one field per each input, and then a label question to rate the quality of the completion in either bad (represented with \ud83d\udc4e) or good (represented with \ud83d\udc4d).</p> Note <p>This step is meant to be used in conjunction with a <code>TextGeneration</code> step and no column mapping is needed, as it will use the default values for the <code>instruction</code> and <code>generation</code> columns.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <p>The name of the dataset in Argilla.</p> <code>dataset_workspace</code> <p>The workspace where the dataset will be created in Argilla. Defaults to <code>None</code>, which means it will be created in the default workspace.</p> <code>api_url</code> <p>The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_URL</code> environment variable.</p> <code>api_key</code> <p>The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> Runtime parameters <ul> <li><code>api_url</code>: The base URL to use for the Argilla API requests.</li> <li><code>api_key</code>: The API key to authenticate the requests to the Argilla API.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the completion.</li> <li>generation (<code>str</code> or <code>List[str]</code>): The completions that were generated based on the input instruction.</li> </ul> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>class TextGenerationToArgilla(Argilla):\n    \"\"\"Creates a text generation dataset in Argilla.\n\n    `Step` that creates a dataset in Argilla during the load phase, and then pushes the input\n    batches into it as records. This dataset is a text-generation dataset, where there's one field\n    per each input, and then a label question to rate the quality of the completion in either bad\n    (represented with \ud83d\udc4e) or good (represented with \ud83d\udc4d).\n\n    Note:\n        This step is meant to be used in conjunction with a `TextGeneration` step and no column mapping\n        is needed, as it will use the default values for the `instruction` and `generation` columns.\n\n    Attributes:\n        dataset_name: The name of the dataset in Argilla.\n        dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to\n            `None`, which means it will be created in the default workspace.\n        api_url: The URL of the Argilla API. Defaults to `None`, which means it will be read from\n            the `ARGILLA_API_URL` environment variable.\n        api_key: The API key to authenticate with Argilla. Defaults to `None`, which means it will\n            be read from the `ARGILLA_API_KEY` environment variable.\n\n    Runtime parameters:\n        - `api_url`: The base URL to use for the Argilla API requests.\n        - `api_key`: The API key to authenticate the requests to the Argilla API.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the completion.\n        - generation (`str` or `List[str]`): The completions that were generated based on the input instruction.\n    \"\"\"\n\n    _id: str = PrivateAttr(default=\"id\")\n    _instruction: str = PrivateAttr(...)\n    _generation: str = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Sets the `_instruction` and `_generation` attributes based on the `inputs_mapping`, otherwise\n        uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n        the text-generation scenario. And then it pushes it to Argilla.\n        \"\"\"\n        super().load()\n\n        self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n        self._generation = self.input_mappings.get(\"generation\", \"generation\")\n\n        if self._rg_dataset_exists():\n            _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n                name=self.dataset_name,\n                workspace=self.dataset_workspace,\n            )\n\n            for field in _rg_dataset.fields:\n                if (\n                    field.name not in [self._id, self._instruction, self._generation]\n                    and field.required\n                ):\n                    raise ValueError(\n                        f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                        f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`\"\n                        f\", nor `{self._generation}`.\"\n                    )\n\n            self._rg_dataset = _rg_dataset\n        else:\n            _rg_dataset = rg.FeedbackDataset(  # type: ignore\n                fields=[\n                    rg.TextField(name=self._id, title=self._id),  # type: ignore\n                    rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                    rg.TextField(name=self._generation, title=self._generation),  # type: ignore\n                ],\n                questions=[\n                    rg.LabelQuestion(  # type: ignore\n                        name=\"quality\",\n                        title=f\"What's the quality of the {self._generation} for the given {self._instruction}?\",\n                        labels={\"bad\": \"\ud83d\udc4e\", \"good\": \"\ud83d\udc4d\"},\n                    )\n                ],\n            )\n            self._rg_dataset = _rg_dataset.push_to_argilla(\n                name=self.dataset_name,  # type: ignore\n                workspace=self.dataset_workspace,\n            )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the step are the `instruction` and the `generation`.\"\"\"\n        return [\"instruction\", \"generation\"]\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        records = []\n        for input in inputs:\n            # Generate the SHA-256 hash of the instruction to use it as the metadata\n            instruction_id = hashlib.sha256(\n                input[\"instruction\"].encode(\"utf-8\")\n            ).hexdigest()\n\n            generations = input[\"generation\"]\n\n            # If the `generation` is not a list, then convert it into a list\n            if not isinstance(generations, list):\n                generations = [generations]\n\n            # Create a `generations_set` to avoid adding duplicates\n            generations_set = set()\n\n            for generation in generations:\n                # If the generation is already in the set, then skip it\n                if generation in generations_set:\n                    continue\n                # Otherwise, add it to the set\n                generations_set.add(generation)\n\n                records.append(\n                    rg.FeedbackRecord(  # type: ignore\n                        fields={\n                            self._id: instruction_id,\n                            self._instruction: input[\"instruction\"],\n                            self._generation: generation,\n                        },\n                    )\n                )\n        self._rg_dataset.add_records(records)  # type: ignore\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.TextGenerationToArgilla.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the step are the <code>instruction</code> and the <code>generation</code>.</p>"},{"location":"reference/distilabel/steps/#distilabel.steps.TextGenerationToArgilla.load","title":"<code>load()</code>","text":"<p>Sets the <code>_instruction</code> and <code>_generation</code> attributes based on the <code>inputs_mapping</code>, otherwise uses the default values; and then uses those values to create a <code>FeedbackDataset</code> suited for the text-generation scenario. And then it pushes it to Argilla.</p> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Sets the `_instruction` and `_generation` attributes based on the `inputs_mapping`, otherwise\n    uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n    the text-generation scenario. And then it pushes it to Argilla.\n    \"\"\"\n    super().load()\n\n    self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n    self._generation = self.input_mappings.get(\"generation\", \"generation\")\n\n    if self._rg_dataset_exists():\n        _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n            name=self.dataset_name,\n            workspace=self.dataset_workspace,\n        )\n\n        for field in _rg_dataset.fields:\n            if (\n                field.name not in [self._id, self._instruction, self._generation]\n                and field.required\n            ):\n                raise ValueError(\n                    f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                    f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`\"\n                    f\", nor `{self._generation}`.\"\n                )\n\n        self._rg_dataset = _rg_dataset\n    else:\n        _rg_dataset = rg.FeedbackDataset(  # type: ignore\n            fields=[\n                rg.TextField(name=self._id, title=self._id),  # type: ignore\n                rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                rg.TextField(name=self._generation, title=self._generation),  # type: ignore\n            ],\n            questions=[\n                rg.LabelQuestion(  # type: ignore\n                    name=\"quality\",\n                    title=f\"What's the quality of the {self._generation} for the given {self._instruction}?\",\n                    labels={\"bad\": \"\ud83d\udc4e\", \"good\": \"\ud83d\udc4d\"},\n                )\n            ],\n        )\n        self._rg_dataset = _rg_dataset.push_to_argilla(\n            name=self.dataset_name,  # type: ignore\n            workspace=self.dataset_workspace,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.TextGenerationToArgilla.process","title":"<code>process(inputs)</code>","text":"<p>Creates and pushes the records as FeedbackRecords to the Argilla dataset.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    records = []\n    for input in inputs:\n        # Generate the SHA-256 hash of the instruction to use it as the metadata\n        instruction_id = hashlib.sha256(\n            input[\"instruction\"].encode(\"utf-8\")\n        ).hexdigest()\n\n        generations = input[\"generation\"]\n\n        # If the `generation` is not a list, then convert it into a list\n        if not isinstance(generations, list):\n            generations = [generations]\n\n        # Create a `generations_set` to avoid adding duplicates\n        generations_set = set()\n\n        for generation in generations:\n            # If the generation is already in the set, then skip it\n            if generation in generations_set:\n                continue\n            # Otherwise, add it to the set\n            generations_set.add(generation)\n\n            records.append(\n                rg.FeedbackRecord(  # type: ignore\n                    fields={\n                        self._id: instruction_id,\n                        self._instruction: input[\"instruction\"],\n                        self._generation: generation,\n                    },\n                )\n            )\n    self._rg_dataset.add_records(records)  # type: ignore\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/#distilabel.steps.step","title":"<code>step(inputs=None, outputs=None, step_type='normal')</code>","text":"<p>Creates an <code>Step</code> from a processing function.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[List[str], None]</code> <p>a list containing the name of the inputs columns/keys expected by this step. If not provided the default will be an empty list <code>[]</code> and it will be assumed that the step doesn't need any specific columns. Defaults to <code>None</code>.</p> <code>None</code> <code>outputs</code> <code>Union[List[str], None]</code> <p>a list containing the name of the outputs columns/keys that the step will generate. If not provided the default will be an empty list <code>[]</code> and it will be assumed that the step doesn't need any specific columns. Defaults to <code>None</code>.</p> <code>None</code> <code>step_type</code> <code>Literal['normal', 'global', 'generator']</code> <p>the kind of step to create. Valid choices are: \"normal\" (<code>Step</code>), \"global\" (<code>GlobalStep</code>) or \"generator\" (<code>GeneratorStep</code>). Defaults to <code>\"normal\"</code>.</p> <code>'normal'</code> <p>Returns:</p> Type Description <code>Callable[..., Type[_Step]]</code> <p>A callable that will generate the type given the processing function.</p> <p>Example:</p> <pre><code># Normal step\n@step(inputs=[\"instruction\"], outputs=[\"generation\"])\ndef GenerationStep(inputs: StepInput, dummy_generation: RuntimeParameter[str]) -&gt; StepOutput:\n    for input in inputs:\n        input[\"generation\"] = dummy_generation\n    yield inputs\n\n# Global step\n@step(inputs=[\"instruction\"], step_type=\"global\")\ndef FilteringStep(inputs: StepInput, max_length: RuntimeParameter[int] = 256) -&gt; StepOutput:\n    yield [\n        input\n        for input in inputs\n        if len(input[\"instruction\"]) &lt;= max_length\n    ]\n\n# Generator step\n@step(outputs=[\"num\"], step_type=\"generator\")\ndef RowGenerator(num_rows: RuntimeParameter[int] = 500) -&gt; GeneratorStepOutput:\n    data = list(range(num_rows))\n    for i in range(0, len(data), 100):\n        last_batch = i + 100 &gt;= len(data)\n        yield [{\"num\": num} for num in data[i : i + 100]], last_batch\n</code></pre> Source code in <code>src/distilabel/steps/decorator.py</code> <pre><code>def step(\n    inputs: Union[List[str], None] = None,\n    outputs: Union[List[str], None] = None,\n    step_type: Literal[\"normal\", \"global\", \"generator\"] = \"normal\",\n) -&gt; Callable[..., Type[\"_Step\"]]:\n    \"\"\"Creates an `Step` from a processing function.\n\n    Args:\n        inputs: a list containing the name of the inputs columns/keys expected by this step.\n            If not provided the default will be an empty list `[]` and it will be assumed\n            that the step doesn't need any specific columns. Defaults to `None`.\n        outputs: a list containing the name of the outputs columns/keys that the step\n            will generate. If not provided the default will be an empty list `[]` and it\n            will be assumed that the step doesn't need any specific columns. Defaults to\n            `None`.\n        step_type: the kind of step to create. Valid choices are: \"normal\" (`Step`),\n            \"global\" (`GlobalStep`) or \"generator\" (`GeneratorStep`). Defaults to\n            `\"normal\"`.\n\n    Returns:\n        A callable that will generate the type given the processing function.\n\n    Example:\n\n    ```python\n    # Normal step\n    @step(inputs=[\"instruction\"], outputs=[\"generation\"])\n    def GenerationStep(inputs: StepInput, dummy_generation: RuntimeParameter[str]) -&gt; StepOutput:\n        for input in inputs:\n            input[\"generation\"] = dummy_generation\n        yield inputs\n\n    # Global step\n    @step(inputs=[\"instruction\"], step_type=\"global\")\n    def FilteringStep(inputs: StepInput, max_length: RuntimeParameter[int] = 256) -&gt; StepOutput:\n        yield [\n            input\n            for input in inputs\n            if len(input[\"instruction\"]) &lt;= max_length\n        ]\n\n    # Generator step\n    @step(outputs=[\"num\"], step_type=\"generator\")\n    def RowGenerator(num_rows: RuntimeParameter[int] = 500) -&gt; GeneratorStepOutput:\n        data = list(range(num_rows))\n        for i in range(0, len(data), 100):\n            last_batch = i + 100 &gt;= len(data)\n            yield [{\"num\": num} for num in data[i : i + 100]], last_batch\n    ```\n    \"\"\"\n\n    inputs = inputs or []\n    outputs = outputs or []\n\n    def decorator(func: ProcessingFunc) -&gt; Type[\"_Step\"]:\n        if step_type not in _STEP_MAPPING:\n            raise ValueError(\n                f\"Invalid step type '{step_type}'. Please, review the '{func.__name__}'\"\n                \" function decorated with the `@step` decorator and provide a valid\"\n                \" `step_type`. Valid choices are: 'normal', 'global' or 'generator'.\"\n            )\n\n        BaseClass = _STEP_MAPPING[step_type]\n\n        signature = inspect.signature(func)\n\n        runtime_parameters = {\n            name: (\n                param.annotation,\n                param.default if param.default != param.empty else None,\n            )\n            for name, param in signature.parameters.items()\n        }\n\n        runtime_parameters = {}\n        step_input_parameter = None\n        for name, param in signature.parameters.items():\n            if is_parameter_annotated_with(param, _RUNTIME_PARAMETER_ANNOTATION):\n                runtime_parameters[name] = (\n                    param.annotation,\n                    param.default if param.default != param.empty else None,\n                )\n\n            if not step_type == \"generator\" and is_parameter_annotated_with(\n                param, _STEP_INPUT_ANNOTATION\n            ):\n                if step_input_parameter is not None:\n                    raise ValueError(\n                        f\"Function '{func.__name__}' has more than one parameter annotated\"\n                        f\" with `StepInput`. Please, review the '{func.__name__}' function\"\n                        \" decorated with the `@step` decorator and provide only one\"\n                        \" argument annotated with `StepInput`.\"\n                    )\n                step_input_parameter = param\n\n        RuntimeParametersModel = create_model(  # type: ignore\n            \"RuntimeParametersModel\",\n            **runtime_parameters,  # type: ignore\n        )\n\n        def inputs_property(self) -&gt; List[str]:\n            return inputs\n\n        def outputs_property(self) -&gt; List[str]:\n            return outputs\n\n        def process(\n            self, *args: Any, **kwargs: Any\n        ) -&gt; Union[\"StepOutput\", \"GeneratorStepOutput\"]:\n            return func(*args, **kwargs)\n\n        return type(  # type: ignore\n            func.__name__,\n            (\n                BaseClass,\n                RuntimeParametersModel,\n            ),\n            {\n                \"process\": process,\n                \"inputs\": property(inputs_property),\n                \"outputs\": property(outputs_property),\n                \"__module__\": func.__module__,\n                \"__doc__\": func.__doc__,\n                \"_built_from_decorator\": True,\n                # Override the `get_process_step_input` method to return the parameter\n                # of the original function annotated with `StepInput`.\n                \"get_process_step_input\": lambda self: step_input_parameter,\n            },\n        )\n\n    return decorator\n</code></pre>"},{"location":"reference/distilabel/steps/base/","title":"Base","text":""},{"location":"reference/distilabel/steps/base/#distilabel.steps.base.StepInput","title":"<code>StepInput = Annotated[List[Dict[str, Any]], _STEP_INPUT_ANNOTATION]</code>  <code>module-attribute</code>","text":"<p>StepInput is just an <code>Annotated</code> alias of the typing <code>List[Dict[str, Any]]</code> with extra metadata that allows <code>distilabel</code> to perform validations over the <code>process</code> step method defined in each <code>Step</code></p>"},{"location":"reference/distilabel/steps/base/#distilabel.steps.base.GeneratorStep","title":"<code>GeneratorStep</code>","text":"<p>               Bases: <code>_Step</code>, <code>ABC</code></p> <p>A special kind of <code>Step</code> that is able to generate data i.e. it doesn't receive any input from the previous steps.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>RuntimeParameter[int]</code> <p>The number of rows that will contain the batches generated by the step. Defaults to <code>50</code>.</p> Runtime parameters <ul> <li><code>batch_size</code>: The number of rows that will contain the batches generated by     the step. Defaults to <code>50</code>.</li> </ul> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class GeneratorStep(_Step, ABC):\n    \"\"\"A special kind of `Step` that is able to generate data i.e. it doesn't receive\n    any input from the previous steps.\n\n    Attributes:\n        batch_size: The number of rows that will contain the batches generated by the\n            step. Defaults to `50`.\n\n    Runtime parameters:\n        - `batch_size`: The number of rows that will contain the batches generated by\n            the step. Defaults to `50`.\n    \"\"\"\n\n    batch_size: RuntimeParameter[int] = Field(\n        default=50,\n        description=\"The number of rows that will contain the batches generated by the\"\n        \" step.\",\n    )\n\n    @abstractmethod\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n        \"\"\"Method that defines the generation logic of the step. It should yield the\n        output rows and a boolean indicating if it's the last batch or not.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            The output rows and a boolean indicating if it's the last batch or not.\n        \"\"\"\n        pass\n\n    def process_applying_mappings(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n        \"\"\"Runs the `process` method of the step applying the `outputs_mappings` to the\n        output rows. This is the function that should be used to run the generation logic\n        of the step.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            The output rows and a boolean indicating if it's the last batch or not.\n        \"\"\"\n\n        # If the `Step` was built using the `@step` decorator, then we need to pass\n        # the runtime parameters as `kwargs`, so they can be used within the processing\n        # function\n        generator = (\n            self.process(offset=offset)\n            if not self._built_from_decorator\n            else self.process(offset=offset, **self._runtime_parameters)\n        )\n\n        for output_rows, last_batch in generator:\n            yield (\n                [\n                    {self.output_mappings.get(k, k): v for k, v in row.items()}\n                    for row in output_rows\n                ],\n                last_batch,\n            )\n</code></pre>"},{"location":"reference/distilabel/steps/base/#distilabel.steps.base.GeneratorStep.process","title":"<code>process(offset=0)</code>  <code>abstractmethod</code>","text":"<p>Method that defines the generation logic of the step. It should yield the output rows and a boolean indicating if it's the last batch or not.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>The output rows and a boolean indicating if it's the last batch or not.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>@abstractmethod\ndef process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n    \"\"\"Method that defines the generation logic of the step. It should yield the\n    output rows and a boolean indicating if it's the last batch or not.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        The output rows and a boolean indicating if it's the last batch or not.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/steps/base/#distilabel.steps.base.GeneratorStep.process_applying_mappings","title":"<code>process_applying_mappings(offset=0)</code>","text":"<p>Runs the <code>process</code> method of the step applying the <code>outputs_mappings</code> to the output rows. This is the function that should be used to run the generation logic of the step.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>The output rows and a boolean indicating if it's the last batch or not.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def process_applying_mappings(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n    \"\"\"Runs the `process` method of the step applying the `outputs_mappings` to the\n    output rows. This is the function that should be used to run the generation logic\n    of the step.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        The output rows and a boolean indicating if it's the last batch or not.\n    \"\"\"\n\n    # If the `Step` was built using the `@step` decorator, then we need to pass\n    # the runtime parameters as `kwargs`, so they can be used within the processing\n    # function\n    generator = (\n        self.process(offset=offset)\n        if not self._built_from_decorator\n        else self.process(offset=offset, **self._runtime_parameters)\n    )\n\n    for output_rows, last_batch in generator:\n        yield (\n            [\n                {self.output_mappings.get(k, k): v for k, v in row.items()}\n                for row in output_rows\n            ],\n            last_batch,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/base/#distilabel.steps.base.GlobalStep","title":"<code>GlobalStep</code>","text":"<p>               Bases: <code>Step</code>, <code>ABC</code></p> <p>A special kind of <code>Step</code> which it's <code>process</code> method receives all the data processed by their previous steps at once, instead of receiving it in batches. This kind of steps are useful when the processing logic requires to have all the data at once, for example to train a model, to perform a global aggregation, etc.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class GlobalStep(Step, ABC):\n    \"\"\"A special kind of `Step` which it's `process` method receives all the data processed\n    by their previous steps at once, instead of receiving it in batches. This kind of steps\n    are useful when the processing logic requires to have all the data at once, for example\n    to train a model, to perform a global aggregation, etc.\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        return []\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return []\n</code></pre>"},{"location":"reference/distilabel/steps/base/#distilabel.steps.base.Step","title":"<code>Step</code>","text":"<p>               Bases: <code>_Step</code>, <code>ABC</code></p> <p>Base class for the steps that can be included in a <code>Pipeline</code>.</p> <p>Attributes:</p> Name Type Description <code>input_batch_size</code> <code>RuntimeParameter[PositiveInt]</code> <p>The number of rows that will contain the batches processed by the step. Defaults to <code>50</code>.</p> Runtime parameters <ul> <li><code>input_batch_size</code>: The number of rows that will contain the batches processed     by the step. Defaults to <code>50</code>.</li> </ul> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>class Step(_Step, ABC):\n    \"\"\"Base class for the steps that can be included in a `Pipeline`.\n\n    Attributes:\n        input_batch_size: The number of rows that will contain the batches processed by\n            the step. Defaults to `50`.\n\n    Runtime parameters:\n        - `input_batch_size`: The number of rows that will contain the batches processed\n            by the step. Defaults to `50`.\n    \"\"\"\n\n    input_batch_size: RuntimeParameter[PositiveInt] = Field(\n        default=DEFAULT_INPUT_BATCH_SIZE,\n        description=\"The number of rows that will contain the batches processed by the\"\n        \" step.\",\n    )\n\n    @abstractmethod\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"Method that defines the processing logic of the step. It should yield the\n        output rows.\n\n        Args:\n            *inputs: An argument used to receive the outputs of the previous steps. The\n                number of arguments depends on the number of previous steps. It doesn't\n                need to be an `*args` argument, it can be a regular argument annotated\n                with `StepInput` if the step has only one previous step.\n        \"\"\"\n        pass\n\n    def process_applying_mappings(self, *args: List[Dict[str, Any]]) -&gt; \"StepOutput\":\n        \"\"\"Runs the `process` method of the step applying the `input_mappings` to the input\n        rows and the `outputs_mappings` to the output rows. This is the function that\n        should be used to run the processing logic of the step.\n\n        Yields:\n            The output rows.\n        \"\"\"\n\n        inputs = self._apply_input_mappings(args) if self.input_mappings else args\n\n        # If the `Step` was built using the `@step` decorator, then we need to pass\n        # the runtime parameters as kwargs, so they can be used within the processing\n        # function\n        generator = (\n            self.process(*inputs)\n            if not self._built_from_decorator\n            else self.process(*inputs, **self._runtime_parameters)\n        )\n\n        for output_rows in generator:\n            yield [\n                {\n                    # Apply output mapping and revert input mapping\n                    self.output_mappings.get(k, None)\n                    or self.input_mappings.get(k, None)\n                    or k: v\n                    for k, v in row.items()\n                }\n                for row in output_rows\n            ]\n\n    def _revert_input_mappings(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Reverts the `input_mappings` of the step to the input row.\n\n        Args:\n            input: The input row.\n\n        Returns:\n            The input row with the `input_mappings` reverted.\n        \"\"\"\n        return {self.input_mappings.get(k, k): v for k, v in input.items()}\n\n    def _apply_input_mappings(\n        self, inputs: Tuple[List[Dict[str, Any]], ...]\n    ) -&gt; List[List[Dict[str, Any]]]:\n        \"\"\"Applies the `input_mappings` to the input rows.\n\n        Args:\n            inputs: The input rows.\n\n        Returns:\n            The input rows with the `input_mappings` applied.\n        \"\"\"\n        reverted_input_mappings = {v: k for k, v in self.input_mappings.items()}\n\n        return [\n            [\n                {reverted_input_mappings.get(k, k): v for k, v in row.items()}\n                for row in row_inputs\n            ]\n            for row_inputs in inputs\n        ]\n</code></pre>"},{"location":"reference/distilabel/steps/base/#distilabel.steps.base.Step.process","title":"<code>process(*inputs)</code>  <code>abstractmethod</code>","text":"<p>Method that defines the processing logic of the step. It should yield the output rows.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>An argument used to receive the outputs of the previous steps. The number of arguments depends on the number of previous steps. It doesn't need to be an <code>*args</code> argument, it can be a regular argument annotated with <code>StepInput</code> if the step has only one previous step.</p> <code>()</code> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>@abstractmethod\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"Method that defines the processing logic of the step. It should yield the\n    output rows.\n\n    Args:\n        *inputs: An argument used to receive the outputs of the previous steps. The\n            number of arguments depends on the number of previous steps. It doesn't\n            need to be an `*args` argument, it can be a regular argument annotated\n            with `StepInput` if the step has only one previous step.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/steps/base/#distilabel.steps.base.Step.process_applying_mappings","title":"<code>process_applying_mappings(*args)</code>","text":"<p>Runs the <code>process</code> method of the step applying the <code>input_mappings</code> to the input rows and the <code>outputs_mappings</code> to the output rows. This is the function that should be used to run the processing logic of the step.</p> <p>Yields:</p> Type Description <code>StepOutput</code> <p>The output rows.</p> Source code in <code>src/distilabel/steps/base.py</code> <pre><code>def process_applying_mappings(self, *args: List[Dict[str, Any]]) -&gt; \"StepOutput\":\n    \"\"\"Runs the `process` method of the step applying the `input_mappings` to the input\n    rows and the `outputs_mappings` to the output rows. This is the function that\n    should be used to run the processing logic of the step.\n\n    Yields:\n        The output rows.\n    \"\"\"\n\n    inputs = self._apply_input_mappings(args) if self.input_mappings else args\n\n    # If the `Step` was built using the `@step` decorator, then we need to pass\n    # the runtime parameters as kwargs, so they can be used within the processing\n    # function\n    generator = (\n        self.process(*inputs)\n        if not self._built_from_decorator\n        else self.process(*inputs, **self._runtime_parameters)\n    )\n\n    for output_rows in generator:\n        yield [\n            {\n                # Apply output mapping and revert input mapping\n                self.output_mappings.get(k, None)\n                or self.input_mappings.get(k, None)\n                or k: v\n                for k, v in row.items()\n            }\n            for row in output_rows\n        ]\n</code></pre>"},{"location":"reference/distilabel/steps/combine/","title":"Combine","text":""},{"location":"reference/distilabel/steps/combine/#distilabel.steps.combine.CombineColumns","title":"<code>CombineColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Combines columns from a list of <code>StepInput</code>.</p> <p><code>CombineColumns</code> is a <code>Step</code> that implements the <code>process</code> method that calls the <code>combine_dicts</code> function to handle and combine a list of <code>StepInput</code>. Also <code>CombineColumns</code> provides two attributes <code>columns</code> and <code>output_columns</code> to specify the columns to merge and the output columns which will override the default value for the properties <code>inputs</code> and <code>outputs</code>, respectively.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[str]</code> <p>List of strings with the names of the columns to merge.</p> <code>output_columns</code> <code>Optional[List[str]]</code> <p>Optional list of strings with the names of the output columns.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to merge.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> and <code>output_columns</code> attributes): The columns     that were merged.</li> </ul> Source code in <code>src/distilabel/steps/combine.py</code> <pre><code>class CombineColumns(Step):\n    \"\"\"Combines columns from a list of `StepInput`.\n\n    `CombineColumns` is a `Step` that implements the `process` method that calls the `combine_dicts`\n    function to handle and combine a list of `StepInput`. Also `CombineColumns` provides two attributes\n    `columns` and `output_columns` to specify the columns to merge and the output columns\n    which will override the default value for the properties `inputs` and `outputs`, respectively.\n\n    Attributes:\n        columns: List of strings with the names of the columns to merge.\n        output_columns: Optional list of strings with the names of the output columns.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to merge.\n\n    Output columns:\n        - dynamic (determined by `columns` and `output_columns` attributes): The columns\n            that were merged.\n    \"\"\"\n\n    columns: List[str]\n    output_columns: Optional[List[str]] = None\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the column names in `output_columns` or\n        `merged_{column}` for each column in `columns`.\"\"\"\n        return (\n            self.output_columns\n            if self.output_columns is not None\n            else [f\"merged_{column}\" for column in self.columns]\n        )\n\n    @override\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"The `process` method calls the `combine_dicts` function to handle and combine a list of `StepInput`.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with the combined `StepInput` using the `combine_dicts` function.\n        \"\"\"\n        yield combine_dicts(\n            *inputs,\n            merge_keys=self.inputs,\n            output_merge_keys=self.outputs,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/combine/#distilabel.steps.combine.CombineColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the column names in <code>columns</code>.</p>"},{"location":"reference/distilabel/steps/combine/#distilabel.steps.combine.CombineColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the column names in <code>output_columns</code> or <code>merged_{column}</code> for each column in <code>columns</code>.</p>"},{"location":"reference/distilabel/steps/combine/#distilabel.steps.combine.CombineColumns.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method calls the <code>combine_dicts</code> function to handle and combine a list of <code>StepInput</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with the combined <code>StepInput</code> using the <code>combine_dicts</code> function.</p> Source code in <code>src/distilabel/steps/combine.py</code> <pre><code>@override\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"The `process` method calls the `combine_dicts` function to handle and combine a list of `StepInput`.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with the combined `StepInput` using the `combine_dicts` function.\n    \"\"\"\n    yield combine_dicts(\n        *inputs,\n        merge_keys=self.inputs,\n        output_merge_keys=self.outputs,\n    )\n</code></pre>"},{"location":"reference/distilabel/steps/constants/","title":"Constants","text":""},{"location":"reference/distilabel/steps/decorator/","title":"Decorator","text":""},{"location":"reference/distilabel/steps/decorator/#distilabel.steps.decorator.step","title":"<code>step(inputs=None, outputs=None, step_type='normal')</code>","text":"<p>Creates an <code>Step</code> from a processing function.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[List[str], None]</code> <p>a list containing the name of the inputs columns/keys expected by this step. If not provided the default will be an empty list <code>[]</code> and it will be assumed that the step doesn't need any specific columns. Defaults to <code>None</code>.</p> <code>None</code> <code>outputs</code> <code>Union[List[str], None]</code> <p>a list containing the name of the outputs columns/keys that the step will generate. If not provided the default will be an empty list <code>[]</code> and it will be assumed that the step doesn't need any specific columns. Defaults to <code>None</code>.</p> <code>None</code> <code>step_type</code> <code>Literal['normal', 'global', 'generator']</code> <p>the kind of step to create. Valid choices are: \"normal\" (<code>Step</code>), \"global\" (<code>GlobalStep</code>) or \"generator\" (<code>GeneratorStep</code>). Defaults to <code>\"normal\"</code>.</p> <code>'normal'</code> <p>Returns:</p> Type Description <code>Callable[..., Type[_Step]]</code> <p>A callable that will generate the type given the processing function.</p> <p>Example:</p> <pre><code># Normal step\n@step(inputs=[\"instruction\"], outputs=[\"generation\"])\ndef GenerationStep(inputs: StepInput, dummy_generation: RuntimeParameter[str]) -&gt; StepOutput:\n    for input in inputs:\n        input[\"generation\"] = dummy_generation\n    yield inputs\n\n# Global step\n@step(inputs=[\"instruction\"], step_type=\"global\")\ndef FilteringStep(inputs: StepInput, max_length: RuntimeParameter[int] = 256) -&gt; StepOutput:\n    yield [\n        input\n        for input in inputs\n        if len(input[\"instruction\"]) &lt;= max_length\n    ]\n\n# Generator step\n@step(outputs=[\"num\"], step_type=\"generator\")\ndef RowGenerator(num_rows: RuntimeParameter[int] = 500) -&gt; GeneratorStepOutput:\n    data = list(range(num_rows))\n    for i in range(0, len(data), 100):\n        last_batch = i + 100 &gt;= len(data)\n        yield [{\"num\": num} for num in data[i : i + 100]], last_batch\n</code></pre> Source code in <code>src/distilabel/steps/decorator.py</code> <pre><code>def step(\n    inputs: Union[List[str], None] = None,\n    outputs: Union[List[str], None] = None,\n    step_type: Literal[\"normal\", \"global\", \"generator\"] = \"normal\",\n) -&gt; Callable[..., Type[\"_Step\"]]:\n    \"\"\"Creates an `Step` from a processing function.\n\n    Args:\n        inputs: a list containing the name of the inputs columns/keys expected by this step.\n            If not provided the default will be an empty list `[]` and it will be assumed\n            that the step doesn't need any specific columns. Defaults to `None`.\n        outputs: a list containing the name of the outputs columns/keys that the step\n            will generate. If not provided the default will be an empty list `[]` and it\n            will be assumed that the step doesn't need any specific columns. Defaults to\n            `None`.\n        step_type: the kind of step to create. Valid choices are: \"normal\" (`Step`),\n            \"global\" (`GlobalStep`) or \"generator\" (`GeneratorStep`). Defaults to\n            `\"normal\"`.\n\n    Returns:\n        A callable that will generate the type given the processing function.\n\n    Example:\n\n    ```python\n    # Normal step\n    @step(inputs=[\"instruction\"], outputs=[\"generation\"])\n    def GenerationStep(inputs: StepInput, dummy_generation: RuntimeParameter[str]) -&gt; StepOutput:\n        for input in inputs:\n            input[\"generation\"] = dummy_generation\n        yield inputs\n\n    # Global step\n    @step(inputs=[\"instruction\"], step_type=\"global\")\n    def FilteringStep(inputs: StepInput, max_length: RuntimeParameter[int] = 256) -&gt; StepOutput:\n        yield [\n            input\n            for input in inputs\n            if len(input[\"instruction\"]) &lt;= max_length\n        ]\n\n    # Generator step\n    @step(outputs=[\"num\"], step_type=\"generator\")\n    def RowGenerator(num_rows: RuntimeParameter[int] = 500) -&gt; GeneratorStepOutput:\n        data = list(range(num_rows))\n        for i in range(0, len(data), 100):\n            last_batch = i + 100 &gt;= len(data)\n            yield [{\"num\": num} for num in data[i : i + 100]], last_batch\n    ```\n    \"\"\"\n\n    inputs = inputs or []\n    outputs = outputs or []\n\n    def decorator(func: ProcessingFunc) -&gt; Type[\"_Step\"]:\n        if step_type not in _STEP_MAPPING:\n            raise ValueError(\n                f\"Invalid step type '{step_type}'. Please, review the '{func.__name__}'\"\n                \" function decorated with the `@step` decorator and provide a valid\"\n                \" `step_type`. Valid choices are: 'normal', 'global' or 'generator'.\"\n            )\n\n        BaseClass = _STEP_MAPPING[step_type]\n\n        signature = inspect.signature(func)\n\n        runtime_parameters = {\n            name: (\n                param.annotation,\n                param.default if param.default != param.empty else None,\n            )\n            for name, param in signature.parameters.items()\n        }\n\n        runtime_parameters = {}\n        step_input_parameter = None\n        for name, param in signature.parameters.items():\n            if is_parameter_annotated_with(param, _RUNTIME_PARAMETER_ANNOTATION):\n                runtime_parameters[name] = (\n                    param.annotation,\n                    param.default if param.default != param.empty else None,\n                )\n\n            if not step_type == \"generator\" and is_parameter_annotated_with(\n                param, _STEP_INPUT_ANNOTATION\n            ):\n                if step_input_parameter is not None:\n                    raise ValueError(\n                        f\"Function '{func.__name__}' has more than one parameter annotated\"\n                        f\" with `StepInput`. Please, review the '{func.__name__}' function\"\n                        \" decorated with the `@step` decorator and provide only one\"\n                        \" argument annotated with `StepInput`.\"\n                    )\n                step_input_parameter = param\n\n        RuntimeParametersModel = create_model(  # type: ignore\n            \"RuntimeParametersModel\",\n            **runtime_parameters,  # type: ignore\n        )\n\n        def inputs_property(self) -&gt; List[str]:\n            return inputs\n\n        def outputs_property(self) -&gt; List[str]:\n            return outputs\n\n        def process(\n            self, *args: Any, **kwargs: Any\n        ) -&gt; Union[\"StepOutput\", \"GeneratorStepOutput\"]:\n            return func(*args, **kwargs)\n\n        return type(  # type: ignore\n            func.__name__,\n            (\n                BaseClass,\n                RuntimeParametersModel,\n            ),\n            {\n                \"process\": process,\n                \"inputs\": property(inputs_property),\n                \"outputs\": property(outputs_property),\n                \"__module__\": func.__module__,\n                \"__doc__\": func.__doc__,\n                \"_built_from_decorator\": True,\n                # Override the `get_process_step_input` method to return the parameter\n                # of the original function annotated with `StepInput`.\n                \"get_process_step_input\": lambda self: step_input_parameter,\n            },\n        )\n\n    return decorator\n</code></pre>"},{"location":"reference/distilabel/steps/deita/","title":"Deita","text":""},{"location":"reference/distilabel/steps/deita/#distilabel.steps.deita.DeitaFiltering","title":"<code>DeitaFiltering</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Filter dataset rows using DEITA filtering strategy.</p> <p>Filter the dataset based on the DEITA score and the cosine distance between the embeddings. It's an implementation of the filtering step from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>data_budget</code> <code>RuntimeParameter[int]</code> <p>The desired size of the dataset after filtering.</p> <code>diversity_threshold</code> <code>RuntimeParameter[float]</code> <p>If a row has a cosine distance with respect to it's nearest neighbor greater than this value, it will be included in the filtered dataset. Defaults to <code>0.9</code>.</p> <code>normalize_embeddings</code> <code>RuntimeParameter[bool]</code> <p>Whether to normalize the embeddings before computing the cosine distance. Defaults to <code>True</code>.</p> Runtime parameters <ul> <li><code>data_budget</code>: The desired size of the dataset after filtering.</li> <li><code>diversity_threshold</code>: If a row has a cosine distance with respect to it's nearest     neighbor greater than this value, it will be included in the filtered dataset.</li> </ul> Input columns <ul> <li>evol_instruction_score (<code>float</code>): The score of the instruction generated by     <code>ComplexityScorer</code> step.</li> <li>evol_response_score (<code>float</code>): The score of the response generated by     <code>QualityScorer</code> step.</li> <li>embedding (<code>List[float]</code>): The embedding generated for the conversation of the     instruction-response pair using <code>GenerateEmbeddings</code> step.</li> </ul> Output columns <ul> <li>deita_score (<code>float</code>): The DEITA score for the instruction-response pair.</li> <li>deita_score_computed_with (<code>List[str]</code>): The scores used to compute the DEITA     score.</li> <li>nearest_neighbor_distance (<code>float</code>): The cosine distance between the embeddings     of the instruction-response pair.</li> </ul> Categories <ul> <li>filtering</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/deita.py</code> <pre><code>class DeitaFiltering(GlobalStep):\n    \"\"\"Filter dataset rows using DEITA filtering strategy.\n\n    Filter the dataset based on the DEITA score and the cosine distance between the embeddings.\n    It's an implementation of the filtering step from the paper 'What Makes Good Data\n    for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        data_budget: The desired size of the dataset after filtering.\n        diversity_threshold: If a row has a cosine distance with respect to it's nearest\n            neighbor greater than this value, it will be included in the filtered dataset.\n            Defaults to `0.9`.\n        normalize_embeddings: Whether to normalize the embeddings before computing the cosine\n            distance. Defaults to `True`.\n\n    Runtime parameters:\n        - `data_budget`: The desired size of the dataset after filtering.\n        - `diversity_threshold`: If a row has a cosine distance with respect to it's nearest\n            neighbor greater than this value, it will be included in the filtered dataset.\n\n    Input columns:\n        - evol_instruction_score (`float`): The score of the instruction generated by\n            `ComplexityScorer` step.\n        - evol_response_score (`float`): The score of the response generated by\n            `QualityScorer` step.\n        - embedding (`List[float]`): The embedding generated for the conversation of the\n            instruction-response pair using `GenerateEmbeddings` step.\n\n    Output columns:\n        - deita_score (`float`): The DEITA score for the instruction-response pair.\n        - deita_score_computed_with (`List[str]`): The scores used to compute the DEITA\n            score.\n        - nearest_neighbor_distance (`float`): The cosine distance between the embeddings\n            of the instruction-response pair.\n\n    Categories:\n        - filtering\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    data_budget: RuntimeParameter[int] = Field(\n        default=None, description=\"The desired size of the dataset after filtering.\"\n    )\n    diversity_threshold: RuntimeParameter[float] = Field(\n        default=0.9,\n        description=\"If a row has a cosine distance with respect to it's nearest neighbor\"\n        \" greater than this value, it will be included in the filtered dataset.\",\n    )\n    normalize_embeddings: RuntimeParameter[bool] = Field(\n        default=True,\n        description=\"Whether to normalize the embeddings before computing the cosine distance.\",\n    )\n    distance_metric: RuntimeParameter[Literal[\"cosine\", \"manhattan\"]] = Field(\n        default=\"cosine\",\n        description=\"The distance metric to use. Currently only 'cosine' is supported.\",\n    )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        return [\"evol_instruction_score\", \"evol_response_score\", \"embedding\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return [\"deita_score\", \"nearest_neighbor_distance\", \"deita_score_computed_with\"]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Filter the dataset based on the DEITA score and the cosine distance between the\n        embeddings.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The filtered dataset.\n        \"\"\"\n        inputs = self._compute_deita_score(inputs)\n        inputs = self._compute_nearest_neighbor(inputs)\n        inputs.sort(key=lambda x: x[\"deita_score\"], reverse=True)\n\n        selected_rows = []\n        for input in inputs:\n            if len(selected_rows) &gt;= self.data_budget:  # type: ignore\n                break\n            if input[\"nearest_neighbor_distance\"] &gt;= self.diversity_threshold:\n                selected_rows.append(input)\n        yield selected_rows\n\n    def _compute_deita_score(self, inputs: StepInput) -&gt; StepInput:\n        \"\"\"Computes the DEITA score for each instruction-response pair. The DEITA score is\n        the product of the instruction score and the response score.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The input data with the DEITA score computed.\n        \"\"\"\n        for input_ in inputs:\n            evol_instruction_score = input_.get(\"evol_instruction_score\")\n            evol_response_score = input_.get(\"evol_response_score\")\n\n            if evol_instruction_score and evol_response_score:\n                deita_score = evol_instruction_score * evol_response_score\n                score_computed_with = [\"evol_instruction_score\", \"evol_response_score\"]\n            elif evol_instruction_score:\n                self._logger.warning(\n                    \"Response score is missing for the instruction-response pair. Using\"\n                    \" instruction score as DEITA score.\"\n                )\n                deita_score = evol_instruction_score\n                score_computed_with = [\"evol_instruction_score\"]\n            elif evol_response_score:\n                self._logger.warning(\n                    \"Instruction score is missing for the instruction-response pair. Using\"\n                    \" response score as DEITA score.\"\n                )\n                deita_score = evol_response_score\n                score_computed_with = [\"evol_response_score\"]\n            else:\n                self._logger.warning(\n                    \"Instruction and response scores are missing for the instruction-response\"\n                    \" pair. Setting DEITA score to 0.\"\n                )\n                deita_score = 0\n                score_computed_with = []\n\n            input_.update(\n                {\n                    \"deita_score\": deita_score,\n                    \"deita_score_computed_with\": score_computed_with,\n                }\n            )\n        return inputs\n\n    def _compute_nearest_neighbor(self, inputs: StepInput) -&gt; StepInput:\n        \"\"\"Computes the cosine distance between the embeddings of the instruction-response\n        pairs and the nearest neighbor.\n\n        Args:\n            inputs: The input data.\n\n        Returns:\n            The input data with the cosine distance computed.\n        \"\"\"\n        embeddings = np.array([input[\"embedding\"] for input in inputs])\n        if self.normalize_embeddings:\n            embeddings = self._normalize_embeddings(embeddings)\n        self._logger.info(\"\ud83d\udccf Computing nearest neighbor distance...\")\n\n        if self.distance_metric == \"cosine\":\n            self._logger.info(\"\ud83d\udccf Using cosine distance.\")\n            distances = self._cosine_distance(embeddings)\n        else:\n            self._logger.info(\"\ud83d\udccf Using manhattan distance.\")\n            distances = self._manhattan_distance(embeddings)\n\n        for distance, input in zip(distances, inputs):\n            input[\"nearest_neighbor_distance\"] = distance\n        return inputs\n\n    def _normalize_embeddings(self, embeddings: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize the embeddings.\n\n        Args:\n            embeddings: The embeddings to normalize.\n\n        Returns:\n            The normalized embeddings.\n        \"\"\"\n        self._logger.info(\"\u2696\ufe0f Normalizing embeddings...\")\n        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n        return embeddings / norms\n\n    def _cosine_distance(self, embeddings: np.array) -&gt; np.array:  # type: ignore\n        \"\"\"Computes the cosine distance between the embeddings.\n\n        Args:\n            embeddings: The embeddings.\n\n        Returns:\n            The cosine distance between the embeddings.\n        \"\"\"\n        cosine_similarity = np.dot(embeddings, embeddings.T)\n        cosine_distance = 1 - cosine_similarity\n        # Ignore self-distance\n        np.fill_diagonal(cosine_distance, np.inf)\n        return np.min(cosine_distance, axis=1)\n\n    def _manhattan_distance(self, embeddings: np.array) -&gt; np.array:  # type: ignore\n        \"\"\"Computes the manhattan distance between the embeddings.\n\n        Args:\n            embeddings: The embeddings.\n\n        Returns:\n            The manhattan distance between the embeddings.\n        \"\"\"\n        manhattan_distance = np.abs(embeddings[:, None] - embeddings).sum(-1)\n        # Ignore self-distance\n        np.fill_diagonal(manhattan_distance, np.inf)\n        return np.min(manhattan_distance, axis=1)\n</code></pre>"},{"location":"reference/distilabel/steps/deita/#distilabel.steps.deita.DeitaFiltering.process","title":"<code>process(inputs)</code>","text":"<p>Filter the dataset based on the DEITA score and the cosine distance between the embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>The input data.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>The filtered dataset.</p> Source code in <code>src/distilabel/steps/deita.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Filter the dataset based on the DEITA score and the cosine distance between the\n    embeddings.\n\n    Args:\n        inputs: The input data.\n\n    Returns:\n        The filtered dataset.\n    \"\"\"\n    inputs = self._compute_deita_score(inputs)\n    inputs = self._compute_nearest_neighbor(inputs)\n    inputs.sort(key=lambda x: x[\"deita_score\"], reverse=True)\n\n    selected_rows = []\n    for input in inputs:\n        if len(selected_rows) &gt;= self.data_budget:  # type: ignore\n            break\n        if input[\"nearest_neighbor_distance\"] &gt;= self.diversity_threshold:\n            selected_rows.append(input)\n    yield selected_rows\n</code></pre>"},{"location":"reference/distilabel/steps/expand/","title":"Expand","text":""},{"location":"reference/distilabel/steps/expand/#distilabel.steps.expand.ExpandColumns","title":"<code>ExpandColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Expand columns that contain lists into multiple rows.</p> <p><code>ExpandColumns</code> is a <code>Step</code> that takes a list of columns and expands them into multiple rows. The new rows will have the same data as the original row, except for the expanded column, which will contain a single item from the original list.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>Union[Dict[str, str], List[str]]</code> <p>A dictionary that maps the column to be expanded to the new column name or a list of columns to be expanded. If a list is provided, the new column name will be the same as the column name.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to be expanded into     multiple rows.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> attribute):  The expanded columns.</li> </ul> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>class ExpandColumns(Step):\n    \"\"\"Expand columns that contain lists into multiple rows.\n\n    `ExpandColumns` is a `Step` that takes a list of columns and expands them into multiple\n    rows. The new rows will have the same data as the original row, except for the expanded\n    column, which will contain a single item from the original list.\n\n    Attributes:\n        columns: A dictionary that maps the column to be expanded to the new column name\n            or a list of columns to be expanded. If a list is provided, the new column name\n            will be the same as the column name.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to be expanded into\n            multiple rows.\n\n    Output columns:\n        - dynamic (determined by `columns` attribute):  The expanded columns.\n    \"\"\"\n\n    columns: Union[Dict[str, str], List[str]]\n\n    @field_validator(\"columns\")\n    @classmethod\n    def always_dict(cls, value: Union[Dict[str, str], List[str]]) -&gt; Dict[str, str]:\n        \"\"\"Ensure that the columns are always a dictionary.\n\n        Args:\n            value: The columns to be expanded.\n\n        Returns:\n            The columns to be expanded as a dictionary.\n        \"\"\"\n        if isinstance(value, list):\n            return {col: col for col in value}\n\n        return value\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The columns to be expanded.\"\"\"\n        return list(self.columns.keys())\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The expanded columns.\"\"\"\n        return [\n            new_column if new_column else expand_column\n            for expand_column, new_column in self.columns.items()\n        ]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Expand the columns in the input data.\n\n        Args:\n            inputs: The input data.\n\n        Yields:\n            The expanded rows.\n        \"\"\"\n        yield [row for input in inputs for row in self._expand_columns(input)]\n\n    def _expand_columns(self, input: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Expand the columns in the input data.\n\n        Args:\n            input: The input data.\n\n        Returns:\n            The expanded rows.\n        \"\"\"\n        expanded_rows = []\n        for expand_column, new_column in self.columns.items():  # type: ignore\n            data = input.get(expand_column)\n            rows = []\n            for item, expanded in zip_longest(*[data, expanded_rows], fillvalue=input):\n                rows.append({**expanded, new_column: item})\n            expanded_rows = rows\n        return expanded_rows\n</code></pre>"},{"location":"reference/distilabel/steps/expand/#distilabel.steps.expand.ExpandColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The columns to be expanded.</p>"},{"location":"reference/distilabel/steps/expand/#distilabel.steps.expand.ExpandColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The expanded columns.</p>"},{"location":"reference/distilabel/steps/expand/#distilabel.steps.expand.ExpandColumns.always_dict","title":"<code>always_dict(value)</code>  <code>classmethod</code>","text":"<p>Ensure that the columns are always a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[Dict[str, str], List[str]]</code> <p>The columns to be expanded.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>The columns to be expanded as a dictionary.</p> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>@field_validator(\"columns\")\n@classmethod\ndef always_dict(cls, value: Union[Dict[str, str], List[str]]) -&gt; Dict[str, str]:\n    \"\"\"Ensure that the columns are always a dictionary.\n\n    Args:\n        value: The columns to be expanded.\n\n    Returns:\n        The columns to be expanded as a dictionary.\n    \"\"\"\n    if isinstance(value, list):\n        return {col: col for col in value}\n\n    return value\n</code></pre>"},{"location":"reference/distilabel/steps/expand/#distilabel.steps.expand.ExpandColumns.process","title":"<code>process(inputs)</code>","text":"<p>Expand the columns in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>The input data.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>The expanded rows.</p> Source code in <code>src/distilabel/steps/expand.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Expand the columns in the input data.\n\n    Args:\n        inputs: The input data.\n\n    Yields:\n        The expanded rows.\n    \"\"\"\n    yield [row for input in inputs for row in self._expand_columns(input)]\n</code></pre>"},{"location":"reference/distilabel/steps/keep/","title":"Keep","text":""},{"location":"reference/distilabel/steps/keep/#distilabel.steps.keep.KeepColumns","title":"<code>KeepColumns</code>","text":"<p>               Bases: <code>Step</code></p> <p>Keeps selected columns in the dataset.</p> <p><code>KeepColumns</code> is a <code>Step</code> that implements the <code>process</code> method that keeps only the columns specified in the <code>columns</code> attribute. Also <code>KeepColumns</code> provides an attribute <code>columns</code> to specify the columns to keep which will override the default value for the properties <code>inputs</code> and <code>outputs</code>.</p> Note <p>The order in which the columns are provided is important, as the output will be sorted using the provided order, which is useful before pushing either a <code>dataset.Dataset</code> via the <code>PushToHub</code> step or a <code>distilabel.Distiset</code> via the <code>Pipeline.run</code> output variable.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[str]</code> <p>List of strings with the names of the columns to keep.</p> Input columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to keep.</li> </ul> Output columns <ul> <li>dynamic (determined by <code>columns</code> attribute): The columns that were kept.</li> </ul> Source code in <code>src/distilabel/steps/keep.py</code> <pre><code>class KeepColumns(Step):\n    \"\"\"Keeps selected columns in the dataset.\n\n    `KeepColumns` is a `Step` that implements the `process` method that keeps only the columns\n    specified in the `columns` attribute. Also `KeepColumns` provides an attribute `columns` to\n    specify the columns to keep which will override the default value for the properties `inputs`\n    and `outputs`.\n\n    Note:\n        The order in which the columns are provided is important, as the output will be sorted\n        using the provided order, which is useful before pushing either a `dataset.Dataset` via\n        the `PushToHub` step or a `distilabel.Distiset` via the `Pipeline.run` output variable.\n\n    Attributes:\n        columns: List of strings with the names of the columns to keep.\n\n    Input columns:\n        - dynamic (determined by `columns` attribute): The columns to keep.\n\n    Output columns:\n        - dynamic (determined by `columns` attribute): The columns that were kept.\n    \"\"\"\n\n    columns: List[str]\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the column names in `columns`.\"\"\"\n        return self.columns\n\n    @override\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n        \"\"\"The `process` method keeps only the columns specified in the `columns` attribute.\n\n        Args:\n            *inputs: A list of dictionaries with the input data.\n\n        Yields:\n            A list of dictionaries with the output data.\n        \"\"\"\n        for input in inputs:\n            outputs = []\n            for item in input:\n                outputs.append({col: item[col] for col in self.columns})\n            yield outputs\n</code></pre>"},{"location":"reference/distilabel/steps/keep/#distilabel.steps.keep.KeepColumns.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the column names in <code>columns</code>.</p>"},{"location":"reference/distilabel/steps/keep/#distilabel.steps.keep.KeepColumns.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the column names in <code>columns</code>.</p>"},{"location":"reference/distilabel/steps/keep/#distilabel.steps.keep.KeepColumns.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method keeps only the columns specified in the <code>columns</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of dictionaries with the input data.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of dictionaries with the output data.</p> Source code in <code>src/distilabel/steps/keep.py</code> <pre><code>@override\ndef process(self, *inputs: StepInput) -&gt; \"StepOutput\":\n    \"\"\"The `process` method keeps only the columns specified in the `columns` attribute.\n\n    Args:\n        *inputs: A list of dictionaries with the input data.\n\n    Yields:\n        A list of dictionaries with the output data.\n    \"\"\"\n    for input in inputs:\n        outputs = []\n        for item in input:\n            outputs.append({col: item[col] for col in self.columns})\n        yield outputs\n</code></pre>"},{"location":"reference/distilabel/steps/typing/","title":"Typing","text":""},{"location":"reference/distilabel/steps/typing/#distilabel.steps.typing.GeneratorStepOutput","title":"<code>GeneratorStepOutput = Iterator[Tuple[List[Dict[str, Any]], bool]]</code>  <code>module-attribute</code>","text":"<p>GeneratorStepOutput is an alias of the typing <code>Iterator[Tuple[List[Dict[str, Any]], bool]]</code></p>"},{"location":"reference/distilabel/steps/typing/#distilabel.steps.typing.StepOutput","title":"<code>StepOutput = Iterator[List[Dict[str, Any]]]</code>  <code>module-attribute</code>","text":"<p>StepOutput is an alias of the typing <code>Iterator[List[Dict[str, Any]]]</code></p>"},{"location":"reference/distilabel/steps/argilla/","title":"Index","text":""},{"location":"reference/distilabel/steps/argilla/base/","title":"Base","text":""},{"location":"reference/distilabel/steps/argilla/base/#distilabel.steps.argilla.base.Argilla","title":"<code>Argilla</code>","text":"<p>               Bases: <code>Step</code>, <code>ABC</code></p> <p>Abstract step that provides a class to subclass from, that contains the boilerplate code required to interact with Argilla, as well as some extra validations on top of it. It also defines the abstract methods that need to be implemented in order to add a new dataset type as a step.</p> Note <p>This class is not intended to be instanced directly, but via subclass.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <code>RuntimeParameter[str]</code> <p>The name of the dataset in Argilla where the records will be added.</p> <code>dataset_workspace</code> <code>Optional[RuntimeParameter[str]]</code> <p>The workspace where the dataset will be created in Argilla. Defaults to <code>None</code>, which means it will be created in the default workspace.</p> <code>api_url</code> <code>Optional[RuntimeParameter[str]]</code> <p>The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_URL</code> environment variable.</p> <code>api_key</code> <code>Optional[RuntimeParameter[SecretStr]]</code> <p>The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> Runtime parameters <ul> <li><code>dataset_name</code>: The name of the dataset in Argilla where the records will be     added.</li> <li><code>dataset_workspace</code>: The workspace where the dataset will be created in Argilla.     Defaults to <code>None</code>, which means it will be created in the default workspace.</li> <li><code>api_url</code>: The base URL to use for the Argilla API requests.</li> <li><code>api_key</code>: The API key to authenticate the requests to the Argilla API.</li> </ul> Input columns <ul> <li>dynamic, based on the <code>inputs</code> value provided</li> </ul> Source code in <code>src/distilabel/steps/argilla/base.py</code> <pre><code>class Argilla(Step, ABC):\n    \"\"\"Abstract step that provides a class to subclass from, that contains the boilerplate code\n    required to interact with Argilla, as well as some extra validations on top of it. It also defines\n    the abstract methods that need to be implemented in order to add a new dataset type as a step.\n\n    Note:\n        This class is not intended to be instanced directly, but via subclass.\n\n    Attributes:\n        dataset_name: The name of the dataset in Argilla where the records will be added.\n        dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to\n            `None`, which means it will be created in the default workspace.\n        api_url: The URL of the Argilla API. Defaults to `None`, which means it will be read from\n            the `ARGILLA_API_URL` environment variable.\n        api_key: The API key to authenticate with Argilla. Defaults to `None`, which means it will\n            be read from the `ARGILLA_API_KEY` environment variable.\n\n    Runtime parameters:\n        - `dataset_name`: The name of the dataset in Argilla where the records will be\n            added.\n        - `dataset_workspace`: The workspace where the dataset will be created in Argilla.\n            Defaults to `None`, which means it will be created in the default workspace.\n        - `api_url`: The base URL to use for the Argilla API requests.\n        - `api_key`: The API key to authenticate the requests to the Argilla API.\n\n    Input columns:\n        - dynamic, based on the `inputs` value provided\n    \"\"\"\n\n    dataset_name: RuntimeParameter[str] = Field(\n        default=None, description=\"The name of the dataset in Argilla.\"\n    )\n    dataset_workspace: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The workspace where the dataset will be created in Argilla. Defaults\"\n        \"to `None` which means it will be created in the default workspace.\",\n    )\n\n    api_url: Optional[RuntimeParameter[str]] = Field(\n        default_factory=lambda: os.getenv(\"ARGILLA_API_URL\"),\n        description=\"The base URL to use for the Argilla API requests.\",\n    )\n    api_key: Optional[RuntimeParameter[SecretStr]] = Field(\n        default_factory=lambda: os.getenv(_ARGILLA_API_KEY_ENV_VAR_NAME),\n        description=\"The API key to authenticate the requests to the Argilla API.\",\n    )\n\n    _rg_dataset: Optional[\"RemoteFeedbackDataset\"] = PrivateAttr(...)\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Checks that the Argilla Python SDK is installed, and then filters the Argilla warnings.\"\"\"\n        super().model_post_init(__context)\n\n        try:\n            import argilla as rg  # noqa\n        except ImportError as ie:\n            raise ImportError(\n                \"Argilla is not installed. Please install it using `pip install argilla`.\"\n            ) from ie\n\n        warnings.filterwarnings(\"ignore\")\n\n    def _rg_init(self) -&gt; None:\n        \"\"\"Initializes the Argilla API client with the provided `api_url` and `api_key`.\"\"\"\n        try:\n            if \"hf.space\" in self.api_url and \"HF_TOKEN\" in os.environ:\n                headers = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n            else:\n                headers = None\n            rg.init(\n                api_url=self.api_url,\n                api_key=self.api_key.get_secret_value(),\n                extra_headers=headers,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(f\"Failed to initialize the Argilla API: {e}\") from e\n\n    def _rg_dataset_exists(self) -&gt; bool:\n        \"\"\"Checks if the dataset already exists in Argilla.\"\"\"\n        return self.dataset_name in [\n            dataset.name\n            for dataset in rg.FeedbackDataset.list(workspace=self.dataset_workspace)  # type: ignore\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs of the step is an empty list, since the steps subclassing from this one, will\n        always be leaf nodes and won't propagate the inputs neither generate any outputs.\n        \"\"\"\n        return []\n\n    def load(self) -&gt; None:\n        \"\"\"Method to perform any initialization logic before the `process` method is\n        called. For example, to load an LLM, stablish a connection to a database, etc.\n        \"\"\"\n        super().load()\n\n        self._rg_init()\n\n    @property\n    @abstractmethod\n    def inputs(self) -&gt; List[str]: ...\n\n    @abstractmethod\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\": ...\n</code></pre>"},{"location":"reference/distilabel/steps/argilla/base/#distilabel.steps.argilla.base.Argilla.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs of the step is an empty list, since the steps subclassing from this one, will always be leaf nodes and won't propagate the inputs neither generate any outputs.</p>"},{"location":"reference/distilabel/steps/argilla/base/#distilabel.steps.argilla.base.Argilla.load","title":"<code>load()</code>","text":"<p>Method to perform any initialization logic before the <code>process</code> method is called. For example, to load an LLM, stablish a connection to a database, etc.</p> Source code in <code>src/distilabel/steps/argilla/base.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Method to perform any initialization logic before the `process` method is\n    called. For example, to load an LLM, stablish a connection to a database, etc.\n    \"\"\"\n    super().load()\n\n    self._rg_init()\n</code></pre>"},{"location":"reference/distilabel/steps/argilla/base/#distilabel.steps.argilla.base.Argilla.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Checks that the Argilla Python SDK is installed, and then filters the Argilla warnings.</p> Source code in <code>src/distilabel/steps/argilla/base.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Checks that the Argilla Python SDK is installed, and then filters the Argilla warnings.\"\"\"\n    super().model_post_init(__context)\n\n    try:\n        import argilla as rg  # noqa\n    except ImportError as ie:\n        raise ImportError(\n            \"Argilla is not installed. Please install it using `pip install argilla`.\"\n        ) from ie\n\n    warnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"reference/distilabel/steps/argilla/preference/","title":"Preference","text":""},{"location":"reference/distilabel/steps/argilla/preference/#distilabel.steps.argilla.preference.PreferenceToArgilla","title":"<code>PreferenceToArgilla</code>","text":"<p>               Bases: <code>Argilla</code></p> <p>Creates a preference dataset in Argilla.</p> <p>Step that creates a dataset in Argilla during the load phase, and then pushes the input batches into it as records. This dataset is a preference dataset, where there's one field for the instruction and one extra field per each generation within the same record, and then a rating question per each of the generation fields. The rating question asks the annotator to set a rating from 1 to 5 for each of the provided generations.</p> Note <p>This step is meant to be used in conjunction with the <code>UltraFeedback</code> step, or any other step generating both ratings and responses for a given set of instruction and generations for the given instruction. But alternatively, it can also be used with any other task or step generating only the <code>instruction</code> and <code>generations</code>, as the <code>ratings</code> and <code>rationales</code> are optional.</p> <p>Attributes:</p> Name Type Description <code>num_generations</code> <code>int</code> <p>The number of generations to include in the dataset.</p> <code>dataset_name</code> <code>int</code> <p>The name of the dataset in Argilla.</p> <code>dataset_workspace</code> <code>int</code> <p>The workspace where the dataset will be created in Argilla. Defaults to <code>None</code>, which means it will be created in the default workspace.</p> <code>api_url</code> <code>int</code> <p>The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_URL</code> environment variable.</p> <code>api_key</code> <code>int</code> <p>The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> Runtime parameters <ul> <li><code>api_url</code>: The base URL to use for the Argilla API requests.</li> <li><code>api_key</code>: The API key to authenticate the requests to the Argilla API.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the completion.</li> <li>generations (<code>List[str]</code>): The completion that was generated based on the input instruction.</li> <li>ratings (<code>List[str]</code>, optional): The ratings for the generations. If not provided, the     generated ratings won't be pushed to Argilla.</li> <li>rationales (<code>List[str]</code>, optional): The rationales for the ratings. If not provided, the     generated rationales won't be pushed to Argilla.</li> </ul> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>class PreferenceToArgilla(Argilla):\n    \"\"\"Creates a preference dataset in Argilla.\n\n    Step that creates a dataset in Argilla during the load phase, and then pushes the input\n    batches into it as records. This dataset is a preference dataset, where there's one field\n    for the instruction and one extra field per each generation within the same record, and then\n    a rating question per each of the generation fields. The rating question asks the annotator to\n    set a rating from 1 to 5 for each of the provided generations.\n\n    Note:\n        This step is meant to be used in conjunction with the `UltraFeedback` step, or any other step\n        generating both ratings and responses for a given set of instruction and generations for the\n        given instruction. But alternatively, it can also be used with any other task or step generating\n        only the `instruction` and `generations`, as the `ratings` and `rationales` are optional.\n\n    Attributes:\n        num_generations: The number of generations to include in the dataset.\n        dataset_name: The name of the dataset in Argilla.\n        dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to\n            `None`, which means it will be created in the default workspace.\n        api_url: The URL of the Argilla API. Defaults to `None`, which means it will be read from\n            the `ARGILLA_API_URL` environment variable.\n        api_key: The API key to authenticate with Argilla. Defaults to `None`, which means it will\n            be read from the `ARGILLA_API_KEY` environment variable.\n\n    Runtime parameters:\n        - `api_url`: The base URL to use for the Argilla API requests.\n        - `api_key`: The API key to authenticate the requests to the Argilla API.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the completion.\n        - generations (`List[str]`): The completion that was generated based on the input instruction.\n        - ratings (`List[str]`, optional): The ratings for the generations. If not provided, the\n            generated ratings won't be pushed to Argilla.\n        - rationales (`List[str]`, optional): The rationales for the ratings. If not provided, the\n            generated rationales won't be pushed to Argilla.\n    \"\"\"\n\n    num_generations: int\n\n    _id: str = PrivateAttr(default=\"id\")\n    _instruction: str = PrivateAttr(...)\n    _generations: str = PrivateAttr(...)\n    _ratings: str = PrivateAttr(...)\n    _rationales: str = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Sets the `_instruction` and `_generations` attributes based on the `inputs_mapping`, otherwise\n        uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n        the text-generation scenario. And then it pushes it to Argilla.\n        \"\"\"\n        super().load()\n\n        # Both `instruction` and `generations` will be used as the fields of the dataset\n        self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n        self._generations = self.input_mappings.get(\"generations\", \"generations\")\n        # Both `ratings` and `rationales` will be used as suggestions to the default questions of the dataset\n        self._ratings = self.input_mappings.get(\"ratings\", \"ratings\")\n        self._rationales = self.input_mappings.get(\"rationales\", \"rationales\")\n\n        if self._rg_dataset_exists():\n            _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n                name=self.dataset_name,\n                workspace=self.dataset_workspace,\n            )\n\n            for field in _rg_dataset.fields:\n                if (\n                    field.name\n                    not in [self._id, self._instruction]\n                    + [\n                        f\"{self._generations}-{idx}\"\n                        for idx in range(self.num_generations)\n                    ]\n                    and field.required\n                ):\n                    raise ValueError(\n                        f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                        f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`,\"\n                        f\" nor `{self._generations}`.\"\n                    )\n\n            self._rg_dataset = _rg_dataset\n        else:\n            _rg_dataset = rg.FeedbackDataset(  # type: ignore\n                fields=[\n                    rg.TextField(name=self._id, title=self._id),  # type: ignore\n                    rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                    *self._generation_fields(),  # type: ignore\n                ],\n                questions=self._rating_rationale_pairs(),  # type: ignore\n            )\n            self._rg_dataset = _rg_dataset.push_to_argilla(\n                name=self.dataset_name,  # type: ignore\n                workspace=self.dataset_workspace,\n            )\n\n    def _generation_fields(self) -&gt; List[\"TextField\"]:\n        \"\"\"Method to generate the fields for each of the generations.\"\"\"\n        return [\n            rg.TextField(  # type: ignore\n                name=f\"{self._generations}-{idx}\",\n                title=f\"{self._generations}-{idx}\",\n                required=True if idx == 0 else False,\n            )\n            for idx in range(self.num_generations)\n        ]\n\n    def _rating_rationale_pairs(\n        self,\n    ) -&gt; List[Union[\"RatingQuestion\", \"TextQuestion\"]]:\n        \"\"\"Method to generate the rating and rationale questions for each of the generations.\"\"\"\n        questions = []\n        for idx in range(self.num_generations):\n            questions.extend(\n                [\n                    rg.RatingQuestion(  # type: ignore\n                        name=f\"{self._generations}-{idx}-rating\",\n                        title=f\"Rate {self._generations}-{idx} given {self._instruction}.\",\n                        description=f\"Ignore this question if the corresponding `{self._generations}-{idx}` field is not available.\"\n                        if idx != 0\n                        else None,\n                        values=[1, 2, 3, 4, 5],\n                        required=True if idx == 0 else False,\n                    ),\n                    rg.TextQuestion(  # type: ignore\n                        name=f\"{self._generations}-{idx}-rationale\",\n                        title=f\"Specify the rationale for {self._generations}-{idx}'s rating.\",\n                        description=f\"Ignore this question if the corresponding `{self._generations}-{idx}` field is not available.\"\n                        if idx != 0\n                        else None,\n                        required=False,\n                    ),\n                ]\n            )\n        return questions\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the step are the `instruction` and the `generations`. Optionally, one could also\n        provide the `ratings` and the `rationales` for the generations.\"\"\"\n        return [\"instruction\", \"generations\"]\n\n    def _add_suggestions_if_any(\n        self, input: Dict[str, Any]\n    ) -&gt; List[\"SuggestionSchema\"]:\n        \"\"\"Method to generate the suggestions for the `FeedbackRecord` based on the input.\"\"\"\n        # Since the `suggestions` i.e. answers to the `questions` are optional, will default to {}\n        suggestions = []\n        # If `ratings` is in `input`, then add those as suggestions\n        if self._ratings in input:\n            suggestions.extend(\n                [\n                    {\n                        \"question_name\": f\"{self._generations}-{idx}-rating\",\n                        \"value\": rating,\n                    }\n                    for idx, rating in enumerate(input[self._ratings])\n                    if rating is not None\n                    and isinstance(rating, int)\n                    and rating in [1, 2, 3, 4, 5]\n                ],\n            )\n        # If `rationales` is in `input`, then add those as suggestions\n        if self._rationales in input:\n            suggestions.extend(\n                [\n                    {\n                        \"question_name\": f\"{self._generations}-{idx}-rationale\",\n                        \"value\": rationale,\n                    }\n                    for idx, rationale in enumerate(input[self._rationales])\n                    if rationale is not None and isinstance(rationale, str)\n                ],\n            )\n        return suggestions\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        records = []\n        for input in inputs:\n            # Generate the SHA-256 hash of the instruction to use it as the metadata\n            instruction_id = hashlib.sha256(\n                input[\"instruction\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            generations = {\n                f\"{self._generations}-{idx}\": generation\n                for idx, generation in enumerate(input[\"generations\"])  # type: ignore\n            }\n\n            records.append(  # type: ignore\n                rg.FeedbackRecord(  # type: ignore\n                    fields={\n                        \"id\": instruction_id,\n                        \"instruction\": input[\"instruction\"],  # type: ignore\n                        **generations,\n                    },\n                    suggestions=self._add_suggestions_if_any(input),  # type: ignore\n                )\n            )\n        self._rg_dataset.add_records(records)  # type: ignore\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/argilla/preference/#distilabel.steps.argilla.preference.PreferenceToArgilla.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the step are the <code>instruction</code> and the <code>generations</code>. Optionally, one could also provide the <code>ratings</code> and the <code>rationales</code> for the generations.</p>"},{"location":"reference/distilabel/steps/argilla/preference/#distilabel.steps.argilla.preference.PreferenceToArgilla.load","title":"<code>load()</code>","text":"<p>Sets the <code>_instruction</code> and <code>_generations</code> attributes based on the <code>inputs_mapping</code>, otherwise uses the default values; and then uses those values to create a <code>FeedbackDataset</code> suited for the text-generation scenario. And then it pushes it to Argilla.</p> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Sets the `_instruction` and `_generations` attributes based on the `inputs_mapping`, otherwise\n    uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n    the text-generation scenario. And then it pushes it to Argilla.\n    \"\"\"\n    super().load()\n\n    # Both `instruction` and `generations` will be used as the fields of the dataset\n    self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n    self._generations = self.input_mappings.get(\"generations\", \"generations\")\n    # Both `ratings` and `rationales` will be used as suggestions to the default questions of the dataset\n    self._ratings = self.input_mappings.get(\"ratings\", \"ratings\")\n    self._rationales = self.input_mappings.get(\"rationales\", \"rationales\")\n\n    if self._rg_dataset_exists():\n        _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n            name=self.dataset_name,\n            workspace=self.dataset_workspace,\n        )\n\n        for field in _rg_dataset.fields:\n            if (\n                field.name\n                not in [self._id, self._instruction]\n                + [\n                    f\"{self._generations}-{idx}\"\n                    for idx in range(self.num_generations)\n                ]\n                and field.required\n            ):\n                raise ValueError(\n                    f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                    f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`,\"\n                    f\" nor `{self._generations}`.\"\n                )\n\n        self._rg_dataset = _rg_dataset\n    else:\n        _rg_dataset = rg.FeedbackDataset(  # type: ignore\n            fields=[\n                rg.TextField(name=self._id, title=self._id),  # type: ignore\n                rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                *self._generation_fields(),  # type: ignore\n            ],\n            questions=self._rating_rationale_pairs(),  # type: ignore\n        )\n        self._rg_dataset = _rg_dataset.push_to_argilla(\n            name=self.dataset_name,  # type: ignore\n            workspace=self.dataset_workspace,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/argilla/preference/#distilabel.steps.argilla.preference.PreferenceToArgilla.process","title":"<code>process(inputs)</code>","text":"<p>Creates and pushes the records as FeedbackRecords to the Argilla dataset.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/argilla/preference.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    records = []\n    for input in inputs:\n        # Generate the SHA-256 hash of the instruction to use it as the metadata\n        instruction_id = hashlib.sha256(\n            input[\"instruction\"].encode(\"utf-8\")  # type: ignore\n        ).hexdigest()\n\n        generations = {\n            f\"{self._generations}-{idx}\": generation\n            for idx, generation in enumerate(input[\"generations\"])  # type: ignore\n        }\n\n        records.append(  # type: ignore\n            rg.FeedbackRecord(  # type: ignore\n                fields={\n                    \"id\": instruction_id,\n                    \"instruction\": input[\"instruction\"],  # type: ignore\n                    **generations,\n                },\n                suggestions=self._add_suggestions_if_any(input),  # type: ignore\n            )\n        )\n    self._rg_dataset.add_records(records)  # type: ignore\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/argilla/text_generation/","title":"Text generation","text":""},{"location":"reference/distilabel/steps/argilla/text_generation/#distilabel.steps.argilla.text_generation.TextGenerationToArgilla","title":"<code>TextGenerationToArgilla</code>","text":"<p>               Bases: <code>Argilla</code></p> <p>Creates a text generation dataset in Argilla.</p> <p><code>Step</code> that creates a dataset in Argilla during the load phase, and then pushes the input batches into it as records. This dataset is a text-generation dataset, where there's one field per each input, and then a label question to rate the quality of the completion in either bad (represented with \ud83d\udc4e) or good (represented with \ud83d\udc4d).</p> Note <p>This step is meant to be used in conjunction with a <code>TextGeneration</code> step and no column mapping is needed, as it will use the default values for the <code>instruction</code> and <code>generation</code> columns.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <p>The name of the dataset in Argilla.</p> <code>dataset_workspace</code> <p>The workspace where the dataset will be created in Argilla. Defaults to <code>None</code>, which means it will be created in the default workspace.</p> <code>api_url</code> <p>The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_URL</code> environment variable.</p> <code>api_key</code> <p>The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> Runtime parameters <ul> <li><code>api_url</code>: The base URL to use for the Argilla API requests.</li> <li><code>api_key</code>: The API key to authenticate the requests to the Argilla API.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the completion.</li> <li>generation (<code>str</code> or <code>List[str]</code>): The completions that were generated based on the input instruction.</li> </ul> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>class TextGenerationToArgilla(Argilla):\n    \"\"\"Creates a text generation dataset in Argilla.\n\n    `Step` that creates a dataset in Argilla during the load phase, and then pushes the input\n    batches into it as records. This dataset is a text-generation dataset, where there's one field\n    per each input, and then a label question to rate the quality of the completion in either bad\n    (represented with \ud83d\udc4e) or good (represented with \ud83d\udc4d).\n\n    Note:\n        This step is meant to be used in conjunction with a `TextGeneration` step and no column mapping\n        is needed, as it will use the default values for the `instruction` and `generation` columns.\n\n    Attributes:\n        dataset_name: The name of the dataset in Argilla.\n        dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to\n            `None`, which means it will be created in the default workspace.\n        api_url: The URL of the Argilla API. Defaults to `None`, which means it will be read from\n            the `ARGILLA_API_URL` environment variable.\n        api_key: The API key to authenticate with Argilla. Defaults to `None`, which means it will\n            be read from the `ARGILLA_API_KEY` environment variable.\n\n    Runtime parameters:\n        - `api_url`: The base URL to use for the Argilla API requests.\n        - `api_key`: The API key to authenticate the requests to the Argilla API.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the completion.\n        - generation (`str` or `List[str]`): The completions that were generated based on the input instruction.\n    \"\"\"\n\n    _id: str = PrivateAttr(default=\"id\")\n    _instruction: str = PrivateAttr(...)\n    _generation: str = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Sets the `_instruction` and `_generation` attributes based on the `inputs_mapping`, otherwise\n        uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n        the text-generation scenario. And then it pushes it to Argilla.\n        \"\"\"\n        super().load()\n\n        self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n        self._generation = self.input_mappings.get(\"generation\", \"generation\")\n\n        if self._rg_dataset_exists():\n            _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n                name=self.dataset_name,\n                workspace=self.dataset_workspace,\n            )\n\n            for field in _rg_dataset.fields:\n                if (\n                    field.name not in [self._id, self._instruction, self._generation]\n                    and field.required\n                ):\n                    raise ValueError(\n                        f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                        f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`\"\n                        f\", nor `{self._generation}`.\"\n                    )\n\n            self._rg_dataset = _rg_dataset\n        else:\n            _rg_dataset = rg.FeedbackDataset(  # type: ignore\n                fields=[\n                    rg.TextField(name=self._id, title=self._id),  # type: ignore\n                    rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                    rg.TextField(name=self._generation, title=self._generation),  # type: ignore\n                ],\n                questions=[\n                    rg.LabelQuestion(  # type: ignore\n                        name=\"quality\",\n                        title=f\"What's the quality of the {self._generation} for the given {self._instruction}?\",\n                        labels={\"bad\": \"\ud83d\udc4e\", \"good\": \"\ud83d\udc4d\"},\n                    )\n                ],\n            )\n            self._rg_dataset = _rg_dataset.push_to_argilla(\n                name=self.dataset_name,  # type: ignore\n                workspace=self.dataset_workspace,\n            )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the step are the `instruction` and the `generation`.\"\"\"\n        return [\"instruction\", \"generation\"]\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        records = []\n        for input in inputs:\n            # Generate the SHA-256 hash of the instruction to use it as the metadata\n            instruction_id = hashlib.sha256(\n                input[\"instruction\"].encode(\"utf-8\")\n            ).hexdigest()\n\n            generations = input[\"generation\"]\n\n            # If the `generation` is not a list, then convert it into a list\n            if not isinstance(generations, list):\n                generations = [generations]\n\n            # Create a `generations_set` to avoid adding duplicates\n            generations_set = set()\n\n            for generation in generations:\n                # If the generation is already in the set, then skip it\n                if generation in generations_set:\n                    continue\n                # Otherwise, add it to the set\n                generations_set.add(generation)\n\n                records.append(\n                    rg.FeedbackRecord(  # type: ignore\n                        fields={\n                            self._id: instruction_id,\n                            self._instruction: input[\"instruction\"],\n                            self._generation: generation,\n                        },\n                    )\n                )\n        self._rg_dataset.add_records(records)  # type: ignore\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/argilla/text_generation/#distilabel.steps.argilla.text_generation.TextGenerationToArgilla.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the step are the <code>instruction</code> and the <code>generation</code>.</p>"},{"location":"reference/distilabel/steps/argilla/text_generation/#distilabel.steps.argilla.text_generation.TextGenerationToArgilla.load","title":"<code>load()</code>","text":"<p>Sets the <code>_instruction</code> and <code>_generation</code> attributes based on the <code>inputs_mapping</code>, otherwise uses the default values; and then uses those values to create a <code>FeedbackDataset</code> suited for the text-generation scenario. And then it pushes it to Argilla.</p> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Sets the `_instruction` and `_generation` attributes based on the `inputs_mapping`, otherwise\n    uses the default values; and then uses those values to create a `FeedbackDataset` suited for\n    the text-generation scenario. And then it pushes it to Argilla.\n    \"\"\"\n    super().load()\n\n    self._instruction = self.input_mappings.get(\"instruction\", \"instruction\")\n    self._generation = self.input_mappings.get(\"generation\", \"generation\")\n\n    if self._rg_dataset_exists():\n        _rg_dataset = rg.FeedbackDataset.from_argilla(  # type: ignore\n            name=self.dataset_name,\n            workspace=self.dataset_workspace,\n        )\n\n        for field in _rg_dataset.fields:\n            if (\n                field.name not in [self._id, self._instruction, self._generation]\n                and field.required\n            ):\n                raise ValueError(\n                    f\"The dataset {self.dataset_name} in the workspace {self.dataset_workspace} already exists,\"\n                    f\" but contains at least a required field that is neither `{self._id}`, `{self._instruction}`\"\n                    f\", nor `{self._generation}`.\"\n                )\n\n        self._rg_dataset = _rg_dataset\n    else:\n        _rg_dataset = rg.FeedbackDataset(  # type: ignore\n            fields=[\n                rg.TextField(name=self._id, title=self._id),  # type: ignore\n                rg.TextField(name=self._instruction, title=self._instruction),  # type: ignore\n                rg.TextField(name=self._generation, title=self._generation),  # type: ignore\n            ],\n            questions=[\n                rg.LabelQuestion(  # type: ignore\n                    name=\"quality\",\n                    title=f\"What's the quality of the {self._generation} for the given {self._instruction}?\",\n                    labels={\"bad\": \"\ud83d\udc4e\", \"good\": \"\ud83d\udc4d\"},\n                )\n            ],\n        )\n        self._rg_dataset = _rg_dataset.push_to_argilla(\n            name=self.dataset_name,  # type: ignore\n            workspace=self.dataset_workspace,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/argilla/text_generation/#distilabel.steps.argilla.text_generation.TextGenerationToArgilla.process","title":"<code>process(inputs)</code>","text":"<p>Creates and pushes the records as FeedbackRecords to the Argilla dataset.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/argilla/text_generation.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Creates and pushes the records as FeedbackRecords to the Argilla dataset.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    records = []\n    for input in inputs:\n        # Generate the SHA-256 hash of the instruction to use it as the metadata\n        instruction_id = hashlib.sha256(\n            input[\"instruction\"].encode(\"utf-8\")\n        ).hexdigest()\n\n        generations = input[\"generation\"]\n\n        # If the `generation` is not a list, then convert it into a list\n        if not isinstance(generations, list):\n            generations = [generations]\n\n        # Create a `generations_set` to avoid adding duplicates\n        generations_set = set()\n\n        for generation in generations:\n            # If the generation is already in the set, then skip it\n            if generation in generations_set:\n                continue\n            # Otherwise, add it to the set\n            generations_set.add(generation)\n\n            records.append(\n                rg.FeedbackRecord(  # type: ignore\n                    fields={\n                        self._id: instruction_id,\n                        self._instruction: input[\"instruction\"],\n                        self._generation: generation,\n                    },\n                )\n            )\n    self._rg_dataset.add_records(records)  # type: ignore\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/","title":"Index","text":""},{"location":"reference/distilabel/steps/formatting/conversation/","title":"Conversation","text":""},{"location":"reference/distilabel/steps/formatting/conversation/#distilabel.steps.formatting.conversation.ConversationTemplate","title":"<code>ConversationTemplate</code>","text":"<p>               Bases: <code>Step</code></p> <p>Generate a conversation template from an instruction and a response.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to be used in the conversation.</li> <li>response (<code>str</code>): The response to be used in the conversation.</li> </ul> Output columns <ul> <li>conversation (<code>ChatType</code>): The conversation template.</li> </ul> Categories <ul> <li>format</li> <li>chat</li> <li>template</li> </ul> Source code in <code>src/distilabel/steps/formatting/conversation.py</code> <pre><code>class ConversationTemplate(Step):\n    \"\"\"Generate a conversation template from an instruction and a response.\n\n    Input columns:\n        - instruction (`str`): The instruction to be used in the conversation.\n        - response (`str`): The response to be used in the conversation.\n\n    Output columns:\n        - conversation (`ChatType`): The conversation template.\n\n    Categories:\n        - format\n        - chat\n        - template\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The instruction and response.\"\"\"\n        return [\"instruction\", \"response\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The conversation template.\"\"\"\n        return [\"conversation\"]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Generate a conversation template from an instruction and a response.\n\n        Args:\n            inputs: The input data.\n\n        Yields:\n            The input data with the conversation template.\n        \"\"\"\n        for input in inputs:\n            input[\"conversation\"] = [\n                {\"role\": \"user\", \"content\": input[\"instruction\"]},\n                {\"role\": \"assistant\", \"content\": input[\"response\"]},\n            ]\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/conversation/#distilabel.steps.formatting.conversation.ConversationTemplate.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The instruction and response.</p>"},{"location":"reference/distilabel/steps/formatting/conversation/#distilabel.steps.formatting.conversation.ConversationTemplate.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The conversation template.</p>"},{"location":"reference/distilabel/steps/formatting/conversation/#distilabel.steps.formatting.conversation.ConversationTemplate.process","title":"<code>process(inputs)</code>","text":"<p>Generate a conversation template from an instruction and a response.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>The input data.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>The input data with the conversation template.</p> Source code in <code>src/distilabel/steps/formatting/conversation.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Generate a conversation template from an instruction and a response.\n\n    Args:\n        inputs: The input data.\n\n    Yields:\n        The input data with the conversation template.\n    \"\"\"\n    for input in inputs:\n        input[\"conversation\"] = [\n            {\"role\": \"user\", \"content\": input[\"instruction\"]},\n            {\"role\": \"assistant\", \"content\": input[\"response\"]},\n        ]\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/dpo/","title":"Dpo","text":""},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatChatGenerationDPO","title":"<code>FormatChatGenerationDPO</code>","text":"<p>               Bases: <code>Step</code></p> <p>Format the output of a combination of a <code>ChatGeneration</code> + a preference task such as <code>UltraFeedback</code>, for Direct Preference Optimization (DPO) following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p> <p><code>FormatChatGenerationDPO</code> is a <code>Step</code> that formats the output of the combination of a <code>ChatGeneration</code> task with a preference <code>Task</code> i.e. a task generating <code>ratings</code>, so that those are used to rank the existing generations and provide the <code>chosen</code> and <code>rejected</code> generations based on the <code>ratings</code>.</p> Note <p>The <code>messages</code> column should contain at least one message from the user, the <code>generations</code> column should contain at least two generations, the <code>ratings</code> column should contain the same number of ratings as generations.</p> Input columns <ul> <li>messages (<code>List[Dict[str, str]]</code>): The conversation messages.</li> <li>generations (<code>List[str]</code>): The generations produced by the <code>LLM</code>.</li> <li>generation_models (<code>List[str]</code>, optional): The model names used to generate the <code>generations</code>,     only available if the <code>model_name</code> from the <code>ChatGeneration</code> task/s is combined into a single     column named this way, otherwise, it will be ignored.</li> <li>ratings (<code>List[float]</code>): The ratings for each of the <code>generations</code>, produced by a preference     task such as <code>UltraFeedback</code>.</li> </ul> Output columns <ul> <li>prompt (<code>str</code>): The user message used to generate the <code>generations</code> with the <code>LLM</code>.</li> <li>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</li> <li>chosen (<code>List[Dict[str, str]]</code>): The <code>chosen</code> generation based on the <code>ratings</code>.</li> <li>chosen_model (<code>str</code>, optional): The model name used to generate the <code>chosen</code> generation,     if the <code>generation_models</code> are available.</li> <li>chosen_rating (<code>float</code>): The rating of the <code>chosen</code> generation.</li> <li>rejected (<code>List[Dict[str, str]]</code>): The <code>rejected</code> generation based on the <code>ratings</code>.</li> <li>rejected_model (<code>str</code>, optional): The model name used to generate the <code>rejected</code> generation,     if the <code>generation_models</code> are available.</li> <li>rejected_rating (<code>float</code>): The rating of the <code>rejected</code> generation.</li> </ul> Categories <ul> <li>format</li> <li>chat-generation</li> <li>preference</li> <li>messages</li> <li>generations</li> </ul> Source code in <code>src/distilabel/steps/formatting/dpo.py</code> <pre><code>class FormatChatGenerationDPO(Step):\n    \"\"\"Format the output of a combination of a `ChatGeneration` + a preference task such as\n    `UltraFeedback`, for Direct Preference Optimization (DPO) following the standard formatting\n    from frameworks such as `axolotl` or `alignment-handbook`.\n\n    `FormatChatGenerationDPO` is a `Step` that formats the output of the combination of a `ChatGeneration`\n    task with a preference `Task` i.e. a task generating `ratings`, so that those are used to rank the\n    existing generations and provide the `chosen` and `rejected` generations based on the `ratings`.\n\n    Note:\n        The `messages` column should contain at least one message from the user, the `generations`\n        column should contain at least two generations, the `ratings` column should contain the same\n        number of ratings as generations.\n\n    Input columns:\n        - messages (`List[Dict[str, str]]`): The conversation messages.\n        - generations (`List[str]`): The generations produced by the `LLM`.\n        - generation_models (`List[str]`, optional): The model names used to generate the `generations`,\n            only available if the `model_name` from the `ChatGeneration` task/s is combined into a single\n            column named this way, otherwise, it will be ignored.\n        - ratings (`List[float]`): The ratings for each of the `generations`, produced by a preference\n            task such as `UltraFeedback`.\n\n    Output columns:\n        - prompt (`str`): The user message used to generate the `generations` with the `LLM`.\n        - prompt_id (`str`): The `SHA256` hash of the `prompt`.\n        - chosen (`List[Dict[str, str]]`): The `chosen` generation based on the `ratings`.\n        - chosen_model (`str`, optional): The model name used to generate the `chosen` generation,\n            if the `generation_models` are available.\n        - chosen_rating (`float`): The rating of the `chosen` generation.\n        - rejected (`List[Dict[str, str]]`): The `rejected` generation based on the `ratings`.\n        - rejected_model (`str`, optional): The model name used to generate the `rejected` generation,\n            if the `generation_models` are available.\n        - rejected_rating (`float`): The rating of the `rejected` generation.\n\n    Categories:\n        - format\n        - chat-generation\n        - preference\n        - messages\n        - generations\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of inputs required by the `Step`, which in this case are: `messages`, `generations`,\n        and `ratings`.\"\"\"\n        return [\"messages\", \"generations\", \"ratings\"]\n\n    @property\n    def optional_inputs(self) -&gt; List[str]:\n        \"\"\"List of optional inputs, which are not required by the `Step` but used if available,\n        which in this case is: `generation_models`.\"\"\"\n        return [\"generation_models\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of outputs generated by the `Step`, which are: `prompt`, `prompt_id`, `chosen`,\n        `chosen_model`, `chosen_rating`, `rejected`, `rejected_model`, `rejected_rating`. Both\n        the `chosen_model` and `rejected_model` being optional and only used if `generation_models`\n        is available.\n\n        Reference:\n            - Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n        \"\"\"\n        return [\n            \"prompt\",\n            \"prompt_id\",\n            \"chosen\",\n            \"chosen_model\",\n            \"chosen_rating\",\n            \"rejected\",\n            \"rejected_model\",\n            \"rejected_rating\",\n        ]\n\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n        according to the DPO formatting standard.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with batches of formatted `StepInput` following the DPO standard.\n        \"\"\"\n        for input in inputs:\n            for item in input:\n                item[\"prompt\"] = next(\n                    (\n                        turn[\"content\"]\n                        for turn in item[\"messages\"]\n                        if turn[\"role\"] == \"user\"\n                    ),\n                    None,\n                )\n                item[\"prompt_id\"] = hashlib.sha256(\n                    item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n                ).hexdigest()\n\n                chosen_idx = max(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n                item[\"chosen\"] = item[\"messages\"] + [\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": item[\"generations\"][chosen_idx],\n                    }\n                ]\n                if \"generation_models\" in item:\n                    item[\"chosen_model\"] = item[\"generation_models\"][chosen_idx]\n                item[\"chosen_rating\"] = item[\"ratings\"][chosen_idx]\n\n                rejected_idx = min(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n                item[\"rejected\"] = item[\"messages\"] + [\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": item[\"generations\"][rejected_idx],\n                    }\n                ]\n                if \"generation_models\" in item:\n                    item[\"rejected_model\"] = item[\"generation_models\"][rejected_idx]\n                item[\"rejected_rating\"] = item[\"ratings\"][rejected_idx]\n\n            yield input\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatChatGenerationDPO.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of inputs required by the <code>Step</code>, which in this case are: <code>messages</code>, <code>generations</code>, and <code>ratings</code>.</p>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatChatGenerationDPO.optional_inputs","title":"<code>optional_inputs: List[str]</code>  <code>property</code>","text":"<p>List of optional inputs, which are not required by the <code>Step</code> but used if available, which in this case is: <code>generation_models</code>.</p>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatChatGenerationDPO.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of outputs generated by the <code>Step</code>, which are: <code>prompt</code>, <code>prompt_id</code>, <code>chosen</code>, <code>chosen_model</code>, <code>chosen_rating</code>, <code>rejected</code>, <code>rejected_model</code>, <code>rejected_rating</code>. Both the <code>chosen_model</code> and <code>rejected_model</code> being optional and only used if <code>generation_models</code> is available.</p> Reference <ul> <li>Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</li> </ul>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatChatGenerationDPO.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method formats the received <code>StepInput</code> or list of <code>StepInput</code> according to the DPO formatting standard.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with batches of formatted <code>StepInput</code> following the DPO standard.</p> Source code in <code>src/distilabel/steps/formatting/dpo.py</code> <pre><code>def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n    according to the DPO formatting standard.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with batches of formatted `StepInput` following the DPO standard.\n    \"\"\"\n    for input in inputs:\n        for item in input:\n            item[\"prompt\"] = next(\n                (\n                    turn[\"content\"]\n                    for turn in item[\"messages\"]\n                    if turn[\"role\"] == \"user\"\n                ),\n                None,\n            )\n            item[\"prompt_id\"] = hashlib.sha256(\n                item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            chosen_idx = max(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n            item[\"chosen\"] = item[\"messages\"] + [\n                {\n                    \"role\": \"assistant\",\n                    \"content\": item[\"generations\"][chosen_idx],\n                }\n            ]\n            if \"generation_models\" in item:\n                item[\"chosen_model\"] = item[\"generation_models\"][chosen_idx]\n            item[\"chosen_rating\"] = item[\"ratings\"][chosen_idx]\n\n            rejected_idx = min(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n            item[\"rejected\"] = item[\"messages\"] + [\n                {\n                    \"role\": \"assistant\",\n                    \"content\": item[\"generations\"][rejected_idx],\n                }\n            ]\n            if \"generation_models\" in item:\n                item[\"rejected_model\"] = item[\"generation_models\"][rejected_idx]\n            item[\"rejected_rating\"] = item[\"ratings\"][rejected_idx]\n\n        yield input\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatTextGenerationDPO","title":"<code>FormatTextGenerationDPO</code>","text":"<p>               Bases: <code>Step</code></p> <p>Format the output of your LLMs for Direct Preference Optimization (DPO).</p> <p><code>FormatTextGenerationDPO</code> is a <code>Step</code> that formats the output of the combination of a <code>TextGeneration</code> task with a preference <code>Task</code> i.e. a task generating <code>ratings</code>, so that those are used to rank the existing generations and provide the <code>chosen</code> and <code>rejected</code> generations based on the <code>ratings</code>. Use this step to transform the output of a combination of a <code>TextGeneration</code> + a preference task such as <code>UltraFeedback</code> following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p> Note <p>The <code>generations</code> column should contain at least two generations, the <code>ratings</code> column should contain the same number of ratings as generations.</p> Input columns <ul> <li>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the     <code>generations</code>, if available.</li> <li>instruction (<code>str</code>): The instruction used to generate the <code>generations</code> with the <code>LLM</code>.</li> <li>generations (<code>List[str]</code>): The generations produced by the <code>LLM</code>.</li> <li>generation_models (<code>List[str]</code>, optional): The model names used to generate the <code>generations</code>,     only available if the <code>model_name</code> from the <code>TextGeneration</code> task/s is combined into a single     column named this way, otherwise, it will be ignored.</li> <li>ratings (<code>List[float]</code>): The ratings for each of the <code>generations</code>, produced by a preference     task such as <code>UltraFeedback</code>.</li> </ul> Output columns <ul> <li>prompt (<code>str</code>): The instruction used to generate the <code>generations</code> with the <code>LLM</code>.</li> <li>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</li> <li>chosen (<code>List[Dict[str, str]]</code>): The <code>chosen</code> generation based on the <code>ratings</code>.</li> <li>chosen_model (<code>str</code>, optional): The model name used to generate the <code>chosen</code> generation,     if the <code>generation_models</code> are available.</li> <li>chosen_rating (<code>float</code>): The rating of the <code>chosen</code> generation.</li> <li>rejected (<code>List[Dict[str, str]]</code>): The <code>rejected</code> generation based on the <code>ratings</code>.</li> <li>rejected_model (<code>str</code>, optional): The model name used to generate the <code>rejected</code> generation,     if the <code>generation_models</code> are available.</li> <li>rejected_rating (<code>float</code>): The rating of the <code>rejected</code> generation.</li> </ul> Categories <ul> <li>format</li> <li>text-generation</li> <li>preference</li> <li>instruction</li> <li>generations</li> </ul> Source code in <code>src/distilabel/steps/formatting/dpo.py</code> <pre><code>class FormatTextGenerationDPO(Step):\n    \"\"\"Format the output of your LLMs for Direct Preference Optimization (DPO).\n\n    `FormatTextGenerationDPO` is a `Step` that formats the output of the combination of a `TextGeneration`\n    task with a preference `Task` i.e. a task generating `ratings`, so that those are used to rank the\n    existing generations and provide the `chosen` and `rejected` generations based on the `ratings`.\n    Use this step to transform the output of a combination of a `TextGeneration` + a preference task such as\n    `UltraFeedback` following the standard formatting from frameworks such as `axolotl` or `alignment-handbook`.\n\n    Note:\n        The `generations` column should contain at least two generations, the `ratings` column should\n        contain the same number of ratings as generations.\n\n    Input columns:\n        - system_prompt (`str`, optional): The system prompt used within the `LLM` to generate the\n            `generations`, if available.\n        - instruction (`str`): The instruction used to generate the `generations` with the `LLM`.\n        - generations (`List[str]`): The generations produced by the `LLM`.\n        - generation_models (`List[str]`, optional): The model names used to generate the `generations`,\n            only available if the `model_name` from the `TextGeneration` task/s is combined into a single\n            column named this way, otherwise, it will be ignored.\n        - ratings (`List[float]`): The ratings for each of the `generations`, produced by a preference\n            task such as `UltraFeedback`.\n\n    Output columns:\n        - prompt (`str`): The instruction used to generate the `generations` with the `LLM`.\n        - prompt_id (`str`): The `SHA256` hash of the `prompt`.\n        - chosen (`List[Dict[str, str]]`): The `chosen` generation based on the `ratings`.\n        - chosen_model (`str`, optional): The model name used to generate the `chosen` generation,\n            if the `generation_models` are available.\n        - chosen_rating (`float`): The rating of the `chosen` generation.\n        - rejected (`List[Dict[str, str]]`): The `rejected` generation based on the `ratings`.\n        - rejected_model (`str`, optional): The model name used to generate the `rejected` generation,\n            if the `generation_models` are available.\n        - rejected_rating (`float`): The rating of the `rejected` generation.\n\n    Categories:\n        - format\n        - text-generation\n        - preference\n        - instruction\n        - generations\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of inputs required by the `Step`, which in this case are: `instruction`, `generations`,\n        and `ratings`.\"\"\"\n        return [\"instruction\", \"generations\", \"ratings\"]\n\n    @property\n    def optional_inputs(self) -&gt; List[str]:\n        \"\"\"List of optional inputs, which are not required by the `Step` but used if available,\n        which in this case are: `system_prompt`, and `generation_models`.\"\"\"\n        return [\"system_prompt\", \"generation_models\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of outputs generated by the `Step`, which are: `prompt`, `prompt_id`, `chosen`,\n        `chosen_model`, `chosen_rating`, `rejected`, `rejected_model`, `rejected_rating`. Both\n        the `chosen_model` and `rejected_model` being optional and only used if `generation_models`\n        is available.\n\n        Reference:\n            - Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n        \"\"\"\n        return [\n            \"prompt\",\n            \"prompt_id\",\n            \"chosen\",\n            \"chosen_model\",\n            \"chosen_rating\",\n            \"rejected\",\n            \"rejected_model\",\n            \"rejected_rating\",\n        ]\n\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n        according to the DPO formatting standard.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with batches of formatted `StepInput` following the DPO standard.\n        \"\"\"\n        for input in inputs:\n            for item in input:\n                messages = [\n                    {\"role\": \"user\", \"content\": item[\"instruction\"]},  # type: ignore\n                ]\n                if (\n                    \"system_prompt\" in item\n                    and isinstance(item[\"system_prompt\"], str)  # type: ignore\n                    and len(item[\"system_prompt\"]) &gt; 0  # type: ignore\n                ):\n                    messages.insert(\n                        0,\n                        {\"role\": \"system\", \"content\": item[\"system_prompt\"]},  # type: ignore\n                    )\n\n                item[\"prompt\"] = item[\"instruction\"]\n                item[\"prompt_id\"] = hashlib.sha256(\n                    item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n                ).hexdigest()\n\n                chosen_idx = max(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n                item[\"chosen\"] = messages + [\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": item[\"generations\"][chosen_idx],\n                    }\n                ]\n                if \"generation_models\" in item:\n                    item[\"chosen_model\"] = item[\"generation_models\"][chosen_idx]\n                item[\"chosen_rating\"] = item[\"ratings\"][chosen_idx]\n\n                rejected_idx = min(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n                item[\"rejected\"] = messages + [\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": item[\"generations\"][rejected_idx],\n                    }\n                ]\n                if \"generation_models\" in item:\n                    item[\"rejected_model\"] = item[\"generation_models\"][rejected_idx]\n                item[\"rejected_rating\"] = item[\"ratings\"][rejected_idx]\n\n            yield input\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatTextGenerationDPO.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of inputs required by the <code>Step</code>, which in this case are: <code>instruction</code>, <code>generations</code>, and <code>ratings</code>.</p>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatTextGenerationDPO.optional_inputs","title":"<code>optional_inputs: List[str]</code>  <code>property</code>","text":"<p>List of optional inputs, which are not required by the <code>Step</code> but used if available, which in this case are: <code>system_prompt</code>, and <code>generation_models</code>.</p>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatTextGenerationDPO.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of outputs generated by the <code>Step</code>, which are: <code>prompt</code>, <code>prompt_id</code>, <code>chosen</code>, <code>chosen_model</code>, <code>chosen_rating</code>, <code>rejected</code>, <code>rejected_model</code>, <code>rejected_rating</code>. Both the <code>chosen_model</code> and <code>rejected_model</code> being optional and only used if <code>generation_models</code> is available.</p> Reference <ul> <li>Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</li> </ul>"},{"location":"reference/distilabel/steps/formatting/dpo/#distilabel.steps.formatting.dpo.FormatTextGenerationDPO.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method formats the received <code>StepInput</code> or list of <code>StepInput</code> according to the DPO formatting standard.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with batches of formatted <code>StepInput</code> following the DPO standard.</p> Source code in <code>src/distilabel/steps/formatting/dpo.py</code> <pre><code>def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n    according to the DPO formatting standard.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with batches of formatted `StepInput` following the DPO standard.\n    \"\"\"\n    for input in inputs:\n        for item in input:\n            messages = [\n                {\"role\": \"user\", \"content\": item[\"instruction\"]},  # type: ignore\n            ]\n            if (\n                \"system_prompt\" in item\n                and isinstance(item[\"system_prompt\"], str)  # type: ignore\n                and len(item[\"system_prompt\"]) &gt; 0  # type: ignore\n            ):\n                messages.insert(\n                    0,\n                    {\"role\": \"system\", \"content\": item[\"system_prompt\"]},  # type: ignore\n                )\n\n            item[\"prompt\"] = item[\"instruction\"]\n            item[\"prompt_id\"] = hashlib.sha256(\n                item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            chosen_idx = max(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n            item[\"chosen\"] = messages + [\n                {\n                    \"role\": \"assistant\",\n                    \"content\": item[\"generations\"][chosen_idx],\n                }\n            ]\n            if \"generation_models\" in item:\n                item[\"chosen_model\"] = item[\"generation_models\"][chosen_idx]\n            item[\"chosen_rating\"] = item[\"ratings\"][chosen_idx]\n\n            rejected_idx = min(enumerate(item[\"ratings\"]), key=lambda x: x[1])[0]\n            item[\"rejected\"] = messages + [\n                {\n                    \"role\": \"assistant\",\n                    \"content\": item[\"generations\"][rejected_idx],\n                }\n            ]\n            if \"generation_models\" in item:\n                item[\"rejected_model\"] = item[\"generation_models\"][rejected_idx]\n            item[\"rejected_rating\"] = item[\"ratings\"][rejected_idx]\n\n        yield input\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/sft/","title":"Sft","text":""},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatChatGenerationSFT","title":"<code>FormatChatGenerationSFT</code>","text":"<p>               Bases: <code>Step</code></p> <p>Format the output of a <code>ChatGeneration</code> task for Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p> <p><code>FormatChatGenerationSFT</code> is a <code>Step</code> that formats the output of a <code>ChatGeneration</code> task for Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>. The output of the <code>ChatGeneration</code> task is formatted into a chat-like conversation with the <code>instruction</code> as the user message and the <code>generation</code> as the assistant message. Optionally, if the <code>system_prompt</code> is available, it is included as the first message in the conversation.</p> Input columns <ul> <li>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the     <code>generation</code>, if available.</li> <li>instruction (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</li> <li>generation (<code>str</code>): The generation produced by the <code>LLM</code>.</li> </ul> Output columns <ul> <li>prompt (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</li> <li>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</li> <li>messages (<code>List[Dict[str, str]]</code>): The chat-like conversation with the <code>instruction</code> as     the user message and the <code>generation</code> as the assistant message.</li> </ul> Categories <ul> <li>format</li> <li>chat-generation</li> <li>instruction</li> <li>generation</li> </ul> Source code in <code>src/distilabel/steps/formatting/sft.py</code> <pre><code>class FormatChatGenerationSFT(Step):\n    \"\"\"Format the output of a `ChatGeneration` task for Supervised Fine-Tuning (SFT) following the\n    standard formatting from frameworks such as `axolotl` or `alignment-handbook`.\n\n    `FormatChatGenerationSFT` is a `Step` that formats the output of a `ChatGeneration` task for\n    Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as `axolotl`\n    or `alignment-handbook`. The output of the `ChatGeneration` task is formatted into a chat-like\n    conversation with the `instruction` as the user message and the `generation` as the assistant\n    message. Optionally, if the `system_prompt` is available, it is included as the first message\n    in the conversation.\n\n    Input columns:\n        - system_prompt (`str`, optional): The system prompt used within the `LLM` to generate the\n            `generation`, if available.\n        - instruction (`str`): The instruction used to generate the `generation` with the `LLM`.\n        - generation (`str`): The generation produced by the `LLM`.\n\n    Output columns:\n        - prompt (`str`): The instruction used to generate the `generation` with the `LLM`.\n        - prompt_id (`str`): The `SHA256` hash of the `prompt`.\n        - messages (`List[Dict[str, str]]`): The chat-like conversation with the `instruction` as\n            the user message and the `generation` as the assistant message.\n\n    Categories:\n        - format\n        - chat-generation\n        - instruction\n        - generation\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of inputs required by the `Step`, which in this case are: `instruction`, and `generation`.\"\"\"\n        return [\"messages\", \"generation\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of outputs generated by the `Step`, which are: `prompt`, `prompt_id`, `messages`.\n\n        Reference:\n            - Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n        \"\"\"\n        return [\"prompt\", \"prompt_id\", \"messages\"]\n\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n        according to the SFT formatting standard.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with batches of formatted `StepInput` following the SFT standard.\n        \"\"\"\n        for input in inputs:\n            for item in input:\n                item[\"prompt\"] = next(\n                    (\n                        turn[\"content\"]\n                        for turn in item[\"messages\"]\n                        if turn[\"role\"] == \"user\"\n                    ),\n                    None,\n                )\n\n                item[\"prompt_id\"] = hashlib.sha256(\n                    item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n                ).hexdigest()\n\n                item[\"messages\"] = item[\"messages\"] + [\n                    {\"role\": \"assistant\", \"content\": item[\"generation\"]},  # type: ignore\n                ]\n            yield input\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatChatGenerationSFT.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of inputs required by the <code>Step</code>, which in this case are: <code>instruction</code>, and <code>generation</code>.</p>"},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatChatGenerationSFT.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of outputs generated by the <code>Step</code>, which are: <code>prompt</code>, <code>prompt_id</code>, <code>messages</code>.</p> Reference <ul> <li>Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</li> </ul>"},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatChatGenerationSFT.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method formats the received <code>StepInput</code> or list of <code>StepInput</code> according to the SFT formatting standard.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with batches of formatted <code>StepInput</code> following the SFT standard.</p> Source code in <code>src/distilabel/steps/formatting/sft.py</code> <pre><code>def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n    according to the SFT formatting standard.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with batches of formatted `StepInput` following the SFT standard.\n    \"\"\"\n    for input in inputs:\n        for item in input:\n            item[\"prompt\"] = next(\n                (\n                    turn[\"content\"]\n                    for turn in item[\"messages\"]\n                    if turn[\"role\"] == \"user\"\n                ),\n                None,\n            )\n\n            item[\"prompt_id\"] = hashlib.sha256(\n                item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            item[\"messages\"] = item[\"messages\"] + [\n                {\"role\": \"assistant\", \"content\": item[\"generation\"]},  # type: ignore\n            ]\n        yield input\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatTextGenerationSFT","title":"<code>FormatTextGenerationSFT</code>","text":"<p>               Bases: <code>Step</code></p> <p>Format the output of a <code>TextGeneration</code> task for Supervised Fine-Tuning (SFT).</p> <p><code>FormatTextGenerationSFT</code> is a <code>Step</code> that formats the output of a <code>TextGeneration</code> task for Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>. The output of the <code>TextGeneration</code> task is formatted into a chat-like conversation with the <code>instruction</code> as the user message and the <code>generation</code> as the assistant message. Optionally, if the <code>system_prompt</code> is available, it is included as the first message in the conversation.</p> Input columns <ul> <li>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the     <code>generation</code>, if available.</li> <li>instruction (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</li> <li>generation (<code>str</code>): The generation produced by the <code>LLM</code>.</li> </ul> Output columns <ul> <li>prompt (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</li> <li>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</li> <li>messages (<code>List[Dict[str, str]]</code>): The chat-like conversation with the <code>instruction</code> as     the user message and the <code>generation</code> as the assistant message.</li> </ul> Categories <ul> <li>format</li> <li>text-generation</li> <li>instruction</li> <li>generation</li> </ul> Source code in <code>src/distilabel/steps/formatting/sft.py</code> <pre><code>class FormatTextGenerationSFT(Step):\n    \"\"\"Format the output of a `TextGeneration` task for Supervised Fine-Tuning (SFT).\n\n    `FormatTextGenerationSFT` is a `Step` that formats the output of a `TextGeneration` task for\n    Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as `axolotl`\n    or `alignment-handbook`. The output of the `TextGeneration` task is formatted into a chat-like\n    conversation with the `instruction` as the user message and the `generation` as the assistant\n    message. Optionally, if the `system_prompt` is available, it is included as the first message\n    in the conversation.\n\n    Input columns:\n        - system_prompt (`str`, optional): The system prompt used within the `LLM` to generate the\n            `generation`, if available.\n        - instruction (`str`): The instruction used to generate the `generation` with the `LLM`.\n        - generation (`str`): The generation produced by the `LLM`.\n\n    Output columns:\n        - prompt (`str`): The instruction used to generate the `generation` with the `LLM`.\n        - prompt_id (`str`): The `SHA256` hash of the `prompt`.\n        - messages (`List[Dict[str, str]]`): The chat-like conversation with the `instruction` as\n            the user message and the `generation` as the assistant message.\n\n    Categories:\n        - format\n        - text-generation\n        - instruction\n        - generation\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"List of inputs required by the `Step`, which in this case are: `instruction`, and `generation`.\"\"\"\n        return [\"instruction\", \"generation\"]\n\n    @property\n    def optional_inputs(self) -&gt; List[str]:\n        \"\"\"List of optional inputs, which are not required by the `Step` but used if available,\n        which in this case is: `system_prompt`.\"\"\"\n        return [\"system_prompt\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"List of outputs generated by the `Step`, which are: `prompt`, `prompt_id`, `messages`.\n\n        Reference:\n            - Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n        \"\"\"\n        return [\"prompt\", \"prompt_id\", \"messages\"]\n\n    def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n        according to the SFT formatting standard.\n\n        Args:\n            *inputs: A list of `StepInput` to be combined.\n\n        Yields:\n            A `StepOutput` with batches of formatted `StepInput` following the SFT standard.\n        \"\"\"\n        for input in inputs:\n            for item in input:\n                item[\"prompt\"] = item[\"instruction\"]\n\n                item[\"prompt_id\"] = hashlib.sha256(\n                    item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n                ).hexdigest()\n\n                item[\"messages\"] = [\n                    {\"role\": \"user\", \"content\": item[\"instruction\"]},  # type: ignore\n                    {\"role\": \"assistant\", \"content\": item[\"generation\"]},  # type: ignore\n                ]\n                if (\n                    \"system_prompt\" in item\n                    and isinstance(item[\"system_prompt\"], str)  # type: ignore\n                    and len(item[\"system_prompt\"]) &gt; 0  # type: ignore\n                ):\n                    item[\"messages\"].insert(\n                        0,\n                        {\"role\": \"system\", \"content\": item[\"system_prompt\"]},  # type: ignore\n                    )\n\n            yield input\n</code></pre>"},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatTextGenerationSFT.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>List of inputs required by the <code>Step</code>, which in this case are: <code>instruction</code>, and <code>generation</code>.</p>"},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatTextGenerationSFT.optional_inputs","title":"<code>optional_inputs: List[str]</code>  <code>property</code>","text":"<p>List of optional inputs, which are not required by the <code>Step</code> but used if available, which in this case is: <code>system_prompt</code>.</p>"},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatTextGenerationSFT.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>List of outputs generated by the <code>Step</code>, which are: <code>prompt</code>, <code>prompt_id</code>, <code>messages</code>.</p> Reference <ul> <li>Format inspired in https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</li> </ul>"},{"location":"reference/distilabel/steps/formatting/sft/#distilabel.steps.formatting.sft.FormatTextGenerationSFT.process","title":"<code>process(*inputs)</code>","text":"<p>The <code>process</code> method formats the received <code>StepInput</code> or list of <code>StepInput</code> according to the SFT formatting standard.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>StepInput</code> <p>A list of <code>StepInput</code> to be combined.</p> <code>()</code> <p>Yields:</p> Type Description <code>StepOutput</code> <p>A <code>StepOutput</code> with batches of formatted <code>StepInput</code> following the SFT standard.</p> Source code in <code>src/distilabel/steps/formatting/sft.py</code> <pre><code>def process(self, *inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"The `process` method formats the received `StepInput` or list of `StepInput`\n    according to the SFT formatting standard.\n\n    Args:\n        *inputs: A list of `StepInput` to be combined.\n\n    Yields:\n        A `StepOutput` with batches of formatted `StepInput` following the SFT standard.\n    \"\"\"\n    for input in inputs:\n        for item in input:\n            item[\"prompt\"] = item[\"instruction\"]\n\n            item[\"prompt_id\"] = hashlib.sha256(\n                item[\"prompt\"].encode(\"utf-8\")  # type: ignore\n            ).hexdigest()\n\n            item[\"messages\"] = [\n                {\"role\": \"user\", \"content\": item[\"instruction\"]},  # type: ignore\n                {\"role\": \"assistant\", \"content\": item[\"generation\"]},  # type: ignore\n            ]\n            if (\n                \"system_prompt\" in item\n                and isinstance(item[\"system_prompt\"], str)  # type: ignore\n                and len(item[\"system_prompt\"]) &gt; 0  # type: ignore\n            ):\n                item[\"messages\"].insert(\n                    0,\n                    {\"role\": \"system\", \"content\": item[\"system_prompt\"]},  # type: ignore\n                )\n\n        yield input\n</code></pre>"},{"location":"reference/distilabel/steps/generators/","title":"Index","text":""},{"location":"reference/distilabel/steps/generators/data/","title":"Data","text":""},{"location":"reference/distilabel/steps/generators/data/#distilabel.steps.generators.data.LoadDataFromDicts","title":"<code>LoadDataFromDicts</code>","text":"<p>               Bases: <code>GeneratorStep</code></p> <p>Loads a dataset from a list of dictionaries.</p> <p><code>GeneratorStep</code> that loads a dataset from a list of dictionaries and yields it in batches.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>List[Dict[str, Any]]</code> <p>The list of dictionaries to load the data from.</p> Runtime parameters <ul> <li><code>batch_size</code>: The batch size to use when processing the data.</li> </ul> Output columns <ul> <li>dynamic (based on the keys found on the first dictionary of the list): The columns     of the dataset.</li> </ul> Categories <ul> <li>load</li> </ul> Source code in <code>src/distilabel/steps/generators/data.py</code> <pre><code>class LoadDataFromDicts(GeneratorStep):\n    \"\"\"Loads a dataset from a list of dictionaries.\n\n    `GeneratorStep` that loads a dataset from a list of dictionaries and yields it in\n    batches.\n\n    Attributes:\n        data: The list of dictionaries to load the data from.\n\n    Runtime parameters:\n        - `batch_size`: The batch size to use when processing the data.\n\n    Output columns:\n        - dynamic (based on the keys found on the first dictionary of the list): The columns\n            of the dataset.\n\n    Categories:\n        - load\n    \"\"\"\n\n    data: List[Dict[str, Any]]\n\n    @override\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n        \"\"\"Yields batches from a list of dictionaries.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to `0`.\n\n        Yields:\n            A list of Python dictionaries as read from the inputs (propagated in batches)\n            and a flag indicating whether the yield batch is the last one.\n        \"\"\"\n        if offset:\n            self.data = self.data[offset:]\n\n        while self.data:\n            batch = self.data[: self.batch_size]\n            self.data = self.data[self.batch_size :]\n            yield (\n                batch,\n                True if len(self.data) == 0 else False,\n            )\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"Returns a list of strings with the names of the columns that the step will generate.\"\"\"\n        return list(self.data[0].keys())\n</code></pre>"},{"location":"reference/distilabel/steps/generators/data/#distilabel.steps.generators.data.LoadDataFromDicts.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>Returns a list of strings with the names of the columns that the step will generate.</p>"},{"location":"reference/distilabel/steps/generators/data/#distilabel.steps.generators.data.LoadDataFromDicts.process","title":"<code>process(offset=0)</code>","text":"<p>Yields batches from a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to <code>0</code>.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>A list of Python dictionaries as read from the inputs (propagated in batches)</p> <code>GeneratorStepOutput</code> <p>and a flag indicating whether the yield batch is the last one.</p> Source code in <code>src/distilabel/steps/generators/data.py</code> <pre><code>@override\ndef process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n    \"\"\"Yields batches from a list of dictionaries.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to `0`.\n\n    Yields:\n        A list of Python dictionaries as read from the inputs (propagated in batches)\n        and a flag indicating whether the yield batch is the last one.\n    \"\"\"\n    if offset:\n        self.data = self.data[offset:]\n\n    while self.data:\n        batch = self.data[: self.batch_size]\n        self.data = self.data[self.batch_size :]\n        yield (\n            batch,\n            True if len(self.data) == 0 else False,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/generators/huggingface/","title":"Huggingface","text":""},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromDisk","title":"<code>LoadDataFromDisk</code>","text":"<p>               Bases: <code>LoadDataFromHub</code></p> <p>Load a dataset that was previously saved to disk.</p> <p>If you previously saved your dataset using the <code>save_to_disk</code> method, or <code>Distiset.save_to_disk</code> you can load it again to build a new pipeline using this class.</p> <p>Attributes:</p> Name Type Description <code>dataset_path</code> <code>RuntimeParameter[Union[str, Path]]</code> <p>The path to the dataset or distiset.</p> <code>split</code> <code>Optional[RuntimeParameter[str]]</code> <p>The split of the dataset to load (typically will be <code>train</code>, <code>test</code> or <code>validation</code>).</p> <code>config</code> <code>RuntimeParameter[str]</code> <p>The configuration of the dataset to load. This is optional and only needed if the dataset has multiple configurations.</p> Runtime parameters <ul> <li><code>batch_size</code>: The batch size to use when processing the data.</li> <li><code>dataset_path</code>: The path to the dataset or distiset.</li> <li><code>is_distiset</code>: Whether the dataset to load is a <code>Distiset</code> or not. Defaults to False.</li> <li><code>split</code>: The split of the dataset to load. Defaults to 'train'.</li> <li><code>config</code>: The configuration of the dataset to load. This is optional and only     needed if the dataset has multiple configurations.</li> <li><code>num_examples</code>: The number of examples to load from the dataset.     By default will load all examples.</li> <li><code>storage_options</code>: Key/value pairs to be passed on to the file-system backend, if any.     Defaults to <code>None</code>.</li> </ul> Output columns <ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the     datasets loaded from the Hugging Face Hub.</li> </ul> Categories <ul> <li>load</li> </ul> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>class LoadDataFromDisk(LoadDataFromHub):\n    \"\"\"Load a dataset that was previously saved to disk.\n\n    If you previously saved your dataset using the `save_to_disk` method, or\n    `Distiset.save_to_disk` you can load it again to build a new pipeline using this class.\n\n    Attributes:\n        dataset_path: The path to the dataset or distiset.\n        split: The split of the dataset to load (typically will be `train`, `test` or `validation`).\n        config: The configuration of the dataset to load. This is optional and only needed\n            if the dataset has multiple configurations.\n\n    Runtime parameters:\n        - `batch_size`: The batch size to use when processing the data.\n        - `dataset_path`: The path to the dataset or distiset.\n        - `is_distiset`: Whether the dataset to load is a `Distiset` or not. Defaults to False.\n        - `split`: The split of the dataset to load. Defaults to 'train'.\n        - `config`: The configuration of the dataset to load. This is optional and only\n            needed if the dataset has multiple configurations.\n        - `num_examples`: The number of examples to load from the dataset.\n            By default will load all examples.\n        - `storage_options`: Key/value pairs to be passed on to the file-system backend, if any.\n            Defaults to `None`.\n\n    Output columns:\n        - dynamic (`all`): The columns that will be generated by this step, based on the\n            datasets loaded from the Hugging Face Hub.\n\n    Categories:\n        - load\n    \"\"\"\n\n    dataset_path: RuntimeParameter[Union[str, Path]] = Field(\n        default=None,\n        description=\"_summary_\",\n    )\n    config: RuntimeParameter[str] = Field(\n        default=None,\n        description=\"The configuration of the dataset to load. This is optional and only\"\n        \" needed if the dataset has multiple configurations.\",\n    )\n    is_distiset: Optional[RuntimeParameter[bool]] = Field(\n        default=False,\n        description=\"Whether the dataset to load is a `Distiset` or not. Defaults to False.\",\n    )\n    keep_in_memory: Optional[RuntimeParameter[bool]] = Field(\n        default=None,\n        description=\"Whether to copy the dataset in-memory, see `datasets.Dataset.load_from_disk` \"\n        \" for more information. Defaults to `None`.\",\n    )\n    split: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The split of the dataset to load. By default will load the whole Dataset/Distiset.\",\n    )\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset from the file/s in disk.\"\"\"\n        super(GeneratorStep, self).load()\n        if self.is_distiset:\n            ds = Distiset.load_from_disk(\n                self.dataset_path,\n                keep_in_memory=self.keep_in_memory,\n                storage_options=self.storage_options,\n            )\n            if self.config:\n                ds = ds[self.config]\n\n        else:\n            ds = load_from_disk(\n                self.dataset_path,\n                keep_in_memory=self.keep_in_memory,\n                storage_options=self.storage_options,\n            )\n\n        if self.split:\n            ds = ds[self.split]\n\n        self._dataset = ds\n\n        if self.num_examples:\n            self._dataset = self._dataset.select(range(self.num_examples))\n        else:\n            self.num_examples = len(self._dataset)\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The columns that will be generated by this step, based on the datasets from a file\n        in disk.\n\n        Returns:\n            The columns that will be generated by this step.\n        \"\"\"\n        # We assume there are Dataset/IterableDataset, not it's ...Dict counterparts\n        if self._dataset is Ellipsis:\n            raise ValueError(\n                \"Dataset not loaded yet, you must call `load` method first.\"\n            )\n\n        return self._dataset.column_names\n</code></pre>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromDisk.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The columns that will be generated by this step, based on the datasets from a file in disk.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The columns that will be generated by this step.</p>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromDisk.load","title":"<code>load()</code>","text":"<p>Load the dataset from the file/s in disk.</p> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset from the file/s in disk.\"\"\"\n    super(GeneratorStep, self).load()\n    if self.is_distiset:\n        ds = Distiset.load_from_disk(\n            self.dataset_path,\n            keep_in_memory=self.keep_in_memory,\n            storage_options=self.storage_options,\n        )\n        if self.config:\n            ds = ds[self.config]\n\n    else:\n        ds = load_from_disk(\n            self.dataset_path,\n            keep_in_memory=self.keep_in_memory,\n            storage_options=self.storage_options,\n        )\n\n    if self.split:\n        ds = ds[self.split]\n\n    self._dataset = ds\n\n    if self.num_examples:\n        self._dataset = self._dataset.select(range(self.num_examples))\n    else:\n        self.num_examples = len(self._dataset)\n</code></pre>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromFileSystem","title":"<code>LoadDataFromFileSystem</code>","text":"<p>               Bases: <code>LoadDataFromHub</code></p> <p>Loads a dataset from a file in your filesystem.</p> <p><code>GeneratorStep</code> that creates a dataset from a file in the filesystem, uses Hugging Face <code>datasets</code> library. Take a look at Hugging Face Datasets for more information of the supported file types.</p> <p>Attributes:</p> Name Type Description <code>data_files</code> <code>RuntimeParameter[Union[str, Path]]</code> <p>The path to the file, or directory containing the files that conform the dataset.</p> <code>split</code> <code>RuntimeParameter[Union[str, Path]]</code> <p>The split of the dataset to load (typically will be <code>train</code>, <code>test</code> or <code>validation</code>).</p> Runtime parameters <ul> <li><code>batch_size</code>: The batch size to use when processing the data.</li> <li><code>data_files</code>: The path to the file, or directory containing the files that conform     the dataset.</li> <li><code>split</code>: The split of the dataset to load. Defaults to 'train'.</li> <li><code>streaming</code>: Whether to load the dataset in streaming mode or not. Defaults to     <code>False</code>.</li> <li><code>num_examples</code>: The number of examples to load from the dataset.     By default will load all examples.</li> <li><code>storage_options</code>: Key/value pairs to be passed on to the file-system backend, if any.     Defaults to <code>None</code>.</li> <li><code>filetype</code>: The expected filetype. If not provided, it will be inferred from the file extension.     For more than one file, it will be inferred from the first file.</li> </ul> Output columns <ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the     datasets loaded from the Hugging Face Hub.</li> </ul> Categories <ul> <li>load</li> </ul> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>class LoadDataFromFileSystem(LoadDataFromHub):\n    \"\"\"Loads a dataset from a file in your filesystem.\n\n    `GeneratorStep` that creates a dataset from a file in the filesystem, uses Hugging Face `datasets`\n    library. Take a look at [Hugging Face Datasets](https://huggingface.co/docs/datasets/loading)\n    for more information of the supported file types.\n\n    Attributes:\n        data_files: The path to the file, or directory containing the files that conform\n            the dataset.\n        split: The split of the dataset to load (typically will be `train`, `test` or `validation`).\n\n    Runtime parameters:\n        - `batch_size`: The batch size to use when processing the data.\n        - `data_files`: The path to the file, or directory containing the files that conform\n            the dataset.\n        - `split`: The split of the dataset to load. Defaults to 'train'.\n        - `streaming`: Whether to load the dataset in streaming mode or not. Defaults to\n            `False`.\n        - `num_examples`: The number of examples to load from the dataset.\n            By default will load all examples.\n        - `storage_options`: Key/value pairs to be passed on to the file-system backend, if any.\n            Defaults to `None`.\n        - `filetype`: The expected filetype. If not provided, it will be inferred from the file extension.\n            For more than one file, it will be inferred from the first file.\n\n    Output columns:\n        - dynamic (`all`): The columns that will be generated by this step, based on the\n            datasets loaded from the Hugging Face Hub.\n\n    Categories:\n        - load\n    \"\"\"\n\n    data_files: RuntimeParameter[Union[str, Path]] = Field(\n        default=None,\n        description=\"The data files, or directory containing the data files, to generate the dataset from.\",\n    )\n    filetype: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The expected filetype. If not provided, it will be inferred from the file extension.\",\n    )\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset from the file/s in disk.\"\"\"\n        super(GeneratorStep, self).load()\n\n        data_path = UPath(self.data_files, storage_options=self.storage_options)\n\n        (data_files, self.filetype) = self._prepare_data_files(data_path)\n\n        self._dataset = load_dataset(\n            self.filetype,\n            data_files=data_files,\n            split=self.split,\n            streaming=self.streaming,\n            storage_options=self.storage_options,\n        )\n\n        if not self.streaming and self.num_examples:\n            self._dataset = self._dataset.select(range(self.num_examples))\n        if not self.num_examples:\n            if self.streaming:\n                # There's no better way to get the number of examples in a streaming dataset,\n                # load it again for the moment.\n                self.num_examples = len(\n                    load_dataset(\n                        self.filetype, data_files=self.data_files, split=self.split\n                    )\n                )\n            else:\n                self.num_examples = len(self._dataset)\n\n    @staticmethod\n    def _prepare_data_files(\n        data_path: UPath,\n    ) -&gt; Tuple[Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]], str]:\n        \"\"\"Prepare the loading process by setting the `data_files` attribute.\n\n        Args:\n            data_path: The path to the data files, or directory containing the data files.\n\n        Returns:\n            Tuple with the data files and the filetype.\n        \"\"\"\n\n        def get_filetype(data_path: UPath) -&gt; str:\n            filetype = data_path.suffix.lstrip(\".\")\n            if filetype == \"jsonl\":\n                filetype = \"json\"\n            return filetype\n\n        if data_path.is_file():\n            filetype = get_filetype(data_path)\n            data_files = str(data_path)\n        elif data_path.is_dir():\n            file_sequence = []\n            file_map = defaultdict(list)\n            for file_or_folder in data_path.iterdir():\n                if file_or_folder.is_file():\n                    file_sequence.append(str(file_or_folder))\n                elif file_or_folder.is_dir():\n                    for file in file_or_folder.iterdir():\n                        file_sequence.append(str(file))\n                        file_map[str(file_or_folder)].append(str(file))\n\n            data_files = file_sequence or file_map\n            # Try to obtain the filetype from any of the files, assuming all files have the same type.\n            if file_sequence:\n                filetype = get_filetype(UPath(file_sequence[0]))\n            else:\n                filetype = get_filetype(UPath(file_map[list(file_map.keys())[0]][0]))\n        return data_files, filetype\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The columns that will be generated by this step, based on the datasets from a file\n        in disk.\n\n        Returns:\n            The columns that will be generated by this step.\n        \"\"\"\n        # We assume there are Dataset/IterableDataset, not it's ...Dict counterparts\n        if self._dataset is Ellipsis:\n            raise ValueError(\n                \"Dataset not loaded yet, you must call `load` method first.\"\n            )\n\n        return self._dataset.column_names\n</code></pre>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromFileSystem.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The columns that will be generated by this step, based on the datasets from a file in disk.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The columns that will be generated by this step.</p>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromFileSystem.load","title":"<code>load()</code>","text":"<p>Load the dataset from the file/s in disk.</p> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset from the file/s in disk.\"\"\"\n    super(GeneratorStep, self).load()\n\n    data_path = UPath(self.data_files, storage_options=self.storage_options)\n\n    (data_files, self.filetype) = self._prepare_data_files(data_path)\n\n    self._dataset = load_dataset(\n        self.filetype,\n        data_files=data_files,\n        split=self.split,\n        streaming=self.streaming,\n        storage_options=self.storage_options,\n    )\n\n    if not self.streaming and self.num_examples:\n        self._dataset = self._dataset.select(range(self.num_examples))\n    if not self.num_examples:\n        if self.streaming:\n            # There's no better way to get the number of examples in a streaming dataset,\n            # load it again for the moment.\n            self.num_examples = len(\n                load_dataset(\n                    self.filetype, data_files=self.data_files, split=self.split\n                )\n            )\n        else:\n            self.num_examples = len(self._dataset)\n</code></pre>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromHub","title":"<code>LoadDataFromHub</code>","text":"<p>               Bases: <code>GeneratorStep</code></p> <p>Loads a dataset from the Hugging Face Hub.</p> <p><code>GeneratorStep</code> that loads a dataset from the Hugging Face Hub using the <code>datasets</code> library.</p> <p>Attributes:</p> Name Type Description <code>repo_id</code> <code>RuntimeParameter[str]</code> <p>The Hugging Face Hub repository ID of the dataset to load.</p> <code>split</code> <code>RuntimeParameter[str]</code> <p>The split of the dataset to load.</p> <code>config</code> <code>Optional[RuntimeParameter[str]]</code> <p>The configuration of the dataset to load. This is optional and only needed if the dataset has multiple configurations.</p> Runtime parameters <ul> <li><code>batch_size</code>: The batch size to use when processing the data.</li> <li><code>repo_id</code>: The Hugging Face Hub repository ID of the dataset to load.</li> <li><code>split</code>: The split of the dataset to load. Defaults to 'train'.</li> <li><code>config</code>: The configuration of the dataset to load. This is optional and only     needed if the dataset has multiple configurations.</li> <li><code>streaming</code>: Whether to load the dataset in streaming mode or not. Defaults to     <code>False</code>.</li> <li><code>num_examples</code>: The number of examples to load from the dataset.     By default will load all examples.</li> <li><code>storage_options</code>: Key/value pairs to be passed on to the file-system backend, if any.     Defaults to <code>None</code>.</li> </ul> Output columns <ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the     datasets loaded from the Hugging Face Hub.</li> </ul> Categories <ul> <li>load</li> </ul> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>class LoadDataFromHub(GeneratorStep):\n    \"\"\"Loads a dataset from the Hugging Face Hub.\n\n    `GeneratorStep` that loads a dataset from the Hugging Face Hub using the `datasets`\n    library.\n\n    Attributes:\n        repo_id: The Hugging Face Hub repository ID of the dataset to load.\n        split: The split of the dataset to load.\n        config: The configuration of the dataset to load. This is optional and only needed\n            if the dataset has multiple configurations.\n\n    Runtime parameters:\n        - `batch_size`: The batch size to use when processing the data.\n        - `repo_id`: The Hugging Face Hub repository ID of the dataset to load.\n        - `split`: The split of the dataset to load. Defaults to 'train'.\n        - `config`: The configuration of the dataset to load. This is optional and only\n            needed if the dataset has multiple configurations.\n        - `streaming`: Whether to load the dataset in streaming mode or not. Defaults to\n            `False`.\n        - `num_examples`: The number of examples to load from the dataset.\n            By default will load all examples.\n        - `storage_options`: Key/value pairs to be passed on to the file-system backend, if any.\n            Defaults to `None`.\n\n    Output columns:\n        - dynamic (`all`): The columns that will be generated by this step, based on the\n            datasets loaded from the Hugging Face Hub.\n\n    Categories:\n        - load\n    \"\"\"\n\n    repo_id: RuntimeParameter[str] = Field(\n        default=None,\n        description=\"The Hugging Face Hub repository ID of the dataset to load.\",\n    )\n    split: RuntimeParameter[str] = Field(\n        default=\"train\",\n        description=\"The split of the dataset to load. Defaults to 'train'.\",\n    )\n    config: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The configuration of the dataset to load. This is optional and only\"\n        \" needed if the dataset has multiple configurations.\",\n    )\n    streaming: RuntimeParameter[bool] = Field(\n        default=False,\n        description=\"Whether to load the dataset in streaming mode or not. Defaults to False.\",\n    )\n    num_examples: Optional[RuntimeParameter[int]] = Field(\n        default=None,\n        description=\"The number of examples to load from the dataset. By default will load all examples.\",\n    )\n    storage_options: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=\"The storage options to use when loading the dataset.\",\n    )\n\n    _dataset: Union[IterableDataset, Dataset, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset from the Hugging Face Hub\"\"\"\n        super().load()\n\n        self._dataset = load_dataset(\n            self.repo_id,  # type: ignore\n            self.config,\n            split=self.split,\n            streaming=self.streaming,\n        )\n        num_examples = self._get_dataset_num_examples()\n        self.num_examples = (\n            min(self.num_examples, num_examples) if self.num_examples else num_examples\n        )\n\n        if not self.streaming:\n            self._dataset = self._dataset.select(range(self.num_examples))\n\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n        \"\"\"Yields batches from the loaded dataset from the Hugging Face Hub.\n\n        Args:\n            offset: The offset to start yielding the data from. Will be used during the caching\n                process to help skipping already processed data.\n\n        Yields:\n            A tuple containing a batch of rows and a boolean indicating if the batch is\n            the last one.\n        \"\"\"\n        num_returned_rows = 0\n        for batch_num, batch in enumerate(\n            self._dataset.iter(batch_size=self.batch_size)  # type: ignore\n        ):\n            if batch_num * self.batch_size &lt; offset:\n                continue\n            transformed_batch = self._transform_batch(batch)\n            batch_size = len(transformed_batch)\n            num_returned_rows += batch_size\n            yield transformed_batch, num_returned_rows &gt;= self.num_examples\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The columns that will be generated by this step, based on the datasets loaded\n        from the Hugging Face Hub.\n\n        Returns:\n            The columns that will be generated by this step.\n        \"\"\"\n        return self._get_dataset_columns()\n\n    def _transform_batch(self, batch: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Transform a batch of data from the Hugging Face Hub into a list of rows.\n\n        Args:\n            batch: The batch of data from the Hugging Face Hub.\n\n        Returns:\n            A list of rows, where each row is a dictionary of column names and values.\n        \"\"\"\n        length = len(next(iter(batch.values())))\n        rows = []\n        for i in range(length):\n            rows.append({col: values[i] for col, values in batch.items()})\n        return rows\n\n    def _get_dataset_num_examples(self) -&gt; int:\n        \"\"\"Get the number of examples in the dataset, based on the `split` and `config`\n        runtime parameters provided.\n\n        Returns:\n            The number of examples in the dataset.\n        \"\"\"\n        return (\n            self._dataset_info[self.config if self.config else \"default\"]\n            .splits[self.split]\n            .num_examples\n        )\n\n    def _get_dataset_columns(self) -&gt; List[str]:\n        \"\"\"Get the columns of the dataset, based on the `config` runtime parameter provided.\n\n        Returns:\n            The columns of the dataset.\n        \"\"\"\n        return list(\n            self._dataset_info[\n                self.config if self.config else \"default\"\n            ].features.keys()\n        )\n\n    @cached_property\n    def _dataset_info(self) -&gt; Dict[str, DatasetInfo]:\n        \"\"\"Calls the Datasets Server API from Hugging Face to obtain the dataset information.\n\n        Returns:\n            The dataset information.\n        \"\"\"\n        repo_id = self.repo_id\n        config = self.config\n\n        try:\n            return get_dataset_infos(repo_id)\n        except Exception as e:\n            # The previous could fail in case of a internet connection issues.\n            # Assuming the dataset is already loaded and we can get the info from the loaded dataset, otherwise it will fail anyway.\n            self._logger.warning(\n                f\"Failed to get dataset info from Hugging Face Hub, trying to get it loading the dataset. Error: {e}\"\n            )\n            ds = load_dataset(repo_id, config=self.config, split=self.split)\n            if config:\n                return ds[config].info\n            return ds.info\n</code></pre>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromHub.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The columns that will be generated by this step, based on the datasets loaded from the Hugging Face Hub.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The columns that will be generated by this step.</p>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromHub.load","title":"<code>load()</code>","text":"<p>Load the dataset from the Hugging Face Hub</p> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset from the Hugging Face Hub\"\"\"\n    super().load()\n\n    self._dataset = load_dataset(\n        self.repo_id,  # type: ignore\n        self.config,\n        split=self.split,\n        streaming=self.streaming,\n    )\n    num_examples = self._get_dataset_num_examples()\n    self.num_examples = (\n        min(self.num_examples, num_examples) if self.num_examples else num_examples\n    )\n\n    if not self.streaming:\n        self._dataset = self._dataset.select(range(self.num_examples))\n</code></pre>"},{"location":"reference/distilabel/steps/generators/huggingface/#distilabel.steps.generators.huggingface.LoadDataFromHub.process","title":"<code>process(offset=0)</code>","text":"<p>Yields batches from the loaded dataset from the Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start yielding the data from. Will be used during the caching process to help skipping already processed data.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>A tuple containing a batch of rows and a boolean indicating if the batch is</p> <code>GeneratorStepOutput</code> <p>the last one.</p> Source code in <code>src/distilabel/steps/generators/huggingface.py</code> <pre><code>def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":\n    \"\"\"Yields batches from the loaded dataset from the Hugging Face Hub.\n\n    Args:\n        offset: The offset to start yielding the data from. Will be used during the caching\n            process to help skipping already processed data.\n\n    Yields:\n        A tuple containing a batch of rows and a boolean indicating if the batch is\n        the last one.\n    \"\"\"\n    num_returned_rows = 0\n    for batch_num, batch in enumerate(\n        self._dataset.iter(batch_size=self.batch_size)  # type: ignore\n    ):\n        if batch_num * self.batch_size &lt; offset:\n            continue\n        transformed_batch = self._transform_batch(batch)\n        batch_size = len(transformed_batch)\n        num_returned_rows += batch_size\n        yield transformed_batch, num_returned_rows &gt;= self.num_examples\n</code></pre>"},{"location":"reference/distilabel/steps/globals/","title":"Index","text":""},{"location":"reference/distilabel/steps/globals/huggingface/","title":"Huggingface","text":""},{"location":"reference/distilabel/steps/globals/huggingface/#distilabel.steps.globals.huggingface.PushToHub","title":"<code>PushToHub</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Push data to a Hugging Face Hub dataset.</p> <p>A <code>GlobalStep</code> which creates a <code>datasets.Dataset</code> with the input data and pushes it to the Hugging Face Hub.</p> <p>Attributes:</p> Name Type Description <code>repo_id</code> <code>RuntimeParameter[str]</code> <p>The Hugging Face Hub repository ID where the dataset will be uploaded.</p> <code>split</code> <code>RuntimeParameter[str]</code> <p>The split of the dataset that will be pushed. Defaults to <code>\"train\"</code>.</p> <code>private</code> <code>RuntimeParameter[bool]</code> <p>Whether the dataset to be pushed should be private or not. Defaults to <code>False</code>.</p> <code>token</code> <code>Optional[RuntimeParameter[str]]</code> <p>The token that will be used to authenticate in the Hub. If not provided, the token will be tried to be obtained from the environment variable <code>HF_TOKEN</code>. If not provided using one of the previous methods, then <code>huggingface_hub</code> library will try to use the token from the local Hugging Face CLI configuration. Defaults to <code>None</code>.</p> Runtime parameters <ul> <li><code>repo_id</code>: The Hugging Face Hub repository ID where the dataset will be uploaded.</li> <li><code>split</code>: The split of the dataset that will be pushed.</li> <li><code>private</code>: Whether the dataset to be pushed should be private or not.</li> <li><code>token</code>: The token that will be used to authenticate in the Hub.</li> </ul> Input columns <ul> <li>dynamic (<code>all</code>): all columns from the input will be used to create the dataset.</li> </ul> Categories <ul> <li>save</li> <li>dataset</li> <li>huggingface</li> </ul> Source code in <code>src/distilabel/steps/globals/huggingface.py</code> <pre><code>class PushToHub(GlobalStep):\n    \"\"\"Push data to a Hugging Face Hub dataset.\n\n    A `GlobalStep` which creates a `datasets.Dataset` with the input data and pushes\n    it to the Hugging Face Hub.\n\n    Attributes:\n        repo_id: The Hugging Face Hub repository ID where the dataset will be uploaded.\n        split: The split of the dataset that will be pushed. Defaults to `\"train\"`.\n        private: Whether the dataset to be pushed should be private or not. Defaults to\n            `False`.\n        token: The token that will be used to authenticate in the Hub. If not provided, the\n            token will be tried to be obtained from the environment variable `HF_TOKEN`.\n            If not provided using one of the previous methods, then `huggingface_hub` library\n            will try to use the token from the local Hugging Face CLI configuration. Defaults\n            to `None`.\n\n    Runtime parameters:\n        - `repo_id`: The Hugging Face Hub repository ID where the dataset will be uploaded.\n        - `split`: The split of the dataset that will be pushed.\n        - `private`: Whether the dataset to be pushed should be private or not.\n        - `token`: The token that will be used to authenticate in the Hub.\n\n    Input columns:\n        - dynamic (`all`): all columns from the input will be used to create the dataset.\n\n    Categories:\n        - save\n        - dataset\n        - huggingface\n    \"\"\"\n\n    repo_id: RuntimeParameter[str] = Field(\n        default=None,\n        description=\"The Hugging Face Hub repository ID where the dataset will be uploaded.\",\n    )\n    split: RuntimeParameter[str] = Field(\n        default=\"train\",\n        description=\"The split of the dataset that will be pushed. Defaults to 'train'.\",\n    )\n    private: RuntimeParameter[bool] = Field(\n        default=False,\n        description=\"Whether the dataset to be pushed should be private or not. Defaults\"\n        \" to `False`.\",\n    )\n    token: Optional[RuntimeParameter[str]] = Field(\n        default=None,\n        description=\"The token that will be used to authenticate in the Hub. If not provided,\"\n        \" the token will be tried to be obtained from the environment variable `HF_TOKEN`.\"\n        \" If not provided using one of the previous methods, then `huggingface_hub` library\"\n        \" will try to use the token from the local Hugging Face CLI configuration. Defaults\"\n        \" to `None`\",\n    )\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Method that processes the input data, respecting the `datasets.Dataset` formatting,\n        and pushes it to the Hugging Face Hub based on the `RuntimeParameter`s attributes.\n\n        Args:\n            inputs: that input data within a single object (as it's a GlobalStep) that\n                will be transformed into a `datasets.Dataset`.\n\n        Yields:\n            Propagates the received inputs so that the `Distiset` can be generated if this is\n            the last step of the `Pipeline`, or if this is not a leaf step and has follow up\n            steps.\n        \"\"\"\n        dataset_dict = defaultdict(list)\n        for input in inputs:\n            for key, value in input.items():\n                dataset_dict[key].append(value)\n        dataset_dict = dict(dataset_dict)\n        dataset = Dataset.from_dict(dataset_dict)\n        dataset.push_to_hub(\n            self.repo_id,  # type: ignore\n            split=self.split,\n            private=self.private,\n            token=self.token or os.getenv(\"HF_TOKEN\"),\n        )\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/globals/huggingface/#distilabel.steps.globals.huggingface.PushToHub.process","title":"<code>process(inputs)</code>","text":"<p>Method that processes the input data, respecting the <code>datasets.Dataset</code> formatting, and pushes it to the Hugging Face Hub based on the <code>RuntimeParameter</code>s attributes.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>that input data within a single object (as it's a GlobalStep) that will be transformed into a <code>datasets.Dataset</code>.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>Propagates the received inputs so that the <code>Distiset</code> can be generated if this is</p> <code>StepOutput</code> <p>the last step of the <code>Pipeline</code>, or if this is not a leaf step and has follow up</p> <code>StepOutput</code> <p>steps.</p> Source code in <code>src/distilabel/steps/globals/huggingface.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Method that processes the input data, respecting the `datasets.Dataset` formatting,\n    and pushes it to the Hugging Face Hub based on the `RuntimeParameter`s attributes.\n\n    Args:\n        inputs: that input data within a single object (as it's a GlobalStep) that\n            will be transformed into a `datasets.Dataset`.\n\n    Yields:\n        Propagates the received inputs so that the `Distiset` can be generated if this is\n        the last step of the `Pipeline`, or if this is not a leaf step and has follow up\n        steps.\n    \"\"\"\n    dataset_dict = defaultdict(list)\n    for input in inputs:\n        for key, value in input.items():\n            dataset_dict[key].append(value)\n    dataset_dict = dict(dataset_dict)\n    dataset = Dataset.from_dict(dataset_dict)\n    dataset.push_to_hub(\n        self.repo_id,  # type: ignore\n        split=self.split,\n        private=self.private,\n        token=self.token or os.getenv(\"HF_TOKEN\"),\n    )\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/","title":"Index","text":""},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ChatType","title":"<code>ChatType = List[ChatItem]</code>  <code>module-attribute</code>","text":"<p>ChatType is a type alias for a <code>list</code> of <code>dict</code>s following the OpenAI conversational format.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ChatGeneration","title":"<code>ChatGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generates text based on a conversation.</p> <p><code>ChatGeneration</code> is a pre-defined task that defines the <code>messages</code> as the input and <code>generation</code> as the output. This task is used to generate text based on a conversation. The <code>model_name</code> is also returned as part of the output in order to enhance it.</p> Input columns <ul> <li>messages (<code>List[Dict[Literal[\"role\", \"content\"], str]]</code>): The messages to generate the     follow up completion from.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text from the assistant.</li> <li>model_name (<code>str</code>): The model name used to generate the text.</li> </ul> Categories <ul> <li>chat-generation</li> </ul> Icon <p><code>:material-chat:</code></p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>class ChatGeneration(Task):\n    \"\"\"Generates text based on a conversation.\n\n    `ChatGeneration` is a pre-defined task that defines the `messages` as the input\n    and `generation` as the output. This task is used to generate text based on a conversation.\n    The `model_name` is also returned as part of the output in order to enhance it.\n\n    Input columns:\n        - messages (`List[Dict[Literal[\"role\", \"content\"], str]]`): The messages to generate the\n            follow up completion from.\n\n    Output columns:\n        - generation (`str`): The generated text from the assistant.\n        - model_name (`str`): The model name used to generate the text.\n\n    Categories:\n        - chat-generation\n\n    Icon:\n        `:material-chat:`\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `messages`.\"\"\"\n        return [\"messages\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the messages provided\n        are already formatted that way i.e. following the OpenAI chat format.\"\"\"\n\n        if not is_openai_format(input[\"messages\"]):\n            raise ValueError(\n                \"Input `instruction` must be a string or an OpenAI chat-like format. \"\n                f\"Got: {input['messages']}. Please check: 'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n            )\n\n        if input[\"messages\"][-1][\"role\"] != \"user\":\n            raise ValueError(\n                \"The last message must be from the user. Please check: \"\n                \"'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n            )\n\n        return input[\"messages\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ChatGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>messages</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ChatGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ChatGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the messages provided are already formatted that way i.e. following the OpenAI chat format.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the messages provided\n    are already formatted that way i.e. following the OpenAI chat format.\"\"\"\n\n    if not is_openai_format(input[\"messages\"]):\n        raise ValueError(\n            \"Input `instruction` must be a string or an OpenAI chat-like format. \"\n            f\"Got: {input['messages']}. Please check: 'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n        )\n\n    if input[\"messages\"][-1][\"role\"] != \"user\":\n        raise ValueError(\n            \"The last message must be from the user. Please check: \"\n            \"'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n        )\n\n    return input[\"messages\"]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ChatGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ComplexityScorer","title":"<code>ComplexityScorer</code>","text":"<p>               Bases: <code>Task</code></p> <p>Score instructions based on their complexity using an <code>LLM</code>.</p> <p><code>ComplexityScorer</code> is a pre-defined task used to rank a list of instructions based in their complexity. It's an implementation of the complexity score task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instructions (<code>List[str]</code>): The list of instructions to be scored.</li> </ul> Output columns <ul> <li>scores (<code>List[float]</code>): The score for each instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the scores.</li> </ul> Categories <ul> <li>scorer</li> <li>complexity</li> <li>instruction</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>class ComplexityScorer(Task):\n    \"\"\"Score instructions based on their complexity using an `LLM`.\n\n    `ComplexityScorer` is a pre-defined task used to rank a list of instructions based in\n    their complexity. It's an implementation of the complexity score task from the paper\n    'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection\n    in Instruction Tuning'.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instructions (`List[str]`): The list of instructions to be scored.\n\n    Output columns:\n        - scores (`List[float]`): The score for each instruction.\n        - model_name (`str`): The model name used to generate the scores.\n\n    Categories:\n        - scorer\n        - complexity\n        - instruction\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"complexity-scorer.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the `instructions`.\"\"\"\n        return [\"instructions\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(instructions=input[\"instructions\"]),  # type: ignore\n            }\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are: a list of `scores` containing the complexity score for each\n        instruction in `instructions`, and the `model_name`.\"\"\"\n        return [\"scores\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the score of each instruction.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the key `scores` containing the scores for each instruction.\n        \"\"\"\n        if output is None:\n            return {\"scores\": [None] * len(input[\"instructions\"])}\n\n        scores = []\n        score_lines = output.split(\"\\n\")\n        for i, line in enumerate(score_lines):\n            match = _PARSE_SCORE_LINE_REGEX.match(line)\n            score = float(match.group(1)) if match else None\n            scores.append(score)\n            if i == len(input[\"instructions\"]) - 1:\n                break\n        return {\"scores\": scores}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ComplexityScorer.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the <code>instructions</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ComplexityScorer.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are: a list of <code>scores</code> containing the complexity score for each instruction in <code>instructions</code>, and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ComplexityScorer.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(instructions=input[\"instructions\"]),  # type: ignore\n        }\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ComplexityScorer.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a list with the score of each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the key <code>scores</code> containing the scores for each instruction.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the score of each instruction.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the key `scores` containing the scores for each instruction.\n    \"\"\"\n    if output is None:\n        return {\"scores\": [None] * len(input[\"instructions\"])}\n\n    scores = []\n    score_lines = output.split(\"\\n\")\n    for i, line in enumerate(score_lines):\n        match = _PARSE_SCORE_LINE_REGEX.match(line)\n        score = float(match.group(1)) if match else None\n        scores.append(score)\n        if i == len(input[\"instructions\"]) - 1:\n            break\n    return {\"scores\": scores}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.ComplexityScorer.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"complexity-scorer.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolComplexity","title":"<code>EvolComplexity</code>","text":"<p>               Bases: <code>EvolInstruct</code></p> <p>Evolve instructions to make them more complex using an <code>LLM</code>.</p> <p><code>EvolComplexity</code> is a task that evolves instructions to make them more complex, and it is based in the EvolInstruct task, but using slight different prompts, but the exact same evolutionary approach.</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>Dict[str, str]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.</li> <li><code>seed</code>: The number of evolutions to be run.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction to evolve.</li> </ul> Output columns <ul> <li>evolved_instruction (<code>str</code>): The evolved instruction.</li> <li>answer (<code>str</code>, optional): The answer to the instruction if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>deita</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/evol_complexity/base.py</code> <pre><code>class EvolComplexity(EvolInstruct):\n    \"\"\"Evolve instructions to make them more complex using an `LLM`.\n\n    `EvolComplexity` is a task that evolves instructions to make them more complex,\n    and it is based in the EvolInstruct task, but using slight different prompts, but the\n    exact same evolutionary approach.\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\n        - `seed`: The number of evolutions to be run.\n\n    Input columns:\n        - instruction (`str`): The instruction to evolve.\n\n    Output columns:\n        - evolved_instruction (`str`): The evolved instruction.\n        - answer (`str`, optional): The answer to the instruction if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - deita\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolComplexityGenerator","title":"<code>EvolComplexityGenerator</code>","text":"<p>               Bases: <code>EvolInstructGenerator</code></p> <p>Generate evolved instructions with increased complexity using an <code>LLM</code>.</p> <p><code>EvolComplexityGenerator</code> is a generation task that evolves instructions to make them more complex, and it is based in the EvolInstruct task, but using slight different prompts, but the exact same evolutionary approach.</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>Dict[str, str]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.</li> <li><code>seed</code>: The number of evolutions to be run.</li> </ul> Output columns <ul> <li>instruction (<code>str</code>): The evolved instruction.</li> <li>answer (<code>str</code>, optional): The answer to the instruction if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>generation</li> <li>deita</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/evol_complexity/generator.py</code> <pre><code>class EvolComplexityGenerator(EvolInstructGenerator):\n    \"\"\"Generate evolved instructions with increased complexity using an `LLM`.\n\n    `EvolComplexityGenerator` is a generation task that evolves instructions to make\n    them more complex, and it is based in the EvolInstruct task, but using slight different\n    prompts, but the exact same evolutionary approach.\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\n        - `seed`: The number of evolutions to be run.\n\n    Output columns:\n        - instruction (`str`): The evolved instruction.\n        - answer (`str`, optional): The answer to the instruction if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - generation\n        - deita\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    mutation_templates: Dict[str, str] = GENERATION_MUTATION_TEMPLATES\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstruct","title":"<code>EvolInstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Evolve instructions using an <code>LLM</code>.</p> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> <p>Attributes:</p> Name Type Description <code>num_evolutions</code> <code>int</code> <p>The number of evolutions to be performed.</p> <code>store_evolutions</code> <code>bool</code> <p>Whether to store all the evolutions or just the last one. Defaults to <code>False</code>.</p> <code>generate_answers</code> <code>bool</code> <p>Whether to generate answers for the evolved instructions. Defaults to <code>False</code>.</p> <code>include_original_instruction</code> <code>bool</code> <p>Whether to include the original instruction in the <code>evolved_instructions</code> output column. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for evolving the instructions. Defaults to the ones provided in the <code>utils.py</code> file.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction to evolve.</li> </ul> Output columns <ul> <li>evolved_instruction (<code>str</code>): The evolved instruction if <code>store_evolutions=False</code>.</li> <li>evolved_instructions (<code>List[str]</code>): The evolved instructions if <code>store_evolutions=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> <li>answer (<code>str</code>): The answer to the evolved instruction if <code>generate_answers=True</code>     and <code>store_evolutions=False</code>.</li> <li>answers (<code>List[str]</code>): The answers to the evolved instructions if <code>generate_answers=True</code>     and <code>store_evolutions=True</code>.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> </ul> References <ul> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> <li>GitHub: h2oai/h2o-wizardlm</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>class EvolInstruct(Task):\n    \"\"\"Evolve instructions using an `LLM`.\n\n    WizardLM: Empowering Large Language Models to Follow Complex Instructions\n\n    Attributes:\n        num_evolutions: The number of evolutions to be performed.\n        store_evolutions: Whether to store all the evolutions or just the last one. Defaults\n            to `False`.\n        generate_answers: Whether to generate answers for the evolved instructions. Defaults\n            to `False`.\n        include_original_instruction: Whether to include the original instruction in the\n            `evolved_instructions` output column. Defaults to `False`.\n        mutation_templates: The mutation templates to be used for evolving the instructions.\n            Defaults to the ones provided in the `utils.py` file.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Input columns:\n        - instruction (`str`): The instruction to evolve.\n\n    Output columns:\n        - evolved_instruction (`str`): The evolved instruction if `store_evolutions=False`.\n        - evolved_instructions (`List[str]`): The evolved instructions if `store_evolutions=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n        - answer (`str`): The answer to the evolved instruction if `generate_answers=True`\n            and `store_evolutions=False`.\n        - answers (`List[str]`): The answers to the evolved instructions if `generate_answers=True`\n            and `store_evolutions=True`.\n\n    Categories:\n        - evol\n        - instruction\n\n    References:\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n        - [GitHub: h2oai/h2o-wizardlm](https://github.com/h2oai/h2o-wizardlm)\n    \"\"\"\n\n    num_evolutions: int\n    store_evolutions: bool = False\n    generate_answers: bool = False\n    include_original_instruction: bool = False\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to seed a random seed.\",\n    )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`.\"\"\"\n        return [\"instruction\"]\n\n    def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation. And the\n        `system_prompt` is added as the first message if it exists.\"\"\"\n        return [{\"role\": \"user\", \"content\": input}]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `evolved_instruction/s`, the `answer` if `generate_answers=True`\n        and the `model_name`.\"\"\"\n        # TODO: having to define a `model_name` column every time as the `Task.outputs` is not ideal,\n        # this could be handled always and the value could be included within the DAG validation when\n        # a `Task` is used, since all the `Task` subclasses will have an `llm` with a `model_name` attr.\n        _outputs = [\n            (\n                \"evolved_instruction\"\n                if not self.store_evolutions\n                else \"evolved_instructions\"\n            ),\n            \"model_name\",\n        ]\n        if self.generate_answers:\n            _outputs.append(\"answer\" if not self.store_evolutions else \"answers\")\n        return _outputs\n\n    @override\n    def format_output(  # type: ignore\n        self, instructions: Union[str, List[str]], answers: Optional[List[str]] = None\n    ) -&gt; Dict[str, Any]:  # type: ignore\n        \"\"\"The output for the task is a dict with: `evolved_instruction` or `evolved_instructions`,\n        depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n        `answer` if `generate_answers=True`; and, finally, the `model_name`.\n\n        Args:\n            instructions: The instructions to be included within the output.\n            answers: The answers to be included within the output if `generate_answers=True`.\n\n        Returns:\n            If `store_evolutions=False` and `generate_answers=True` return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};\n            if `store_evolutions=True` and `generate_answers=True` return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};\n            if `store_evolutions=False` and `generate_answers=False` return {\"evolved_instruction\": ..., \"model_name\": ...};\n            if `store_evolutions=True` and `generate_answers=False` return {\"evolved_instructions\": ..., \"model_name\": ...}.\n        \"\"\"\n        _output = {}\n        if not self.store_evolutions:\n            _output[\"evolved_instruction\"] = instructions[-1]\n        else:\n            _output[\"evolved_instructions\"] = instructions\n\n        if self.generate_answers and answers:\n            if not self.store_evolutions:\n                _output[\"answer\"] = answers[-1]\n            else:\n                _output[\"answers\"] = answers\n\n        _output[\"model_name\"] = self.llm.model_name\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates`.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, instruction: str) -&gt; str:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            instruction: The instruction to be included within the mutation prompt.\n\n        Returns:\n            A random mutation prompt with the provided instruction.\n        \"\"\"\n        mutation = np.random.choice(self.mutation_templates_names)\n        return self.mutation_templates[mutation].replace(\"&lt;PROMPT&gt;\", instruction)  # type: ignore\n\n    def _evolve_instructions(self, inputs: \"StepInput\") -&gt; List[List[str]]:\n        \"\"\"Evolves the instructions provided as part of the inputs of the task.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list where each item is a list with either the last evolved instruction if\n            `store_evolutions=False` or all the evolved instructions if `store_evolutions=True`.\n        \"\"\"\n\n        instructions: List[List[str]] = [[input[\"instruction\"]] for input in inputs]\n\n        for iter_no in range(self.num_evolutions):\n            formatted_prompts = []\n            for instruction in instructions:\n                formatted_prompts.append(self._apply_random_mutation(instruction[-1]))\n\n            formatted_prompts = [\n                self.format_input(prompt) for prompt in formatted_prompts\n            ]\n            generated_prompts = flatten_responses(\n                self.llm.generate(\n                    formatted_prompts,\n                    **self.llm.generation_kwargs,  # type: ignore\n                )\n            )\n\n            evolved_instructions = []\n            for generated_prompt in generated_prompts:\n                generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n                evolved_instructions.append(generated_prompt)\n\n            if self.store_evolutions:\n                instructions = [\n                    instruction + [evolved_instruction]\n                    for instruction, evolved_instruction in zip(\n                        instructions, evolved_instructions\n                    )\n                ]\n            else:\n                instructions = [\n                    [evolved_instruction]\n                    for evolved_instruction in evolved_instructions\n                ]\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} evolving {len(instructions)} instructions!\"\n            )\n\n        return instructions\n\n    def _generate_answers(\n        self, evolved_instructions: List[List[str]]\n    ) -&gt; List[List[str]]:\n        \"\"\"Generates the answer for the instructions in `instructions`.\n\n        Args:\n            evolved_instructions: A list of lists where each item is a list with either the last\n                evolved instruction if `store_evolutions=False` or all the evolved instructions\n                if `store_evolutions=True`.\n\n        Returns:\n            A list of answers for each instruction.\n        \"\"\"\n        formatted_instructions = [\n            self.format_input(instruction)\n            for instructions in evolved_instructions\n            for instruction in instructions\n        ]\n\n        responses = self.llm.generate(\n            formatted_instructions,\n            num_generations=1,\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n\n        step = (\n            self.num_evolutions\n            if not self.include_original_instruction\n            else self.num_evolutions + 1\n        )\n        return [\n            flatten_responses(responses[i : i + step])\n            for i in range(0, len(responses), step)\n        ]\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        evolved_instructions = self._evolve_instructions(inputs)\n\n        if self.store_evolutions:\n            # Remove the input instruction from the `evolved_instructions` list\n            from_ = 1 if not self.include_original_instruction else 0\n            evolved_instructions = [\n                instruction[from_:] for instruction in evolved_instructions\n            ]\n\n        if not self.generate_answers:\n            for input, instruction in zip(inputs, evolved_instructions):\n                input.update(self.format_output(instruction))\n            yield inputs\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished evolving {len(evolved_instructions)} instructions!\"\n        )\n\n        if self.generate_answers:\n            self._logger.info(\n                f\"\ud83e\udde0 Generating answers for the {len(evolved_instructions)} evolved instructions!\"\n            )\n\n            answers = self._generate_answers(evolved_instructions)\n\n            self._logger.info(\n                f\"\ud83c\udf89 Finished generating answers for the {len(evolved_instructions)} evolved\"\n                \" instructions!\"\n            )\n\n            for idx, (input, instruction) in enumerate(\n                zip(inputs, evolved_instructions)\n            ):\n                input.update(self.format_output(instruction, answers[idx]))\n            yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstruct.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstruct.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>evolved_instruction/s</code>, the <code>answer</code> if <code>generate_answers=True</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation. And the <code>system_prompt</code> is added as the first message if it exists.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation. And the\n    `system_prompt` is added as the first message if it exists.\"\"\"\n    return [{\"role\": \"user\", \"content\": input}]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstruct.format_output","title":"<code>format_output(instructions, answers=None)</code>","text":"<p>The output for the task is a dict with: <code>evolved_instruction</code> or <code>evolved_instructions</code>, depending whether the value is either <code>False</code> or <code>True</code> for <code>store_evolutions</code>, respectively; <code>answer</code> if <code>generate_answers=True</code>; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>Union[str, List[str]]</code> <p>The instructions to be included within the output.</p> required <code>answers</code> <code>Optional[List[str]]</code> <p>The answers to be included within the output if <code>generate_answers=True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>If <code>store_evolutions=False</code> and <code>generate_answers=True</code> return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> and <code>generate_answers=True</code> return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=False</code> and <code>generate_answers=False</code> return {\"evolved_instruction\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> and <code>generate_answers=False</code> return {\"evolved_instructions\": ..., \"model_name\": ...}.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>@override\ndef format_output(  # type: ignore\n    self, instructions: Union[str, List[str]], answers: Optional[List[str]] = None\n) -&gt; Dict[str, Any]:  # type: ignore\n    \"\"\"The output for the task is a dict with: `evolved_instruction` or `evolved_instructions`,\n    depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n    `answer` if `generate_answers=True`; and, finally, the `model_name`.\n\n    Args:\n        instructions: The instructions to be included within the output.\n        answers: The answers to be included within the output if `generate_answers=True`.\n\n    Returns:\n        If `store_evolutions=False` and `generate_answers=True` return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};\n        if `store_evolutions=True` and `generate_answers=True` return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};\n        if `store_evolutions=False` and `generate_answers=False` return {\"evolved_instruction\": ..., \"model_name\": ...};\n        if `store_evolutions=True` and `generate_answers=False` return {\"evolved_instructions\": ..., \"model_name\": ...}.\n    \"\"\"\n    _output = {}\n    if not self.store_evolutions:\n        _output[\"evolved_instruction\"] = instructions[-1]\n    else:\n        _output[\"evolved_instructions\"] = instructions\n\n    if self.generate_answers and answers:\n        if not self.store_evolutions:\n            _output[\"answer\"] = answers[-1]\n        else:\n            _output[\"answers\"] = answers\n\n    _output[\"model_name\"] = self.llm.model_name\n    return _output\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstruct.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    evolved_instructions = self._evolve_instructions(inputs)\n\n    if self.store_evolutions:\n        # Remove the input instruction from the `evolved_instructions` list\n        from_ = 1 if not self.include_original_instruction else 0\n        evolved_instructions = [\n            instruction[from_:] for instruction in evolved_instructions\n        ]\n\n    if not self.generate_answers:\n        for input, instruction in zip(inputs, evolved_instructions):\n            input.update(self.format_output(instruction))\n        yield inputs\n\n    self._logger.info(\n        f\"\ud83c\udf89 Finished evolving {len(evolved_instructions)} instructions!\"\n    )\n\n    if self.generate_answers:\n        self._logger.info(\n            f\"\ud83e\udde0 Generating answers for the {len(evolved_instructions)} evolved instructions!\"\n        )\n\n        answers = self._generate_answers(evolved_instructions)\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished generating answers for the {len(evolved_instructions)} evolved\"\n            \" instructions!\"\n        )\n\n        for idx, (input, instruction) in enumerate(\n            zip(inputs, evolved_instructions)\n        ):\n            input.update(self.format_output(instruction, answers[idx]))\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstructGenerator","title":"<code>EvolInstructGenerator</code>","text":"<p>               Bases: <code>GeneratorTask</code></p> <p>Generate evolved instructions using an <code>LLM</code>.</p> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <code>int</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <code>bool</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>RuntimeParameter[int]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>RuntimeParameter[int]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs     to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs     to be lower than, to be considered valid.</li> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Output columns <ul> <li>instruction (<code>str</code>): The generated instruction if <code>generate_answers=False</code>.</li> <li>answer (<code>str</code>): The generated answer if <code>generate_answers=True</code>.</li> <li>instructions (<code>List[str]</code>): The generated instructions if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to generate and evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>generation</li> </ul> References <ul> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> <li>GitHub: h2oai/h2o-wizardlm</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>class EvolInstructGenerator(GeneratorTask):\n    \"\"\"Generate evolved instructions using an `LLM`.\n\n    WizardLM: Empowering Large Language Models to Follow Complex Instructions\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs\n            to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs\n            to be lower than, to be considered valid.\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Output columns:\n        - instruction (`str`): The generated instruction if `generate_answers=False`.\n        - answer (`str`): The generated answer if `generate_answers=True`.\n        - instructions (`List[str]`): The generated instructions if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to generate and evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - generation\n\n    References:\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n        - [GitHub: h2oai/h2o-wizardlm](https://github.com/h2oai/h2o-wizardlm)\n    \"\"\"\n\n    num_instructions: int\n    generate_answers: bool = False\n    mutation_templates: Dict[str, str] = GENERATION_MUTATION_TEMPLATES\n\n    min_length: RuntimeParameter[int] = Field(\n        default=512,\n        description=\"Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\",\n    )\n    max_length: RuntimeParameter[int] = Field(\n        default=1024,\n        description=\"Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\",\n    )\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to seed a random seed.\",\n    )\n    _seed_texts: Optional[List[str]] = PrivateAttr(default_factory=list)\n    _prompts: Optional[List[str]] = PrivateAttr(default_factory=list)\n\n    def _generate_seed_texts(self) -&gt; List[str]:\n        \"\"\"Generates a list of seed texts to be used as part of the starting prompts for the task.\n\n        It will use the `FRESH_START` mutation template, as it needs to generate text from scratch; and\n        a list of English words will be used to generate the seed texts that will be provided to the\n        mutation method and included within the prompt.\n\n        Returns:\n            A list of seed texts to be used as part of the starting prompts for the task.\n        \"\"\"\n        seed_texts = []\n        for _ in range(self.num_instructions * 10):\n            num_words = np.random.choice([1, 2, 3, 4])\n            seed_texts.append(\n                self.mutation_templates[\"FRESH_START\"].replace(  # type: ignore\n                    \"&lt;PROMPT&gt;\",\n                    \", \".join(\n                        [\n                            np.random.choice(self._english_nouns).strip()\n                            for _ in range(num_words)\n                        ]\n                    ),\n                )\n            )\n        return seed_texts\n\n    @override\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n        This is useful if you want to do some validation that requires the entire model to be initialized.\n        \"\"\"\n        super().model_post_init(__context)\n\n        np.random.seed(self.seed)\n\n        self._seed_texts = self._generate_seed_texts()\n        self._prompts = [\n            np.random.choice(self._seed_texts) for _ in range(self.num_instructions)\n        ]\n\n    @cached_property\n    def _english_nouns(self) -&gt; List[str]:\n        \"\"\"A list of English nouns to be used as part of the starting prompts for the task.\n\n        References:\n            - https://github.com/h2oai/h2o-wizardlm\n        \"\"\"\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps/tasks/evol_instruct/english_nouns.txt\"\n        )\n        with open(_path, mode=\"r\") as f:\n            return [line.strip() for line in f.readlines()]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `instruction`, the `answer` if `generate_answers=True`\n        and the `model_name`.\"\"\"\n        _outputs = [\"instruction\", \"model_name\"]\n        if self.generate_answers:\n            _outputs.append(\"answer\")\n        return _outputs\n\n    def format_output(  # type: ignore\n        self, instruction: str, answer: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output for the task is a dict with: `instruction`; `answer` if `generate_answers=True`;\n        and, finally, the `model_name`.\n\n        Args:\n            instruction: The instruction to be included within the output.\n            answer: The answer to be included within the output if `generate_answers=True`.\n\n        Returns:\n            If `generate_answers=True` return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};\n            if `generate_answers=False` return {\"instruction\": ..., \"model_name\": ...};\n        \"\"\"\n        _output = {\n            \"instruction\": instruction,\n            \"model_name\": self.llm.model_name,\n        }\n        if self.generate_answers and answer is not None:\n            _output[\"answer\"] = answer\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates`.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, iter_no: int) -&gt; List[\"ChatType\"]:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            iter_no: The iteration number to be used to check whether the iteration is the\n                first one i.e. FRESH_START, or not.\n\n        Returns:\n            A random mutation prompt with the provided instruction formatted as an OpenAI conversation.\n        \"\"\"\n        prompts = []\n        for idx in range(self.num_instructions):\n            if (\n                iter_no == 0\n                or \"Write one question or request containing\" in self._prompts[idx]  # type: ignore\n            ):\n                mutation = \"FRESH_START\"\n            else:\n                mutation = np.random.choice(self.mutation_templates_names)\n                if mutation == \"FRESH_START\":\n                    self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n\n            prompt_with_template = (\n                self.mutation_templates[mutation].replace(  # type: ignore\n                    \"&lt;PROMPT&gt;\",\n                    self._prompts[idx],  # type: ignore\n                )  # type: ignore\n                if iter_no != 0\n                else self._prompts[idx]  # type: ignore\n            )\n            prompts.append([{\"role\": \"user\", \"content\": prompt_with_template}])\n        return prompts\n\n    def _generate_answers(self, instructions: List[List[str]]) -&gt; List[str]:\n        \"\"\"Generates the answer for the last instruction in `instructions`.\n\n        Args:\n            instructions: A list of lists where each item is a list with either the last\n                evolved instruction if `store_evolutions=False` or all the evolved instructions\n                if `store_evolutions=True`.\n\n        Returns:\n            A list of answers for the last instruction in `instructions`.\n        \"\"\"\n        # TODO: update to generate answers for all the instructions\n        _formatted_instructions = [\n            [{\"role\": \"user\", \"content\": instruction[-1]}]\n            for instruction in instructions\n        ]\n        responses = self.llm.generate(\n            _formatted_instructions,\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n        return flatten_responses(responses)\n\n    @override\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task, and a boolean\n            flag indicating whether the task has finished or not i.e. is the last batch.\n        \"\"\"\n        instructions = []\n        mutation_no = 0\n\n        iter_no = 0\n        while len(instructions) &lt; self.num_instructions:\n            prompts = self._apply_random_mutation(iter_no=iter_no)\n\n            generated_prompts = flatten_responses(\n                self.llm.generate(prompts, **self.llm.generation_kwargs)  # type: ignore\n            )\n            for idx, generated_prompt in enumerate(generated_prompts):\n                generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n                if self.max_length &gt;= len(generated_prompt) &gt;= self.min_length:  # type: ignore\n                    instructions.append(generated_prompt)\n                    self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n                else:\n                    self._prompts[idx] = generated_prompt  # type: ignore\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} with {len(instructions)} instructions already evolved!\"\n            )\n            iter_no += 1\n\n            if len(instructions) &gt; self.num_instructions:\n                instructions = instructions[: self.num_instructions]\n            if len(instructions) &gt; mutation_no:\n                mutation_no = len(instructions) - mutation_no\n\n            if not self.generate_answers and len(instructions[-mutation_no:]) &gt; 0:\n                yield (\n                    [\n                        self.format_output(mutated_instruction)\n                        for mutated_instruction in instructions[-mutation_no:]\n                    ],\n                    len(instructions) &gt;= self.num_instructions,\n                )\n\n        self._logger.info(f\"\ud83c\udf89 Finished evolving {len(instructions)} instructions!\")\n\n        if self.generate_answers:\n            self._logger.info(\n                f\"\ud83e\udde0 Generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            answers = self._generate_answers(instructions)\n\n            self._logger.info(\n                f\"\ud83c\udf89 Finished generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            yield (\n                [\n                    self.format_output(instruction, answer)\n                    for instruction, answer in zip(instructions, answers)\n                ],\n                True,\n            )\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstructGenerator.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstructGenerator.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>instruction</code>, the <code>answer</code> if <code>generate_answers=True</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstructGenerator.format_output","title":"<code>format_output(instruction, answer=None)</code>","text":"<p>The output for the task is a dict with: <code>instruction</code>; <code>answer</code> if <code>generate_answers=True</code>; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction to be included within the output.</p> required <code>answer</code> <code>Optional[str]</code> <p>The answer to be included within the output if <code>generate_answers=True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>If <code>generate_answers=True</code> return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>generate_answers=False</code> return {\"instruction\": ..., \"model_name\": ...};</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>def format_output(  # type: ignore\n    self, instruction: str, answer: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"The output for the task is a dict with: `instruction`; `answer` if `generate_answers=True`;\n    and, finally, the `model_name`.\n\n    Args:\n        instruction: The instruction to be included within the output.\n        answer: The answer to be included within the output if `generate_answers=True`.\n\n    Returns:\n        If `generate_answers=True` return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};\n        if `generate_answers=False` return {\"instruction\": ..., \"model_name\": ...};\n    \"\"\"\n    _output = {\n        \"instruction\": instruction,\n        \"model_name\": self.llm.model_name,\n    }\n    if self.generate_answers and answer is not None:\n        _output[\"answer\"] = answer\n    return _output\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstructGenerator.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Override this method to perform additional initialization after <code>__init__</code> and <code>model_construct</code>. This is useful if you want to do some validation that requires the entire model to be initialized.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>@override\ndef model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n    This is useful if you want to do some validation that requires the entire model to be initialized.\n    \"\"\"\n    super().model_post_init(__context)\n\n    np.random.seed(self.seed)\n\n    self._seed_texts = self._generate_seed_texts()\n    self._prompts = [\n        np.random.choice(self._seed_texts) for _ in range(self.num_instructions)\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolInstructGenerator.process","title":"<code>process(offset=0)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>A list of Python dictionaries with the outputs of the task, and a boolean</p> <code>GeneratorStepOutput</code> <p>flag indicating whether the task has finished or not i.e. is the last batch.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>@override\ndef process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task, and a boolean\n        flag indicating whether the task has finished or not i.e. is the last batch.\n    \"\"\"\n    instructions = []\n    mutation_no = 0\n\n    iter_no = 0\n    while len(instructions) &lt; self.num_instructions:\n        prompts = self._apply_random_mutation(iter_no=iter_no)\n\n        generated_prompts = flatten_responses(\n            self.llm.generate(prompts, **self.llm.generation_kwargs)  # type: ignore\n        )\n        for idx, generated_prompt in enumerate(generated_prompts):\n            generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n            if self.max_length &gt;= len(generated_prompt) &gt;= self.min_length:  # type: ignore\n                instructions.append(generated_prompt)\n                self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n            else:\n                self._prompts[idx] = generated_prompt  # type: ignore\n\n        self._logger.info(\n            f\"\ud83d\udd04 Ran iteration {iter_no} with {len(instructions)} instructions already evolved!\"\n        )\n        iter_no += 1\n\n        if len(instructions) &gt; self.num_instructions:\n            instructions = instructions[: self.num_instructions]\n        if len(instructions) &gt; mutation_no:\n            mutation_no = len(instructions) - mutation_no\n\n        if not self.generate_answers and len(instructions[-mutation_no:]) &gt; 0:\n            yield (\n                [\n                    self.format_output(mutated_instruction)\n                    for mutated_instruction in instructions[-mutation_no:]\n                ],\n                len(instructions) &gt;= self.num_instructions,\n            )\n\n    self._logger.info(f\"\ud83c\udf89 Finished evolving {len(instructions)} instructions!\")\n\n    if self.generate_answers:\n        self._logger.info(\n            f\"\ud83e\udde0 Generating answers for the {len(instructions)} evolved instructions!\"\n        )\n\n        answers = self._generate_answers(instructions)\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished generating answers for the {len(instructions)} evolved instructions!\"\n        )\n\n        yield (\n            [\n                self.format_output(instruction, answer)\n                for instruction, answer in zip(instructions, answers)\n            ],\n            True,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolQuality","title":"<code>EvolQuality</code>","text":"<p>               Bases: <code>Task</code></p> <p>Evolve the quality of the responses using an <code>LLM</code>.</p> <p><code>EvolQuality</code> task is used to evolve the quality of the responses given a prompt, by generating a new response with a language model. This step implements the evolution quality task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>num_evolutions</code> <code>int</code> <p>The number of evolutions to be performed on the responses.</p> <code>store_evolutions</code> <code>bool</code> <p>Whether to store all the evolved responses or just the last one. Defaults to <code>False</code>.</p> <code>include_original_response</code> <code>bool</code> <p>Whether to include the original response within the evolved responses. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used to evolve the responses.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the <code>responses</code>.</li> <li>response (<code>str</code>): The responses to be rewritten.</li> </ul> Output columns <ul> <li>evolved_response (<code>str</code>): The evolved response if <code>store_evolutions=False</code>.</li> <li>evolved_responses (<code>List[str]</code>): The evolved responses if <code>store_evolutions=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the responses.</li> </ul> Categories <ul> <li>evol</li> <li>response</li> <li>deita</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>class EvolQuality(Task):\n    \"\"\"Evolve the quality of the responses using an `LLM`.\n\n    `EvolQuality` task is used to evolve the quality of the responses given a prompt,\n    by generating a new response with a language model. This step implements the evolution\n    quality task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of\n    Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        num_evolutions: The number of evolutions to be performed on the responses.\n        store_evolutions: Whether to store all the evolved responses or just the last one.\n            Defaults to `False`.\n        include_original_response: Whether to include the original response within the evolved\n            responses. Defaults to `False`.\n        mutation_templates: The mutation templates to be used to evolve the responses.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the `responses`.\n        - response (`str`): The responses to be rewritten.\n\n    Output columns:\n        - evolved_response (`str`): The evolved response if `store_evolutions=False`.\n        - evolved_responses (`List[str]`): The evolved responses if `store_evolutions=True`.\n        - model_name (`str`): The name of the LLM used to evolve the responses.\n\n    Categories:\n        - evol\n        - response\n        - deita\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    num_evolutions: int\n    store_evolutions: bool = False\n    include_original_response: bool = False\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to set a random seed.\",\n    )\n\n    @override\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n        This is useful if you want to do some validation that requires the entire model to be initialized.\n        \"\"\"\n        super().model_post_init(__context)\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `instruction` and `response`.\"\"\"\n        return [\"instruction\", \"response\"]\n\n    def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation. And the\n        `system_prompt` is added as the first message if it exists.\"\"\"\n        return [{\"role\": \"user\", \"content\": input}]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `evolved_response/s` and the `model_name`.\"\"\"\n        # TODO: having to define a `model_name` column every time as the `Task.outputs` is not ideal,\n        # this could be handled always and the value could be included within the DAG validation when\n        # a `Task` is used, since all the `Task` subclasses will have an `llm` with a `model_name` attr.\n        _outputs = [\n            (\"evolved_response\" if not self.store_evolutions else \"evolved_responses\"),\n            \"model_name\",\n        ]\n\n        return _outputs\n\n    def format_output(self, responses: Union[str, List[str]]) -&gt; Dict[str, Any]:  # type: ignore\n        \"\"\"The output for the task is a dict with: `evolved_response` or `evolved_responses`,\n        depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n        and, finally, the `model_name`.\n\n        Args:\n            responses: The responses to be included within the output.\n\n        Returns:\n            if `store_evolutions=False` return {\"evolved_response\": ..., \"model_name\": ...};\n            if `store_evolutions=True` return {\"evolved_responses\": ..., \"model_name\": ...}.\n        \"\"\"\n        _output = {}\n\n        if not self.store_evolutions:\n            _output[\"evolved_response\"] = responses[-1]\n        else:\n            _output[\"evolved_responses\"] = responses\n\n        _output[\"model_name\"] = self.llm.model_name\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates` enum.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, instruction: str, response: str) -&gt; str:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            instruction: The instruction to be included within the mutation prompt.\n\n        Returns:\n            A random mutation prompt with the provided instruction.\n        \"\"\"\n        mutation = np.random.choice(self.mutation_templates_names)\n        return (\n            self.mutation_templates[mutation]\n            .replace(\"&lt;PROMPT&gt;\", instruction)\n            .replace(\"&lt;RESPONSE&gt;\", response)\n        )\n\n    def _evolve_reponses(self, inputs: \"StepInput\") -&gt; List[List[str]]:\n        \"\"\"Evolves the instructions provided as part of the inputs of the task.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list where each item is a list with either the last evolved instruction if\n            `store_evolutions=False` or all the evolved instructions if `store_evolutions=True`.\n        \"\"\"\n        np.random.seed(self.seed)\n        instructions: List[List[str]] = [[input[\"instruction\"]] for input in inputs]\n        responses: List[List[str]] = [[input[\"response\"]] for input in inputs]\n\n        for iter_no in range(self.num_evolutions):\n            formatted_prompts = []\n            for instruction, response in zip(instructions, responses):\n                formatted_prompts.append(\n                    self._apply_random_mutation(instruction[-1], response[-1])\n                )\n\n            formatted_prompts = [\n                self.format_input(prompt) for prompt in formatted_prompts\n            ]\n\n            generated_responses = self.llm.generate(\n                formatted_prompts,\n                **self.llm.generation_kwargs,  # type: ignore\n            )\n\n            if self.store_evolutions:\n                responses = [\n                    response + [evolved_response[0]]\n                    for response, evolved_response in zip(\n                        responses, generated_responses\n                    )\n                ]\n            else:\n                responses = [\n                    [evolved_response[0]] for evolved_response in generated_responses\n                ]\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} evolving {len(responses)} responses!\"\n            )\n\n        return responses\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        responses = self._evolve_reponses(inputs)\n\n        if self.store_evolutions:\n            # Remove the input instruction from the `evolved_responses` list\n            from_ = 1 if not self.include_original_response else 0\n            responses = [response[from_:] for response in responses]\n\n        for input, response in zip(inputs, responses):\n            input.update(self.format_output(response))\n        yield inputs\n\n        self._logger.info(f\"\ud83c\udf89 Finished evolving {len(responses)} instructions!\")\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolQuality.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>instruction</code> and <code>response</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolQuality.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code> enum.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolQuality.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>evolved_response/s</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolQuality.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation. And the <code>system_prompt</code> is added as the first message if it exists.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation. And the\n    `system_prompt` is added as the first message if it exists.\"\"\"\n    return [{\"role\": \"user\", \"content\": input}]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolQuality.format_output","title":"<code>format_output(responses)</code>","text":"<p>The output for the task is a dict with: <code>evolved_response</code> or <code>evolved_responses</code>, depending whether the value is either <code>False</code> or <code>True</code> for <code>store_evolutions</code>, respectively; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>Union[str, List[str]]</code> <p>The responses to be included within the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>if <code>store_evolutions=False</code> return {\"evolved_response\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> return {\"evolved_responses\": ..., \"model_name\": ...}.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>def format_output(self, responses: Union[str, List[str]]) -&gt; Dict[str, Any]:  # type: ignore\n    \"\"\"The output for the task is a dict with: `evolved_response` or `evolved_responses`,\n    depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n    and, finally, the `model_name`.\n\n    Args:\n        responses: The responses to be included within the output.\n\n    Returns:\n        if `store_evolutions=False` return {\"evolved_response\": ..., \"model_name\": ...};\n        if `store_evolutions=True` return {\"evolved_responses\": ..., \"model_name\": ...}.\n    \"\"\"\n    _output = {}\n\n    if not self.store_evolutions:\n        _output[\"evolved_response\"] = responses[-1]\n    else:\n        _output[\"evolved_responses\"] = responses\n\n    _output[\"model_name\"] = self.llm.model_name\n    return _output\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolQuality.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Override this method to perform additional initialization after <code>__init__</code> and <code>model_construct</code>. This is useful if you want to do some validation that requires the entire model to be initialized.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>@override\ndef model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n    This is useful if you want to do some validation that requires the entire model to be initialized.\n    \"\"\"\n    super().model_post_init(__context)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.EvolQuality.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    responses = self._evolve_reponses(inputs)\n\n    if self.store_evolutions:\n        # Remove the input instruction from the `evolved_responses` list\n        from_ = 1 if not self.include_original_response else 0\n        responses = [response[from_:] for response in responses]\n\n    for input, response in zip(inputs, responses):\n        input.update(self.format_output(response))\n    yield inputs\n\n    self._logger.info(f\"\ud83c\udf89 Finished evolving {len(responses)} instructions!\")\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateEmbeddings","title":"<code>GenerateEmbeddings</code>","text":"<p>               Bases: <code>Step</code></p> <p>Generate embeddings using the last hidden state of an <code>LLM</code>.</p> <p>Generate embeddings for a text input using the last hidden state of an <code>LLM</code>, as described in the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>The <code>LLM</code> to use to generate the embeddings.</p> Input columns <ul> <li>text (<code>str</code>, <code>List[Dict[str, str]]</code>): The input text or conversation to generate     embeddings for.</li> </ul> Output columns <ul> <li>embedding (<code>List[float]</code>): The embedding of the input text or conversation.</li> <li>model_name (<code>str</code>): The model name used to generate the embeddings.</li> </ul> Categories <ul> <li>embedding</li> <li>llm</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> </ul> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>class GenerateEmbeddings(Step):\n    \"\"\"Generate embeddings using the last hidden state of an `LLM`.\n\n    Generate embeddings for a text input using the last hidden state of an `LLM`, as\n    described in the paper 'What Makes Good Data for Alignment? A Comprehensive Study of\n    Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        llm: The `LLM` to use to generate the embeddings.\n\n    Input columns:\n        - text (`str`, `List[Dict[str, str]]`): The input text or conversation to generate\n            embeddings for.\n\n    Output columns:\n        - embedding (`List[float]`): The embedding of the input text or conversation.\n        - model_name (`str`): The model name used to generate the embeddings.\n\n    Categories:\n        - embedding\n        - llm\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    llm: LLM\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `LLM` used to generate the embeddings.\"\"\"\n        super().load()\n\n        self.llm.load()\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task is a `text` column containing either a string or a\n        list of dictionaries in OpenAI chat-like format.\"\"\"\n        return [\"text\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task is an `embedding` column containing the embedding of\n        the `text` input.\"\"\"\n        return [\"embedding\", \"model_name\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"Formats the input to be used by the LLM to generate the embeddings. The input\n        can be in `ChatType` format or a string. If a string, it will be converted to a\n        list of dictionaries in OpenAI chat-like format.\n\n        Args:\n            input: The input to format.\n\n        Returns:\n            The OpenAI chat-like format of the input.\n        \"\"\"\n        text = input[\"text\"] = input[\"text\"]\n\n        # input is in `ChatType` format\n        if isinstance(text, str):\n            return [{\"role\": \"user\", \"content\": text}]\n\n        if is_openai_format(text):\n            return text\n\n        raise ValueError(\n            f\"Couldn't format input for step {self.name}. The `text` input column has to\"\n            \" be a string or a list of dictionaries in OpenAI chat-like format.\"\n        )\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Generates an embedding for each input using the last hidden state of the `LLM`.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        formatted_inputs = [self.format_input(input) for input in inputs]\n        last_hidden_states = self.llm.get_last_hidden_states(formatted_inputs)\n        for input, hidden_state in zip(inputs, last_hidden_states):\n            input[\"embedding\"] = hidden_state[-1].tolist()\n            input[\"model_name\"] = self.llm.model_name\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateEmbeddings.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task is a <code>text</code> column containing either a string or a list of dictionaries in OpenAI chat-like format.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateEmbeddings.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task is an <code>embedding</code> column containing the embedding of the <code>text</code> input.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateEmbeddings.format_input","title":"<code>format_input(input)</code>","text":"<p>Formats the input to be used by the LLM to generate the embeddings. The input can be in <code>ChatType</code> format or a string. If a string, it will be converted to a list of dictionaries in OpenAI chat-like format.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, Any]</code> <p>The input to format.</p> required <p>Returns:</p> Type Description <code>ChatType</code> <p>The OpenAI chat-like format of the input.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"Formats the input to be used by the LLM to generate the embeddings. The input\n    can be in `ChatType` format or a string. If a string, it will be converted to a\n    list of dictionaries in OpenAI chat-like format.\n\n    Args:\n        input: The input to format.\n\n    Returns:\n        The OpenAI chat-like format of the input.\n    \"\"\"\n    text = input[\"text\"] = input[\"text\"]\n\n    # input is in `ChatType` format\n    if isinstance(text, str):\n        return [{\"role\": \"user\", \"content\": text}]\n\n    if is_openai_format(text):\n        return text\n\n    raise ValueError(\n        f\"Couldn't format input for step {self.name}. The `text` input column has to\"\n        \" be a string or a list of dictionaries in OpenAI chat-like format.\"\n    )\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateEmbeddings.load","title":"<code>load()</code>","text":"<p>Loads the <code>LLM</code> used to generate the embeddings.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `LLM` used to generate the embeddings.\"\"\"\n    super().load()\n\n    self.llm.load()\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateEmbeddings.process","title":"<code>process(inputs)</code>","text":"<p>Generates an embedding for each input using the last hidden state of the <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Generates an embedding for each input using the last hidden state of the `LLM`.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    formatted_inputs = [self.format_input(input) for input in inputs]\n    last_hidden_states = self.llm.get_last_hidden_states(formatted_inputs)\n    for input, hidden_state in zip(inputs, last_hidden_states):\n        input[\"embedding\"] = hidden_state[-1].tolist()\n        input[\"model_name\"] = self.llm.model_name\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateSentencePair","title":"<code>GenerateSentencePair</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate a positive and negative (optionally) sentences given an anchor sentence.</p> <p><code>GenerateSentencePair</code> is a pre-defined task that given an anchor sentence generates a positive sentence related to the anchor and optionally a negative sentence unrelated to the anchor. This task is useful to generate training datasets for training embeddings models.</p> <p>Attributes:</p> Name Type Description <code>triplet</code> <code>bool</code> <p>a flag to indicate if the task should generate a triplet of sentences (anchor, positive, negative). Defaults to <code>False</code>.</p> <code>action</code> <code>GenerationAction</code> <p>the action to perform to generate the positive sentence.</p> Input columns <ul> <li>anchor (<code>str</code>): The anchor sentence to generate the positive and negative sentences.</li> </ul> Output columns <ul> <li>positive (<code>str</code>): The positive sentence related to the <code>anchor</code>.</li> <li>negative (<code>str</code>): The negative sentence unrelated to the <code>anchor</code> if <code>triplet=True</code>.</li> <li>model_name (<code>str</code>): The name of the model that was used to generate the sentences.</li> </ul> Categories <ul> <li>embedding</li> </ul> <p>Examples:</p> <pre><code>Paraphrasing:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"paraphrase\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n```\n\nGenerating semantically similar sentences:\n\n```python\nfrom distilabel.llms import InferenceEndpointsLLM\nfrom distilabel.steps.tasks import GenerateSentencePair\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"semantically-similar\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"How does 3D printing work?\"}])\n```\n\nGenerating queries:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"query\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"Argilla is an open-source data curation platform for LLMs. Using Argilla, ...\"}])\n```\n\nGenerating answers:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"answer\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n```\n</code></pre> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>class GenerateSentencePair(Task):\n    \"\"\"Generate a positive and negative (optionally) sentences given an anchor sentence.\n\n    `GenerateSentencePair` is a pre-defined task that given an anchor sentence generates\n    a positive sentence related to the anchor and optionally a negative sentence unrelated\n    to the anchor. This task is useful to generate training datasets for training embeddings\n    models.\n\n    Attributes:\n        triplet: a flag to indicate if the task should generate a triplet of sentences\n            (anchor, positive, negative). Defaults to `False`.\n        action: the action to perform to generate the positive sentence.\n\n    Input columns:\n        - anchor (`str`): The anchor sentence to generate the positive and negative sentences.\n\n    Output columns:\n        - positive (`str`): The positive sentence related to the `anchor`.\n        - negative (`str`): The negative sentence unrelated to the `anchor` if `triplet=True`.\n        - model_name (`str`): The name of the model that was used to generate the sentences.\n\n    Categories:\n        - embedding\n\n    Examples:\n\n        Paraphrasing:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"paraphrase\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n        ```\n\n        Generating semantically similar sentences:\n\n        ```python\n        from distilabel.llms import InferenceEndpointsLLM\n        from distilabel.steps.tasks import GenerateSentencePair\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"semantically-similar\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"How does 3D printing work?\"}])\n        ```\n\n        Generating queries:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"query\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"Argilla is an open-source data curation platform for LLMs. Using Argilla, ...\"}])\n        ```\n\n        Generating answers:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"answer\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n        ```\n    \"\"\"\n\n    triplet: bool = False\n    action: GenerationAction\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"generate-sentence-pair.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task is the `anchor` sentence.\"\"\"\n        return [\"anchor\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The inputs are formatted as a `ChatType`, with a system prompt describing the\n        task of generating a positive and negative sentences for the anchor sentence. The\n        anchor is provided as the first user interaction in the conversation.\n\n        Args:\n            input: The input containing the `anchor` sentence.\n\n        Returns:\n            A list of dictionaries containing the system and user interactions.\n        \"\"\"\n        action_sentence = GENERATION_ACTION_SENTENCES[self.action]\n        system_prompt = (\n            POSITIVE_NEGATIVE_SYSTEM_PROMPT if self.triplet else POSITIVE_SYSTEM_PROMPT\n        ).format(action_sentence=action_sentence)\n\n        return [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": self._template.render(anchor=input[\"anchor\"])},\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the `positive` and `negative` sentences, as well\n        as the `model_name` used to generate the sentences.\"\"\"\n        columns = [\"positive\", \"negative\"] if self.triplet else [\"positive\"]\n        columns += [\"model_name\"]\n        return columns\n\n    def format_output(\n        self, output: Union[str, None], input: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Formats the output of the LLM, to extract the `positive` and `negative` sentences\n        generated. If the output is `None` or the regex doesn't match, then the outputs\n        will be set to `None` as well.\n\n        Args:\n            output: The output of the LLM.\n            input: The input used to generate the output.\n\n        Returns:\n            The formatted output containing the `positive` and `negative` sentences.\n        \"\"\"\n        if output is None:\n            return {\"positive\": None, \"negative\": None}\n\n        match = POSITIVE_NEGATIVE_PAIR_REGEX.match(output)\n        if match is None:\n            formatted_output = {\"positive\": None}\n            if self.triplet:\n                formatted_output[\"negative\"] = None\n            return formatted_output\n\n        groups = match.groups()\n        if self.triplet:\n            return {\n                \"positive\": groups[0].strip(),\n                \"negative\": groups[1].strip()\n                if len(groups) &gt; 1 and groups[1] is not None\n                else None,\n            }\n\n        return {\"positive\": groups[0].strip()}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateSentencePair.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task is the <code>anchor</code> sentence.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateSentencePair.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the <code>positive</code> and <code>negative</code> sentences, as well as the <code>model_name</code> used to generate the sentences.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateSentencePair.format_input","title":"<code>format_input(input)</code>","text":"<p>The inputs are formatted as a <code>ChatType</code>, with a system prompt describing the task of generating a positive and negative sentences for the anchor sentence. The anchor is provided as the first user interaction in the conversation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, Any]</code> <p>The input containing the <code>anchor</code> sentence.</p> required <p>Returns:</p> Type Description <code>ChatType</code> <p>A list of dictionaries containing the system and user interactions.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The inputs are formatted as a `ChatType`, with a system prompt describing the\n    task of generating a positive and negative sentences for the anchor sentence. The\n    anchor is provided as the first user interaction in the conversation.\n\n    Args:\n        input: The input containing the `anchor` sentence.\n\n    Returns:\n        A list of dictionaries containing the system and user interactions.\n    \"\"\"\n    action_sentence = GENERATION_ACTION_SENTENCES[self.action]\n    system_prompt = (\n        POSITIVE_NEGATIVE_SYSTEM_PROMPT if self.triplet else POSITIVE_SYSTEM_PROMPT\n    ).format(action_sentence=action_sentence)\n\n    return [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": self._template.render(anchor=input[\"anchor\"])},\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateSentencePair.format_output","title":"<code>format_output(output, input=None)</code>","text":"<p>Formats the output of the LLM, to extract the <code>positive</code> and <code>negative</code> sentences generated. If the output is <code>None</code> or the regex doesn't match, then the outputs will be set to <code>None</code> as well.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>The output of the LLM.</p> required <code>input</code> <code>Optional[Dict[str, Any]]</code> <p>The input used to generate the output.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The formatted output containing the <code>positive</code> and <code>negative</code> sentences.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Optional[Dict[str, Any]] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Formats the output of the LLM, to extract the `positive` and `negative` sentences\n    generated. If the output is `None` or the regex doesn't match, then the outputs\n    will be set to `None` as well.\n\n    Args:\n        output: The output of the LLM.\n        input: The input used to generate the output.\n\n    Returns:\n        The formatted output containing the `positive` and `negative` sentences.\n    \"\"\"\n    if output is None:\n        return {\"positive\": None, \"negative\": None}\n\n    match = POSITIVE_NEGATIVE_PAIR_REGEX.match(output)\n    if match is None:\n        formatted_output = {\"positive\": None}\n        if self.triplet:\n            formatted_output[\"negative\"] = None\n        return formatted_output\n\n    groups = match.groups()\n    if self.triplet:\n        return {\n            \"positive\": groups[0].strip(),\n            \"negative\": groups[1].strip()\n            if len(groups) &gt; 1 and groups[1] is not None\n            else None,\n        }\n\n    return {\"positive\": groups[0].strip()}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GenerateSentencePair.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"generate-sentence-pair.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.GeneratorTask","title":"<code>GeneratorTask</code>","text":"<p>               Bases: <code>_Task</code>, <code>GeneratorStep</code></p> <p>GeneratorTask is a class that implements the <code>_Task</code> abstract class and adds the <code>GeneratorStep</code> interface to be used as a step in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>the <code>LLM</code> to be used to generate the outputs of the task.</p> <code>group_generations</code> <p>whether to group the <code>num_generations</code> generated per input in a list or create a row per generation. Defaults to <code>False</code>.</p> <code>num_generations</code> <p>The number of generations to be produced per input.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>class GeneratorTask(_Task, GeneratorStep):\n    \"\"\"GeneratorTask is a class that implements the `_Task` abstract class and adds the\n    `GeneratorStep` interface to be used as a step in the pipeline.\n\n    Attributes:\n        llm: the `LLM` to be used to generate the outputs of the task.\n        group_generations: whether to group the `num_generations` generated per input in\n            a list or create a row per generation. Defaults to `False`.\n        num_generations: The number of generations to be produced per input.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Genstruct","title":"<code>Genstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate a pair of instruction-response from a document using an <code>LLM</code>.</p> <p><code>Genstruct</code> is a pre-defined task designed to generate valid instructions from a given raw document, with the title and the content, enabling the creation of new, partially synthetic instruction finetuning datasets from any raw-text corpus. The task is based on the Genstruct 7B model by Nous Research, which is inspired in the Ada-Instruct paper.</p> Note <p>The Genstruct prompt i.e. the task, can be used with any model really, but the safest / recommended option is to use <code>NousResearch/Genstruct-7B</code> as the LLM provided to the task, since it was trained for this specific task.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>title (<code>str</code>): The title of the document.</li> <li>content (<code>str</code>): The content of the document.</li> </ul> Output columns <ul> <li>user (<code>str</code>): The user's instruction based on the document.</li> <li>assistant (<code>str</code>): The assistant's response based on the user's instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the <code>feedback</code> and <code>result</code>.</li> </ul> Categories <ul> <li>text-generation</li> <li>instruction</li> <li>response</li> </ul> References <ul> <li>Genstruct 7B by Nous Research</li> <li>Ada-Instruct: Adapting Instruction Generators for Complex Reasoning</li> </ul> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>class Genstruct(Task):\n    \"\"\"Generate a pair of instruction-response from a document using an `LLM`.\n\n    `Genstruct` is a pre-defined task designed to generate valid instructions from a given raw document,\n    with the title and the content, enabling the creation of new, partially synthetic instruction finetuning\n    datasets from any raw-text corpus. The task is based on the Genstruct 7B model by Nous Research, which is\n    inspired in the Ada-Instruct paper.\n\n    Note:\n        The Genstruct prompt i.e. the task, can be used with any model really, but the safest / recommended\n        option is to use `NousResearch/Genstruct-7B` as the LLM provided to the task, since it was trained\n        for this specific task.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - title (`str`): The title of the document.\n        - content (`str`): The content of the document.\n\n    Output columns:\n        - user (`str`): The user's instruction based on the document.\n        - assistant (`str`): The assistant's response based on the user's instruction.\n        - model_name (`str`): The model name used to generate the `feedback` and `result`.\n\n    Categories:\n        - text-generation\n        - instruction\n        - response\n\n    References:\n        - [Genstruct 7B by Nous Research](https://huggingface.co/NousResearch/Genstruct-7B)\n        - [Ada-Instruct: Adapting Instruction Generators for Complex Reasoning](https://arxiv.org/abs/2310.04484)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"genstruct.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the `title` and the `content`.\"\"\"\n        return [\"title\", \"content\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    title=input[\"title\"], content=input[\"content\"]\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `user` instruction based on the provided document\n        and the `assistant` response based on the user's instruction.\"\"\"\n        return [\"user\", \"assistant\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted so that both the user and the assistant messages are\n        captured.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the keys `user` and `assistant` containing the content for each role.\n        \"\"\"\n        if output is None:\n            return {\"user\": None, \"assistant\": None}\n\n        matches = re.search(_PARSE_GENSTRUCT_OUTPUT_REGEX, output, re.DOTALL)\n        if not matches:\n            return {\"user\": None, \"assistant\": None}\n\n        return {\n            \"user\": matches.group(1).strip(),\n            \"assistant\": matches.group(2).strip(),\n        }\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Genstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the <code>title</code> and the <code>content</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Genstruct.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>user</code> instruction based on the provided document and the <code>assistant</code> response based on the user's instruction.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Genstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                title=input[\"title\"], content=input[\"content\"]\n            ),\n        }\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Genstruct.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted so that both the user and the assistant messages are captured.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the keys <code>user</code> and <code>assistant</code> containing the content for each role.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted so that both the user and the assistant messages are\n    captured.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the keys `user` and `assistant` containing the content for each role.\n    \"\"\"\n    if output is None:\n        return {\"user\": None, \"assistant\": None}\n\n    matches = re.search(_PARSE_GENSTRUCT_OUTPUT_REGEX, output, re.DOTALL)\n    if not matches:\n        return {\"user\": None, \"assistant\": None}\n\n    return {\n        \"user\": matches.group(1).strip(),\n        \"assistant\": matches.group(2).strip(),\n    }\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Genstruct.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"genstruct.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.InstructionBacktranslation","title":"<code>InstructionBacktranslation</code>","text":"<p>               Bases: <code>Task</code></p> <p>Self-Alignment with Instruction Backtranslation.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Optional[Template]</code> <p>the Jinja2 template to use for the Instruction Backtranslation task.</p> Input columns <ul> <li>instruction (<code>str</code>): The reference instruction to evaluate the text output.</li> <li>generation (<code>str</code>): The text output to evaluate for the given instruction.</li> </ul> Output columns <ul> <li>score (<code>str</code>): The score for the generation based on the given instruction.</li> <li>reason (<code>str</code>): The reason for the provided score.</li> <li>model_name (<code>str</code>): The model name used to score the generation.</li> </ul> Categories <ul> <li>critique</li> </ul> References <ul> <li><code>Self-Alignment with Instruction Backtranslation</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>class InstructionBacktranslation(Task):\n    \"\"\"Self-Alignment with Instruction Backtranslation.\n\n    Attributes:\n        _template: the Jinja2 template to use for the Instruction Backtranslation task.\n\n    Input columns:\n        - instruction (`str`): The reference instruction to evaluate the text output.\n        - generation (`str`): The text output to evaluate for the given instruction.\n\n    Output columns:\n        - score (`str`): The score for the generation based on the given instruction.\n        - reason (`str`): The reason for the provided score.\n        - model_name (`str`): The model name used to score the generation.\n\n    Categories:\n        - critique\n\n    References:\n        - [`Self-Alignment with Instruction Backtranslation`](https://arxiv.org/abs/2308.06259)\n    \"\"\"\n\n    _template: Optional[\"Template\"] = PrivateAttr(default=...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"instruction-backtranslation.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`, and the `generation` for it.\"\"\"\n        return [\"instruction\", \"generation\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], generation=input[\"generation\"]\n                ),\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `score`, `reason` and the `model_name`.\"\"\"\n        return [\"score\", \"reason\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `score` and `reason`. The\n        `model_name` will be automatically included within the `process` method of `Task`.\n\n        Args:\n            output: a string representing the output of the LLM via the `process` method.\n            input: the input to the task, as required by some tasks to format the output.\n\n        Returns:\n            A dictionary containing the `score` and the `reason` for the provided `score`.\n        \"\"\"\n        pattern = r\"(.+?)Score: (\\d)\"\n\n        matches = None\n        if output is not None:\n            matches = re.findall(pattern, output, re.DOTALL)\n        if matches is None:\n            return {\"score\": None, \"reason\": None}\n\n        return {\n            \"score\": int(matches[0][1]),\n            \"reason\": matches[0][0].strip(),\n        }\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.InstructionBacktranslation.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>, and the <code>generation</code> for it.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.InstructionBacktranslation.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>score</code>, <code>reason</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.InstructionBacktranslation.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], generation=input[\"generation\"]\n            ),\n        },\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.InstructionBacktranslation.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>score</code> and <code>reason</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>a string representing the output of the LLM via the <code>process</code> method.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task, as required by some tasks to format the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the <code>score</code> and the <code>reason</code> for the provided <code>score</code>.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `score` and `reason`. The\n    `model_name` will be automatically included within the `process` method of `Task`.\n\n    Args:\n        output: a string representing the output of the LLM via the `process` method.\n        input: the input to the task, as required by some tasks to format the output.\n\n    Returns:\n        A dictionary containing the `score` and the `reason` for the provided `score`.\n    \"\"\"\n    pattern = r\"(.+?)Score: (\\d)\"\n\n    matches = None\n    if output is not None:\n        matches = re.findall(pattern, output, re.DOTALL)\n    if matches is None:\n        return {\"score\": None, \"reason\": None}\n\n    return {\n        \"score\": int(matches[0][1]),\n        \"reason\": matches[0][0].strip(),\n    }\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.InstructionBacktranslation.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"instruction-backtranslation.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PairRM","title":"<code>PairRM</code>","text":"<p>               Bases: <code>Step</code></p> <p>Rank the candidates based on the input using the <code>LLM</code> model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The model to use for the ranking. Defaults to <code>\"llm-blender/PairRM\"</code>.</p> <code>instructions</code> <code>Optional[str]</code> <p>The instructions to use for the model. Defaults to <code>None</code>.</p> Input columns <ul> <li>inputs (<code>List[Dict[str, Any]]</code>): The input text or conversation to rank the candidates for.</li> <li>candidates (<code>List[Dict[str, Any]]</code>): The candidates to rank.</li> </ul> Output columns <ul> <li>ranks (<code>List[int]</code>): The ranks of the candidates based on the input.</li> <li>ranked_candidates (<code>List[Dict[str, Any]]</code>): The candidates ranked based on the input.</li> <li>model_name (<code>str</code>): The model name used to rank the candidate responses. Defaults to <code>\"llm-blender/PairRM\"</code>.</li> </ul> References <ul> <li>LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion.</li> <li>Pair Ranking Model.</li> </ul> Categories <ul> <li>preference</li> </ul> Note <p>This step differs to other tasks as there is a single implementation of this model currently, and we will use a specific <code>LLM</code>.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>class PairRM(Step):\n    \"\"\"Rank the candidates based on the input using the `LLM` model.\n\n    Attributes:\n        model: The model to use for the ranking. Defaults to `\"llm-blender/PairRM\"`.\n        instructions: The instructions to use for the model. Defaults to `None`.\n\n    Input columns:\n        - inputs (`List[Dict[str, Any]]`): The input text or conversation to rank the candidates for.\n        - candidates (`List[Dict[str, Any]]`): The candidates to rank.\n\n    Output columns:\n        - ranks (`List[int]`): The ranks of the candidates based on the input.\n        - ranked_candidates (`List[Dict[str, Any]]`): The candidates ranked based on the input.\n        - model_name (`str`): The model name used to rank the candidate responses. Defaults to `\"llm-blender/PairRM\"`.\n\n    References:\n        - [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://arxiv.org/abs/2306.02561).\n        - [Pair Ranking Model](https://huggingface.co/llm-blender/PairRM).\n\n    Categories:\n        - preference\n\n    Note:\n        This step differs to other tasks as there is a single implementation of this model\n        currently, and we will use a specific `LLM`.\n    \"\"\"\n\n    model: str = \"llm-blender/PairRM\"\n    instructions: Optional[str] = None\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the PairRM model provided via `model` with `llm_blender.Blender`, which is the\n        custom library for running the inference for the PairRM models.\"\"\"\n        try:\n            import llm_blender\n        except ImportError as e:\n            raise ImportError(\n                \"The `llm_blender` package is required to use the `PairRM` class.\"\n                \"Please install it with `pip install git+https://github.com/yuchenlin/LLM-Blender.git`.\"\n            ) from e\n\n        self._blender = llm_blender.Blender()\n        self._blender.loadranker(self.model)\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input columns correspond to the two required arguments from `Blender.rank`:\n        `inputs` and `candidates`.\"\"\"\n        return [\"input\", \"candidates\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs will include the `ranks` and the `ranked_candidates`.\"\"\"\n        return [\"ranks\", \"ranked_candidates\", \"model_name\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"The input is expected to be a dictionary with the keys `input` and `candidates`,\n        where the `input` corresponds to the instruction of a model and `candidates` are a\n        list of responses to be ranked.\n        \"\"\"\n        return {\"input\": input[\"input\"], \"candidates\": input[\"candidates\"]}\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Generates the ranks for the candidates based on the input.\n\n        The ranks are the positions of the candidates, where lower is better,\n        and the ranked candidates correspond to the candidates sorted according to the\n        ranks obtained.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            An iterator with the inputs containing the `ranks`, `ranked_candidates`, and `model_name`.\n        \"\"\"\n        input_texts = []\n        candidates = []\n        for input in inputs:\n            formatted_input = self.format_input(input)\n            input_texts.append(formatted_input[\"input\"])\n            candidates.append(formatted_input[\"candidates\"])\n\n        instructions = (\n            [self.instructions] * len(input_texts) if self.instructions else None\n        )\n\n        ranks = self._blender.rank(\n            input_texts,\n            candidates,\n            instructions=instructions,\n            return_scores=False,\n            batch_size=self.input_batch_size,\n        )\n        # Sort the candidates based on the ranks\n        ranked_candidates = np.take_along_axis(\n            np.array(candidates), ranks - 1, axis=1\n        ).tolist()\n        ranks = ranks.tolist()\n        for input, rank, ranked_candidate in zip(inputs, ranks, ranked_candidates):\n            input[\"ranks\"] = rank\n            input[\"ranked_candidates\"] = ranked_candidate\n            input[\"model_name\"] = self.model\n\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PairRM.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input columns correspond to the two required arguments from <code>Blender.rank</code>: <code>inputs</code> and <code>candidates</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PairRM.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs will include the <code>ranks</code> and the <code>ranked_candidates</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PairRM.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is expected to be a dictionary with the keys <code>input</code> and <code>candidates</code>, where the <code>input</code> corresponds to the instruction of a model and <code>candidates</code> are a list of responses to be ranked.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"The input is expected to be a dictionary with the keys `input` and `candidates`,\n    where the `input` corresponds to the instruction of a model and `candidates` are a\n    list of responses to be ranked.\n    \"\"\"\n    return {\"input\": input[\"input\"], \"candidates\": input[\"candidates\"]}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PairRM.load","title":"<code>load()</code>","text":"<p>Loads the PairRM model provided via <code>model</code> with <code>llm_blender.Blender</code>, which is the custom library for running the inference for the PairRM models.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the PairRM model provided via `model` with `llm_blender.Blender`, which is the\n    custom library for running the inference for the PairRM models.\"\"\"\n    try:\n        import llm_blender\n    except ImportError as e:\n        raise ImportError(\n            \"The `llm_blender` package is required to use the `PairRM` class.\"\n            \"Please install it with `pip install git+https://github.com/yuchenlin/LLM-Blender.git`.\"\n        ) from e\n\n    self._blender = llm_blender.Blender()\n    self._blender.loadranker(self.model)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PairRM.process","title":"<code>process(inputs)</code>","text":"<p>Generates the ranks for the candidates based on the input.</p> <p>The ranks are the positions of the candidates, where lower is better, and the ranked candidates correspond to the candidates sorted according to the ranks obtained.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>An iterator with the inputs containing the <code>ranks</code>, <code>ranked_candidates</code>, and <code>model_name</code>.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Generates the ranks for the candidates based on the input.\n\n    The ranks are the positions of the candidates, where lower is better,\n    and the ranked candidates correspond to the candidates sorted according to the\n    ranks obtained.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        An iterator with the inputs containing the `ranks`, `ranked_candidates`, and `model_name`.\n    \"\"\"\n    input_texts = []\n    candidates = []\n    for input in inputs:\n        formatted_input = self.format_input(input)\n        input_texts.append(formatted_input[\"input\"])\n        candidates.append(formatted_input[\"candidates\"])\n\n    instructions = (\n        [self.instructions] * len(input_texts) if self.instructions else None\n    )\n\n    ranks = self._blender.rank(\n        input_texts,\n        candidates,\n        instructions=instructions,\n        return_scores=False,\n        batch_size=self.input_batch_size,\n    )\n    # Sort the candidates based on the ranks\n    ranked_candidates = np.take_along_axis(\n        np.array(candidates), ranks - 1, axis=1\n    ).tolist()\n    ranks = ranks.tolist()\n    for input, rank, ranked_candidate in zip(inputs, ranks, ranked_candidates):\n        input[\"ranks\"] = rank\n        input[\"ranked_candidates\"] = ranked_candidate\n        input[\"model_name\"] = self.model\n\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PrometheusEval","title":"<code>PrometheusEval</code>","text":"<p>               Bases: <code>Task</code></p> <p>Critique and rank the quality of generations from an <code>LLM</code> using Prometheus 2.0.</p> <p><code>PrometheusEval</code> is a task created for Prometheus 2.0, covering both the absolute and relative evaluations.</p> <ul> <li>The absolute evaluation i.e. <code>mode=\"absolute\"</code> is used to evaluate a single generation from     an LLM for a given instruction.</li> <li>The relative evaluation i.e. <code>mode=\"relative\"</code> is used to evaluate two generations from an LLM     for a given instruction.</li> </ul> <p>Both evaluations provide the possibility whether to use a reference answer to compare with or not via the <code>reference</code> attribute, and both are based on a score rubric that critiques the generation/s based on the following default aspects: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, and <code>reasoning</code>, that can be overridden via <code>rubrics</code>, and the selected rubric is set via the attribute <code>rubric</code>.</p> Note <p>The <code>PrometheusEval</code> task is better suited and intended to be used with any of the Prometheus 2.0 models released by Kaist AI, being: https://huggingface.co/prometheus-eval/prometheus-7b-v2.0, and https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0. The critique assessment formatting and quality is not guaranteed if using another model, even though some other models may be able to correctly follow the formatting and generate insightful critiques too.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>Literal['absolute', 'relative']</code> <p>the evaluation mode to use, either <code>absolute</code> or <code>relative</code>. It defines whether the task will evaluate one or two generations.</p> <code>rubric</code> <code>str</code> <p>the score rubric to use within the prompt to run the critique based on different aspects. Can be any existing key in the <code>rubrics</code> attribute, which by default means that it can be: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, or <code>reasoning</code>. Those will only work if using the default <code>rubrics</code>, otherwise, the provided <code>rubrics</code> should be used.</p> <code>rubrics</code> <code>Optional[Dict[str, str]]</code> <p>a dictionary containing the different rubrics to use for the critique, where the keys are the rubric names and the values are the rubric descriptions. The default rubrics are the following: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, and <code>reasoning</code>.</p> <code>reference</code> <code>bool</code> <p>a boolean flag to indicate whether a reference answer / completion will be provided, so that the model critique is based on the comparison with it. It implies that the column <code>reference</code> needs to be provided within the input data in addition to the rest of the inputs.</p> <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to use as reference.</li> <li>generation (<code>str</code>, optional): The generated text from the given <code>instruction</code>. This column is required     if <code>mode=absolute</code>.</li> <li>generations (<code>List[str]</code>, optional): The generated texts from the given <code>instruction</code>. It should     contain 2 generations only. This column is required if <code>mode=relative</code>.</li> <li>reference (<code>str</code>, optional): The reference / golden answer for the <code>instruction</code>, to be used by the LLM     for comparison against.</li> </ul> Output columns <ul> <li>feedback (<code>str</code>): The feedback explaining the result below, as critiqued by the LLM using the     pre-defined score rubric, compared against <code>reference</code> if provided.</li> <li>result (<code>Union[int, Literal[\"A\", \"B\"]]</code>): If <code>mode=absolute</code>, then the result contains the score for the     <code>generation</code> in a likert-scale from 1-5, otherwise, if <code>mode=relative</code>, then the result contains either     \"A\" or \"B\", the \"winning\" one being the generation in the index 0 of <code>generations</code> if <code>result='A'</code> or the     index 1 if <code>result='B'</code>.</li> <li>model_name (<code>str</code>): The model name used to generate the <code>feedback</code> and <code>result</code>.</li> </ul> Categories <ul> <li>critique</li> <li>preference</li> </ul> References <ul> <li>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</li> <li>prometheus-eval: Evaluate your LLM's response with Prometheus \ud83d\udcaf</li> </ul> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>class PrometheusEval(Task):\n    \"\"\"Critique and rank the quality of generations from an `LLM` using Prometheus 2.0.\n\n    `PrometheusEval` is a task created for Prometheus 2.0, covering both the absolute and relative\n    evaluations.\n\n    - The absolute evaluation i.e. `mode=\"absolute\"` is used to evaluate a single generation from\n        an LLM for a given instruction.\n    - The relative evaluation i.e. `mode=\"relative\"` is used to evaluate two generations from an LLM\n        for a given instruction.\n\n    Both evaluations provide the possibility whether to use a reference answer to compare with or not\n    via the `reference` attribute, and both are based on a score rubric that critiques the generation/s\n    based on the following default aspects: `helpfulness`, `harmlessness`, `honesty`, `factual-validity`,\n    and `reasoning`, that can be overridden via `rubrics`, and the selected rubric is set via the attribute\n    `rubric`.\n\n    Note:\n        The `PrometheusEval` task is better suited and intended to be used with any of the Prometheus 2.0\n        models released by Kaist AI, being: https://huggingface.co/prometheus-eval/prometheus-7b-v2.0,\n        and https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0. The critique assessment formatting\n        and quality is not guaranteed if using another model, even though some other models may be able to\n        correctly follow the formatting and generate insightful critiques too.\n\n    Attributes:\n        mode: the evaluation mode to use, either `absolute` or `relative`. It defines whether the task\n            will evaluate one or two generations.\n        rubric: the score rubric to use within the prompt to run the critique based on different aspects.\n            Can be any existing key in the `rubrics` attribute, which by default means that it can be:\n            `helpfulness`, `harmlessness`, `honesty`, `factual-validity`, or `reasoning`. Those will only\n            work if using the default `rubrics`, otherwise, the provided `rubrics` should be used.\n        rubrics: a dictionary containing the different rubrics to use for the critique, where the keys are\n            the rubric names and the values are the rubric descriptions. The default rubrics are the following:\n            `helpfulness`, `harmlessness`, `honesty`, `factual-validity`, and `reasoning`.\n        reference: a boolean flag to indicate whether a reference answer / completion will be provided, so\n            that the model critique is based on the comparison with it. It implies that the column `reference`\n            needs to be provided within the input data in addition to the rest of the inputs.\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instruction (`str`): The instruction to use as reference.\n        - generation (`str`, optional): The generated text from the given `instruction`. This column is required\n            if `mode=absolute`.\n        - generations (`List[str]`, optional): The generated texts from the given `instruction`. It should\n            contain 2 generations only. This column is required if `mode=relative`.\n        - reference (`str`, optional): The reference / golden answer for the `instruction`, to be used by the LLM\n            for comparison against.\n\n    Output columns:\n        - feedback (`str`): The feedback explaining the result below, as critiqued by the LLM using the\n            pre-defined score rubric, compared against `reference` if provided.\n        - result (`Union[int, Literal[\"A\", \"B\"]]`): If `mode=absolute`, then the result contains the score for the\n            `generation` in a likert-scale from 1-5, otherwise, if `mode=relative`, then the result contains either\n            \"A\" or \"B\", the \"winning\" one being the generation in the index 0 of `generations` if `result='A'` or the\n            index 1 if `result='B'`.\n        - model_name (`str`): The model name used to generate the `feedback` and `result`.\n\n    Categories:\n        - critique\n        - preference\n\n    References:\n        - [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535)\n        - [prometheus-eval: Evaluate your LLM's response with Prometheus \ud83d\udcaf](https://github.com/prometheus-eval/prometheus-eval)\n    \"\"\"\n\n    mode: Literal[\"absolute\", \"relative\"]\n    rubric: str\n    rubrics: Optional[Dict[str, str]] = Field(default=_DEFAULT_RUBRICS)\n    reference: bool = False\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    @model_validator(mode=\"after\")\n    def validate_rubric_and_rubrics(self) -&gt; Self:\n        if not isinstance(self.rubrics, dict) or len(self.rubrics) &lt; 1:\n            raise ValueError(\n                \"Provided `rubrics` must be a Python dictionary with string keys and string values.\"\n            )\n\n        def rubric_matches_pattern(rubric: str) -&gt; bool:\n            \"\"\"Checks if the provided rubric matches the pattern of the default rubrics.\"\"\"\n            pattern = r\"^\\[.*?\\]\\n(?:Score [1-4]: .*?\\n){4}(?:Score 5: .*?)\"\n            return bool(re.match(pattern, rubric, re.MULTILINE))\n\n        if not all(rubric_matches_pattern(value) for value in self.rubrics.values()):\n            raise ValueError(\n                \"Provided rubrics should match the format of the default rubrics, which\"\n                \" is as follows: `[&lt;scoring criteria&gt;]\\nScore 1: &lt;description&gt;\\nScore 2: &lt;description&gt;\\n\"\n                \"Score 3: &lt;description&gt;\\nScore 4: &lt;description&gt;\\nScore 5: &lt;description&gt;`; replacing\"\n                \" `&lt;scoring criteria&gt;` and `&lt;description&gt;` with the actual criteria and description\"\n                \" for each or the scores, respectively.\"\n            )\n\n        if self.rubric not in self.rubrics:\n            raise ValueError(\n                f\"Provided rubric '{self.rubric}' is not among the available rubrics: {', '.join(self.rubrics.keys())}.\"\n            )\n\n        return self\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation\n        depending on the `mode` value, and either with or without reference, depending on the\n        value of `reference`.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"prometheus\"\n            / (\n                f\"{self.mode}_without_reference.jinja2\"\n                if self.reference is False\n                else f\"{self.mode}_with_reference.jinja2\"\n            )\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The default inputs for the task are the `instruction` and the `generation`\n        if `reference=False`, otherwise, the inputs are `instruction`, `generation`, and\n        `reference`.\"\"\"\n        if self.mode == \"absolute\":\n            if self.reference:\n                return [\"instruction\", \"generation\", \"reference\"]\n            return [\"instruction\", \"generation\"]\n        else:  # self.mode == \"relative\"\n            if self.reference:\n                return [\"instruction\", \"generations\", \"reference\"]\n            return [\"instruction\", \"generations\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` where the prompt is formatted according\n        to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction\n        from the user, including a pre-defined system prompt.\"\"\"\n        template_kwargs = {\n            \"instruction\": input[\"instruction\"],\n            \"rubric\": self.rubrics[self.rubric],\n        }\n        if self.reference:\n            template_kwargs[\"reference\"] = input[\"reference\"]\n\n        if self.mode == \"absolute\":\n            if not isinstance(input[\"generation\"], str):\n                raise ValueError(\n                    f\"Provided `generation` is of type {type(input['generation'])} but a string\"\n                    \" should be provided instead.\",\n                )\n\n            template_kwargs[\"generation\"] = input[\"generation\"]\n            system_message = (\n                \"You are a fair judge assistant tasked with providing clear, objective feedback based\"\n                \" on specific criteria, ensuring each assessment reflects the absolute standards set\"\n                \" for performance.\"\n            )\n        else:  # self.mode == \"relative\"\n            if (\n                not isinstance(input[\"generations\"], list)\n                or not all(\n                    isinstance(generation, str) for generation in input[\"generations\"]\n                )\n                or len(input[\"generations\"]) != 2\n            ):\n                raise ValueError(\n                    f\"Provided `generations` is of type {type(input['generations'])} but a list of strings with length 2 should be provided instead.\"\n                )\n\n            template_kwargs[\"generations\"] = input[\"generations\"]\n            system_message = (\n                \"You are a fair judge assistant assigned to deliver insightful feedback that compares\"\n                \" individual performances, highlighting how each stands relative to others within the\"\n                \" same cohort.\"\n            )\n\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": system_message,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(**template_kwargs),  # type: ignore\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `feedback` and the `result` generated by Prometheus,\n        as well as the `model_name` which is automatically included based on the `LLM` used.\n        \"\"\"\n        return [\"feedback\", \"result\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dict with the keys `feedback` and `result` captured\n        using a regex from the Prometheus output.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Optionally provided in case it's useful to build the output.\n\n        Returns:\n            A dict with the keys `feedback` and `result` generated by the LLM.\n        \"\"\"\n        if output is None:\n            return {\"feedback\": None, \"result\": None}\n\n        parts = output.split(\"[RESULT]\")\n        if len(parts) != 2:\n            return {\"feedback\": None, \"result\": None}\n\n        feedback, result = parts[0].strip(), parts[1].strip()\n        if feedback.startswith(\"Feedback:\"):\n            feedback = feedback[len(\"Feedback:\") :].strip()\n        if self.mode == \"absolute\":\n            if not result.isdigit() or result not in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n                return {\"feedback\": None, \"result\": None}\n            return {\"feedback\": feedback, \"result\": int(result)}\n        else:  # self.mode == \"relative\"\n            if result not in [\"A\", \"B\"]:\n                return {\"feedback\": None, \"result\": None}\n            return {\"feedback\": feedback, \"result\": result}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PrometheusEval.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The default inputs for the task are the <code>instruction</code> and the <code>generation</code> if <code>reference=False</code>, otherwise, the inputs are <code>instruction</code>, <code>generation</code>, and <code>reference</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PrometheusEval.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>feedback</code> and the <code>result</code> generated by Prometheus, as well as the <code>model_name</code> which is automatically included based on the <code>LLM</code> used.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PrometheusEval.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> where the prompt is formatted according to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction from the user, including a pre-defined system prompt.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` where the prompt is formatted according\n    to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction\n    from the user, including a pre-defined system prompt.\"\"\"\n    template_kwargs = {\n        \"instruction\": input[\"instruction\"],\n        \"rubric\": self.rubrics[self.rubric],\n    }\n    if self.reference:\n        template_kwargs[\"reference\"] = input[\"reference\"]\n\n    if self.mode == \"absolute\":\n        if not isinstance(input[\"generation\"], str):\n            raise ValueError(\n                f\"Provided `generation` is of type {type(input['generation'])} but a string\"\n                \" should be provided instead.\",\n            )\n\n        template_kwargs[\"generation\"] = input[\"generation\"]\n        system_message = (\n            \"You are a fair judge assistant tasked with providing clear, objective feedback based\"\n            \" on specific criteria, ensuring each assessment reflects the absolute standards set\"\n            \" for performance.\"\n        )\n    else:  # self.mode == \"relative\"\n        if (\n            not isinstance(input[\"generations\"], list)\n            or not all(\n                isinstance(generation, str) for generation in input[\"generations\"]\n            )\n            or len(input[\"generations\"]) != 2\n        ):\n            raise ValueError(\n                f\"Provided `generations` is of type {type(input['generations'])} but a list of strings with length 2 should be provided instead.\"\n            )\n\n        template_kwargs[\"generations\"] = input[\"generations\"]\n        system_message = (\n            \"You are a fair judge assistant assigned to deliver insightful feedback that compares\"\n            \" individual performances, highlighting how each stands relative to others within the\"\n            \" same cohort.\"\n        )\n\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": system_message,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(**template_kwargs),  # type: ignore\n        },\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PrometheusEval.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dict with the keys <code>feedback</code> and <code>result</code> captured using a regex from the Prometheus output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Optionally provided in case it's useful to build the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the keys <code>feedback</code> and <code>result</code> generated by the LLM.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dict with the keys `feedback` and `result` captured\n    using a regex from the Prometheus output.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Optionally provided in case it's useful to build the output.\n\n    Returns:\n        A dict with the keys `feedback` and `result` generated by the LLM.\n    \"\"\"\n    if output is None:\n        return {\"feedback\": None, \"result\": None}\n\n    parts = output.split(\"[RESULT]\")\n    if len(parts) != 2:\n        return {\"feedback\": None, \"result\": None}\n\n    feedback, result = parts[0].strip(), parts[1].strip()\n    if feedback.startswith(\"Feedback:\"):\n        feedback = feedback[len(\"Feedback:\") :].strip()\n    if self.mode == \"absolute\":\n        if not result.isdigit() or result not in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n            return {\"feedback\": None, \"result\": None}\n        return {\"feedback\": feedback, \"result\": int(result)}\n    else:  # self.mode == \"relative\"\n        if result not in [\"A\", \"B\"]:\n            return {\"feedback\": None, \"result\": None}\n        return {\"feedback\": feedback, \"result\": result}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.PrometheusEval.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation depending on the <code>mode</code> value, and either with or without reference, depending on the value of <code>reference</code>.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation\n    depending on the `mode` value, and either with or without reference, depending on the\n    value of `reference`.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"prometheus\"\n        / (\n            f\"{self.mode}_without_reference.jinja2\"\n            if self.reference is False\n            else f\"{self.mode}_with_reference.jinja2\"\n        )\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.QualityScorer","title":"<code>QualityScorer</code>","text":"<p>               Bases: <code>Task</code></p> <p>Score responses based on their quality using an <code>LLM</code>.</p> <p><code>QualityScorer</code> is a pre-defined task that defines the <code>instruction</code> as the input and <code>score</code> as the output. This task is used to rate the quality of instructions and responses. It's an implementation of the quality score task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'. The task follows the same scheme as the Complexity Scorer, but the instruction-response pairs are scored in terms of quality, obtaining a quality score for each instruction.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the <code>responses</code>.</li> <li>responses (<code>List[str]</code>): The responses to be scored. Each response forms a pair with the instruction.</li> </ul> Output columns <ul> <li>scores (<code>List[float]</code>): The score for each instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the scores.</li> </ul> Categories <ul> <li>scorer</li> <li>quality</li> <li>response</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>class QualityScorer(Task):\n    \"\"\"Score responses based on their quality using an `LLM`.\n\n    `QualityScorer` is a pre-defined task that defines the `instruction` as the input\n    and `score` as the output. This task is used to rate the quality of instructions and responses.\n    It's an implementation of the quality score task from the paper 'What Makes Good Data\n    for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.\n    The task follows the same scheme as the Complexity Scorer, but the instruction-response pairs\n    are scored in terms of quality, obtaining a quality score for each instruction.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the `responses`.\n        - responses (`List[str]`): The responses to be scored. Each response forms a pair with the instruction.\n\n    Output columns:\n        - scores (`List[float]`): The score for each instruction.\n        - model_name (`str`): The model name used to generate the scores.\n\n    Categories:\n        - scorer\n        - quality\n        - response\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"quality-scorer.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are `instruction` and `responses`.\"\"\"\n        return [\"instruction\", \"responses\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], responses=input[\"responses\"]\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self):\n        \"\"\"The output for the task is a list of `scores` containing the quality score for each\n        response in `responses`.\"\"\"\n        return [\"scores\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the score of each instruction-response pair.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the key `scores` containing the scores for each instruction-response pair.\n        \"\"\"\n        if output is None:\n            return {\"scores\": [None] * len(input[\"responses\"])}\n\n        scores = []\n        score_lines = output.split(\"\\n\")\n\n        for i, line in enumerate(score_lines):\n            match = _PARSE_SCORE_LINE_REGEX.match(line)\n            score = float(match.group(1)) if match else None\n            scores.append(score)\n            if i == len(input[\"responses\"]) - 1:\n                break\n        return {\"scores\": scores}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.QualityScorer.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are <code>instruction</code> and <code>responses</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.QualityScorer.outputs","title":"<code>outputs</code>  <code>property</code>","text":"<p>The output for the task is a list of <code>scores</code> containing the quality score for each response in <code>responses</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.QualityScorer.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], responses=input[\"responses\"]\n            ),\n        }\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.QualityScorer.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a list with the score of each instruction-response pair.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the key <code>scores</code> containing the scores for each instruction-response pair.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the score of each instruction-response pair.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the key `scores` containing the scores for each instruction-response pair.\n    \"\"\"\n    if output is None:\n        return {\"scores\": [None] * len(input[\"responses\"])}\n\n    scores = []\n    score_lines = output.split(\"\\n\")\n\n    for i, line in enumerate(score_lines):\n        match = _PARSE_SCORE_LINE_REGEX.match(line)\n        score = float(match.group(1)) if match else None\n        scores.append(score)\n        if i == len(input[\"responses\"]) - 1:\n            break\n    return {\"scores\": scores}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.QualityScorer.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"quality-scorer.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.SelfInstruct","title":"<code>SelfInstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate instructions based on a given input using an <code>LLM</code>.</p> <p><code>SelfInstruct</code> is a pre-defined task that, given a number of instructions, a certain criteria for query generations, an application description, and an input, generates a number of instruction related to the given input and following what is stated in the criteria for query generation and the application description. It is based in the SelfInstruct framework from the paper \"Self-Instruct: Aligning Language Models with Self-Generated Instructions\".</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <code>int</code> <p>The number of instructions to be generated. Defaults to 5.</p> <code>criteria_for_query_generation</code> <code>str</code> <p>The criteria for the query generation. Defaults to the criteria defined within the paper.</p> <code>application_description</code> <code>str</code> <p>The description of the AI application that one want to build with these instructions. Defaults to <code>AI assistant</code>.</p> Input columns <ul> <li>input (<code>str</code>): The input to generate the instructions. It's also called seed in     the paper.</li> </ul> Output columns <ul> <li>instructions (<code>List[str]</code>): The generated instructions.</li> <li>model_name (<code>str</code>): The model name used to generate the instructions.</li> </ul> Categories <ul> <li>text-generation</li> </ul> Reference <ul> <li><code>Self-Instruct: Aligning Language Models with Self-Generated Instructions</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>class SelfInstruct(Task):\n    \"\"\"Generate instructions based on a given input using an `LLM`.\n\n    `SelfInstruct` is a pre-defined task that, given a number of instructions, a\n    certain criteria for query generations, an application description, and an input,\n    generates a number of instruction related to the given input and following what\n    is stated in the criteria for query generation and the application description.\n    It is based in the SelfInstruct framework from the paper \"Self-Instruct: Aligning\n    Language Models with Self-Generated Instructions\".\n\n    Attributes:\n        num_instructions: The number of instructions to be generated. Defaults to 5.\n        criteria_for_query_generation: The criteria for the query generation. Defaults\n            to the criteria defined within the paper.\n        application_description: The description of the AI application that one want\n            to build with these instructions. Defaults to `AI assistant`.\n\n    Input columns:\n        - input (`str`): The input to generate the instructions. It's also called seed in\n            the paper.\n\n    Output columns:\n        - instructions (`List[str]`): The generated instructions.\n        - model_name (`str`): The model name used to generate the instructions.\n\n    Categories:\n        - text-generation\n\n    Reference:\n        - [`Self-Instruct: Aligning Language Models with Self-Generated Instructions`](https://arxiv.org/abs/2212.10560)\n    \"\"\"\n\n    num_instructions: int = 5\n    criteria_for_query_generation: str = (\n        \"Incorporate a diverse range of verbs, avoiding repetition.\\n\"\n        \"Ensure queries are compatible with AI model's text generation functions and are limited to 1-2 sentences.\\n\"\n        \"Design queries to be self-contained and standalone.\\n\"\n        'Blend interrogative (e.g., \"What is the significance of x?\") and imperative (e.g., \"Detail the process of x.\") styles.'\n    )\n    application_description: str = \"AI assistant\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"self-instruct.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `input` i.e. seed text.\"\"\"\n        return [\"input\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(\n                    input=input[\"input\"],\n                    application_description=self.application_description,\n                    criteria_for_query_generation=self.criteria_for_query_generation,\n                    num_instructions=self.num_instructions,\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self):\n        \"\"\"The output for the task is a list of `instructions` containing the generated instructions.\"\"\"\n        return [\"instructions\", \"model_name\"]\n\n    def format_output(\n        self,\n        output: Union[str, None],\n        input: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the generated instructions.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with containing the generated instructions.\n        \"\"\"\n        if output is None:\n            return {\"instructions\": []}\n        return {\"instructions\": [line for line in output.split(\"\\n\") if line != \"\"]}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.SelfInstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>input</code> i.e. seed text.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.SelfInstruct.outputs","title":"<code>outputs</code>  <code>property</code>","text":"<p>The output for the task is a list of <code>instructions</code> containing the generated instructions.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.SelfInstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(\n                input=input[\"input\"],\n                application_description=self.application_description,\n                criteria_for_query_generation=self.criteria_for_query_generation,\n                num_instructions=self.num_instructions,\n            ),\n        }\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.SelfInstruct.format_output","title":"<code>format_output(output, input=None)</code>","text":"<p>The output is formatted as a list with the generated instructions.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Optional[Dict[str, Any]]</code> <p>the input to the task. Used for obtaining the number of responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with containing the generated instructions.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def format_output(\n    self,\n    output: Union[str, None],\n    input: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the generated instructions.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with containing the generated instructions.\n    \"\"\"\n    if output is None:\n        return {\"instructions\": []}\n    return {\"instructions\": [line for line in output.split(\"\\n\") if line != \"\"]}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.SelfInstruct.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"self-instruct.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.StructuredGeneration","title":"<code>StructuredGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate structured content for a given <code>instruction</code> using an <code>LLM</code>.</p> <p><code>StructuredGeneration</code> is a pre-defined task that defines the <code>instruction</code> and the <code>grammar</code> as the inputs, and <code>generation</code> as the output. This task is used to generate structured content based on the input instruction and following the schema provided within the <code>grammar</code> column per each <code>instruction</code>. The <code>model_name</code> also returned as part of the output in order to enhance it.</p> <p>Attributes:</p> Name Type Description <code>use_system_prompt</code> <code>bool</code> <p>Whether to use the system prompt in the generation. Defaults to <code>True</code>, which means that if the column <code>system_prompt</code> is  defined within the input batch, then the <code>system_prompt</code> will be used, otherwise, it will be ignored.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to generate structured content from.</li> <li>grammar (<code>Dict[str, Any]</code>): The grammar to generate structured content from. It should be a     Python dictionary with the keys <code>type</code> and <code>value</code>, where <code>type</code> should be one of <code>json</code> or     <code>regex</code>, and the <code>value</code> should be either the JSON schema or the regex pattern, respectively.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text matching the provided schema, if possible.</li> <li>model_name (<code>str</code>): The name of the model used to generate the text.</li> </ul> Categories <ul> <li>outlines</li> <li>structured-generation</li> </ul> <p>Examples:</p> <pre><code>from distilabel.steps.tasks import StructuredGeneration\n\ntask = StructuredGeneration(llm=LLM(...))\n</code></pre> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>class StructuredGeneration(Task):\n    \"\"\"Generate structured content for a given `instruction` using an `LLM`.\n\n    `StructuredGeneration` is a pre-defined task that defines the `instruction` and the `grammar`\n    as the inputs, and `generation` as the output. This task is used to generate structured content based on\n    the input instruction and following the schema provided within the `grammar` column per each\n    `instruction`. The `model_name` also returned as part of the output in order to enhance it.\n\n    Attributes:\n        use_system_prompt: Whether to use the system prompt in the generation. Defaults to `True`,\n            which means that if the column `system_prompt` is  defined within the input batch, then\n            the `system_prompt` will be used, otherwise, it will be ignored.\n\n    Input columns:\n        - instruction (`str`): The instruction to generate structured content from.\n        - grammar (`Dict[str, Any]`): The grammar to generate structured content from. It should be a\n            Python dictionary with the keys `type` and `value`, where `type` should be one of `json` or\n            `regex`, and the `value` should be either the JSON schema or the regex pattern, respectively.\n\n    Output columns:\n        - generation (`str`): The generated text matching the provided schema, if possible.\n        - model_name (`str`): The name of the model used to generate the text.\n\n    Categories:\n        - outlines\n        - structured-generation\n\n    Examples:\n        ```python\n        from distilabel.steps.tasks import StructuredGeneration\n\n        task = StructuredGeneration(llm=LLM(...))\n        ```\n    \"\"\"\n\n    use_system_prompt: bool = False\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `instruction` and the `grammar`.\n        Optionally, if the `use_system_prompt` flag is set to True, then the\n        `system_prompt` will be used too.\"\"\"\n        columns = [\"instruction\", \"grammar\"]\n        if self.use_system_prompt:\n            columns = [\"system_prompt\"] + columns\n        return columns\n\n    def format_input(self, input: Dict[str, Any]) -&gt; StructuredInput:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        if not isinstance(input[\"instruction\"], str):\n            raise ValueError(\n                f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n            )\n\n        messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n        if self.use_system_prompt:\n            if \"system_prompt\" in input:\n                messages.insert(\n                    0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n                )\n            else:\n                warnings.warn(\n                    \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n        return (messages, input.get(\"grammar\", None))  # type: ignore\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`. Note that even\n        if the `grammar` is defined to produce a JSON schema, this method will return the raw\n        output i.e. a string without any parsing.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.StructuredGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>instruction</code> and the <code>grammar</code>. Optionally, if the <code>use_system_prompt</code> flag is set to True, then the <code>system_prompt</code> will be used too.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.StructuredGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.StructuredGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; StructuredInput:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    if not isinstance(input[\"instruction\"], str):\n        raise ValueError(\n            f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n        )\n\n    messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n    if self.use_system_prompt:\n        if \"system_prompt\" in input:\n            messages.insert(\n                0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n            )\n        else:\n            warnings.warn(\n                \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    return (messages, input.get(\"grammar\", None))  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.StructuredGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>. Note that even if the <code>grammar</code> is defined to produce a JSON schema, this method will return the raw output i.e. a string without any parsing.</p> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`. Note that even\n    if the `grammar` is defined to produce a JSON schema, this method will return the raw\n    output i.e. a string without any parsing.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>_Task</code>, <code>Step</code></p> <p>Task is a class that implements the <code>_Task</code> abstract class and adds the <code>Step</code> interface to be used as a step in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>the <code>LLM</code> to be used to generate the outputs of the task.</p> <code>group_generations</code> <p>whether to group the <code>num_generations</code> generated per input in a list or create a row per generation. Defaults to <code>False</code>.</p> <code>num_generations</code> <p>The number of generations to be produced per input.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>class Task(_Task, Step):\n    \"\"\"Task is a class that implements the `_Task` abstract class and adds the `Step`\n    interface to be used as a step in the pipeline.\n\n    Attributes:\n        llm: the `LLM` to be used to generate the outputs of the task.\n        group_generations: whether to group the `num_generations` generated per input in\n            a list or create a row per generation. Defaults to `False`.\n        num_generations: The number of generations to be produced per input.\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, input: Dict[str, Any]) -&gt; \"FormattedInput\":\n        \"\"\"Abstract method to format the inputs of the task. It needs to receive an input\n        as a Python dictionary, and generates an OpenAI chat-like list of dicts.\"\"\"\n        pass\n\n    def _format_inputs(self, inputs: List[Dict[str, Any]]) -&gt; List[\"FormattedInput\"]:\n        \"\"\"Formats the inputs of the task using the `format_input` method.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list containing the formatted inputs, which are `ChatType`-like following\n            the OpenAI formatting.\n        \"\"\"\n        return [self.format_input(input) for input in inputs]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        formatted_inputs = self._format_inputs(inputs)\n        outputs = self.llm.generate(\n            inputs=formatted_inputs,\n            num_generations=self.num_generations,  # type: ignore\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n\n        task_outputs = []\n        for input, input_outputs in zip(inputs, outputs):\n            formatted_outputs = self._format_outputs(input_outputs, inputs)\n\n            if self.group_generations:\n                combined = combine_dicts(*formatted_outputs)\n                task_outputs.append(\n                    {**input, \"model_name\": self.llm.model_name, **combined}\n                )\n                continue\n\n            # Create a row per generation\n            for formatted_output in formatted_outputs:\n                task_outputs.append(\n                    {**input, \"model_name\": self.llm.model_name, **formatted_output}\n                )\n\n        yield task_outputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Task.format_input","title":"<code>format_input(input)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to format the inputs of the task. It needs to receive an input as a Python dictionary, and generates an OpenAI chat-like list of dicts.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, input: Dict[str, Any]) -&gt; \"FormattedInput\":\n    \"\"\"Abstract method to format the inputs of the task. It needs to receive an input\n    as a Python dictionary, and generates an OpenAI chat-like list of dicts.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.Task.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    formatted_inputs = self._format_inputs(inputs)\n    outputs = self.llm.generate(\n        inputs=formatted_inputs,\n        num_generations=self.num_generations,  # type: ignore\n        **self.llm.generation_kwargs,  # type: ignore\n    )\n\n    task_outputs = []\n    for input, input_outputs in zip(inputs, outputs):\n        formatted_outputs = self._format_outputs(input_outputs, inputs)\n\n        if self.group_generations:\n            combined = combine_dicts(*formatted_outputs)\n            task_outputs.append(\n                {**input, \"model_name\": self.llm.model_name, **combined}\n            )\n            continue\n\n        # Create a row per generation\n        for formatted_output in formatted_outputs:\n            task_outputs.append(\n                {**input, \"model_name\": self.llm.model_name, **formatted_output}\n            )\n\n    yield task_outputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.TextGeneration","title":"<code>TextGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Simple text generation with an <code>LLM</code> given an instruction.</p> <p><code>TextGeneration</code> is a pre-defined task that defines the <code>instruction</code> as the input and <code>generation</code> as the output. This task is used to generate text based on the input instruction. The model_name is also returned as part of the output in order to enhance it.</p> <p>Attributes:</p> Name Type Description <code>use_system_prompt</code> <code>bool</code> <p>Whether to use the system prompt in the generation. Defaults to <code>True</code>, which means that if the column <code>system_prompt</code> is defined within the input batch, then the <code>system_prompt</code> will be used, otherwise, it will be ignored.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to generate text from.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text.</li> <li>model_name (<code>str</code>): The name of the model used to generate the text.</li> </ul> Categories <ul> <li>text-generation</li> </ul> <p>Examples:</p> <pre><code>from distilabel.steps.tasks import TextGeneration\n\ntask = TextGeneration(llm=LLM(...))\n</code></pre> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>class TextGeneration(Task):\n    \"\"\"Simple text generation with an `LLM` given an instruction.\n\n    `TextGeneration` is a pre-defined task that defines the `instruction` as the input\n    and `generation` as the output. This task is used to generate text based on the input\n    instruction. The model_name is also returned as part of the output in order to enhance it.\n\n    Attributes:\n        use_system_prompt: Whether to use the system prompt in the generation. Defaults to `True`,\n            which means that if the column `system_prompt` is defined within the input batch, then\n            the `system_prompt` will be used, otherwise, it will be ignored.\n\n    Input columns:\n        - instruction (`str`): The instruction to generate text from.\n\n    Output columns:\n        - generation (`str`): The generated text.\n        - model_name (`str`): The name of the model used to generate the text.\n\n    Categories:\n        - text-generation\n\n    Examples:\n        ```python\n        from distilabel.steps.tasks import TextGeneration\n\n        task = TextGeneration(llm=LLM(...))\n        ```\n    \"\"\"\n\n    use_system_prompt: bool = True\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`.\"\"\"\n        return [\"instruction\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n\n        if is_openai_format(input[\"instruction\"]):\n            raise ValueError(\n                \"Providing `instruction` formatted as an OpenAI chat / conversation is\"\n                \" deprecated, you should use `ChatGeneration` with `messages` as input instead.\",\n            )\n\n        if not isinstance(input[\"instruction\"], str):\n            raise ValueError(\n                f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n            )\n\n        messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n        if self.use_system_prompt:\n            if \"system_prompt\" in input:\n                messages.insert(\n                    0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n                )\n            else:\n                warnings.warn(\n                    \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        return messages  # type: ignore\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.TextGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.TextGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.TextGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n\n    if is_openai_format(input[\"instruction\"]):\n        raise ValueError(\n            \"Providing `instruction` formatted as an OpenAI chat / conversation is\"\n            \" deprecated, you should use `ChatGeneration` with `messages` as input instead.\",\n        )\n\n    if not isinstance(input[\"instruction\"], str):\n        raise ValueError(\n            f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n        )\n\n    messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n    if self.use_system_prompt:\n        if \"system_prompt\" in input:\n            messages.insert(\n                0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n            )\n        else:\n            warnings.warn(\n                \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    return messages  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.TextGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.UltraFeedback","title":"<code>UltraFeedback</code>","text":"<p>               Bases: <code>Task</code></p> <p>Rank generations focusing on different aspects using an <code>LLM</code>.</p> <p>UltraFeedback: Boosting Language Models with High-quality Feedback.</p> <p>Attributes:</p> Name Type Description <code>aspect</code> <code>Literal['helpfulness', 'honesty', 'instruction-following', 'truthfulness', 'overall-rating']</code> <p>The aspect to perform with the <code>UltraFeedback</code> model. The available aspects are: - <code>helpfulness</code>: Evaluate text outputs based on helpfulness. - <code>honesty</code>: Evaluate text outputs based on honesty. - <code>instruction-following</code>: Evaluate text outputs based on given instructions. - <code>truthfulness</code>: Evaluate text outputs based on truthfulness. Additionally, a custom aspect has been defined by Argilla, so as to evaluate the overall assessment of the text outputs within a single prompt. The custom aspect is: - <code>overall-rating</code>: Evaluate text outputs based on an overall assessment.</p> Input columns <ul> <li>instruction (<code>str</code>): The reference instruction to evaluate the text outputs.</li> <li>generations (<code>List[str]</code>): The text outputs to evaluate for the given instruction.</li> </ul> Output columns <ul> <li>ratings (<code>List[float]</code>): The ratings for each of the provided text outputs.</li> <li>rationales (<code>List[str]</code>): The rationales for each of the provided text outputs.</li> <li>model_name (<code>str</code>): The name of the model used to generate the ratings and rationales.</li> </ul> Categories <ul> <li>preference</li> </ul> References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>UltraFeedback - GitHub Repository</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>class UltraFeedback(Task):\n    \"\"\"Rank generations focusing on different aspects using an `LLM`.\n\n    UltraFeedback: Boosting Language Models with High-quality Feedback.\n\n    Attributes:\n        aspect: The aspect to perform with the `UltraFeedback` model. The available aspects are:\n            - `helpfulness`: Evaluate text outputs based on helpfulness.\n            - `honesty`: Evaluate text outputs based on honesty.\n            - `instruction-following`: Evaluate text outputs based on given instructions.\n            - `truthfulness`: Evaluate text outputs based on truthfulness.\n            Additionally, a custom aspect has been defined by Argilla, so as to evaluate the overall\n            assessment of the text outputs within a single prompt. The custom aspect is:\n            - `overall-rating`: Evaluate text outputs based on an overall assessment.\n\n    Input columns:\n        - instruction (`str`): The reference instruction to evaluate the text outputs.\n        - generations (`List[str]`): The text outputs to evaluate for the given instruction.\n\n    Output columns:\n        - ratings (`List[float]`): The ratings for each of the provided text outputs.\n        - rationales (`List[str]`): The rationales for each of the provided text outputs.\n        - model_name (`str`): The name of the model used to generate the ratings and rationales.\n\n    Categories:\n        - preference\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`UltraFeedback - GitHub Repository`](https://github.com/OpenBMB/UltraFeedback)\n    \"\"\"\n\n    aspect: Literal[\n        \"helpfulness\",\n        \"honesty\",\n        \"instruction-following\",\n        \"truthfulness\",\n        # Custom aspects\n        \"overall-rating\",\n    ]\n\n    _system_prompt: str = PrivateAttr(\n        default=(\n            \"Your role is to evaluate text quality based on given criteria.\\n\"\n            'You\\'ll receive an instructional description (\"Instruction\") and {no_texts} text outputs (\"Text\").\\n'\n            \"Understand and interpret instructions to evaluate effectively.\\n\"\n            \"Provide annotations for each text with a rating and rationale.\\n\"\n            \"The {no_texts} texts given are independent, and should be evaluated separately.\\n\"\n        )\n    )\n    _template: Optional[\"Template\"] = PrivateAttr(default=...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template for the given `aspect`.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"ultrafeedback\"\n            / f\"{self.aspect}.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`, and the `generations` for it.\"\"\"\n        return [\"instruction\", \"generations\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": self._system_prompt.format(\n                    no_texts=len(input[\"generations\"])\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], generations=input[\"generations\"]\n                ),\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        columns = []\n        if self.aspect in [\"honesty\", \"instruction-following\", \"overall-rating\"]:\n            columns = [\"ratings\", \"rationales\"]\n        elif self.aspect in [\"helpfulness\", \"truthfulness\"]:\n            columns = [\"types\", \"rationales\", \"ratings\", \"rationales-for-ratings\"]\n        return columns + [\"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `ratings` and `rationales` for\n        each of the provided `generations` for the given `instruction`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\n\n        Args:\n            output: a string representing the output of the LLM via the `process` method.\n            input: the input to the task, as required by some tasks to format the output.\n\n        Returns:\n            A dictionary containing either the `ratings` and `rationales` for each of the provided\n            `generations` for the given `instruction` if the provided aspect is either `honesty`,\n            `instruction-following`, or `overall-rating`; or the `types`, `rationales`,\n            `ratings`, and `rationales-for-ratings` for each of the provided `generations` for the\n            given `instruction` if the provided aspect is either `helpfulness` or `truthfulness`.\n        \"\"\"\n        if self.aspect in [\n            \"honesty\",\n            \"instruction-following\",\n            \"overall-rating\",\n        ]:\n            return self._format_ratings_rationales_output(output, input)\n        return self._format_types_ratings_rationales_output(output, input)\n\n    def _format_ratings_rationales_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, List[Any]]:\n        \"\"\"Formats the output when the aspect is either `honesty`, `instruction-following`, or `overall-rating`.\"\"\"\n        if output is None:\n            return {\n                \"ratings\": [None] * len(input[\"generations\"]),\n                \"rationales\": [None] * len(input[\"generations\"]),\n            }\n\n        pattern = r\"Rating: (.+?)\\nRationale: (.+)\"\n        sections = output.split(\"\\n\\n\")\n\n        formatted_outputs = []\n        for section in sections:\n            matches = None\n            if section is not None and section != \"\":\n                matches = re.search(pattern, section, re.DOTALL)\n            if not matches:\n                formatted_outputs.append({\"ratings\": None, \"rationales\": None})\n                continue\n\n            formatted_outputs.append(\n                {\n                    \"ratings\": int(re.findall(r\"\\b\\d+\\b\", matches.group(1))[0])\n                    if matches.group(1) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales\": matches.group(2),\n                }\n            )\n        return combine_dicts(*formatted_outputs)\n\n    def _format_types_ratings_rationales_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, List[Any]]:\n        \"\"\"Formats the output when the aspect is either `helpfulness` or `truthfulness`.\"\"\"\n        if output is None:\n            return {\n                \"types\": [None] * len(input[\"generations\"]),\n                \"rationales\": [None] * len(input[\"generations\"]),\n                \"ratings\": [None] * len(input[\"generations\"]),\n                \"rationales-for-ratings\": [None] * len(input[\"generations\"]),\n            }\n\n        pattern = r\"Type: (.+?)\\nRationale: (.+?)\\nRating: (.+?)\\nRationale: (.+)\"\n\n        sections = output.split(\"\\n\\n\")\n\n        formatted_outputs = []\n        for section in sections:\n            matches = None\n            if section is not None and section != \"\":\n                matches = re.search(pattern, section, re.DOTALL)\n            if not matches:\n                formatted_outputs.append(\n                    {\n                        \"types\": None,\n                        \"rationales\": None,\n                        \"ratings\": None,\n                        \"rationales-for-ratings\": None,\n                    }\n                )\n                continue\n\n            formatted_outputs.append(\n                {\n                    \"types\": int(re.findall(r\"\\b\\d+\\b\", matches.group(1))[0])\n                    if matches.group(1) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales\": matches.group(2),\n                    \"ratings\": int(re.findall(r\"\\b\\d+\\b\", matches.group(3))[0])\n                    if matches.group(3) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales-for-ratings\": matches.group(4),\n                }\n            )\n        return combine_dicts(*formatted_outputs)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.UltraFeedback.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>, and the <code>generations</code> for it.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.UltraFeedback.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.UltraFeedback.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": self._system_prompt.format(\n                no_texts=len(input[\"generations\"])\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], generations=input[\"generations\"]\n            ),\n        },\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.UltraFeedback.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>ratings</code> and <code>rationales</code> for each of the provided <code>generations</code> for the given <code>instruction</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>a string representing the output of the LLM via the <code>process</code> method.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task, as required by some tasks to format the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing either the <code>ratings</code> and <code>rationales</code> for each of the provided</p> <code>Dict[str, Any]</code> <p><code>generations</code> for the given <code>instruction</code> if the provided aspect is either <code>honesty</code>,</p> <code>Dict[str, Any]</code> <p><code>instruction-following</code>, or <code>overall-rating</code>; or the <code>types</code>, <code>rationales</code>,</p> <code>Dict[str, Any]</code> <p><code>ratings</code>, and <code>rationales-for-ratings</code> for each of the provided <code>generations</code> for the</p> <code>Dict[str, Any]</code> <p>given <code>instruction</code> if the provided aspect is either <code>helpfulness</code> or <code>truthfulness</code>.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `ratings` and `rationales` for\n    each of the provided `generations` for the given `instruction`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\n\n    Args:\n        output: a string representing the output of the LLM via the `process` method.\n        input: the input to the task, as required by some tasks to format the output.\n\n    Returns:\n        A dictionary containing either the `ratings` and `rationales` for each of the provided\n        `generations` for the given `instruction` if the provided aspect is either `honesty`,\n        `instruction-following`, or `overall-rating`; or the `types`, `rationales`,\n        `ratings`, and `rationales-for-ratings` for each of the provided `generations` for the\n        given `instruction` if the provided aspect is either `helpfulness` or `truthfulness`.\n    \"\"\"\n    if self.aspect in [\n        \"honesty\",\n        \"instruction-following\",\n        \"overall-rating\",\n    ]:\n        return self._format_ratings_rationales_output(output, input)\n    return self._format_types_ratings_rationales_output(output, input)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/#distilabel.steps.tasks.UltraFeedback.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template for the given <code>aspect</code>.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template for the given `aspect`.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"ultrafeedback\"\n        / f\"{self.aspect}.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/base/","title":"Base","text":""},{"location":"reference/distilabel/steps/tasks/base/#distilabel.steps.tasks.base.GeneratorTask","title":"<code>GeneratorTask</code>","text":"<p>               Bases: <code>_Task</code>, <code>GeneratorStep</code></p> <p>GeneratorTask is a class that implements the <code>_Task</code> abstract class and adds the <code>GeneratorStep</code> interface to be used as a step in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>the <code>LLM</code> to be used to generate the outputs of the task.</p> <code>group_generations</code> <p>whether to group the <code>num_generations</code> generated per input in a list or create a row per generation. Defaults to <code>False</code>.</p> <code>num_generations</code> <p>The number of generations to be produced per input.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>class GeneratorTask(_Task, GeneratorStep):\n    \"\"\"GeneratorTask is a class that implements the `_Task` abstract class and adds the\n    `GeneratorStep` interface to be used as a step in the pipeline.\n\n    Attributes:\n        llm: the `LLM` to be used to generate the outputs of the task.\n        group_generations: whether to group the `num_generations` generated per input in\n            a list or create a row per generation. Defaults to `False`.\n        num_generations: The number of generations to be produced per input.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/base/#distilabel.steps.tasks.base.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>_Task</code>, <code>Step</code></p> <p>Task is a class that implements the <code>_Task</code> abstract class and adds the <code>Step</code> interface to be used as a step in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>the <code>LLM</code> to be used to generate the outputs of the task.</p> <code>group_generations</code> <p>whether to group the <code>num_generations</code> generated per input in a list or create a row per generation. Defaults to <code>False</code>.</p> <code>num_generations</code> <p>The number of generations to be produced per input.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>class Task(_Task, Step):\n    \"\"\"Task is a class that implements the `_Task` abstract class and adds the `Step`\n    interface to be used as a step in the pipeline.\n\n    Attributes:\n        llm: the `LLM` to be used to generate the outputs of the task.\n        group_generations: whether to group the `num_generations` generated per input in\n            a list or create a row per generation. Defaults to `False`.\n        num_generations: The number of generations to be produced per input.\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, input: Dict[str, Any]) -&gt; \"FormattedInput\":\n        \"\"\"Abstract method to format the inputs of the task. It needs to receive an input\n        as a Python dictionary, and generates an OpenAI chat-like list of dicts.\"\"\"\n        pass\n\n    def _format_inputs(self, inputs: List[Dict[str, Any]]) -&gt; List[\"FormattedInput\"]:\n        \"\"\"Formats the inputs of the task using the `format_input` method.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list containing the formatted inputs, which are `ChatType`-like following\n            the OpenAI formatting.\n        \"\"\"\n        return [self.format_input(input) for input in inputs]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        formatted_inputs = self._format_inputs(inputs)\n        outputs = self.llm.generate(\n            inputs=formatted_inputs,\n            num_generations=self.num_generations,  # type: ignore\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n\n        task_outputs = []\n        for input, input_outputs in zip(inputs, outputs):\n            formatted_outputs = self._format_outputs(input_outputs, inputs)\n\n            if self.group_generations:\n                combined = combine_dicts(*formatted_outputs)\n                task_outputs.append(\n                    {**input, \"model_name\": self.llm.model_name, **combined}\n                )\n                continue\n\n            # Create a row per generation\n            for formatted_output in formatted_outputs:\n                task_outputs.append(\n                    {**input, \"model_name\": self.llm.model_name, **formatted_output}\n                )\n\n        yield task_outputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/base/#distilabel.steps.tasks.base.Task.format_input","title":"<code>format_input(input)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to format the inputs of the task. It needs to receive an input as a Python dictionary, and generates an OpenAI chat-like list of dicts.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, input: Dict[str, Any]) -&gt; \"FormattedInput\":\n    \"\"\"Abstract method to format the inputs of the task. It needs to receive an input\n    as a Python dictionary, and generates an OpenAI chat-like list of dicts.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/base/#distilabel.steps.tasks.base.Task.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/base.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    formatted_inputs = self._format_inputs(inputs)\n    outputs = self.llm.generate(\n        inputs=formatted_inputs,\n        num_generations=self.num_generations,  # type: ignore\n        **self.llm.generation_kwargs,  # type: ignore\n    )\n\n    task_outputs = []\n    for input, input_outputs in zip(inputs, outputs):\n        formatted_outputs = self._format_outputs(input_outputs, inputs)\n\n        if self.group_generations:\n            combined = combine_dicts(*formatted_outputs)\n            task_outputs.append(\n                {**input, \"model_name\": self.llm.model_name, **combined}\n            )\n            continue\n\n        # Create a row per generation\n        for formatted_output in formatted_outputs:\n            task_outputs.append(\n                {**input, \"model_name\": self.llm.model_name, **formatted_output}\n            )\n\n    yield task_outputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/complexity_scorer/","title":"Complexity scorer","text":""},{"location":"reference/distilabel/steps/tasks/complexity_scorer/#distilabel.steps.tasks.complexity_scorer.ComplexityScorer","title":"<code>ComplexityScorer</code>","text":"<p>               Bases: <code>Task</code></p> <p>Score instructions based on their complexity using an <code>LLM</code>.</p> <p><code>ComplexityScorer</code> is a pre-defined task used to rank a list of instructions based in their complexity. It's an implementation of the complexity score task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instructions (<code>List[str]</code>): The list of instructions to be scored.</li> </ul> Output columns <ul> <li>scores (<code>List[float]</code>): The score for each instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the scores.</li> </ul> Categories <ul> <li>scorer</li> <li>complexity</li> <li>instruction</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>class ComplexityScorer(Task):\n    \"\"\"Score instructions based on their complexity using an `LLM`.\n\n    `ComplexityScorer` is a pre-defined task used to rank a list of instructions based in\n    their complexity. It's an implementation of the complexity score task from the paper\n    'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection\n    in Instruction Tuning'.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instructions (`List[str]`): The list of instructions to be scored.\n\n    Output columns:\n        - scores (`List[float]`): The score for each instruction.\n        - model_name (`str`): The model name used to generate the scores.\n\n    Categories:\n        - scorer\n        - complexity\n        - instruction\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"complexity-scorer.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the `instructions`.\"\"\"\n        return [\"instructions\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(instructions=input[\"instructions\"]),  # type: ignore\n            }\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are: a list of `scores` containing the complexity score for each\n        instruction in `instructions`, and the `model_name`.\"\"\"\n        return [\"scores\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the score of each instruction.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the key `scores` containing the scores for each instruction.\n        \"\"\"\n        if output is None:\n            return {\"scores\": [None] * len(input[\"instructions\"])}\n\n        scores = []\n        score_lines = output.split(\"\\n\")\n        for i, line in enumerate(score_lines):\n            match = _PARSE_SCORE_LINE_REGEX.match(line)\n            score = float(match.group(1)) if match else None\n            scores.append(score)\n            if i == len(input[\"instructions\"]) - 1:\n                break\n        return {\"scores\": scores}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/complexity_scorer/#distilabel.steps.tasks.complexity_scorer.ComplexityScorer.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the <code>instructions</code>.</p>"},{"location":"reference/distilabel/steps/tasks/complexity_scorer/#distilabel.steps.tasks.complexity_scorer.ComplexityScorer.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are: a list of <code>scores</code> containing the complexity score for each instruction in <code>instructions</code>, and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/complexity_scorer/#distilabel.steps.tasks.complexity_scorer.ComplexityScorer.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(instructions=input[\"instructions\"]),  # type: ignore\n        }\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/complexity_scorer/#distilabel.steps.tasks.complexity_scorer.ComplexityScorer.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a list with the score of each instruction.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the key <code>scores</code> containing the scores for each instruction.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the score of each instruction.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the key `scores` containing the scores for each instruction.\n    \"\"\"\n    if output is None:\n        return {\"scores\": [None] * len(input[\"instructions\"])}\n\n    scores = []\n    score_lines = output.split(\"\\n\")\n    for i, line in enumerate(score_lines):\n        match = _PARSE_SCORE_LINE_REGEX.match(line)\n        score = float(match.group(1)) if match else None\n        scores.append(score)\n        if i == len(input[\"instructions\"]) - 1:\n            break\n    return {\"scores\": scores}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/complexity_scorer/#distilabel.steps.tasks.complexity_scorer.ComplexityScorer.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/complexity_scorer.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"complexity-scorer.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/generate_embeddings/","title":"Generate embeddings","text":""},{"location":"reference/distilabel/steps/tasks/generate_embeddings/#distilabel.steps.tasks.generate_embeddings.GenerateEmbeddings","title":"<code>GenerateEmbeddings</code>","text":"<p>               Bases: <code>Step</code></p> <p>Generate embeddings using the last hidden state of an <code>LLM</code>.</p> <p>Generate embeddings for a text input using the last hidden state of an <code>LLM</code>, as described in the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>The <code>LLM</code> to use to generate the embeddings.</p> Input columns <ul> <li>text (<code>str</code>, <code>List[Dict[str, str]]</code>): The input text or conversation to generate     embeddings for.</li> </ul> Output columns <ul> <li>embedding (<code>List[float]</code>): The embedding of the input text or conversation.</li> <li>model_name (<code>str</code>): The model name used to generate the embeddings.</li> </ul> Categories <ul> <li>embedding</li> <li>llm</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> </ul> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>class GenerateEmbeddings(Step):\n    \"\"\"Generate embeddings using the last hidden state of an `LLM`.\n\n    Generate embeddings for a text input using the last hidden state of an `LLM`, as\n    described in the paper 'What Makes Good Data for Alignment? A Comprehensive Study of\n    Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        llm: The `LLM` to use to generate the embeddings.\n\n    Input columns:\n        - text (`str`, `List[Dict[str, str]]`): The input text or conversation to generate\n            embeddings for.\n\n    Output columns:\n        - embedding (`List[float]`): The embedding of the input text or conversation.\n        - model_name (`str`): The model name used to generate the embeddings.\n\n    Categories:\n        - embedding\n        - llm\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    llm: LLM\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the `LLM` used to generate the embeddings.\"\"\"\n        super().load()\n\n        self.llm.load()\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task is a `text` column containing either a string or a\n        list of dictionaries in OpenAI chat-like format.\"\"\"\n        return [\"text\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task is an `embedding` column containing the embedding of\n        the `text` input.\"\"\"\n        return [\"embedding\", \"model_name\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"Formats the input to be used by the LLM to generate the embeddings. The input\n        can be in `ChatType` format or a string. If a string, it will be converted to a\n        list of dictionaries in OpenAI chat-like format.\n\n        Args:\n            input: The input to format.\n\n        Returns:\n            The OpenAI chat-like format of the input.\n        \"\"\"\n        text = input[\"text\"] = input[\"text\"]\n\n        # input is in `ChatType` format\n        if isinstance(text, str):\n            return [{\"role\": \"user\", \"content\": text}]\n\n        if is_openai_format(text):\n            return text\n\n        raise ValueError(\n            f\"Couldn't format input for step {self.name}. The `text` input column has to\"\n            \" be a string or a list of dictionaries in OpenAI chat-like format.\"\n        )\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Generates an embedding for each input using the last hidden state of the `LLM`.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n        formatted_inputs = [self.format_input(input) for input in inputs]\n        last_hidden_states = self.llm.get_last_hidden_states(formatted_inputs)\n        for input, hidden_state in zip(inputs, last_hidden_states):\n            input[\"embedding\"] = hidden_state[-1].tolist()\n            input[\"model_name\"] = self.llm.model_name\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/generate_embeddings/#distilabel.steps.tasks.generate_embeddings.GenerateEmbeddings.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task is a <code>text</code> column containing either a string or a list of dictionaries in OpenAI chat-like format.</p>"},{"location":"reference/distilabel/steps/tasks/generate_embeddings/#distilabel.steps.tasks.generate_embeddings.GenerateEmbeddings.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task is an <code>embedding</code> column containing the embedding of the <code>text</code> input.</p>"},{"location":"reference/distilabel/steps/tasks/generate_embeddings/#distilabel.steps.tasks.generate_embeddings.GenerateEmbeddings.format_input","title":"<code>format_input(input)</code>","text":"<p>Formats the input to be used by the LLM to generate the embeddings. The input can be in <code>ChatType</code> format or a string. If a string, it will be converted to a list of dictionaries in OpenAI chat-like format.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, Any]</code> <p>The input to format.</p> required <p>Returns:</p> Type Description <code>ChatType</code> <p>The OpenAI chat-like format of the input.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"Formats the input to be used by the LLM to generate the embeddings. The input\n    can be in `ChatType` format or a string. If a string, it will be converted to a\n    list of dictionaries in OpenAI chat-like format.\n\n    Args:\n        input: The input to format.\n\n    Returns:\n        The OpenAI chat-like format of the input.\n    \"\"\"\n    text = input[\"text\"] = input[\"text\"]\n\n    # input is in `ChatType` format\n    if isinstance(text, str):\n        return [{\"role\": \"user\", \"content\": text}]\n\n    if is_openai_format(text):\n        return text\n\n    raise ValueError(\n        f\"Couldn't format input for step {self.name}. The `text` input column has to\"\n        \" be a string or a list of dictionaries in OpenAI chat-like format.\"\n    )\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/generate_embeddings/#distilabel.steps.tasks.generate_embeddings.GenerateEmbeddings.load","title":"<code>load()</code>","text":"<p>Loads the <code>LLM</code> used to generate the embeddings.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the `LLM` used to generate the embeddings.\"\"\"\n    super().load()\n\n    self.llm.load()\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/generate_embeddings/#distilabel.steps.tasks.generate_embeddings.GenerateEmbeddings.process","title":"<code>process(inputs)</code>","text":"<p>Generates an embedding for each input using the last hidden state of the <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/generate_embeddings.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Generates an embedding for each input using the last hidden state of the `LLM`.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n    formatted_inputs = [self.format_input(input) for input in inputs]\n    last_hidden_states = self.llm.get_last_hidden_states(formatted_inputs)\n    for input, hidden_state in zip(inputs, last_hidden_states):\n        input[\"embedding\"] = hidden_state[-1].tolist()\n        input[\"model_name\"] = self.llm.model_name\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/genstruct/","title":"Genstruct","text":""},{"location":"reference/distilabel/steps/tasks/genstruct/#distilabel.steps.tasks.genstruct.Genstruct","title":"<code>Genstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate a pair of instruction-response from a document using an <code>LLM</code>.</p> <p><code>Genstruct</code> is a pre-defined task designed to generate valid instructions from a given raw document, with the title and the content, enabling the creation of new, partially synthetic instruction finetuning datasets from any raw-text corpus. The task is based on the Genstruct 7B model by Nous Research, which is inspired in the Ada-Instruct paper.</p> Note <p>The Genstruct prompt i.e. the task, can be used with any model really, but the safest / recommended option is to use <code>NousResearch/Genstruct-7B</code> as the LLM provided to the task, since it was trained for this specific task.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>title (<code>str</code>): The title of the document.</li> <li>content (<code>str</code>): The content of the document.</li> </ul> Output columns <ul> <li>user (<code>str</code>): The user's instruction based on the document.</li> <li>assistant (<code>str</code>): The assistant's response based on the user's instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the <code>feedback</code> and <code>result</code>.</li> </ul> Categories <ul> <li>text-generation</li> <li>instruction</li> <li>response</li> </ul> References <ul> <li>Genstruct 7B by Nous Research</li> <li>Ada-Instruct: Adapting Instruction Generators for Complex Reasoning</li> </ul> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>class Genstruct(Task):\n    \"\"\"Generate a pair of instruction-response from a document using an `LLM`.\n\n    `Genstruct` is a pre-defined task designed to generate valid instructions from a given raw document,\n    with the title and the content, enabling the creation of new, partially synthetic instruction finetuning\n    datasets from any raw-text corpus. The task is based on the Genstruct 7B model by Nous Research, which is\n    inspired in the Ada-Instruct paper.\n\n    Note:\n        The Genstruct prompt i.e. the task, can be used with any model really, but the safest / recommended\n        option is to use `NousResearch/Genstruct-7B` as the LLM provided to the task, since it was trained\n        for this specific task.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - title (`str`): The title of the document.\n        - content (`str`): The content of the document.\n\n    Output columns:\n        - user (`str`): The user's instruction based on the document.\n        - assistant (`str`): The assistant's response based on the user's instruction.\n        - model_name (`str`): The model name used to generate the `feedback` and `result`.\n\n    Categories:\n        - text-generation\n        - instruction\n        - response\n\n    References:\n        - [Genstruct 7B by Nous Research](https://huggingface.co/NousResearch/Genstruct-7B)\n        - [Ada-Instruct: Adapting Instruction Generators for Complex Reasoning](https://arxiv.org/abs/2310.04484)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"genstruct.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are the `title` and the `content`.\"\"\"\n        return [\"title\", \"content\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    title=input[\"title\"], content=input[\"content\"]\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `user` instruction based on the provided document\n        and the `assistant` response based on the user's instruction.\"\"\"\n        return [\"user\", \"assistant\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted so that both the user and the assistant messages are\n        captured.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the keys `user` and `assistant` containing the content for each role.\n        \"\"\"\n        if output is None:\n            return {\"user\": None, \"assistant\": None}\n\n        matches = re.search(_PARSE_GENSTRUCT_OUTPUT_REGEX, output, re.DOTALL)\n        if not matches:\n            return {\"user\": None, \"assistant\": None}\n\n        return {\n            \"user\": matches.group(1).strip(),\n            \"assistant\": matches.group(2).strip(),\n        }\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/genstruct/#distilabel.steps.tasks.genstruct.Genstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are the <code>title</code> and the <code>content</code>.</p>"},{"location":"reference/distilabel/steps/tasks/genstruct/#distilabel.steps.tasks.genstruct.Genstruct.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>user</code> instruction based on the provided document and the <code>assistant</code> response based on the user's instruction.</p>"},{"location":"reference/distilabel/steps/tasks/genstruct/#distilabel.steps.tasks.genstruct.Genstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                title=input[\"title\"], content=input[\"content\"]\n            ),\n        }\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/genstruct/#distilabel.steps.tasks.genstruct.Genstruct.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted so that both the user and the assistant messages are captured.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the keys <code>user</code> and <code>assistant</code> containing the content for each role.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted so that both the user and the assistant messages are\n    captured.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the keys `user` and `assistant` containing the content for each role.\n    \"\"\"\n    if output is None:\n        return {\"user\": None, \"assistant\": None}\n\n    matches = re.search(_PARSE_GENSTRUCT_OUTPUT_REGEX, output, re.DOTALL)\n    if not matches:\n        return {\"user\": None, \"assistant\": None}\n\n    return {\n        \"user\": matches.group(1).strip(),\n        \"assistant\": matches.group(2).strip(),\n    }\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/genstruct/#distilabel.steps.tasks.genstruct.Genstruct.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/genstruct.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"genstruct.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/instruction_backtranslation/","title":"Instruction backtranslation","text":""},{"location":"reference/distilabel/steps/tasks/instruction_backtranslation/#distilabel.steps.tasks.instruction_backtranslation.InstructionBacktranslation","title":"<code>InstructionBacktranslation</code>","text":"<p>               Bases: <code>Task</code></p> <p>Self-Alignment with Instruction Backtranslation.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Optional[Template]</code> <p>the Jinja2 template to use for the Instruction Backtranslation task.</p> Input columns <ul> <li>instruction (<code>str</code>): The reference instruction to evaluate the text output.</li> <li>generation (<code>str</code>): The text output to evaluate for the given instruction.</li> </ul> Output columns <ul> <li>score (<code>str</code>): The score for the generation based on the given instruction.</li> <li>reason (<code>str</code>): The reason for the provided score.</li> <li>model_name (<code>str</code>): The model name used to score the generation.</li> </ul> Categories <ul> <li>critique</li> </ul> References <ul> <li><code>Self-Alignment with Instruction Backtranslation</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>class InstructionBacktranslation(Task):\n    \"\"\"Self-Alignment with Instruction Backtranslation.\n\n    Attributes:\n        _template: the Jinja2 template to use for the Instruction Backtranslation task.\n\n    Input columns:\n        - instruction (`str`): The reference instruction to evaluate the text output.\n        - generation (`str`): The text output to evaluate for the given instruction.\n\n    Output columns:\n        - score (`str`): The score for the generation based on the given instruction.\n        - reason (`str`): The reason for the provided score.\n        - model_name (`str`): The model name used to score the generation.\n\n    Categories:\n        - critique\n\n    References:\n        - [`Self-Alignment with Instruction Backtranslation`](https://arxiv.org/abs/2308.06259)\n    \"\"\"\n\n    _template: Optional[\"Template\"] = PrivateAttr(default=...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"instruction-backtranslation.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`, and the `generation` for it.\"\"\"\n        return [\"instruction\", \"generation\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], generation=input[\"generation\"]\n                ),\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `score`, `reason` and the `model_name`.\"\"\"\n        return [\"score\", \"reason\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `score` and `reason`. The\n        `model_name` will be automatically included within the `process` method of `Task`.\n\n        Args:\n            output: a string representing the output of the LLM via the `process` method.\n            input: the input to the task, as required by some tasks to format the output.\n\n        Returns:\n            A dictionary containing the `score` and the `reason` for the provided `score`.\n        \"\"\"\n        pattern = r\"(.+?)Score: (\\d)\"\n\n        matches = None\n        if output is not None:\n            matches = re.findall(pattern, output, re.DOTALL)\n        if matches is None:\n            return {\"score\": None, \"reason\": None}\n\n        return {\n            \"score\": int(matches[0][1]),\n            \"reason\": matches[0][0].strip(),\n        }\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/instruction_backtranslation/#distilabel.steps.tasks.instruction_backtranslation.InstructionBacktranslation.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>, and the <code>generation</code> for it.</p>"},{"location":"reference/distilabel/steps/tasks/instruction_backtranslation/#distilabel.steps.tasks.instruction_backtranslation.InstructionBacktranslation.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>score</code>, <code>reason</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/instruction_backtranslation/#distilabel.steps.tasks.instruction_backtranslation.InstructionBacktranslation.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], generation=input[\"generation\"]\n            ),\n        },\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/instruction_backtranslation/#distilabel.steps.tasks.instruction_backtranslation.InstructionBacktranslation.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>score</code> and <code>reason</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>a string representing the output of the LLM via the <code>process</code> method.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task, as required by some tasks to format the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the <code>score</code> and the <code>reason</code> for the provided <code>score</code>.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `score` and `reason`. The\n    `model_name` will be automatically included within the `process` method of `Task`.\n\n    Args:\n        output: a string representing the output of the LLM via the `process` method.\n        input: the input to the task, as required by some tasks to format the output.\n\n    Returns:\n        A dictionary containing the `score` and the `reason` for the provided `score`.\n    \"\"\"\n    pattern = r\"(.+?)Score: (\\d)\"\n\n    matches = None\n    if output is not None:\n        matches = re.findall(pattern, output, re.DOTALL)\n    if matches is None:\n        return {\"score\": None, \"reason\": None}\n\n    return {\n        \"score\": int(matches[0][1]),\n        \"reason\": matches[0][0].strip(),\n    }\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/instruction_backtranslation/#distilabel.steps.tasks.instruction_backtranslation.InstructionBacktranslation.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/instruction_backtranslation.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"instruction-backtranslation.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/pair_rm/","title":"Pair rm","text":""},{"location":"reference/distilabel/steps/tasks/pair_rm/#distilabel.steps.tasks.pair_rm.PairRM","title":"<code>PairRM</code>","text":"<p>               Bases: <code>Step</code></p> <p>Rank the candidates based on the input using the <code>LLM</code> model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The model to use for the ranking. Defaults to <code>\"llm-blender/PairRM\"</code>.</p> <code>instructions</code> <code>Optional[str]</code> <p>The instructions to use for the model. Defaults to <code>None</code>.</p> Input columns <ul> <li>inputs (<code>List[Dict[str, Any]]</code>): The input text or conversation to rank the candidates for.</li> <li>candidates (<code>List[Dict[str, Any]]</code>): The candidates to rank.</li> </ul> Output columns <ul> <li>ranks (<code>List[int]</code>): The ranks of the candidates based on the input.</li> <li>ranked_candidates (<code>List[Dict[str, Any]]</code>): The candidates ranked based on the input.</li> <li>model_name (<code>str</code>): The model name used to rank the candidate responses. Defaults to <code>\"llm-blender/PairRM\"</code>.</li> </ul> References <ul> <li>LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion.</li> <li>Pair Ranking Model.</li> </ul> Categories <ul> <li>preference</li> </ul> Note <p>This step differs to other tasks as there is a single implementation of this model currently, and we will use a specific <code>LLM</code>.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>class PairRM(Step):\n    \"\"\"Rank the candidates based on the input using the `LLM` model.\n\n    Attributes:\n        model: The model to use for the ranking. Defaults to `\"llm-blender/PairRM\"`.\n        instructions: The instructions to use for the model. Defaults to `None`.\n\n    Input columns:\n        - inputs (`List[Dict[str, Any]]`): The input text or conversation to rank the candidates for.\n        - candidates (`List[Dict[str, Any]]`): The candidates to rank.\n\n    Output columns:\n        - ranks (`List[int]`): The ranks of the candidates based on the input.\n        - ranked_candidates (`List[Dict[str, Any]]`): The candidates ranked based on the input.\n        - model_name (`str`): The model name used to rank the candidate responses. Defaults to `\"llm-blender/PairRM\"`.\n\n    References:\n        - [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://arxiv.org/abs/2306.02561).\n        - [Pair Ranking Model](https://huggingface.co/llm-blender/PairRM).\n\n    Categories:\n        - preference\n\n    Note:\n        This step differs to other tasks as there is a single implementation of this model\n        currently, and we will use a specific `LLM`.\n    \"\"\"\n\n    model: str = \"llm-blender/PairRM\"\n    instructions: Optional[str] = None\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the PairRM model provided via `model` with `llm_blender.Blender`, which is the\n        custom library for running the inference for the PairRM models.\"\"\"\n        try:\n            import llm_blender\n        except ImportError as e:\n            raise ImportError(\n                \"The `llm_blender` package is required to use the `PairRM` class.\"\n                \"Please install it with `pip install git+https://github.com/yuchenlin/LLM-Blender.git`.\"\n            ) from e\n\n        self._blender = llm_blender.Blender()\n        self._blender.loadranker(self.model)\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input columns correspond to the two required arguments from `Blender.rank`:\n        `inputs` and `candidates`.\"\"\"\n        return [\"input\", \"candidates\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs will include the `ranks` and the `ranked_candidates`.\"\"\"\n        return [\"ranks\", \"ranked_candidates\", \"model_name\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"The input is expected to be a dictionary with the keys `input` and `candidates`,\n        where the `input` corresponds to the instruction of a model and `candidates` are a\n        list of responses to be ranked.\n        \"\"\"\n        return {\"input\": input[\"input\"], \"candidates\": input[\"candidates\"]}\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Generates the ranks for the candidates based on the input.\n\n        The ranks are the positions of the candidates, where lower is better,\n        and the ranked candidates correspond to the candidates sorted according to the\n        ranks obtained.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            An iterator with the inputs containing the `ranks`, `ranked_candidates`, and `model_name`.\n        \"\"\"\n        input_texts = []\n        candidates = []\n        for input in inputs:\n            formatted_input = self.format_input(input)\n            input_texts.append(formatted_input[\"input\"])\n            candidates.append(formatted_input[\"candidates\"])\n\n        instructions = (\n            [self.instructions] * len(input_texts) if self.instructions else None\n        )\n\n        ranks = self._blender.rank(\n            input_texts,\n            candidates,\n            instructions=instructions,\n            return_scores=False,\n            batch_size=self.input_batch_size,\n        )\n        # Sort the candidates based on the ranks\n        ranked_candidates = np.take_along_axis(\n            np.array(candidates), ranks - 1, axis=1\n        ).tolist()\n        ranks = ranks.tolist()\n        for input, rank, ranked_candidate in zip(inputs, ranks, ranked_candidates):\n            input[\"ranks\"] = rank\n            input[\"ranked_candidates\"] = ranked_candidate\n            input[\"model_name\"] = self.model\n\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/pair_rm/#distilabel.steps.tasks.pair_rm.PairRM.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input columns correspond to the two required arguments from <code>Blender.rank</code>: <code>inputs</code> and <code>candidates</code>.</p>"},{"location":"reference/distilabel/steps/tasks/pair_rm/#distilabel.steps.tasks.pair_rm.PairRM.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs will include the <code>ranks</code> and the <code>ranked_candidates</code>.</p>"},{"location":"reference/distilabel/steps/tasks/pair_rm/#distilabel.steps.tasks.pair_rm.PairRM.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is expected to be a dictionary with the keys <code>input</code> and <code>candidates</code>, where the <code>input</code> corresponds to the instruction of a model and <code>candidates</code> are a list of responses to be ranked.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"The input is expected to be a dictionary with the keys `input` and `candidates`,\n    where the `input` corresponds to the instruction of a model and `candidates` are a\n    list of responses to be ranked.\n    \"\"\"\n    return {\"input\": input[\"input\"], \"candidates\": input[\"candidates\"]}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/pair_rm/#distilabel.steps.tasks.pair_rm.PairRM.load","title":"<code>load()</code>","text":"<p>Loads the PairRM model provided via <code>model</code> with <code>llm_blender.Blender</code>, which is the custom library for running the inference for the PairRM models.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the PairRM model provided via `model` with `llm_blender.Blender`, which is the\n    custom library for running the inference for the PairRM models.\"\"\"\n    try:\n        import llm_blender\n    except ImportError as e:\n        raise ImportError(\n            \"The `llm_blender` package is required to use the `PairRM` class.\"\n            \"Please install it with `pip install git+https://github.com/yuchenlin/LLM-Blender.git`.\"\n        ) from e\n\n    self._blender = llm_blender.Blender()\n    self._blender.loadranker(self.model)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/pair_rm/#distilabel.steps.tasks.pair_rm.PairRM.process","title":"<code>process(inputs)</code>","text":"<p>Generates the ranks for the candidates based on the input.</p> <p>The ranks are the positions of the candidates, where lower is better, and the ranked candidates correspond to the candidates sorted according to the ranks obtained.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>An iterator with the inputs containing the <code>ranks</code>, <code>ranked_candidates</code>, and <code>model_name</code>.</p> Source code in <code>src/distilabel/steps/tasks/pair_rm.py</code> <pre><code>def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Generates the ranks for the candidates based on the input.\n\n    The ranks are the positions of the candidates, where lower is better,\n    and the ranked candidates correspond to the candidates sorted according to the\n    ranks obtained.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        An iterator with the inputs containing the `ranks`, `ranked_candidates`, and `model_name`.\n    \"\"\"\n    input_texts = []\n    candidates = []\n    for input in inputs:\n        formatted_input = self.format_input(input)\n        input_texts.append(formatted_input[\"input\"])\n        candidates.append(formatted_input[\"candidates\"])\n\n    instructions = (\n        [self.instructions] * len(input_texts) if self.instructions else None\n    )\n\n    ranks = self._blender.rank(\n        input_texts,\n        candidates,\n        instructions=instructions,\n        return_scores=False,\n        batch_size=self.input_batch_size,\n    )\n    # Sort the candidates based on the ranks\n    ranked_candidates = np.take_along_axis(\n        np.array(candidates), ranks - 1, axis=1\n    ).tolist()\n    ranks = ranks.tolist()\n    for input, rank, ranked_candidate in zip(inputs, ranks, ranked_candidates):\n        input[\"ranks\"] = rank\n        input[\"ranked_candidates\"] = ranked_candidate\n        input[\"model_name\"] = self.model\n\n    yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/prometheus_eval/","title":"Prometheus eval","text":""},{"location":"reference/distilabel/steps/tasks/prometheus_eval/#distilabel.steps.tasks.prometheus_eval.PrometheusEval","title":"<code>PrometheusEval</code>","text":"<p>               Bases: <code>Task</code></p> <p>Critique and rank the quality of generations from an <code>LLM</code> using Prometheus 2.0.</p> <p><code>PrometheusEval</code> is a task created for Prometheus 2.0, covering both the absolute and relative evaluations.</p> <ul> <li>The absolute evaluation i.e. <code>mode=\"absolute\"</code> is used to evaluate a single generation from     an LLM for a given instruction.</li> <li>The relative evaluation i.e. <code>mode=\"relative\"</code> is used to evaluate two generations from an LLM     for a given instruction.</li> </ul> <p>Both evaluations provide the possibility whether to use a reference answer to compare with or not via the <code>reference</code> attribute, and both are based on a score rubric that critiques the generation/s based on the following default aspects: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, and <code>reasoning</code>, that can be overridden via <code>rubrics</code>, and the selected rubric is set via the attribute <code>rubric</code>.</p> Note <p>The <code>PrometheusEval</code> task is better suited and intended to be used with any of the Prometheus 2.0 models released by Kaist AI, being: https://huggingface.co/prometheus-eval/prometheus-7b-v2.0, and https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0. The critique assessment formatting and quality is not guaranteed if using another model, even though some other models may be able to correctly follow the formatting and generate insightful critiques too.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>Literal['absolute', 'relative']</code> <p>the evaluation mode to use, either <code>absolute</code> or <code>relative</code>. It defines whether the task will evaluate one or two generations.</p> <code>rubric</code> <code>str</code> <p>the score rubric to use within the prompt to run the critique based on different aspects. Can be any existing key in the <code>rubrics</code> attribute, which by default means that it can be: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, or <code>reasoning</code>. Those will only work if using the default <code>rubrics</code>, otherwise, the provided <code>rubrics</code> should be used.</p> <code>rubrics</code> <code>Optional[Dict[str, str]]</code> <p>a dictionary containing the different rubrics to use for the critique, where the keys are the rubric names and the values are the rubric descriptions. The default rubrics are the following: <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, and <code>reasoning</code>.</p> <code>reference</code> <code>bool</code> <p>a boolean flag to indicate whether a reference answer / completion will be provided, so that the model critique is based on the comparison with it. It implies that the column <code>reference</code> needs to be provided within the input data in addition to the rest of the inputs.</p> <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to use as reference.</li> <li>generation (<code>str</code>, optional): The generated text from the given <code>instruction</code>. This column is required     if <code>mode=absolute</code>.</li> <li>generations (<code>List[str]</code>, optional): The generated texts from the given <code>instruction</code>. It should     contain 2 generations only. This column is required if <code>mode=relative</code>.</li> <li>reference (<code>str</code>, optional): The reference / golden answer for the <code>instruction</code>, to be used by the LLM     for comparison against.</li> </ul> Output columns <ul> <li>feedback (<code>str</code>): The feedback explaining the result below, as critiqued by the LLM using the     pre-defined score rubric, compared against <code>reference</code> if provided.</li> <li>result (<code>Union[int, Literal[\"A\", \"B\"]]</code>): If <code>mode=absolute</code>, then the result contains the score for the     <code>generation</code> in a likert-scale from 1-5, otherwise, if <code>mode=relative</code>, then the result contains either     \"A\" or \"B\", the \"winning\" one being the generation in the index 0 of <code>generations</code> if <code>result='A'</code> or the     index 1 if <code>result='B'</code>.</li> <li>model_name (<code>str</code>): The model name used to generate the <code>feedback</code> and <code>result</code>.</li> </ul> Categories <ul> <li>critique</li> <li>preference</li> </ul> References <ul> <li>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</li> <li>prometheus-eval: Evaluate your LLM's response with Prometheus \ud83d\udcaf</li> </ul> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>class PrometheusEval(Task):\n    \"\"\"Critique and rank the quality of generations from an `LLM` using Prometheus 2.0.\n\n    `PrometheusEval` is a task created for Prometheus 2.0, covering both the absolute and relative\n    evaluations.\n\n    - The absolute evaluation i.e. `mode=\"absolute\"` is used to evaluate a single generation from\n        an LLM for a given instruction.\n    - The relative evaluation i.e. `mode=\"relative\"` is used to evaluate two generations from an LLM\n        for a given instruction.\n\n    Both evaluations provide the possibility whether to use a reference answer to compare with or not\n    via the `reference` attribute, and both are based on a score rubric that critiques the generation/s\n    based on the following default aspects: `helpfulness`, `harmlessness`, `honesty`, `factual-validity`,\n    and `reasoning`, that can be overridden via `rubrics`, and the selected rubric is set via the attribute\n    `rubric`.\n\n    Note:\n        The `PrometheusEval` task is better suited and intended to be used with any of the Prometheus 2.0\n        models released by Kaist AI, being: https://huggingface.co/prometheus-eval/prometheus-7b-v2.0,\n        and https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0. The critique assessment formatting\n        and quality is not guaranteed if using another model, even though some other models may be able to\n        correctly follow the formatting and generate insightful critiques too.\n\n    Attributes:\n        mode: the evaluation mode to use, either `absolute` or `relative`. It defines whether the task\n            will evaluate one or two generations.\n        rubric: the score rubric to use within the prompt to run the critique based on different aspects.\n            Can be any existing key in the `rubrics` attribute, which by default means that it can be:\n            `helpfulness`, `harmlessness`, `honesty`, `factual-validity`, or `reasoning`. Those will only\n            work if using the default `rubrics`, otherwise, the provided `rubrics` should be used.\n        rubrics: a dictionary containing the different rubrics to use for the critique, where the keys are\n            the rubric names and the values are the rubric descriptions. The default rubrics are the following:\n            `helpfulness`, `harmlessness`, `honesty`, `factual-validity`, and `reasoning`.\n        reference: a boolean flag to indicate whether a reference answer / completion will be provided, so\n            that the model critique is based on the comparison with it. It implies that the column `reference`\n            needs to be provided within the input data in addition to the rest of the inputs.\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instruction (`str`): The instruction to use as reference.\n        - generation (`str`, optional): The generated text from the given `instruction`. This column is required\n            if `mode=absolute`.\n        - generations (`List[str]`, optional): The generated texts from the given `instruction`. It should\n            contain 2 generations only. This column is required if `mode=relative`.\n        - reference (`str`, optional): The reference / golden answer for the `instruction`, to be used by the LLM\n            for comparison against.\n\n    Output columns:\n        - feedback (`str`): The feedback explaining the result below, as critiqued by the LLM using the\n            pre-defined score rubric, compared against `reference` if provided.\n        - result (`Union[int, Literal[\"A\", \"B\"]]`): If `mode=absolute`, then the result contains the score for the\n            `generation` in a likert-scale from 1-5, otherwise, if `mode=relative`, then the result contains either\n            \"A\" or \"B\", the \"winning\" one being the generation in the index 0 of `generations` if `result='A'` or the\n            index 1 if `result='B'`.\n        - model_name (`str`): The model name used to generate the `feedback` and `result`.\n\n    Categories:\n        - critique\n        - preference\n\n    References:\n        - [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535)\n        - [prometheus-eval: Evaluate your LLM's response with Prometheus \ud83d\udcaf](https://github.com/prometheus-eval/prometheus-eval)\n    \"\"\"\n\n    mode: Literal[\"absolute\", \"relative\"]\n    rubric: str\n    rubrics: Optional[Dict[str, str]] = Field(default=_DEFAULT_RUBRICS)\n    reference: bool = False\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    @model_validator(mode=\"after\")\n    def validate_rubric_and_rubrics(self) -&gt; Self:\n        if not isinstance(self.rubrics, dict) or len(self.rubrics) &lt; 1:\n            raise ValueError(\n                \"Provided `rubrics` must be a Python dictionary with string keys and string values.\"\n            )\n\n        def rubric_matches_pattern(rubric: str) -&gt; bool:\n            \"\"\"Checks if the provided rubric matches the pattern of the default rubrics.\"\"\"\n            pattern = r\"^\\[.*?\\]\\n(?:Score [1-4]: .*?\\n){4}(?:Score 5: .*?)\"\n            return bool(re.match(pattern, rubric, re.MULTILINE))\n\n        if not all(rubric_matches_pattern(value) for value in self.rubrics.values()):\n            raise ValueError(\n                \"Provided rubrics should match the format of the default rubrics, which\"\n                \" is as follows: `[&lt;scoring criteria&gt;]\\nScore 1: &lt;description&gt;\\nScore 2: &lt;description&gt;\\n\"\n                \"Score 3: &lt;description&gt;\\nScore 4: &lt;description&gt;\\nScore 5: &lt;description&gt;`; replacing\"\n                \" `&lt;scoring criteria&gt;` and `&lt;description&gt;` with the actual criteria and description\"\n                \" for each or the scores, respectively.\"\n            )\n\n        if self.rubric not in self.rubrics:\n            raise ValueError(\n                f\"Provided rubric '{self.rubric}' is not among the available rubrics: {', '.join(self.rubrics.keys())}.\"\n            )\n\n        return self\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation\n        depending on the `mode` value, and either with or without reference, depending on the\n        value of `reference`.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"prometheus\"\n            / (\n                f\"{self.mode}_without_reference.jinja2\"\n                if self.reference is False\n                else f\"{self.mode}_with_reference.jinja2\"\n            )\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The default inputs for the task are the `instruction` and the `generation`\n        if `reference=False`, otherwise, the inputs are `instruction`, `generation`, and\n        `reference`.\"\"\"\n        if self.mode == \"absolute\":\n            if self.reference:\n                return [\"instruction\", \"generation\", \"reference\"]\n            return [\"instruction\", \"generation\"]\n        else:  # self.mode == \"relative\"\n            if self.reference:\n                return [\"instruction\", \"generations\", \"reference\"]\n            return [\"instruction\", \"generations\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` where the prompt is formatted according\n        to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction\n        from the user, including a pre-defined system prompt.\"\"\"\n        template_kwargs = {\n            \"instruction\": input[\"instruction\"],\n            \"rubric\": self.rubrics[self.rubric],\n        }\n        if self.reference:\n            template_kwargs[\"reference\"] = input[\"reference\"]\n\n        if self.mode == \"absolute\":\n            if not isinstance(input[\"generation\"], str):\n                raise ValueError(\n                    f\"Provided `generation` is of type {type(input['generation'])} but a string\"\n                    \" should be provided instead.\",\n                )\n\n            template_kwargs[\"generation\"] = input[\"generation\"]\n            system_message = (\n                \"You are a fair judge assistant tasked with providing clear, objective feedback based\"\n                \" on specific criteria, ensuring each assessment reflects the absolute standards set\"\n                \" for performance.\"\n            )\n        else:  # self.mode == \"relative\"\n            if (\n                not isinstance(input[\"generations\"], list)\n                or not all(\n                    isinstance(generation, str) for generation in input[\"generations\"]\n                )\n                or len(input[\"generations\"]) != 2\n            ):\n                raise ValueError(\n                    f\"Provided `generations` is of type {type(input['generations'])} but a list of strings with length 2 should be provided instead.\"\n                )\n\n            template_kwargs[\"generations\"] = input[\"generations\"]\n            system_message = (\n                \"You are a fair judge assistant assigned to deliver insightful feedback that compares\"\n                \" individual performances, highlighting how each stands relative to others within the\"\n                \" same cohort.\"\n            )\n\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": system_message,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(**template_kwargs),  # type: ignore\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `feedback` and the `result` generated by Prometheus,\n        as well as the `model_name` which is automatically included based on the `LLM` used.\n        \"\"\"\n        return [\"feedback\", \"result\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dict with the keys `feedback` and `result` captured\n        using a regex from the Prometheus output.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Optionally provided in case it's useful to build the output.\n\n        Returns:\n            A dict with the keys `feedback` and `result` generated by the LLM.\n        \"\"\"\n        if output is None:\n            return {\"feedback\": None, \"result\": None}\n\n        parts = output.split(\"[RESULT]\")\n        if len(parts) != 2:\n            return {\"feedback\": None, \"result\": None}\n\n        feedback, result = parts[0].strip(), parts[1].strip()\n        if feedback.startswith(\"Feedback:\"):\n            feedback = feedback[len(\"Feedback:\") :].strip()\n        if self.mode == \"absolute\":\n            if not result.isdigit() or result not in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n                return {\"feedback\": None, \"result\": None}\n            return {\"feedback\": feedback, \"result\": int(result)}\n        else:  # self.mode == \"relative\"\n            if result not in [\"A\", \"B\"]:\n                return {\"feedback\": None, \"result\": None}\n            return {\"feedback\": feedback, \"result\": result}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/prometheus_eval/#distilabel.steps.tasks.prometheus_eval.PrometheusEval.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The default inputs for the task are the <code>instruction</code> and the <code>generation</code> if <code>reference=False</code>, otherwise, the inputs are <code>instruction</code>, <code>generation</code>, and <code>reference</code>.</p>"},{"location":"reference/distilabel/steps/tasks/prometheus_eval/#distilabel.steps.tasks.prometheus_eval.PrometheusEval.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>feedback</code> and the <code>result</code> generated by Prometheus, as well as the <code>model_name</code> which is automatically included based on the <code>LLM</code> used.</p>"},{"location":"reference/distilabel/steps/tasks/prometheus_eval/#distilabel.steps.tasks.prometheus_eval.PrometheusEval.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> where the prompt is formatted according to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction from the user, including a pre-defined system prompt.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` where the prompt is formatted according\n    to the selected Jinja2 template for Prometheus 2.0, assuming that's the first interaction\n    from the user, including a pre-defined system prompt.\"\"\"\n    template_kwargs = {\n        \"instruction\": input[\"instruction\"],\n        \"rubric\": self.rubrics[self.rubric],\n    }\n    if self.reference:\n        template_kwargs[\"reference\"] = input[\"reference\"]\n\n    if self.mode == \"absolute\":\n        if not isinstance(input[\"generation\"], str):\n            raise ValueError(\n                f\"Provided `generation` is of type {type(input['generation'])} but a string\"\n                \" should be provided instead.\",\n            )\n\n        template_kwargs[\"generation\"] = input[\"generation\"]\n        system_message = (\n            \"You are a fair judge assistant tasked with providing clear, objective feedback based\"\n            \" on specific criteria, ensuring each assessment reflects the absolute standards set\"\n            \" for performance.\"\n        )\n    else:  # self.mode == \"relative\"\n        if (\n            not isinstance(input[\"generations\"], list)\n            or not all(\n                isinstance(generation, str) for generation in input[\"generations\"]\n            )\n            or len(input[\"generations\"]) != 2\n        ):\n            raise ValueError(\n                f\"Provided `generations` is of type {type(input['generations'])} but a list of strings with length 2 should be provided instead.\"\n            )\n\n        template_kwargs[\"generations\"] = input[\"generations\"]\n        system_message = (\n            \"You are a fair judge assistant assigned to deliver insightful feedback that compares\"\n            \" individual performances, highlighting how each stands relative to others within the\"\n            \" same cohort.\"\n        )\n\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": system_message,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(**template_kwargs),  # type: ignore\n        },\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/prometheus_eval/#distilabel.steps.tasks.prometheus_eval.PrometheusEval.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dict with the keys <code>feedback</code> and <code>result</code> captured using a regex from the Prometheus output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Optionally provided in case it's useful to build the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the keys <code>feedback</code> and <code>result</code> generated by the LLM.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dict with the keys `feedback` and `result` captured\n    using a regex from the Prometheus output.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Optionally provided in case it's useful to build the output.\n\n    Returns:\n        A dict with the keys `feedback` and `result` generated by the LLM.\n    \"\"\"\n    if output is None:\n        return {\"feedback\": None, \"result\": None}\n\n    parts = output.split(\"[RESULT]\")\n    if len(parts) != 2:\n        return {\"feedback\": None, \"result\": None}\n\n    feedback, result = parts[0].strip(), parts[1].strip()\n    if feedback.startswith(\"Feedback:\"):\n        feedback = feedback[len(\"Feedback:\") :].strip()\n    if self.mode == \"absolute\":\n        if not result.isdigit() or result not in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n            return {\"feedback\": None, \"result\": None}\n        return {\"feedback\": feedback, \"result\": int(result)}\n    else:  # self.mode == \"relative\"\n        if result not in [\"A\", \"B\"]:\n            return {\"feedback\": None, \"result\": None}\n        return {\"feedback\": feedback, \"result\": result}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/prometheus_eval/#distilabel.steps.tasks.prometheus_eval.PrometheusEval.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation depending on the <code>mode</code> value, and either with or without reference, depending on the value of <code>reference</code>.</p> Source code in <code>src/distilabel/steps/tasks/prometheus_eval.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template for Prometheus 2.0 either absolute or relative evaluation\n    depending on the `mode` value, and either with or without reference, depending on the\n    value of `reference`.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"prometheus\"\n        / (\n            f\"{self.mode}_without_reference.jinja2\"\n            if self.reference is False\n            else f\"{self.mode}_with_reference.jinja2\"\n        )\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/quality_scorer/","title":"Quality scorer","text":""},{"location":"reference/distilabel/steps/tasks/quality_scorer/#distilabel.steps.tasks.quality_scorer.QualityScorer","title":"<code>QualityScorer</code>","text":"<p>               Bases: <code>Task</code></p> <p>Score responses based on their quality using an <code>LLM</code>.</p> <p><code>QualityScorer</code> is a pre-defined task that defines the <code>instruction</code> as the input and <code>score</code> as the output. This task is used to rate the quality of instructions and responses. It's an implementation of the quality score task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'. The task follows the same scheme as the Complexity Scorer, but the instruction-response pairs are scored in terms of quality, obtaining a quality score for each instruction.</p> <p>Attributes:</p> Name Type Description <code>_template</code> <code>Union[Template, None]</code> <p>a Jinja2 template used to format the input for the LLM.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the <code>responses</code>.</li> <li>responses (<code>List[str]</code>): The responses to be scored. Each response forms a pair with the instruction.</li> </ul> Output columns <ul> <li>scores (<code>List[float]</code>): The score for each instruction.</li> <li>model_name (<code>str</code>): The model name used to generate the scores.</li> </ul> Categories <ul> <li>scorer</li> <li>quality</li> <li>response</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>class QualityScorer(Task):\n    \"\"\"Score responses based on their quality using an `LLM`.\n\n    `QualityScorer` is a pre-defined task that defines the `instruction` as the input\n    and `score` as the output. This task is used to rate the quality of instructions and responses.\n    It's an implementation of the quality score task from the paper 'What Makes Good Data\n    for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.\n    The task follows the same scheme as the Complexity Scorer, but the instruction-response pairs\n    are scored in terms of quality, obtaining a quality score for each instruction.\n\n    Attributes:\n        _template: a Jinja2 template used to format the input for the LLM.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the `responses`.\n        - responses (`List[str]`): The responses to be scored. Each response forms a pair with the instruction.\n\n    Output columns:\n        - scores (`List[float]`): The score for each instruction.\n        - model_name (`str`): The model name used to generate the scores.\n\n    Categories:\n        - scorer\n        - quality\n        - response\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"quality-scorer.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task are `instruction` and `responses`.\"\"\"\n        return [\"instruction\", \"responses\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], responses=input[\"responses\"]\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self):\n        \"\"\"The output for the task is a list of `scores` containing the quality score for each\n        response in `responses`.\"\"\"\n        return [\"scores\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the score of each instruction-response pair.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with the key `scores` containing the scores for each instruction-response pair.\n        \"\"\"\n        if output is None:\n            return {\"scores\": [None] * len(input[\"responses\"])}\n\n        scores = []\n        score_lines = output.split(\"\\n\")\n\n        for i, line in enumerate(score_lines):\n            match = _PARSE_SCORE_LINE_REGEX.match(line)\n            score = float(match.group(1)) if match else None\n            scores.append(score)\n            if i == len(input[\"responses\"]) - 1:\n                break\n        return {\"scores\": scores}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/quality_scorer/#distilabel.steps.tasks.quality_scorer.QualityScorer.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task are <code>instruction</code> and <code>responses</code>.</p>"},{"location":"reference/distilabel/steps/tasks/quality_scorer/#distilabel.steps.tasks.quality_scorer.QualityScorer.outputs","title":"<code>outputs</code>  <code>property</code>","text":"<p>The output for the task is a list of <code>scores</code> containing the quality score for each response in <code>responses</code>.</p>"},{"location":"reference/distilabel/steps/tasks/quality_scorer/#distilabel.steps.tasks.quality_scorer.QualityScorer.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], responses=input[\"responses\"]\n            ),\n        }\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/quality_scorer/#distilabel.steps.tasks.quality_scorer.QualityScorer.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a list with the score of each instruction-response pair.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task. Used for obtaining the number of responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with the key <code>scores</code> containing the scores for each instruction-response pair.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the score of each instruction-response pair.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with the key `scores` containing the scores for each instruction-response pair.\n    \"\"\"\n    if output is None:\n        return {\"scores\": [None] * len(input[\"responses\"])}\n\n    scores = []\n    score_lines = output.split(\"\\n\")\n\n    for i, line in enumerate(score_lines):\n        match = _PARSE_SCORE_LINE_REGEX.match(line)\n        score = float(match.group(1)) if match else None\n        scores.append(score)\n        if i == len(input[\"responses\"]) - 1:\n            break\n    return {\"scores\": scores}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/quality_scorer/#distilabel.steps.tasks.quality_scorer.QualityScorer.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/quality_scorer.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"quality-scorer.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/self_instruct/","title":"Self instruct","text":""},{"location":"reference/distilabel/steps/tasks/self_instruct/#distilabel.steps.tasks.self_instruct.SelfInstruct","title":"<code>SelfInstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate instructions based on a given input using an <code>LLM</code>.</p> <p><code>SelfInstruct</code> is a pre-defined task that, given a number of instructions, a certain criteria for query generations, an application description, and an input, generates a number of instruction related to the given input and following what is stated in the criteria for query generation and the application description. It is based in the SelfInstruct framework from the paper \"Self-Instruct: Aligning Language Models with Self-Generated Instructions\".</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <code>int</code> <p>The number of instructions to be generated. Defaults to 5.</p> <code>criteria_for_query_generation</code> <code>str</code> <p>The criteria for the query generation. Defaults to the criteria defined within the paper.</p> <code>application_description</code> <code>str</code> <p>The description of the AI application that one want to build with these instructions. Defaults to <code>AI assistant</code>.</p> Input columns <ul> <li>input (<code>str</code>): The input to generate the instructions. It's also called seed in     the paper.</li> </ul> Output columns <ul> <li>instructions (<code>List[str]</code>): The generated instructions.</li> <li>model_name (<code>str</code>): The model name used to generate the instructions.</li> </ul> Categories <ul> <li>text-generation</li> </ul> Reference <ul> <li><code>Self-Instruct: Aligning Language Models with Self-Generated Instructions</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>class SelfInstruct(Task):\n    \"\"\"Generate instructions based on a given input using an `LLM`.\n\n    `SelfInstruct` is a pre-defined task that, given a number of instructions, a\n    certain criteria for query generations, an application description, and an input,\n    generates a number of instruction related to the given input and following what\n    is stated in the criteria for query generation and the application description.\n    It is based in the SelfInstruct framework from the paper \"Self-Instruct: Aligning\n    Language Models with Self-Generated Instructions\".\n\n    Attributes:\n        num_instructions: The number of instructions to be generated. Defaults to 5.\n        criteria_for_query_generation: The criteria for the query generation. Defaults\n            to the criteria defined within the paper.\n        application_description: The description of the AI application that one want\n            to build with these instructions. Defaults to `AI assistant`.\n\n    Input columns:\n        - input (`str`): The input to generate the instructions. It's also called seed in\n            the paper.\n\n    Output columns:\n        - instructions (`List[str]`): The generated instructions.\n        - model_name (`str`): The model name used to generate the instructions.\n\n    Categories:\n        - text-generation\n\n    Reference:\n        - [`Self-Instruct: Aligning Language Models with Self-Generated Instructions`](https://arxiv.org/abs/2212.10560)\n    \"\"\"\n\n    num_instructions: int = 5\n    criteria_for_query_generation: str = (\n        \"Incorporate a diverse range of verbs, avoiding repetition.\\n\"\n        \"Ensure queries are compatible with AI model's text generation functions and are limited to 1-2 sentences.\\n\"\n        \"Design queries to be self-contained and standalone.\\n\"\n        'Blend interrogative (e.g., \"What is the significance of x?\") and imperative (e.g., \"Detail the process of x.\") styles.'\n    )\n    application_description: str = \"AI assistant\"\n\n    _template: Union[Template, None] = PrivateAttr(...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"self-instruct.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `input` i.e. seed text.\"\"\"\n        return [\"input\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(\n                    input=input[\"input\"],\n                    application_description=self.application_description,\n                    criteria_for_query_generation=self.criteria_for_query_generation,\n                    num_instructions=self.num_instructions,\n                ),\n            }\n        ]\n\n    @property\n    def outputs(self):\n        \"\"\"The output for the task is a list of `instructions` containing the generated instructions.\"\"\"\n        return [\"instructions\", \"model_name\"]\n\n    def format_output(\n        self,\n        output: Union[str, None],\n        input: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a list with the generated instructions.\n\n        Args:\n            output: the raw output of the LLM.\n            input: the input to the task. Used for obtaining the number of responses.\n\n        Returns:\n            A dict with containing the generated instructions.\n        \"\"\"\n        if output is None:\n            return {\"instructions\": []}\n        return {\"instructions\": [line for line in output.split(\"\\n\") if line != \"\"]}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/self_instruct/#distilabel.steps.tasks.self_instruct.SelfInstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>input</code> i.e. seed text.</p>"},{"location":"reference/distilabel/steps/tasks/self_instruct/#distilabel.steps.tasks.self_instruct.SelfInstruct.outputs","title":"<code>outputs</code>  <code>property</code>","text":"<p>The output for the task is a list of <code>instructions</code> containing the generated instructions.</p>"},{"location":"reference/distilabel/steps/tasks/self_instruct/#distilabel.steps.tasks.self_instruct.SelfInstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(\n                input=input[\"input\"],\n                application_description=self.application_description,\n                criteria_for_query_generation=self.criteria_for_query_generation,\n                num_instructions=self.num_instructions,\n            ),\n        }\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/self_instruct/#distilabel.steps.tasks.self_instruct.SelfInstruct.format_output","title":"<code>format_output(output, input=None)</code>","text":"<p>The output is formatted as a list with the generated instructions.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>the raw output of the LLM.</p> required <code>input</code> <code>Optional[Dict[str, Any]]</code> <p>the input to the task. Used for obtaining the number of responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict with containing the generated instructions.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def format_output(\n    self,\n    output: Union[str, None],\n    input: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a list with the generated instructions.\n\n    Args:\n        output: the raw output of the LLM.\n        input: the input to the task. Used for obtaining the number of responses.\n\n    Returns:\n        A dict with containing the generated instructions.\n    \"\"\"\n    if output is None:\n        return {\"instructions\": []}\n    return {\"instructions\": [line for line in output.split(\"\\n\") if line != \"\"]}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/self_instruct/#distilabel.steps.tasks.self_instruct.SelfInstruct.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/self_instruct.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"self-instruct.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/sentence_transformers/","title":"Sentence transformers","text":""},{"location":"reference/distilabel/steps/tasks/sentence_transformers/#distilabel.steps.tasks.sentence_transformers.GenerateSentencePair","title":"<code>GenerateSentencePair</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate a positive and negative (optionally) sentences given an anchor sentence.</p> <p><code>GenerateSentencePair</code> is a pre-defined task that given an anchor sentence generates a positive sentence related to the anchor and optionally a negative sentence unrelated to the anchor. This task is useful to generate training datasets for training embeddings models.</p> <p>Attributes:</p> Name Type Description <code>triplet</code> <code>bool</code> <p>a flag to indicate if the task should generate a triplet of sentences (anchor, positive, negative). Defaults to <code>False</code>.</p> <code>action</code> <code>GenerationAction</code> <p>the action to perform to generate the positive sentence.</p> Input columns <ul> <li>anchor (<code>str</code>): The anchor sentence to generate the positive and negative sentences.</li> </ul> Output columns <ul> <li>positive (<code>str</code>): The positive sentence related to the <code>anchor</code>.</li> <li>negative (<code>str</code>): The negative sentence unrelated to the <code>anchor</code> if <code>triplet=True</code>.</li> <li>model_name (<code>str</code>): The name of the model that was used to generate the sentences.</li> </ul> Categories <ul> <li>embedding</li> </ul> <p>Examples:</p> <pre><code>Paraphrasing:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"paraphrase\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n```\n\nGenerating semantically similar sentences:\n\n```python\nfrom distilabel.llms import InferenceEndpointsLLM\nfrom distilabel.steps.tasks import GenerateSentencePair\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"semantically-similar\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"How does 3D printing work?\"}])\n```\n\nGenerating queries:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"query\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"Argilla is an open-source data curation platform for LLMs. Using Argilla, ...\"}])\n```\n\nGenerating answers:\n\n```python\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"answer\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n```\n</code></pre> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>class GenerateSentencePair(Task):\n    \"\"\"Generate a positive and negative (optionally) sentences given an anchor sentence.\n\n    `GenerateSentencePair` is a pre-defined task that given an anchor sentence generates\n    a positive sentence related to the anchor and optionally a negative sentence unrelated\n    to the anchor. This task is useful to generate training datasets for training embeddings\n    models.\n\n    Attributes:\n        triplet: a flag to indicate if the task should generate a triplet of sentences\n            (anchor, positive, negative). Defaults to `False`.\n        action: the action to perform to generate the positive sentence.\n\n    Input columns:\n        - anchor (`str`): The anchor sentence to generate the positive and negative sentences.\n\n    Output columns:\n        - positive (`str`): The positive sentence related to the `anchor`.\n        - negative (`str`): The negative sentence unrelated to the `anchor` if `triplet=True`.\n        - model_name (`str`): The name of the model that was used to generate the sentences.\n\n    Categories:\n        - embedding\n\n    Examples:\n\n        Paraphrasing:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"paraphrase\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n        ```\n\n        Generating semantically similar sentences:\n\n        ```python\n        from distilabel.llms import InferenceEndpointsLLM\n        from distilabel.steps.tasks import GenerateSentencePair\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"semantically-similar\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"How does 3D printing work?\"}])\n        ```\n\n        Generating queries:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"query\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"Argilla is an open-source data curation platform for LLMs. Using Argilla, ...\"}])\n        ```\n\n        Generating answers:\n\n        ```python\n        from distilabel.steps.tasks import GenerateSentencePair\n        from distilabel.llms import InferenceEndpointsLLM\n\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True, # `False` to generate only positive\n            action=\"answer\",\n            llm=InferenceEndpointsLLM(\n                model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n            ),\n            input_batch_size=10,\n        )\n\n        generate_sentence_pair.load()\n\n        result = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n        ```\n    \"\"\"\n\n    triplet: bool = False\n    action: GenerationAction\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"generate-sentence-pair.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The inputs for the task is the `anchor` sentence.\"\"\"\n        return [\"anchor\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n        \"\"\"The inputs are formatted as a `ChatType`, with a system prompt describing the\n        task of generating a positive and negative sentences for the anchor sentence. The\n        anchor is provided as the first user interaction in the conversation.\n\n        Args:\n            input: The input containing the `anchor` sentence.\n\n        Returns:\n            A list of dictionaries containing the system and user interactions.\n        \"\"\"\n        action_sentence = GENERATION_ACTION_SENTENCES[self.action]\n        system_prompt = (\n            POSITIVE_NEGATIVE_SYSTEM_PROMPT if self.triplet else POSITIVE_SYSTEM_PROMPT\n        ).format(action_sentence=action_sentence)\n\n        return [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": self._template.render(anchor=input[\"anchor\"])},\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The outputs for the task are the `positive` and `negative` sentences, as well\n        as the `model_name` used to generate the sentences.\"\"\"\n        columns = [\"positive\", \"negative\"] if self.triplet else [\"positive\"]\n        columns += [\"model_name\"]\n        return columns\n\n    def format_output(\n        self, output: Union[str, None], input: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Formats the output of the LLM, to extract the `positive` and `negative` sentences\n        generated. If the output is `None` or the regex doesn't match, then the outputs\n        will be set to `None` as well.\n\n        Args:\n            output: The output of the LLM.\n            input: The input used to generate the output.\n\n        Returns:\n            The formatted output containing the `positive` and `negative` sentences.\n        \"\"\"\n        if output is None:\n            return {\"positive\": None, \"negative\": None}\n\n        match = POSITIVE_NEGATIVE_PAIR_REGEX.match(output)\n        if match is None:\n            formatted_output = {\"positive\": None}\n            if self.triplet:\n                formatted_output[\"negative\"] = None\n            return formatted_output\n\n        groups = match.groups()\n        if self.triplet:\n            return {\n                \"positive\": groups[0].strip(),\n                \"negative\": groups[1].strip()\n                if len(groups) &gt; 1 and groups[1] is not None\n                else None,\n            }\n\n        return {\"positive\": groups[0].strip()}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/sentence_transformers/#distilabel.steps.tasks.sentence_transformers.GenerateSentencePair.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The inputs for the task is the <code>anchor</code> sentence.</p>"},{"location":"reference/distilabel/steps/tasks/sentence_transformers/#distilabel.steps.tasks.sentence_transformers.GenerateSentencePair.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The outputs for the task are the <code>positive</code> and <code>negative</code> sentences, as well as the <code>model_name</code> used to generate the sentences.</p>"},{"location":"reference/distilabel/steps/tasks/sentence_transformers/#distilabel.steps.tasks.sentence_transformers.GenerateSentencePair.format_input","title":"<code>format_input(input)</code>","text":"<p>The inputs are formatted as a <code>ChatType</code>, with a system prompt describing the task of generating a positive and negative sentences for the anchor sentence. The anchor is provided as the first user interaction in the conversation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, Any]</code> <p>The input containing the <code>anchor</code> sentence.</p> required <p>Returns:</p> Type Description <code>ChatType</code> <p>A list of dictionaries containing the system and user interactions.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; \"ChatType\":\n    \"\"\"The inputs are formatted as a `ChatType`, with a system prompt describing the\n    task of generating a positive and negative sentences for the anchor sentence. The\n    anchor is provided as the first user interaction in the conversation.\n\n    Args:\n        input: The input containing the `anchor` sentence.\n\n    Returns:\n        A list of dictionaries containing the system and user interactions.\n    \"\"\"\n    action_sentence = GENERATION_ACTION_SENTENCES[self.action]\n    system_prompt = (\n        POSITIVE_NEGATIVE_SYSTEM_PROMPT if self.triplet else POSITIVE_SYSTEM_PROMPT\n    ).format(action_sentence=action_sentence)\n\n    return [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": self._template.render(anchor=input[\"anchor\"])},\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/sentence_transformers/#distilabel.steps.tasks.sentence_transformers.GenerateSentencePair.format_output","title":"<code>format_output(output, input=None)</code>","text":"<p>Formats the output of the LLM, to extract the <code>positive</code> and <code>negative</code> sentences generated. If the output is <code>None</code> or the regex doesn't match, then the outputs will be set to <code>None</code> as well.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>The output of the LLM.</p> required <code>input</code> <code>Optional[Dict[str, Any]]</code> <p>The input used to generate the output.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The formatted output containing the <code>positive</code> and <code>negative</code> sentences.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Optional[Dict[str, Any]] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Formats the output of the LLM, to extract the `positive` and `negative` sentences\n    generated. If the output is `None` or the regex doesn't match, then the outputs\n    will be set to `None` as well.\n\n    Args:\n        output: The output of the LLM.\n        input: The input used to generate the output.\n\n    Returns:\n        The formatted output containing the `positive` and `negative` sentences.\n    \"\"\"\n    if output is None:\n        return {\"positive\": None, \"negative\": None}\n\n    match = POSITIVE_NEGATIVE_PAIR_REGEX.match(output)\n    if match is None:\n        formatted_output = {\"positive\": None}\n        if self.triplet:\n            formatted_output[\"negative\"] = None\n        return formatted_output\n\n    groups = match.groups()\n    if self.triplet:\n        return {\n            \"positive\": groups[0].strip(),\n            \"negative\": groups[1].strip()\n            if len(groups) &gt; 1 and groups[1] is not None\n            else None,\n        }\n\n    return {\"positive\": groups[0].strip()}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/sentence_transformers/#distilabel.steps.tasks.sentence_transformers.GenerateSentencePair.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template.</p> Source code in <code>src/distilabel/steps/tasks/sentence_transformers.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"generate-sentence-pair.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_generation/","title":"Structured generation","text":""},{"location":"reference/distilabel/steps/tasks/structured_generation/#distilabel.steps.tasks.structured_generation.StructuredGeneration","title":"<code>StructuredGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generate structured content for a given <code>instruction</code> using an <code>LLM</code>.</p> <p><code>StructuredGeneration</code> is a pre-defined task that defines the <code>instruction</code> and the <code>grammar</code> as the inputs, and <code>generation</code> as the output. This task is used to generate structured content based on the input instruction and following the schema provided within the <code>grammar</code> column per each <code>instruction</code>. The <code>model_name</code> also returned as part of the output in order to enhance it.</p> <p>Attributes:</p> Name Type Description <code>use_system_prompt</code> <code>bool</code> <p>Whether to use the system prompt in the generation. Defaults to <code>True</code>, which means that if the column <code>system_prompt</code> is  defined within the input batch, then the <code>system_prompt</code> will be used, otherwise, it will be ignored.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to generate structured content from.</li> <li>grammar (<code>Dict[str, Any]</code>): The grammar to generate structured content from. It should be a     Python dictionary with the keys <code>type</code> and <code>value</code>, where <code>type</code> should be one of <code>json</code> or     <code>regex</code>, and the <code>value</code> should be either the JSON schema or the regex pattern, respectively.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text matching the provided schema, if possible.</li> <li>model_name (<code>str</code>): The name of the model used to generate the text.</li> </ul> Categories <ul> <li>outlines</li> <li>structured-generation</li> </ul> <p>Examples:</p> <pre><code>from distilabel.steps.tasks import StructuredGeneration\n\ntask = StructuredGeneration(llm=LLM(...))\n</code></pre> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>class StructuredGeneration(Task):\n    \"\"\"Generate structured content for a given `instruction` using an `LLM`.\n\n    `StructuredGeneration` is a pre-defined task that defines the `instruction` and the `grammar`\n    as the inputs, and `generation` as the output. This task is used to generate structured content based on\n    the input instruction and following the schema provided within the `grammar` column per each\n    `instruction`. The `model_name` also returned as part of the output in order to enhance it.\n\n    Attributes:\n        use_system_prompt: Whether to use the system prompt in the generation. Defaults to `True`,\n            which means that if the column `system_prompt` is  defined within the input batch, then\n            the `system_prompt` will be used, otherwise, it will be ignored.\n\n    Input columns:\n        - instruction (`str`): The instruction to generate structured content from.\n        - grammar (`Dict[str, Any]`): The grammar to generate structured content from. It should be a\n            Python dictionary with the keys `type` and `value`, where `type` should be one of `json` or\n            `regex`, and the `value` should be either the JSON schema or the regex pattern, respectively.\n\n    Output columns:\n        - generation (`str`): The generated text matching the provided schema, if possible.\n        - model_name (`str`): The name of the model used to generate the text.\n\n    Categories:\n        - outlines\n        - structured-generation\n\n    Examples:\n        ```python\n        from distilabel.steps.tasks import StructuredGeneration\n\n        task = StructuredGeneration(llm=LLM(...))\n        ```\n    \"\"\"\n\n    use_system_prompt: bool = False\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `instruction` and the `grammar`.\n        Optionally, if the `use_system_prompt` flag is set to True, then the\n        `system_prompt` will be used too.\"\"\"\n        columns = [\"instruction\", \"grammar\"]\n        if self.use_system_prompt:\n            columns = [\"system_prompt\"] + columns\n        return columns\n\n    def format_input(self, input: Dict[str, Any]) -&gt; StructuredInput:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        if not isinstance(input[\"instruction\"], str):\n            raise ValueError(\n                f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n            )\n\n        messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n        if self.use_system_prompt:\n            if \"system_prompt\" in input:\n                messages.insert(\n                    0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n                )\n            else:\n                warnings.warn(\n                    \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n        return (messages, input.get(\"grammar\", None))  # type: ignore\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`. Note that even\n        if the `grammar` is defined to produce a JSON schema, this method will return the raw\n        output i.e. a string without any parsing.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_generation/#distilabel.steps.tasks.structured_generation.StructuredGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>instruction</code> and the <code>grammar</code>. Optionally, if the <code>use_system_prompt</code> flag is set to True, then the <code>system_prompt</code> will be used too.</p>"},{"location":"reference/distilabel/steps/tasks/structured_generation/#distilabel.steps.tasks.structured_generation.StructuredGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/structured_generation/#distilabel.steps.tasks.structured_generation.StructuredGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; StructuredInput:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    if not isinstance(input[\"instruction\"], str):\n        raise ValueError(\n            f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n        )\n\n    messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n    if self.use_system_prompt:\n        if \"system_prompt\" in input:\n            messages.insert(\n                0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n            )\n        else:\n            warnings.warn(\n                \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    return (messages, input.get(\"grammar\", None))  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_generation/#distilabel.steps.tasks.structured_generation.StructuredGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>. Note that even if the <code>grammar</code> is defined to produce a JSON schema, this method will return the raw output i.e. a string without any parsing.</p> Source code in <code>src/distilabel/steps/tasks/structured_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`. Note that even\n    if the `grammar` is defined to produce a JSON schema, this method will return the raw\n    output i.e. a string without any parsing.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/text_generation/","title":"Text generation","text":""},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.ChatGeneration","title":"<code>ChatGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Generates text based on a conversation.</p> <p><code>ChatGeneration</code> is a pre-defined task that defines the <code>messages</code> as the input and <code>generation</code> as the output. This task is used to generate text based on a conversation. The <code>model_name</code> is also returned as part of the output in order to enhance it.</p> Input columns <ul> <li>messages (<code>List[Dict[Literal[\"role\", \"content\"], str]]</code>): The messages to generate the     follow up completion from.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text from the assistant.</li> <li>model_name (<code>str</code>): The model name used to generate the text.</li> </ul> Categories <ul> <li>chat-generation</li> </ul> Icon <p><code>:material-chat:</code></p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>class ChatGeneration(Task):\n    \"\"\"Generates text based on a conversation.\n\n    `ChatGeneration` is a pre-defined task that defines the `messages` as the input\n    and `generation` as the output. This task is used to generate text based on a conversation.\n    The `model_name` is also returned as part of the output in order to enhance it.\n\n    Input columns:\n        - messages (`List[Dict[Literal[\"role\", \"content\"], str]]`): The messages to generate the\n            follow up completion from.\n\n    Output columns:\n        - generation (`str`): The generated text from the assistant.\n        - model_name (`str`): The model name used to generate the text.\n\n    Categories:\n        - chat-generation\n\n    Icon:\n        `:material-chat:`\n    \"\"\"\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `messages`.\"\"\"\n        return [\"messages\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the messages provided\n        are already formatted that way i.e. following the OpenAI chat format.\"\"\"\n\n        if not is_openai_format(input[\"messages\"]):\n            raise ValueError(\n                \"Input `instruction` must be a string or an OpenAI chat-like format. \"\n                f\"Got: {input['messages']}. Please check: 'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n            )\n\n        if input[\"messages\"][-1][\"role\"] != \"user\":\n            raise ValueError(\n                \"The last message must be from the user. Please check: \"\n                \"'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n            )\n\n        return input[\"messages\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.ChatGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>messages</code>.</p>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.ChatGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.ChatGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the messages provided are already formatted that way i.e. following the OpenAI chat format.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the messages provided\n    are already formatted that way i.e. following the OpenAI chat format.\"\"\"\n\n    if not is_openai_format(input[\"messages\"]):\n        raise ValueError(\n            \"Input `instruction` must be a string or an OpenAI chat-like format. \"\n            f\"Got: {input['messages']}. Please check: 'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n        )\n\n    if input[\"messages\"][-1][\"role\"] != \"user\":\n        raise ValueError(\n            \"The last message must be from the user. Please check: \"\n            \"'https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models'.\"\n        )\n\n    return input[\"messages\"]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.ChatGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.TextGeneration","title":"<code>TextGeneration</code>","text":"<p>               Bases: <code>Task</code></p> <p>Simple text generation with an <code>LLM</code> given an instruction.</p> <p><code>TextGeneration</code> is a pre-defined task that defines the <code>instruction</code> as the input and <code>generation</code> as the output. This task is used to generate text based on the input instruction. The model_name is also returned as part of the output in order to enhance it.</p> <p>Attributes:</p> Name Type Description <code>use_system_prompt</code> <code>bool</code> <p>Whether to use the system prompt in the generation. Defaults to <code>True</code>, which means that if the column <code>system_prompt</code> is defined within the input batch, then the <code>system_prompt</code> will be used, otherwise, it will be ignored.</p> Input columns <ul> <li>instruction (<code>str</code>): The instruction to generate text from.</li> </ul> Output columns <ul> <li>generation (<code>str</code>): The generated text.</li> <li>model_name (<code>str</code>): The name of the model used to generate the text.</li> </ul> Categories <ul> <li>text-generation</li> </ul> <p>Examples:</p> <pre><code>from distilabel.steps.tasks import TextGeneration\n\ntask = TextGeneration(llm=LLM(...))\n</code></pre> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>class TextGeneration(Task):\n    \"\"\"Simple text generation with an `LLM` given an instruction.\n\n    `TextGeneration` is a pre-defined task that defines the `instruction` as the input\n    and `generation` as the output. This task is used to generate text based on the input\n    instruction. The model_name is also returned as part of the output in order to enhance it.\n\n    Attributes:\n        use_system_prompt: Whether to use the system prompt in the generation. Defaults to `True`,\n            which means that if the column `system_prompt` is defined within the input batch, then\n            the `system_prompt` will be used, otherwise, it will be ignored.\n\n    Input columns:\n        - instruction (`str`): The instruction to generate text from.\n\n    Output columns:\n        - generation (`str`): The generated text.\n        - model_name (`str`): The name of the model used to generate the text.\n\n    Categories:\n        - text-generation\n\n    Examples:\n        ```python\n        from distilabel.steps.tasks import TextGeneration\n\n        task = TextGeneration(llm=LLM(...))\n        ```\n    \"\"\"\n\n    use_system_prompt: bool = True\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`.\"\"\"\n        return [\"instruction\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n\n        if is_openai_format(input[\"instruction\"]):\n            raise ValueError(\n                \"Providing `instruction` formatted as an OpenAI chat / conversation is\"\n                \" deprecated, you should use `ChatGeneration` with `messages` as input instead.\",\n            )\n\n        if not isinstance(input[\"instruction\"], str):\n            raise ValueError(\n                f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n            )\n\n        messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n        if self.use_system_prompt:\n            if \"system_prompt\" in input:\n                messages.insert(\n                    0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n                )\n            else:\n                warnings.warn(\n                    \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        return messages  # type: ignore\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        return [\"generation\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\"\"\"\n        return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.TextGeneration.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>.</p>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.TextGeneration.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.TextGeneration.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n\n    if is_openai_format(input[\"instruction\"]):\n        raise ValueError(\n            \"Providing `instruction` formatted as an OpenAI chat / conversation is\"\n            \" deprecated, you should use `ChatGeneration` with `messages` as input instead.\",\n        )\n\n    if not isinstance(input[\"instruction\"], str):\n        raise ValueError(\n            f\"Input `instruction` must be a string. Got: {input['instruction']}.\"\n        )\n\n    messages = [{\"role\": \"user\", \"content\": input[\"instruction\"]}]\n    if self.use_system_prompt:\n        if \"system_prompt\" in input:\n            messages.insert(\n                0, {\"role\": \"system\", \"content\": input[\"system_prompt\"]}\n            )\n        else:\n            warnings.warn(\n                \"`use_system_prompt` is set to `True`, but no `system_prompt` in input batch, so it will be ignored.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    return messages  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/text_generation/#distilabel.steps.tasks.text_generation.TextGeneration.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>generation</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> Source code in <code>src/distilabel/steps/tasks/text_generation.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `generation`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\"\"\"\n    return {\"generation\": output}\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/typing/","title":"Typing","text":""},{"location":"reference/distilabel/steps/tasks/typing/#distilabel.steps.tasks.typing.ChatType","title":"<code>ChatType = List[ChatItem]</code>  <code>module-attribute</code>","text":"<p>ChatType is a type alias for a <code>list</code> of <code>dict</code>s following the OpenAI conversational format.</p>"},{"location":"reference/distilabel/steps/tasks/typing/#distilabel.steps.tasks.typing.FormattedInput","title":"<code>FormattedInput = Union[StandardInput, StructuredInput]</code>  <code>module-attribute</code>","text":"<p>FormattedInput is an alias for the union of <code>StandardInput</code> and <code>StructuredInput</code> as generated by <code>format_input</code> and expected by the <code>LLM</code>s.</p>"},{"location":"reference/distilabel/steps/tasks/typing/#distilabel.steps.tasks.typing.StandardInput","title":"<code>StandardInput = ChatType</code>  <code>module-attribute</code>","text":"<p>StandardInput is an alias for ChatType that defines the default / standard input produced by <code>format_input</code>.</p>"},{"location":"reference/distilabel/steps/tasks/typing/#distilabel.steps.tasks.typing.StructuredInput","title":"<code>StructuredInput = Tuple[StandardInput, Union[Grammar, None]]</code>  <code>module-attribute</code>","text":"<p>StructuredInput defines a type produced by <code>format_input</code> when using either <code>StructuredGeneration</code> or a subclass of it.</p>"},{"location":"reference/distilabel/steps/tasks/ultrafeedback/","title":"Ultrafeedback","text":""},{"location":"reference/distilabel/steps/tasks/ultrafeedback/#distilabel.steps.tasks.ultrafeedback.UltraFeedback","title":"<code>UltraFeedback</code>","text":"<p>               Bases: <code>Task</code></p> <p>Rank generations focusing on different aspects using an <code>LLM</code>.</p> <p>UltraFeedback: Boosting Language Models with High-quality Feedback.</p> <p>Attributes:</p> Name Type Description <code>aspect</code> <code>Literal['helpfulness', 'honesty', 'instruction-following', 'truthfulness', 'overall-rating']</code> <p>The aspect to perform with the <code>UltraFeedback</code> model. The available aspects are: - <code>helpfulness</code>: Evaluate text outputs based on helpfulness. - <code>honesty</code>: Evaluate text outputs based on honesty. - <code>instruction-following</code>: Evaluate text outputs based on given instructions. - <code>truthfulness</code>: Evaluate text outputs based on truthfulness. Additionally, a custom aspect has been defined by Argilla, so as to evaluate the overall assessment of the text outputs within a single prompt. The custom aspect is: - <code>overall-rating</code>: Evaluate text outputs based on an overall assessment.</p> Input columns <ul> <li>instruction (<code>str</code>): The reference instruction to evaluate the text outputs.</li> <li>generations (<code>List[str]</code>): The text outputs to evaluate for the given instruction.</li> </ul> Output columns <ul> <li>ratings (<code>List[float]</code>): The ratings for each of the provided text outputs.</li> <li>rationales (<code>List[str]</code>): The rationales for each of the provided text outputs.</li> <li>model_name (<code>str</code>): The name of the model used to generate the ratings and rationales.</li> </ul> Categories <ul> <li>preference</li> </ul> References <ul> <li><code>UltraFeedback: Boosting Language Models with High-quality Feedback</code></li> <li><code>UltraFeedback - GitHub Repository</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>class UltraFeedback(Task):\n    \"\"\"Rank generations focusing on different aspects using an `LLM`.\n\n    UltraFeedback: Boosting Language Models with High-quality Feedback.\n\n    Attributes:\n        aspect: The aspect to perform with the `UltraFeedback` model. The available aspects are:\n            - `helpfulness`: Evaluate text outputs based on helpfulness.\n            - `honesty`: Evaluate text outputs based on honesty.\n            - `instruction-following`: Evaluate text outputs based on given instructions.\n            - `truthfulness`: Evaluate text outputs based on truthfulness.\n            Additionally, a custom aspect has been defined by Argilla, so as to evaluate the overall\n            assessment of the text outputs within a single prompt. The custom aspect is:\n            - `overall-rating`: Evaluate text outputs based on an overall assessment.\n\n    Input columns:\n        - instruction (`str`): The reference instruction to evaluate the text outputs.\n        - generations (`List[str]`): The text outputs to evaluate for the given instruction.\n\n    Output columns:\n        - ratings (`List[float]`): The ratings for each of the provided text outputs.\n        - rationales (`List[str]`): The rationales for each of the provided text outputs.\n        - model_name (`str`): The name of the model used to generate the ratings and rationales.\n\n    Categories:\n        - preference\n\n    References:\n        - [`UltraFeedback: Boosting Language Models with High-quality Feedback`](https://arxiv.org/abs/2310.01377)\n        - [`UltraFeedback - GitHub Repository`](https://github.com/OpenBMB/UltraFeedback)\n    \"\"\"\n\n    aspect: Literal[\n        \"helpfulness\",\n        \"honesty\",\n        \"instruction-following\",\n        \"truthfulness\",\n        # Custom aspects\n        \"overall-rating\",\n    ]\n\n    _system_prompt: str = PrivateAttr(\n        default=(\n            \"Your role is to evaluate text quality based on given criteria.\\n\"\n            'You\\'ll receive an instructional description (\"Instruction\") and {no_texts} text outputs (\"Text\").\\n'\n            \"Understand and interpret instructions to evaluate effectively.\\n\"\n            \"Provide annotations for each text with a rating and rationale.\\n\"\n            \"The {no_texts} texts given are independent, and should be evaluated separately.\\n\"\n        )\n    )\n    _template: Optional[\"Template\"] = PrivateAttr(default=...)\n\n    def load(self) -&gt; None:\n        \"\"\"Loads the Jinja2 template for the given `aspect`.\"\"\"\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"ultrafeedback\"\n            / f\"{self.aspect}.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`, and the `generations` for it.\"\"\"\n        return [\"instruction\", \"generations\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation.\"\"\"\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": self._system_prompt.format(\n                    no_texts=len(input[\"generations\"])\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": self._template.render(  # type: ignore\n                    instruction=input[\"instruction\"], generations=input[\"generations\"]\n                ),\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task is the `generation` and the `model_name`.\"\"\"\n        columns = []\n        if self.aspect in [\"honesty\", \"instruction-following\", \"overall-rating\"]:\n            columns = [\"ratings\", \"rationales\"]\n        elif self.aspect in [\"helpfulness\", \"truthfulness\"]:\n            columns = [\"types\", \"rationales\", \"ratings\", \"rationales-for-ratings\"]\n        return columns + [\"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output is formatted as a dictionary with the `ratings` and `rationales` for\n        each of the provided `generations` for the given `instruction`. The `model_name`\n        will be automatically included within the `process` method of `Task`.\n\n        Args:\n            output: a string representing the output of the LLM via the `process` method.\n            input: the input to the task, as required by some tasks to format the output.\n\n        Returns:\n            A dictionary containing either the `ratings` and `rationales` for each of the provided\n            `generations` for the given `instruction` if the provided aspect is either `honesty`,\n            `instruction-following`, or `overall-rating`; or the `types`, `rationales`,\n            `ratings`, and `rationales-for-ratings` for each of the provided `generations` for the\n            given `instruction` if the provided aspect is either `helpfulness` or `truthfulness`.\n        \"\"\"\n        if self.aspect in [\n            \"honesty\",\n            \"instruction-following\",\n            \"overall-rating\",\n        ]:\n            return self._format_ratings_rationales_output(output, input)\n        return self._format_types_ratings_rationales_output(output, input)\n\n    def _format_ratings_rationales_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, List[Any]]:\n        \"\"\"Formats the output when the aspect is either `honesty`, `instruction-following`, or `overall-rating`.\"\"\"\n        if output is None:\n            return {\n                \"ratings\": [None] * len(input[\"generations\"]),\n                \"rationales\": [None] * len(input[\"generations\"]),\n            }\n\n        pattern = r\"Rating: (.+?)\\nRationale: (.+)\"\n        sections = output.split(\"\\n\\n\")\n\n        formatted_outputs = []\n        for section in sections:\n            matches = None\n            if section is not None and section != \"\":\n                matches = re.search(pattern, section, re.DOTALL)\n            if not matches:\n                formatted_outputs.append({\"ratings\": None, \"rationales\": None})\n                continue\n\n            formatted_outputs.append(\n                {\n                    \"ratings\": int(re.findall(r\"\\b\\d+\\b\", matches.group(1))[0])\n                    if matches.group(1) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales\": matches.group(2),\n                }\n            )\n        return combine_dicts(*formatted_outputs)\n\n    def _format_types_ratings_rationales_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, List[Any]]:\n        \"\"\"Formats the output when the aspect is either `helpfulness` or `truthfulness`.\"\"\"\n        if output is None:\n            return {\n                \"types\": [None] * len(input[\"generations\"]),\n                \"rationales\": [None] * len(input[\"generations\"]),\n                \"ratings\": [None] * len(input[\"generations\"]),\n                \"rationales-for-ratings\": [None] * len(input[\"generations\"]),\n            }\n\n        pattern = r\"Type: (.+?)\\nRationale: (.+?)\\nRating: (.+?)\\nRationale: (.+)\"\n\n        sections = output.split(\"\\n\\n\")\n\n        formatted_outputs = []\n        for section in sections:\n            matches = None\n            if section is not None and section != \"\":\n                matches = re.search(pattern, section, re.DOTALL)\n            if not matches:\n                formatted_outputs.append(\n                    {\n                        \"types\": None,\n                        \"rationales\": None,\n                        \"ratings\": None,\n                        \"rationales-for-ratings\": None,\n                    }\n                )\n                continue\n\n            formatted_outputs.append(\n                {\n                    \"types\": int(re.findall(r\"\\b\\d+\\b\", matches.group(1))[0])\n                    if matches.group(1) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales\": matches.group(2),\n                    \"ratings\": int(re.findall(r\"\\b\\d+\\b\", matches.group(3))[0])\n                    if matches.group(3) not in [\"None\", \"N/A\"]\n                    else None,\n                    \"rationales-for-ratings\": matches.group(4),\n                }\n            )\n        return combine_dicts(*formatted_outputs)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/ultrafeedback/#distilabel.steps.tasks.ultrafeedback.UltraFeedback.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>, and the <code>generations</code> for it.</p>"},{"location":"reference/distilabel/steps/tasks/ultrafeedback/#distilabel.steps.tasks.ultrafeedback.UltraFeedback.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task is the <code>generation</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/ultrafeedback/#distilabel.steps.tasks.ultrafeedback.UltraFeedback.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation.\"\"\"\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": self._system_prompt.format(\n                no_texts=len(input[\"generations\"])\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": self._template.render(  # type: ignore\n                instruction=input[\"instruction\"], generations=input[\"generations\"]\n            ),\n        },\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/ultrafeedback/#distilabel.steps.tasks.ultrafeedback.UltraFeedback.format_output","title":"<code>format_output(output, input)</code>","text":"<p>The output is formatted as a dictionary with the <code>ratings</code> and <code>rationales</code> for each of the provided <code>generations</code> for the given <code>instruction</code>. The <code>model_name</code> will be automatically included within the <code>process</code> method of <code>Task</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, None]</code> <p>a string representing the output of the LLM via the <code>process</code> method.</p> required <code>input</code> <code>Dict[str, Any]</code> <p>the input to the task, as required by some tasks to format the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing either the <code>ratings</code> and <code>rationales</code> for each of the provided</p> <code>Dict[str, Any]</code> <p><code>generations</code> for the given <code>instruction</code> if the provided aspect is either <code>honesty</code>,</p> <code>Dict[str, Any]</code> <p><code>instruction-following</code>, or <code>overall-rating</code>; or the <code>types</code>, <code>rationales</code>,</p> <code>Dict[str, Any]</code> <p><code>ratings</code>, and <code>rationales-for-ratings</code> for each of the provided <code>generations</code> for the</p> <code>Dict[str, Any]</code> <p>given <code>instruction</code> if the provided aspect is either <code>helpfulness</code> or <code>truthfulness</code>.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def format_output(\n    self, output: Union[str, None], input: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"The output is formatted as a dictionary with the `ratings` and `rationales` for\n    each of the provided `generations` for the given `instruction`. The `model_name`\n    will be automatically included within the `process` method of `Task`.\n\n    Args:\n        output: a string representing the output of the LLM via the `process` method.\n        input: the input to the task, as required by some tasks to format the output.\n\n    Returns:\n        A dictionary containing either the `ratings` and `rationales` for each of the provided\n        `generations` for the given `instruction` if the provided aspect is either `honesty`,\n        `instruction-following`, or `overall-rating`; or the `types`, `rationales`,\n        `ratings`, and `rationales-for-ratings` for each of the provided `generations` for the\n        given `instruction` if the provided aspect is either `helpfulness` or `truthfulness`.\n    \"\"\"\n    if self.aspect in [\n        \"honesty\",\n        \"instruction-following\",\n        \"overall-rating\",\n    ]:\n        return self._format_ratings_rationales_output(output, input)\n    return self._format_types_ratings_rationales_output(output, input)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/ultrafeedback/#distilabel.steps.tasks.ultrafeedback.UltraFeedback.load","title":"<code>load()</code>","text":"<p>Loads the Jinja2 template for the given <code>aspect</code>.</p> Source code in <code>src/distilabel/steps/tasks/ultrafeedback.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the Jinja2 template for the given `aspect`.\"\"\"\n    super().load()\n\n    _path = str(\n        importlib_resources.files(\"distilabel\")\n        / \"steps\"\n        / \"tasks\"\n        / \"templates\"\n        / \"ultrafeedback\"\n        / f\"{self.aspect}.jinja2\"\n    )\n\n    self._template = Template(open(_path).read())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/","title":"Index","text":""},{"location":"reference/distilabel/steps/tasks/evol_instruct/base/","title":"Base","text":""},{"location":"reference/distilabel/steps/tasks/evol_instruct/base/#distilabel.steps.tasks.evol_instruct.base.EvolInstruct","title":"<code>EvolInstruct</code>","text":"<p>               Bases: <code>Task</code></p> <p>Evolve instructions using an <code>LLM</code>.</p> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> <p>Attributes:</p> Name Type Description <code>num_evolutions</code> <code>int</code> <p>The number of evolutions to be performed.</p> <code>store_evolutions</code> <code>bool</code> <p>Whether to store all the evolutions or just the last one. Defaults to <code>False</code>.</p> <code>generate_answers</code> <code>bool</code> <p>Whether to generate answers for the evolved instructions. Defaults to <code>False</code>.</p> <code>include_original_instruction</code> <code>bool</code> <p>Whether to include the original instruction in the <code>evolved_instructions</code> output column. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for evolving the instructions. Defaults to the ones provided in the <code>utils.py</code> file.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction to evolve.</li> </ul> Output columns <ul> <li>evolved_instruction (<code>str</code>): The evolved instruction if <code>store_evolutions=False</code>.</li> <li>evolved_instructions (<code>List[str]</code>): The evolved instructions if <code>store_evolutions=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> <li>answer (<code>str</code>): The answer to the evolved instruction if <code>generate_answers=True</code>     and <code>store_evolutions=False</code>.</li> <li>answers (<code>List[str]</code>): The answers to the evolved instructions if <code>generate_answers=True</code>     and <code>store_evolutions=True</code>.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> </ul> References <ul> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> <li>GitHub: h2oai/h2o-wizardlm</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>class EvolInstruct(Task):\n    \"\"\"Evolve instructions using an `LLM`.\n\n    WizardLM: Empowering Large Language Models to Follow Complex Instructions\n\n    Attributes:\n        num_evolutions: The number of evolutions to be performed.\n        store_evolutions: Whether to store all the evolutions or just the last one. Defaults\n            to `False`.\n        generate_answers: Whether to generate answers for the evolved instructions. Defaults\n            to `False`.\n        include_original_instruction: Whether to include the original instruction in the\n            `evolved_instructions` output column. Defaults to `False`.\n        mutation_templates: The mutation templates to be used for evolving the instructions.\n            Defaults to the ones provided in the `utils.py` file.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Input columns:\n        - instruction (`str`): The instruction to evolve.\n\n    Output columns:\n        - evolved_instruction (`str`): The evolved instruction if `store_evolutions=False`.\n        - evolved_instructions (`List[str]`): The evolved instructions if `store_evolutions=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n        - answer (`str`): The answer to the evolved instruction if `generate_answers=True`\n            and `store_evolutions=False`.\n        - answers (`List[str]`): The answers to the evolved instructions if `generate_answers=True`\n            and `store_evolutions=True`.\n\n    Categories:\n        - evol\n        - instruction\n\n    References:\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n        - [GitHub: h2oai/h2o-wizardlm](https://github.com/h2oai/h2o-wizardlm)\n    \"\"\"\n\n    num_evolutions: int\n    store_evolutions: bool = False\n    generate_answers: bool = False\n    include_original_instruction: bool = False\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to seed a random seed.\",\n    )\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task is the `instruction`.\"\"\"\n        return [\"instruction\"]\n\n    def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation. And the\n        `system_prompt` is added as the first message if it exists.\"\"\"\n        return [{\"role\": \"user\", \"content\": input}]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `evolved_instruction/s`, the `answer` if `generate_answers=True`\n        and the `model_name`.\"\"\"\n        # TODO: having to define a `model_name` column every time as the `Task.outputs` is not ideal,\n        # this could be handled always and the value could be included within the DAG validation when\n        # a `Task` is used, since all the `Task` subclasses will have an `llm` with a `model_name` attr.\n        _outputs = [\n            (\n                \"evolved_instruction\"\n                if not self.store_evolutions\n                else \"evolved_instructions\"\n            ),\n            \"model_name\",\n        ]\n        if self.generate_answers:\n            _outputs.append(\"answer\" if not self.store_evolutions else \"answers\")\n        return _outputs\n\n    @override\n    def format_output(  # type: ignore\n        self, instructions: Union[str, List[str]], answers: Optional[List[str]] = None\n    ) -&gt; Dict[str, Any]:  # type: ignore\n        \"\"\"The output for the task is a dict with: `evolved_instruction` or `evolved_instructions`,\n        depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n        `answer` if `generate_answers=True`; and, finally, the `model_name`.\n\n        Args:\n            instructions: The instructions to be included within the output.\n            answers: The answers to be included within the output if `generate_answers=True`.\n\n        Returns:\n            If `store_evolutions=False` and `generate_answers=True` return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};\n            if `store_evolutions=True` and `generate_answers=True` return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};\n            if `store_evolutions=False` and `generate_answers=False` return {\"evolved_instruction\": ..., \"model_name\": ...};\n            if `store_evolutions=True` and `generate_answers=False` return {\"evolved_instructions\": ..., \"model_name\": ...}.\n        \"\"\"\n        _output = {}\n        if not self.store_evolutions:\n            _output[\"evolved_instruction\"] = instructions[-1]\n        else:\n            _output[\"evolved_instructions\"] = instructions\n\n        if self.generate_answers and answers:\n            if not self.store_evolutions:\n                _output[\"answer\"] = answers[-1]\n            else:\n                _output[\"answers\"] = answers\n\n        _output[\"model_name\"] = self.llm.model_name\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates`.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, instruction: str) -&gt; str:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            instruction: The instruction to be included within the mutation prompt.\n\n        Returns:\n            A random mutation prompt with the provided instruction.\n        \"\"\"\n        mutation = np.random.choice(self.mutation_templates_names)\n        return self.mutation_templates[mutation].replace(\"&lt;PROMPT&gt;\", instruction)  # type: ignore\n\n    def _evolve_instructions(self, inputs: \"StepInput\") -&gt; List[List[str]]:\n        \"\"\"Evolves the instructions provided as part of the inputs of the task.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list where each item is a list with either the last evolved instruction if\n            `store_evolutions=False` or all the evolved instructions if `store_evolutions=True`.\n        \"\"\"\n\n        instructions: List[List[str]] = [[input[\"instruction\"]] for input in inputs]\n\n        for iter_no in range(self.num_evolutions):\n            formatted_prompts = []\n            for instruction in instructions:\n                formatted_prompts.append(self._apply_random_mutation(instruction[-1]))\n\n            formatted_prompts = [\n                self.format_input(prompt) for prompt in formatted_prompts\n            ]\n            generated_prompts = flatten_responses(\n                self.llm.generate(\n                    formatted_prompts,\n                    **self.llm.generation_kwargs,  # type: ignore\n                )\n            )\n\n            evolved_instructions = []\n            for generated_prompt in generated_prompts:\n                generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n                evolved_instructions.append(generated_prompt)\n\n            if self.store_evolutions:\n                instructions = [\n                    instruction + [evolved_instruction]\n                    for instruction, evolved_instruction in zip(\n                        instructions, evolved_instructions\n                    )\n                ]\n            else:\n                instructions = [\n                    [evolved_instruction]\n                    for evolved_instruction in evolved_instructions\n                ]\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} evolving {len(instructions)} instructions!\"\n            )\n\n        return instructions\n\n    def _generate_answers(\n        self, evolved_instructions: List[List[str]]\n    ) -&gt; List[List[str]]:\n        \"\"\"Generates the answer for the instructions in `instructions`.\n\n        Args:\n            evolved_instructions: A list of lists where each item is a list with either the last\n                evolved instruction if `store_evolutions=False` or all the evolved instructions\n                if `store_evolutions=True`.\n\n        Returns:\n            A list of answers for each instruction.\n        \"\"\"\n        formatted_instructions = [\n            self.format_input(instruction)\n            for instructions in evolved_instructions\n            for instruction in instructions\n        ]\n\n        responses = self.llm.generate(\n            formatted_instructions,\n            num_generations=1,\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n\n        step = (\n            self.num_evolutions\n            if not self.include_original_instruction\n            else self.num_evolutions + 1\n        )\n        return [\n            flatten_responses(responses[i : i + step])\n            for i in range(0, len(responses), step)\n        ]\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        evolved_instructions = self._evolve_instructions(inputs)\n\n        if self.store_evolutions:\n            # Remove the input instruction from the `evolved_instructions` list\n            from_ = 1 if not self.include_original_instruction else 0\n            evolved_instructions = [\n                instruction[from_:] for instruction in evolved_instructions\n            ]\n\n        if not self.generate_answers:\n            for input, instruction in zip(inputs, evolved_instructions):\n                input.update(self.format_output(instruction))\n            yield inputs\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished evolving {len(evolved_instructions)} instructions!\"\n        )\n\n        if self.generate_answers:\n            self._logger.info(\n                f\"\ud83e\udde0 Generating answers for the {len(evolved_instructions)} evolved instructions!\"\n            )\n\n            answers = self._generate_answers(evolved_instructions)\n\n            self._logger.info(\n                f\"\ud83c\udf89 Finished generating answers for the {len(evolved_instructions)} evolved\"\n                \" instructions!\"\n            )\n\n            for idx, (input, instruction) in enumerate(\n                zip(inputs, evolved_instructions)\n            ):\n                input.update(self.format_output(instruction, answers[idx]))\n            yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/base/#distilabel.steps.tasks.evol_instruct.base.EvolInstruct.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task is the <code>instruction</code>.</p>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/base/#distilabel.steps.tasks.evol_instruct.base.EvolInstruct.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code>.</p>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/base/#distilabel.steps.tasks.evol_instruct.base.EvolInstruct.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>evolved_instruction/s</code>, the <code>answer</code> if <code>generate_answers=True</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/base/#distilabel.steps.tasks.evol_instruct.base.EvolInstruct.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation. And the <code>system_prompt</code> is added as the first message if it exists.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation. And the\n    `system_prompt` is added as the first message if it exists.\"\"\"\n    return [{\"role\": \"user\", \"content\": input}]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/base/#distilabel.steps.tasks.evol_instruct.base.EvolInstruct.format_output","title":"<code>format_output(instructions, answers=None)</code>","text":"<p>The output for the task is a dict with: <code>evolved_instruction</code> or <code>evolved_instructions</code>, depending whether the value is either <code>False</code> or <code>True</code> for <code>store_evolutions</code>, respectively; <code>answer</code> if <code>generate_answers=True</code>; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>Union[str, List[str]]</code> <p>The instructions to be included within the output.</p> required <code>answers</code> <code>Optional[List[str]]</code> <p>The answers to be included within the output if <code>generate_answers=True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>If <code>store_evolutions=False</code> and <code>generate_answers=True</code> return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> and <code>generate_answers=True</code> return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=False</code> and <code>generate_answers=False</code> return {\"evolved_instruction\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> and <code>generate_answers=False</code> return {\"evolved_instructions\": ..., \"model_name\": ...}.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>@override\ndef format_output(  # type: ignore\n    self, instructions: Union[str, List[str]], answers: Optional[List[str]] = None\n) -&gt; Dict[str, Any]:  # type: ignore\n    \"\"\"The output for the task is a dict with: `evolved_instruction` or `evolved_instructions`,\n    depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n    `answer` if `generate_answers=True`; and, finally, the `model_name`.\n\n    Args:\n        instructions: The instructions to be included within the output.\n        answers: The answers to be included within the output if `generate_answers=True`.\n\n    Returns:\n        If `store_evolutions=False` and `generate_answers=True` return {\"evolved_instruction\": ..., \"model_name\": ..., \"answer\": ...};\n        if `store_evolutions=True` and `generate_answers=True` return {\"evolved_instructions\": ..., \"model_name\": ..., \"answer\": ...};\n        if `store_evolutions=False` and `generate_answers=False` return {\"evolved_instruction\": ..., \"model_name\": ...};\n        if `store_evolutions=True` and `generate_answers=False` return {\"evolved_instructions\": ..., \"model_name\": ...}.\n    \"\"\"\n    _output = {}\n    if not self.store_evolutions:\n        _output[\"evolved_instruction\"] = instructions[-1]\n    else:\n        _output[\"evolved_instructions\"] = instructions\n\n    if self.generate_answers and answers:\n        if not self.store_evolutions:\n            _output[\"answer\"] = answers[-1]\n        else:\n            _output[\"answers\"] = answers\n\n    _output[\"model_name\"] = self.llm.model_name\n    return _output\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/base/#distilabel.steps.tasks.evol_instruct.base.EvolInstruct.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Yields:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/base.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    evolved_instructions = self._evolve_instructions(inputs)\n\n    if self.store_evolutions:\n        # Remove the input instruction from the `evolved_instructions` list\n        from_ = 1 if not self.include_original_instruction else 0\n        evolved_instructions = [\n            instruction[from_:] for instruction in evolved_instructions\n        ]\n\n    if not self.generate_answers:\n        for input, instruction in zip(inputs, evolved_instructions):\n            input.update(self.format_output(instruction))\n        yield inputs\n\n    self._logger.info(\n        f\"\ud83c\udf89 Finished evolving {len(evolved_instructions)} instructions!\"\n    )\n\n    if self.generate_answers:\n        self._logger.info(\n            f\"\ud83e\udde0 Generating answers for the {len(evolved_instructions)} evolved instructions!\"\n        )\n\n        answers = self._generate_answers(evolved_instructions)\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished generating answers for the {len(evolved_instructions)} evolved\"\n            \" instructions!\"\n        )\n\n        for idx, (input, instruction) in enumerate(\n            zip(inputs, evolved_instructions)\n        ):\n            input.update(self.format_output(instruction, answers[idx]))\n        yield inputs\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/generator/","title":"Generator","text":""},{"location":"reference/distilabel/steps/tasks/evol_instruct/generator/#distilabel.steps.tasks.evol_instruct.generator.EvolInstructGenerator","title":"<code>EvolInstructGenerator</code>","text":"<p>               Bases: <code>GeneratorTask</code></p> <p>Generate evolved instructions using an <code>LLM</code>.</p> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <code>int</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <code>bool</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>RuntimeParameter[int]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>RuntimeParameter[int]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs     to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs     to be lower than, to be considered valid.</li> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Output columns <ul> <li>instruction (<code>str</code>): The generated instruction if <code>generate_answers=False</code>.</li> <li>answer (<code>str</code>): The generated answer if <code>generate_answers=True</code>.</li> <li>instructions (<code>List[str]</code>): The generated instructions if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to generate and evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>generation</li> </ul> References <ul> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> <li>GitHub: h2oai/h2o-wizardlm</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>class EvolInstructGenerator(GeneratorTask):\n    \"\"\"Generate evolved instructions using an `LLM`.\n\n    WizardLM: Empowering Large Language Models to Follow Complex Instructions\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs\n            to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs\n            to be lower than, to be considered valid.\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Output columns:\n        - instruction (`str`): The generated instruction if `generate_answers=False`.\n        - answer (`str`): The generated answer if `generate_answers=True`.\n        - instructions (`List[str]`): The generated instructions if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to generate and evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - generation\n\n    References:\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n        - [GitHub: h2oai/h2o-wizardlm](https://github.com/h2oai/h2o-wizardlm)\n    \"\"\"\n\n    num_instructions: int\n    generate_answers: bool = False\n    mutation_templates: Dict[str, str] = GENERATION_MUTATION_TEMPLATES\n\n    min_length: RuntimeParameter[int] = Field(\n        default=512,\n        description=\"Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\",\n    )\n    max_length: RuntimeParameter[int] = Field(\n        default=1024,\n        description=\"Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\",\n    )\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to seed a random seed.\",\n    )\n    _seed_texts: Optional[List[str]] = PrivateAttr(default_factory=list)\n    _prompts: Optional[List[str]] = PrivateAttr(default_factory=list)\n\n    def _generate_seed_texts(self) -&gt; List[str]:\n        \"\"\"Generates a list of seed texts to be used as part of the starting prompts for the task.\n\n        It will use the `FRESH_START` mutation template, as it needs to generate text from scratch; and\n        a list of English words will be used to generate the seed texts that will be provided to the\n        mutation method and included within the prompt.\n\n        Returns:\n            A list of seed texts to be used as part of the starting prompts for the task.\n        \"\"\"\n        seed_texts = []\n        for _ in range(self.num_instructions * 10):\n            num_words = np.random.choice([1, 2, 3, 4])\n            seed_texts.append(\n                self.mutation_templates[\"FRESH_START\"].replace(  # type: ignore\n                    \"&lt;PROMPT&gt;\",\n                    \", \".join(\n                        [\n                            np.random.choice(self._english_nouns).strip()\n                            for _ in range(num_words)\n                        ]\n                    ),\n                )\n            )\n        return seed_texts\n\n    @override\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n        This is useful if you want to do some validation that requires the entire model to be initialized.\n        \"\"\"\n        super().model_post_init(__context)\n\n        np.random.seed(self.seed)\n\n        self._seed_texts = self._generate_seed_texts()\n        self._prompts = [\n            np.random.choice(self._seed_texts) for _ in range(self.num_instructions)\n        ]\n\n    @cached_property\n    def _english_nouns(self) -&gt; List[str]:\n        \"\"\"A list of English nouns to be used as part of the starting prompts for the task.\n\n        References:\n            - https://github.com/h2oai/h2o-wizardlm\n        \"\"\"\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps/tasks/evol_instruct/english_nouns.txt\"\n        )\n        with open(_path, mode=\"r\") as f:\n            return [line.strip() for line in f.readlines()]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `instruction`, the `answer` if `generate_answers=True`\n        and the `model_name`.\"\"\"\n        _outputs = [\"instruction\", \"model_name\"]\n        if self.generate_answers:\n            _outputs.append(\"answer\")\n        return _outputs\n\n    def format_output(  # type: ignore\n        self, instruction: str, answer: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"The output for the task is a dict with: `instruction`; `answer` if `generate_answers=True`;\n        and, finally, the `model_name`.\n\n        Args:\n            instruction: The instruction to be included within the output.\n            answer: The answer to be included within the output if `generate_answers=True`.\n\n        Returns:\n            If `generate_answers=True` return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};\n            if `generate_answers=False` return {\"instruction\": ..., \"model_name\": ...};\n        \"\"\"\n        _output = {\n            \"instruction\": instruction,\n            \"model_name\": self.llm.model_name,\n        }\n        if self.generate_answers and answer is not None:\n            _output[\"answer\"] = answer\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates`.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, iter_no: int) -&gt; List[\"ChatType\"]:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            iter_no: The iteration number to be used to check whether the iteration is the\n                first one i.e. FRESH_START, or not.\n\n        Returns:\n            A random mutation prompt with the provided instruction formatted as an OpenAI conversation.\n        \"\"\"\n        prompts = []\n        for idx in range(self.num_instructions):\n            if (\n                iter_no == 0\n                or \"Write one question or request containing\" in self._prompts[idx]  # type: ignore\n            ):\n                mutation = \"FRESH_START\"\n            else:\n                mutation = np.random.choice(self.mutation_templates_names)\n                if mutation == \"FRESH_START\":\n                    self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n\n            prompt_with_template = (\n                self.mutation_templates[mutation].replace(  # type: ignore\n                    \"&lt;PROMPT&gt;\",\n                    self._prompts[idx],  # type: ignore\n                )  # type: ignore\n                if iter_no != 0\n                else self._prompts[idx]  # type: ignore\n            )\n            prompts.append([{\"role\": \"user\", \"content\": prompt_with_template}])\n        return prompts\n\n    def _generate_answers(self, instructions: List[List[str]]) -&gt; List[str]:\n        \"\"\"Generates the answer for the last instruction in `instructions`.\n\n        Args:\n            instructions: A list of lists where each item is a list with either the last\n                evolved instruction if `store_evolutions=False` or all the evolved instructions\n                if `store_evolutions=True`.\n\n        Returns:\n            A list of answers for the last instruction in `instructions`.\n        \"\"\"\n        # TODO: update to generate answers for all the instructions\n        _formatted_instructions = [\n            [{\"role\": \"user\", \"content\": instruction[-1]}]\n            for instruction in instructions\n        ]\n        responses = self.llm.generate(\n            _formatted_instructions,\n            **self.llm.generation_kwargs,  # type: ignore\n        )\n        return flatten_responses(responses)\n\n    @override\n    def process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            offset: The offset to start the generation from. Defaults to 0.\n\n        Yields:\n            A list of Python dictionaries with the outputs of the task, and a boolean\n            flag indicating whether the task has finished or not i.e. is the last batch.\n        \"\"\"\n        instructions = []\n        mutation_no = 0\n\n        iter_no = 0\n        while len(instructions) &lt; self.num_instructions:\n            prompts = self._apply_random_mutation(iter_no=iter_no)\n\n            generated_prompts = flatten_responses(\n                self.llm.generate(prompts, **self.llm.generation_kwargs)  # type: ignore\n            )\n            for idx, generated_prompt in enumerate(generated_prompts):\n                generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n                if self.max_length &gt;= len(generated_prompt) &gt;= self.min_length:  # type: ignore\n                    instructions.append(generated_prompt)\n                    self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n                else:\n                    self._prompts[idx] = generated_prompt  # type: ignore\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} with {len(instructions)} instructions already evolved!\"\n            )\n            iter_no += 1\n\n            if len(instructions) &gt; self.num_instructions:\n                instructions = instructions[: self.num_instructions]\n            if len(instructions) &gt; mutation_no:\n                mutation_no = len(instructions) - mutation_no\n\n            if not self.generate_answers and len(instructions[-mutation_no:]) &gt; 0:\n                yield (\n                    [\n                        self.format_output(mutated_instruction)\n                        for mutated_instruction in instructions[-mutation_no:]\n                    ],\n                    len(instructions) &gt;= self.num_instructions,\n                )\n\n        self._logger.info(f\"\ud83c\udf89 Finished evolving {len(instructions)} instructions!\")\n\n        if self.generate_answers:\n            self._logger.info(\n                f\"\ud83e\udde0 Generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            answers = self._generate_answers(instructions)\n\n            self._logger.info(\n                f\"\ud83c\udf89 Finished generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            yield (\n                [\n                    self.format_output(instruction, answer)\n                    for instruction, answer in zip(instructions, answers)\n                ],\n                True,\n            )\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/generator/#distilabel.steps.tasks.evol_instruct.generator.EvolInstructGenerator.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code>.</p>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/generator/#distilabel.steps.tasks.evol_instruct.generator.EvolInstructGenerator.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>instruction</code>, the <code>answer</code> if <code>generate_answers=True</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/generator/#distilabel.steps.tasks.evol_instruct.generator.EvolInstructGenerator.format_output","title":"<code>format_output(instruction, answer=None)</code>","text":"<p>The output for the task is a dict with: <code>instruction</code>; <code>answer</code> if <code>generate_answers=True</code>; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction to be included within the output.</p> required <code>answer</code> <code>Optional[str]</code> <p>The answer to be included within the output if <code>generate_answers=True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>If <code>generate_answers=True</code> return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>generate_answers=False</code> return {\"instruction\": ..., \"model_name\": ...};</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>def format_output(  # type: ignore\n    self, instruction: str, answer: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"The output for the task is a dict with: `instruction`; `answer` if `generate_answers=True`;\n    and, finally, the `model_name`.\n\n    Args:\n        instruction: The instruction to be included within the output.\n        answer: The answer to be included within the output if `generate_answers=True`.\n\n    Returns:\n        If `generate_answers=True` return {\"instruction\": ..., \"answer\": ..., \"model_name\": ...};\n        if `generate_answers=False` return {\"instruction\": ..., \"model_name\": ...};\n    \"\"\"\n    _output = {\n        \"instruction\": instruction,\n        \"model_name\": self.llm.model_name,\n    }\n    if self.generate_answers and answer is not None:\n        _output[\"answer\"] = answer\n    return _output\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/generator/#distilabel.steps.tasks.evol_instruct.generator.EvolInstructGenerator.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Override this method to perform additional initialization after <code>__init__</code> and <code>model_construct</code>. This is useful if you want to do some validation that requires the entire model to be initialized.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>@override\ndef model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n    This is useful if you want to do some validation that requires the entire model to be initialized.\n    \"\"\"\n    super().model_post_init(__context)\n\n    np.random.seed(self.seed)\n\n    self._seed_texts = self._generate_seed_texts()\n    self._prompts = [\n        np.random.choice(self._seed_texts) for _ in range(self.num_instructions)\n    ]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/generator/#distilabel.steps.tasks.evol_instruct.generator.EvolInstructGenerator.process","title":"<code>process(offset=0)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The offset to start the generation from. Defaults to 0.</p> <code>0</code> <p>Yields:</p> Type Description <code>GeneratorStepOutput</code> <p>A list of Python dictionaries with the outputs of the task, and a boolean</p> <code>GeneratorStepOutput</code> <p>flag indicating whether the task has finished or not i.e. is the last batch.</p> Source code in <code>src/distilabel/steps/tasks/evol_instruct/generator.py</code> <pre><code>@override\ndef process(self, offset: int = 0) -&gt; \"GeneratorStepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task, and a boolean\n        flag indicating whether the task has finished or not i.e. is the last batch.\n    \"\"\"\n    instructions = []\n    mutation_no = 0\n\n    iter_no = 0\n    while len(instructions) &lt; self.num_instructions:\n        prompts = self._apply_random_mutation(iter_no=iter_no)\n\n        generated_prompts = flatten_responses(\n            self.llm.generate(prompts, **self.llm.generation_kwargs)  # type: ignore\n        )\n        for idx, generated_prompt in enumerate(generated_prompts):\n            generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n            if self.max_length &gt;= len(generated_prompt) &gt;= self.min_length:  # type: ignore\n                instructions.append(generated_prompt)\n                self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n            else:\n                self._prompts[idx] = generated_prompt  # type: ignore\n\n        self._logger.info(\n            f\"\ud83d\udd04 Ran iteration {iter_no} with {len(instructions)} instructions already evolved!\"\n        )\n        iter_no += 1\n\n        if len(instructions) &gt; self.num_instructions:\n            instructions = instructions[: self.num_instructions]\n        if len(instructions) &gt; mutation_no:\n            mutation_no = len(instructions) - mutation_no\n\n        if not self.generate_answers and len(instructions[-mutation_no:]) &gt; 0:\n            yield (\n                [\n                    self.format_output(mutated_instruction)\n                    for mutated_instruction in instructions[-mutation_no:]\n                ],\n                len(instructions) &gt;= self.num_instructions,\n            )\n\n    self._logger.info(f\"\ud83c\udf89 Finished evolving {len(instructions)} instructions!\")\n\n    if self.generate_answers:\n        self._logger.info(\n            f\"\ud83e\udde0 Generating answers for the {len(instructions)} evolved instructions!\"\n        )\n\n        answers = self._generate_answers(instructions)\n\n        self._logger.info(\n            f\"\ud83c\udf89 Finished generating answers for the {len(instructions)} evolved instructions!\"\n        )\n\n        yield (\n            [\n                self.format_output(instruction, answer)\n                for instruction, answer in zip(instructions, answers)\n            ],\n            True,\n        )\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/utils/","title":"Utils","text":""},{"location":"reference/distilabel/steps/tasks/evol_instruct/evol_complexity/","title":"Index","text":""},{"location":"reference/distilabel/steps/tasks/evol_instruct/evol_complexity/base/","title":"Base","text":""},{"location":"reference/distilabel/steps/tasks/evol_instruct/evol_complexity/base/#distilabel.steps.tasks.evol_instruct.evol_complexity.base.EvolComplexity","title":"<code>EvolComplexity</code>","text":"<p>               Bases: <code>EvolInstruct</code></p> <p>Evolve instructions to make them more complex using an <code>LLM</code>.</p> <p><code>EvolComplexity</code> is a task that evolves instructions to make them more complex, and it is based in the EvolInstruct task, but using slight different prompts, but the exact same evolutionary approach.</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>Dict[str, str]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.</li> <li><code>seed</code>: The number of evolutions to be run.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction to evolve.</li> </ul> Output columns <ul> <li>evolved_instruction (<code>str</code>): The evolved instruction.</li> <li>answer (<code>str</code>, optional): The answer to the instruction if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>deita</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/evol_complexity/base.py</code> <pre><code>class EvolComplexity(EvolInstruct):\n    \"\"\"Evolve instructions to make them more complex using an `LLM`.\n\n    `EvolComplexity` is a task that evolves instructions to make them more complex,\n    and it is based in the EvolInstruct task, but using slight different prompts, but the\n    exact same evolutionary approach.\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\n        - `seed`: The number of evolutions to be run.\n\n    Input columns:\n        - instruction (`str`): The instruction to evolve.\n\n    Output columns:\n        - evolved_instruction (`str`): The evolved instruction.\n        - answer (`str`, optional): The answer to the instruction if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - deita\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/evol_complexity/generator/","title":"Generator","text":""},{"location":"reference/distilabel/steps/tasks/evol_instruct/evol_complexity/generator/#distilabel.steps.tasks.evol_instruct.evol_complexity.generator.EvolComplexityGenerator","title":"<code>EvolComplexityGenerator</code>","text":"<p>               Bases: <code>EvolInstructGenerator</code></p> <p>Generate evolved instructions with increased complexity using an <code>LLM</code>.</p> <p><code>EvolComplexityGenerator</code> is a generation task that evolves instructions to make them more complex, and it is based in the EvolInstruct task, but using slight different prompts, but the exact same evolutionary approach.</p> <p>Attributes:</p> Name Type Description <code>num_instructions</code> <p>The number of instructions to be generated.</p> <code>generate_answers</code> <p>Whether to generate answers for the instructions or not. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used for the generation of the instructions.</p> <code>min_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid. Defaults to <code>512</code>.</p> <code>max_length</code> <code>Dict[str, str]</code> <p>Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid. Defaults to <code>1024</code>.</p> <code>seed</code> <code>Dict[str, str]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>min_length</code>: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.</li> <li><code>max_length</code>: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.</li> <li><code>seed</code>: The number of evolutions to be run.</li> </ul> Output columns <ul> <li>instruction (<code>str</code>): The evolved instruction.</li> <li>answer (<code>str</code>, optional): The answer to the instruction if <code>generate_answers=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</li> </ul> Categories <ul> <li>evol</li> <li>instruction</li> <li>generation</li> <li>deita</li> </ul> References <ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> <li>WizardLM: Empowering Large Language Models to Follow Complex Instructions</li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_instruct/evol_complexity/generator.py</code> <pre><code>class EvolComplexityGenerator(EvolInstructGenerator):\n    \"\"\"Generate evolved instructions with increased complexity using an `LLM`.\n\n    `EvolComplexityGenerator` is a generation task that evolves instructions to make\n    them more complex, and it is based in the EvolInstruct task, but using slight different\n    prompts, but the exact same evolutionary approach.\n\n    Attributes:\n        num_instructions: The number of instructions to be generated.\n        generate_answers: Whether to generate answers for the instructions or not. Defaults\n            to `False`.\n        mutation_templates: The mutation templates to be used for the generation of the\n            instructions.\n        min_length: Defines the length (in bytes) that the generated instruction needs to\n            be higher than, to be considered valid. Defaults to `512`.\n        max_length: Defines the length (in bytes) that the generated instruction needs to\n            be lower than, to be considered valid. Defaults to `1024`.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `min_length`: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.\n        - `max_length`: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.\n        - `seed`: The number of evolutions to be run.\n\n    Output columns:\n        - instruction (`str`): The evolved instruction.\n        - answer (`str`, optional): The answer to the instruction if `generate_answers=True`.\n        - model_name (`str`): The name of the LLM used to evolve the instructions.\n\n    Categories:\n        - evol\n        - instruction\n        - generation\n        - deita\n\n    References:\n        - [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://arxiv.org/abs/2312.15685)\n        - [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)\n    \"\"\"\n\n    mutation_templates: Dict[str, str] = GENERATION_MUTATION_TEMPLATES\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_instruct/evol_complexity/utils/","title":"Utils","text":""},{"location":"reference/distilabel/steps/tasks/evol_quality/","title":"Index","text":""},{"location":"reference/distilabel/steps/tasks/evol_quality/base/","title":"Base","text":""},{"location":"reference/distilabel/steps/tasks/evol_quality/base/#distilabel.steps.tasks.evol_quality.base.EvolQuality","title":"<code>EvolQuality</code>","text":"<p>               Bases: <code>Task</code></p> <p>Evolve the quality of the responses using an <code>LLM</code>.</p> <p><code>EvolQuality</code> task is used to evolve the quality of the responses given a prompt, by generating a new response with a language model. This step implements the evolution quality task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p> <p>Attributes:</p> Name Type Description <code>num_evolutions</code> <code>int</code> <p>The number of evolutions to be performed on the responses.</p> <code>store_evolutions</code> <code>bool</code> <p>Whether to store all the evolved responses or just the last one. Defaults to <code>False</code>.</p> <code>include_original_response</code> <code>bool</code> <p>Whether to include the original response within the evolved responses. Defaults to <code>False</code>.</p> <code>mutation_templates</code> <code>Dict[str, str]</code> <p>The mutation templates to be used to evolve the responses.</p> <code>seed</code> <code>RuntimeParameter[int]</code> <p>The seed to be set for <code>numpy</code> in order to randomly pick a mutation method. Defaults to <code>42</code>.</p> Runtime parameters <ul> <li><code>seed</code>: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul> Input columns <ul> <li>instruction (<code>str</code>): The instruction that was used to generate the <code>responses</code>.</li> <li>response (<code>str</code>): The responses to be rewritten.</li> </ul> Output columns <ul> <li>evolved_response (<code>str</code>): The evolved response if <code>store_evolutions=False</code>.</li> <li>evolved_responses (<code>List[str]</code>): The evolved responses if <code>store_evolutions=True</code>.</li> <li>model_name (<code>str</code>): The name of the LLM used to evolve the responses.</li> </ul> Categories <ul> <li>evol</li> <li>response</li> <li>deita</li> </ul> References <ul> <li><code>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</code></li> </ul> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>class EvolQuality(Task):\n    \"\"\"Evolve the quality of the responses using an `LLM`.\n\n    `EvolQuality` task is used to evolve the quality of the responses given a prompt,\n    by generating a new response with a language model. This step implements the evolution\n    quality task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of\n    Automatic Data Selection in Instruction Tuning'.\n\n    Attributes:\n        num_evolutions: The number of evolutions to be performed on the responses.\n        store_evolutions: Whether to store all the evolved responses or just the last one.\n            Defaults to `False`.\n        include_original_response: Whether to include the original response within the evolved\n            responses. Defaults to `False`.\n        mutation_templates: The mutation templates to be used to evolve the responses.\n        seed: The seed to be set for `numpy` in order to randomly pick a mutation method.\n            Defaults to `42`.\n\n    Runtime parameters:\n        - `seed`: The seed to be set for `numpy` in order to randomly pick a mutation method.\n\n    Input columns:\n        - instruction (`str`): The instruction that was used to generate the `responses`.\n        - response (`str`): The responses to be rewritten.\n\n    Output columns:\n        - evolved_response (`str`): The evolved response if `store_evolutions=False`.\n        - evolved_responses (`List[str]`): The evolved responses if `store_evolutions=True`.\n        - model_name (`str`): The name of the LLM used to evolve the responses.\n\n    Categories:\n        - evol\n        - response\n        - deita\n\n    References:\n        - [`What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning`](https://arxiv.org/abs/2312.15685)\n    \"\"\"\n\n    num_evolutions: int\n    store_evolutions: bool = False\n    include_original_response: bool = False\n    mutation_templates: Dict[str, str] = MUTATION_TEMPLATES\n\n    seed: RuntimeParameter[int] = Field(\n        default=42,\n        description=\"As `numpy` is being used in order to randomly pick a mutation method, then is nice to set a random seed.\",\n    )\n\n    @override\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n        This is useful if you want to do some validation that requires the entire model to be initialized.\n        \"\"\"\n        super().model_post_init(__context)\n\n    @property\n    def inputs(self) -&gt; List[str]:\n        \"\"\"The input for the task are the `instruction` and `response`.\"\"\"\n        return [\"instruction\", \"response\"]\n\n    def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n        \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n        is the first interaction from the user within a conversation. And the\n        `system_prompt` is added as the first message if it exists.\"\"\"\n        return [{\"role\": \"user\", \"content\": input}]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        \"\"\"The output for the task are the `evolved_response/s` and the `model_name`.\"\"\"\n        # TODO: having to define a `model_name` column every time as the `Task.outputs` is not ideal,\n        # this could be handled always and the value could be included within the DAG validation when\n        # a `Task` is used, since all the `Task` subclasses will have an `llm` with a `model_name` attr.\n        _outputs = [\n            (\"evolved_response\" if not self.store_evolutions else \"evolved_responses\"),\n            \"model_name\",\n        ]\n\n        return _outputs\n\n    def format_output(self, responses: Union[str, List[str]]) -&gt; Dict[str, Any]:  # type: ignore\n        \"\"\"The output for the task is a dict with: `evolved_response` or `evolved_responses`,\n        depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n        and, finally, the `model_name`.\n\n        Args:\n            responses: The responses to be included within the output.\n\n        Returns:\n            if `store_evolutions=False` return {\"evolved_response\": ..., \"model_name\": ...};\n            if `store_evolutions=True` return {\"evolved_responses\": ..., \"model_name\": ...}.\n        \"\"\"\n        _output = {}\n\n        if not self.store_evolutions:\n            _output[\"evolved_response\"] = responses[-1]\n        else:\n            _output[\"evolved_responses\"] = responses\n\n        _output[\"model_name\"] = self.llm.model_name\n        return _output\n\n    @property\n    def mutation_templates_names(self) -&gt; List[str]:\n        \"\"\"Returns the names i.e. keys of the provided `mutation_templates` enum.\"\"\"\n        return list(self.mutation_templates.keys())\n\n    def _apply_random_mutation(self, instruction: str, response: str) -&gt; str:\n        \"\"\"Applies a random mutation from the ones provided as part of the `mutation_templates`\n        enum, and returns the provided instruction within the mutation prompt.\n\n        Args:\n            instruction: The instruction to be included within the mutation prompt.\n\n        Returns:\n            A random mutation prompt with the provided instruction.\n        \"\"\"\n        mutation = np.random.choice(self.mutation_templates_names)\n        return (\n            self.mutation_templates[mutation]\n            .replace(\"&lt;PROMPT&gt;\", instruction)\n            .replace(\"&lt;RESPONSE&gt;\", response)\n        )\n\n    def _evolve_reponses(self, inputs: \"StepInput\") -&gt; List[List[str]]:\n        \"\"\"Evolves the instructions provided as part of the inputs of the task.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list where each item is a list with either the last evolved instruction if\n            `store_evolutions=False` or all the evolved instructions if `store_evolutions=True`.\n        \"\"\"\n        np.random.seed(self.seed)\n        instructions: List[List[str]] = [[input[\"instruction\"]] for input in inputs]\n        responses: List[List[str]] = [[input[\"response\"]] for input in inputs]\n\n        for iter_no in range(self.num_evolutions):\n            formatted_prompts = []\n            for instruction, response in zip(instructions, responses):\n                formatted_prompts.append(\n                    self._apply_random_mutation(instruction[-1], response[-1])\n                )\n\n            formatted_prompts = [\n                self.format_input(prompt) for prompt in formatted_prompts\n            ]\n\n            generated_responses = self.llm.generate(\n                formatted_prompts,\n                **self.llm.generation_kwargs,  # type: ignore\n            )\n\n            if self.store_evolutions:\n                responses = [\n                    response + [evolved_response[0]]\n                    for response, evolved_response in zip(\n                        responses, generated_responses\n                    )\n                ]\n            else:\n                responses = [\n                    [evolved_response[0]] for evolved_response in generated_responses\n                ]\n\n            self._logger.info(\n                f\"\ud83d\udd04 Ran iteration {iter_no} evolving {len(responses)} responses!\"\n            )\n\n        return responses\n\n    @override\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n        \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n        Args:\n            inputs: A list of Python dictionaries with the inputs of the task.\n\n        Returns:\n            A list of Python dictionaries with the outputs of the task.\n        \"\"\"\n\n        responses = self._evolve_reponses(inputs)\n\n        if self.store_evolutions:\n            # Remove the input instruction from the `evolved_responses` list\n            from_ = 1 if not self.include_original_response else 0\n            responses = [response[from_:] for response in responses]\n\n        for input, response in zip(inputs, responses):\n            input.update(self.format_output(response))\n        yield inputs\n\n        self._logger.info(f\"\ud83c\udf89 Finished evolving {len(responses)} instructions!\")\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_quality/base/#distilabel.steps.tasks.evol_quality.base.EvolQuality.inputs","title":"<code>inputs: List[str]</code>  <code>property</code>","text":"<p>The input for the task are the <code>instruction</code> and <code>response</code>.</p>"},{"location":"reference/distilabel/steps/tasks/evol_quality/base/#distilabel.steps.tasks.evol_quality.base.EvolQuality.mutation_templates_names","title":"<code>mutation_templates_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names i.e. keys of the provided <code>mutation_templates</code> enum.</p>"},{"location":"reference/distilabel/steps/tasks/evol_quality/base/#distilabel.steps.tasks.evol_quality.base.EvolQuality.outputs","title":"<code>outputs: List[str]</code>  <code>property</code>","text":"<p>The output for the task are the <code>evolved_response/s</code> and the <code>model_name</code>.</p>"},{"location":"reference/distilabel/steps/tasks/evol_quality/base/#distilabel.steps.tasks.evol_quality.base.EvolQuality.format_input","title":"<code>format_input(input)</code>","text":"<p>The input is formatted as a <code>ChatType</code> assuming that the instruction is the first interaction from the user within a conversation. And the <code>system_prompt</code> is added as the first message if it exists.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>def format_input(self, input: str) -&gt; ChatType:  # type: ignore\n    \"\"\"The input is formatted as a `ChatType` assuming that the instruction\n    is the first interaction from the user within a conversation. And the\n    `system_prompt` is added as the first message if it exists.\"\"\"\n    return [{\"role\": \"user\", \"content\": input}]\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_quality/base/#distilabel.steps.tasks.evol_quality.base.EvolQuality.format_output","title":"<code>format_output(responses)</code>","text":"<p>The output for the task is a dict with: <code>evolved_response</code> or <code>evolved_responses</code>, depending whether the value is either <code>False</code> or <code>True</code> for <code>store_evolutions</code>, respectively; and, finally, the <code>model_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>Union[str, List[str]]</code> <p>The responses to be included within the output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>if <code>store_evolutions=False</code> return {\"evolved_response\": ..., \"model_name\": ...};</p> <code>Dict[str, Any]</code> <p>if <code>store_evolutions=True</code> return {\"evolved_responses\": ..., \"model_name\": ...}.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>def format_output(self, responses: Union[str, List[str]]) -&gt; Dict[str, Any]:  # type: ignore\n    \"\"\"The output for the task is a dict with: `evolved_response` or `evolved_responses`,\n    depending whether the value is either `False` or `True` for `store_evolutions`, respectively;\n    and, finally, the `model_name`.\n\n    Args:\n        responses: The responses to be included within the output.\n\n    Returns:\n        if `store_evolutions=False` return {\"evolved_response\": ..., \"model_name\": ...};\n        if `store_evolutions=True` return {\"evolved_responses\": ..., \"model_name\": ...}.\n    \"\"\"\n    _output = {}\n\n    if not self.store_evolutions:\n        _output[\"evolved_response\"] = responses[-1]\n    else:\n        _output[\"evolved_responses\"] = responses\n\n    _output[\"model_name\"] = self.llm.model_name\n    return _output\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_quality/base/#distilabel.steps.tasks.evol_quality.base.EvolQuality.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Override this method to perform additional initialization after <code>__init__</code> and <code>model_construct</code>. This is useful if you want to do some validation that requires the entire model to be initialized.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>@override\ndef model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`.\n    This is useful if you want to do some validation that requires the entire model to be initialized.\n    \"\"\"\n    super().model_post_init(__context)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_quality/base/#distilabel.steps.tasks.evol_quality.base.EvolQuality.process","title":"<code>process(inputs)</code>","text":"<p>Processes the inputs of the task and generates the outputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StepInput</code> <p>A list of Python dictionaries with the inputs of the task.</p> required <p>Returns:</p> Type Description <code>StepOutput</code> <p>A list of Python dictionaries with the outputs of the task.</p> Source code in <code>src/distilabel/steps/tasks/evol_quality/base.py</code> <pre><code>@override\ndef process(self, inputs: StepInput) -&gt; \"StepOutput\":  # type: ignore\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        inputs: A list of Python dictionaries with the inputs of the task.\n\n    Returns:\n        A list of Python dictionaries with the outputs of the task.\n    \"\"\"\n\n    responses = self._evolve_reponses(inputs)\n\n    if self.store_evolutions:\n        # Remove the input instruction from the `evolved_responses` list\n        from_ = 1 if not self.include_original_response else 0\n        responses = [response[from_:] for response in responses]\n\n    for input, response in zip(inputs, responses):\n        input.update(self.format_output(response))\n    yield inputs\n\n    self._logger.info(f\"\ud83c\udf89 Finished evolving {len(responses)} instructions!\")\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/evol_quality/utils/","title":"Utils","text":""},{"location":"reference/distilabel/steps/tasks/structured_outputs/","title":"Index","text":""},{"location":"reference/distilabel/steps/tasks/structured_outputs/instructor/","title":"Instructor","text":""},{"location":"reference/distilabel/steps/tasks/structured_outputs/instructor/#distilabel.steps.tasks.structured_outputs.instructor.InstructorAvailableClients","title":"<code>InstructorAvailableClients: TypeAlias = Union['AsyncAnthropic', 'AsyncAzureOpenAI', 'AsyncCohere', 'AsyncGroq', 'AsyncOpenAI', 'MistralAsyncClient']</code>  <code>module-attribute</code>","text":"<p>Available clients that can be wrapped with <code>instructor</code>.</p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/instructor/#distilabel.steps.tasks.structured_outputs.instructor.InstructorFrameworks","title":"<code>InstructorFrameworks = Literal['openai', 'azure_openai', 'anthropic', 'cohere', 'groq', 'litellm', 'mistral']</code>  <code>module-attribute</code>","text":"<p>Available frameworks for the structured output configuration with <code>instructor</code>.</p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/instructor/#distilabel.steps.tasks.structured_outputs.instructor.InstructorStructuredOutputType","title":"<code>InstructorStructuredOutputType</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict to represent the structured output configuration from <code>instructor</code>.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/instructor.py</code> <pre><code>class InstructorStructuredOutputType(TypedDict):\n    \"\"\"TypedDict to represent the structured output configuration from `instructor`.\"\"\"\n\n    schema: Type[BaseModel]\n    \"\"\"The schema to use for the structured output, a `pydantic.BaseModel` class. \"\"\"\n    mode: Optional[\"instructor.Mode\"]\n    \"\"\"Generation mode. Take a look at `instructor.Mode` for more information, if not informed it will\n    be determined automatically. \"\"\"\n    max_retries: int\n    \"\"\"Number of times to reask the model in case of error, if not set will default to the model's default. \"\"\"\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/instructor/#distilabel.steps.tasks.structured_outputs.instructor.InstructorStructuredOutputType.max_retries","title":"<code>max_retries: int</code>  <code>instance-attribute</code>","text":"<p>Number of times to reask the model in case of error, if not set will default to the model's default.</p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/instructor/#distilabel.steps.tasks.structured_outputs.instructor.InstructorStructuredOutputType.mode","title":"<code>mode: Optional[instructor.Mode]</code>  <code>instance-attribute</code>","text":"<p>Generation mode. Take a look at <code>instructor.Mode</code> for more information, if not informed it will be determined automatically.</p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/instructor/#distilabel.steps.tasks.structured_outputs.instructor.InstructorStructuredOutputType.schema","title":"<code>schema: Type[BaseModel]</code>  <code>instance-attribute</code>","text":"<p>The schema to use for the structured output, a <code>pydantic.BaseModel</code> class.</p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/instructor/#distilabel.steps.tasks.structured_outputs.instructor.prepare_instructor","title":"<code>prepare_instructor(client, mode=None, framework=None)</code>","text":"<p>Wraps the given client with the instructor client for the given framework.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>InstructorAvailableClients</code> <p>The client to wrap with the instructor client, corresponds to the internal client we wrap on <code>LLM</code>, and one of the implemented in <code>instructor</code>.</p> required <code>mode</code> <code>Optional[Mode]</code> <p>One of the <code>instructor.Mode</code> values. Defaults to None.</p> <code>None</code> <code>framework</code> <code>Optional[InstructorFrameworks]</code> <p>The framework corresponding to the client. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>instructor</code> is not installed.</p> <code>ValueError</code> <p>If the mode is not one of the available modes.</p> <p>Returns:</p> Name Type Description <code>patched_client</code> <code>AsyncInstructor</code> <p>The instructor wrapping the original client to be used for structured generation.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/instructor.py</code> <pre><code>def prepare_instructor(\n    client: InstructorAvailableClients,\n    mode: Optional[\"instructor.Mode\"] = None,\n    framework: Optional[InstructorFrameworks] = None,\n) -&gt; \"instructor.AsyncInstructor\":\n    \"\"\"Wraps the given client with the instructor client for the given framework.\n\n    Args:\n        client: The client to wrap with the instructor client, corresponds to the internal\n            client we wrap on `LLM`, and one of the implemented in `instructor`.\n        mode: One of the `instructor.Mode` values. Defaults to None.\n        framework: The framework corresponding to the client. Defaults to None.\n\n    Raises:\n        ImportError: If `instructor` is not installed.\n        ValueError: If the mode is not one of the available modes.\n\n    Returns:\n        patched_client: The instructor wrapping the original client to be used for\n            structured generation.\n    \"\"\"\n    if not importlib.util.find_spec(\"instructor\"):\n        raise ImportError(\n            \"`instructor` is not installed. Please install it using `pip install instructor`.\"\n        )\n    import instructor\n\n    builder, default_mode = _client_patcher(framework)\n\n    mode = mode or default_mode\n    if mode.value not in [m.value for m in instructor.mode.Mode]:\n        raise ValueError(\n            f\"Invalid mode '{mode}'. Must be one of {[m.value for m in instructor.mode.Mode]}\"\n        )\n\n    patched_client: instructor.AsyncInstructor = builder(client, mode=mode)\n\n    return patched_client\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/outlines/","title":"Outlines","text":""},{"location":"reference/distilabel/steps/tasks/structured_outputs/outlines/#distilabel.steps.tasks.structured_outputs.outlines.Frameworks","title":"<code>Frameworks = Literal['transformers', 'llamacpp', 'vllm']</code>  <code>module-attribute</code>","text":"<p>Available frameworks for the structured output configuration.</p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/outlines/#distilabel.steps.tasks.structured_outputs.outlines.StructuredOutputType","title":"<code>StructuredOutputType</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict to represent the structured output configuration from outlines.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/outlines.py</code> <pre><code>class StructuredOutputType(TypedDict):\n    \"\"\"TypedDict to represent the structured output configuration from outlines.\"\"\"\n\n    format: Literal[\"json\", \"regex\"]\n    \"\"\"One of \"json\" or \"regex\".\"\"\"\n    schema: Union[str, Type[BaseModel]]\n    \"\"\"The schema to use for the structured output. If \"json\", it\n    can be a pydantic.BaseModel class, or the schema as a string,\n    as obtained from `model_to_schema(BaseModel)`, if \"regex\", it\n    should be a regex pattern as a string.\n    \"\"\"\n    whitespace_pattern: Optional[Union[str, List[str]]]\n    \"\"\"If \"json\" corresponds to a string or a list of\n    strings with a pattern (doesn't impact string literals).\n    For example, to allow only a single space or newline with\n    `whitespace_pattern=r\"[\\n ]?\"`\n    \"\"\"\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/outlines/#distilabel.steps.tasks.structured_outputs.outlines.StructuredOutputType.format","title":"<code>format: Literal['json', 'regex']</code>  <code>instance-attribute</code>","text":"<p>One of \"json\" or \"regex\".</p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/outlines/#distilabel.steps.tasks.structured_outputs.outlines.StructuredOutputType.schema","title":"<code>schema: Union[str, Type[BaseModel]]</code>  <code>instance-attribute</code>","text":"<p>The schema to use for the structured output. If \"json\", it can be a pydantic.BaseModel class, or the schema as a string, as obtained from <code>model_to_schema(BaseModel)</code>, if \"regex\", it should be a regex pattern as a string.</p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/outlines/#distilabel.steps.tasks.structured_outputs.outlines.StructuredOutputType.whitespace_pattern","title":"<code>whitespace_pattern: Optional[Union[str, List[str]]]</code>  <code>instance-attribute</code>","text":"<p>If \"json\" corresponds to a string or a list of    strings with a pattern (doesn't impact string literals).    For example, to allow only a single space or newline with    <code>whitespace_pattern=r\"[ ]?\"</code></p>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/outlines/#distilabel.steps.tasks.structured_outputs.outlines.model_to_schema","title":"<code>model_to_schema(schema)</code>","text":"<p>Helper function to return a string representation of the schema from a <code>pydantic.BaseModel</code> class.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/outlines.py</code> <pre><code>def model_to_schema(schema: Type[BaseModel]) -&gt; Dict[str, Any]:\n    \"\"\"Helper function to return a string representation of the schema from a `pydantic.BaseModel` class.\"\"\"\n    return json.dumps(schema.model_json_schema())\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/outlines/#distilabel.steps.tasks.structured_outputs.outlines.prepare_guided_output","title":"<code>prepare_guided_output(structured_output, framework, llm)</code>","text":"<p>Prepares the <code>LLM</code> to generate guided output using <code>outlines</code>.</p> <p>It allows to generate JSON or Regex structured outputs for the integrated frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>structured_output</code> <code>StructuredOutputType</code> <p>the structured output configuration.</p> required <code>framework</code> <code>Frameworks</code> <p>the framework to use for the structured output.</p> required <code>llm</code> <code>Any</code> <p>the <code>LLM</code> instance, each framework requires one thing so it should be obtained in the <code>LLM</code> itself.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the format is not \"json\" or \"regex\".</p> <p>Returns:</p> Type Description <code>Dict[str, Union[Callable, None]]</code> <p>A dictionary containing the processor to use for the guided output, and in</p> <code>Dict[str, Union[Callable, None]]</code> <p>case of \"json\" will also include the schema as a dict, to simplify serialization</p> <code>Dict[str, Union[Callable, None]]</code> <p>and deserialization.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/outlines.py</code> <pre><code>def prepare_guided_output(\n    structured_output: StructuredOutputType,\n    framework: Frameworks,\n    llm: Any,\n) -&gt; Dict[str, Union[Callable, None]]:\n    \"\"\"Prepares the `LLM` to generate guided output using `outlines`.\n\n    It allows to generate JSON or Regex structured outputs for the integrated\n    frameworks.\n\n    Args:\n        structured_output: the structured output configuration.\n        framework: the framework to use for the structured output.\n        llm: the `LLM` instance, each framework requires one thing so it should\n            be obtained in the `LLM` itself.\n\n    Raises:\n        ValueError: if the format is not \"json\" or \"regex\".\n\n    Returns:\n        A dictionary containing the processor to use for the guided output, and in\n        case of \"json\" will also include the schema as a dict, to simplify serialization\n        and deserialization.\n    \"\"\"\n    if not importlib.util.find_spec(\"outlines\"):\n        raise ImportError(\n            \"Outlines is not installed. Please install it using `pip install outlines`.\"\n        )\n\n    json_processor, regex_processor = _get_logits_processor(framework)\n\n    format = structured_output.get(\"format\")\n    schema = structured_output.get(\"schema\")\n\n    if format == \"json\":\n        return {\n            \"processor\": json_processor(\n                schema,\n                llm,\n                whitespace_pattern=structured_output.get(\"whitespace_pattern\"),\n            ),\n            \"schema\": schema_as_dict(schema),\n        }\n\n    if format == \"regex\":\n        return {\"processor\": regex_processor(schema, llm)}\n\n    raise ValueError(f\"Invalid format '{format}'. Must be either 'json' or 'regex'.\")\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/utils/","title":"Utils","text":""},{"location":"reference/distilabel/steps/tasks/structured_outputs/utils/#distilabel.steps.tasks.structured_outputs.utils.json_schema_to_model","title":"<code>json_schema_to_model(json_schema)</code>","text":"<p>Converts a JSON schema to a <code>pydantic.BaseModel</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>Dict[str, Any]</code> <p>The JSON schema to convert.</p> required <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>A <code>pydantic.BaseModel</code> class.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/utils.py</code> <pre><code>def json_schema_to_model(json_schema: Dict[str, Any]) -&gt; Type[BaseModel]:\n    \"\"\"Converts a JSON schema to a `pydantic.BaseModel` class.\n\n    Args:\n        json_schema: The JSON schema to convert.\n\n    Returns:\n        A `pydantic.BaseModel` class.\n    \"\"\"\n\n    # Extract the model name from the schema title.\n    model_name = json_schema.get(\"title\")\n    if defs := json_schema.get(\"$defs\", None):\n        # This is done to grab the content of nested classes that need to dereference\n        # the objects (those should be in a higher level).\n        pass\n\n    # Extract the field definitions from the schema properties.\n    field_definitions = {\n        name: json_schema_to_pydantic_field(\n            name, prop, json_schema.get(\"required\", []), defs=defs\n        )\n        for name, prop in json_schema.get(\"properties\", {}).items()\n    }\n\n    # Create the BaseModel class using create_model().\n    return create_model(model_name, **field_definitions)\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/utils/#distilabel.steps.tasks.structured_outputs.utils.json_schema_to_pydantic_field","title":"<code>json_schema_to_pydantic_field(name, json_schema, required, defs=None)</code>","text":"<p>Converts a JSON schema property to a <code>pydantic.Field</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The field name.</p> required <code>json_schema</code> <code>Dict[str, Any]</code> <p>The JSON schema property.</p> required <code>required</code> <code>List[str]</code> <p>The list of required fields.</p> required <code>defs</code> <code>Optional[Dict[str, Any]]</code> <p>The definitions of the JSON schema. It's used to dereference nested classes, so we can grab the original definition from the json schema (it won't work out of the box with just the reference).</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A <code>pydantic.Field</code>.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/utils.py</code> <pre><code>def json_schema_to_pydantic_field(\n    name: str,\n    json_schema: Dict[str, Any],\n    required: List[str],\n    defs: Optional[Dict[str, Any]] = None,\n) -&gt; Any:\n    \"\"\"Converts a JSON schema property to a `pydantic.Field`.\n\n    Args:\n        name: The field name.\n        json_schema: The JSON schema property.\n        required: The list of required fields.\n        defs: The definitions of the JSON schema. It's used to dereference nested classes,\n            so we can grab the original definition from the json schema (it won't\n            work out of the box with just the reference).\n\n    Returns:\n        A `pydantic.Field`.\n    \"\"\"\n\n    # NOTE(plaguss): This needs more testing, nested classes need extra work to be converted\n    # here if we pass a reference to another class it will crash, we have to find the original\n    # definition and insert it here\n    # This takes into account single items referred to other classes\n    if ref := json_schema.get(\"$ref\"):\n        json_schema = defs.get(ref.split(\"/\")[-1])\n\n    # This takes into account lists of items referred to other classes\n    if \"items\" in json_schema and (ref := json_schema[\"items\"].get(\"$ref\")):\n        json_schema[\"items\"] = defs.get(ref.split(\"/\")[-1])\n\n    # Get the field type.\n    type_ = json_schema_to_pydantic_type(json_schema)\n\n    # Get the field description.\n    description = json_schema.get(\"description\")\n\n    # Get the field examples.\n    examples = json_schema.get(\"examples\")\n\n    # Create a Field object with the type, description, and examples.\n    # The \"required\" flag will be set later when creating the model.\n    return (\n        type_,\n        Field(\n            description=description,\n            examples=examples,\n            default=... if name in required else None,\n        ),\n    )\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/utils/#distilabel.steps.tasks.structured_outputs.utils.json_schema_to_pydantic_type","title":"<code>json_schema_to_pydantic_type(json_schema)</code>","text":"<p>Converts a JSON schema type to a Pydantic type.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>Dict[str, Any]</code> <p>The JSON schema to convert.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic type.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/utils.py</code> <pre><code>def json_schema_to_pydantic_type(json_schema: Dict[str, Any]) -&gt; Any:\n    \"\"\"Converts a JSON schema type to a Pydantic type.\n\n    Args:\n        json_schema: The JSON schema to convert.\n\n    Returns:\n        A Pydantic type.\n    \"\"\"\n    type_ = json_schema.get(\"type\")\n\n    if type_ == \"string\":\n        type_val = str\n    elif type_ == \"integer\":\n        type_val = int\n    elif type_ == \"number\":\n        type_val = float\n    elif type_ == \"boolean\":\n        type_val = bool\n    elif type_ == \"array\":\n        items_schema = json_schema.get(\"items\")\n        if items_schema:\n            item_type = json_schema_to_pydantic_type(items_schema)\n            type_val = List[item_type]\n        else:\n            type_val = List\n    elif type_ == \"object\":\n        # Handle nested models.\n        properties = json_schema.get(\"properties\")\n        if properties:\n            nested_model = json_schema_to_model(json_schema)\n            type_val = nested_model\n        else:\n            type_val = Dict\n    elif type_ == \"null\":\n        type_val = Optional[Any]  # Use Optional[Any] for nullable fields\n    else:\n        raise ValueError(f\"Unsupported JSON schema type: {type_}\")\n\n    return type_val\n</code></pre>"},{"location":"reference/distilabel/steps/tasks/structured_outputs/utils/#distilabel.steps.tasks.structured_outputs.utils.schema_as_dict","title":"<code>schema_as_dict(schema)</code>","text":"<p>Helper function to obtain the schema and simplify serialization.</p> Source code in <code>src/distilabel/steps/tasks/structured_outputs/utils.py</code> <pre><code>def schema_as_dict(schema: Union[str, Type[BaseModel]]) -&gt; Dict[str, Any]:\n    \"\"\"Helper function to obtain the schema and simplify serialization.\"\"\"\n    if type(schema) == type(BaseModel):\n        return schema.model_json_schema()\n    elif isinstance(schema, str):\n        return json.loads(schema)\n    return schema\n</code></pre>"},{"location":"reference/distilabel/utils/","title":"Index","text":""},{"location":"reference/distilabel/utils/chat/","title":"Chat","text":""},{"location":"reference/distilabel/utils/chat/#distilabel.utils.chat.is_openai_format","title":"<code>is_openai_format(input)</code>","text":"<p>Checks if the input is in OpenAI chat-like format:</p> <pre><code>[\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n    {\"role\": \"assistant\", \"content\": \"Hi! How can I help you?\"},\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean indicating if the input is in OpenAI chat-like format.</p> Source code in <code>src/distilabel/utils/chat.py</code> <pre><code>def is_openai_format(input: Any) -&gt; bool:\n    \"\"\"Checks if the input is in OpenAI chat-like format:\n\n    ```python\n    [\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n        {\"role\": \"assistant\", \"content\": \"Hi! How can I help you?\"},\n    ]\n    ```\n\n    Args:\n        input: The input to check.\n\n    Returns:\n        A boolean indicating if the input is in OpenAI chat-like format.\n    \"\"\"\n    if not isinstance(input, list):\n        return False\n    return all(\n        isinstance(x, dict) and \"role\" in x.keys() and \"content\" in x.keys()\n        for x in input\n    )\n</code></pre>"},{"location":"reference/distilabel/utils/dicts/","title":"Dicts","text":""},{"location":"reference/distilabel/utils/dicts/#distilabel.utils.dicts.combine_dicts","title":"<code>combine_dicts(*dicts)</code>","text":"<p>Combines multiple dictionaries into a single dictionary joining the values as a list for each key.</p> <p>Parameters:</p> Name Type Description Default <code>*dicts</code> <code>Dict[_K, Any]</code> <p>the dictionaries to be combined.</p> <code>()</code> <p>Returns:</p> Type Description <code>Dict[_K, List[Any]]</code> <p>The combined dictionary.</p> Source code in <code>src/distilabel/utils/dicts.py</code> <pre><code>def combine_dicts(*dicts: Dict[_K, Any]) -&gt; Dict[_K, List[Any]]:\n    \"\"\"Combines multiple dictionaries into a single dictionary joining the values\n    as a list for each key.\n\n    Args:\n        *dicts: the dictionaries to be combined.\n\n    Returns:\n        The combined dictionary.\n    \"\"\"\n    combined_dict = defaultdict(list)\n    for d in dicts:\n        for key, value in d.items():\n            combined_dict[key].append(value)\n    return dict(combined_dict)\n</code></pre>"},{"location":"reference/distilabel/utils/docstring/","title":"Docstring","text":""},{"location":"reference/distilabel/utils/docstring/#distilabel.utils.docstring.parse_google_docstring","title":"<code>parse_google_docstring(func)</code>","text":"<p>Parses a Google-style docstring and returns a dictionary with its sections. It takes into account the peculiarities of the docstrings used in <code>distilabel</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function or class to parse the docstring from.</p> required <p>Returns:</p> Type Description <code>Docstring</code> <p>A dictionary with the parsed docstring sections.</p> Source code in <code>src/distilabel/utils/docstring.py</code> <pre><code>def parse_google_docstring(func: Callable) -&gt; Docstring:  # noqa: C901\n    \"\"\"Parses a Google-style docstring and returns a dictionary with its sections. It takes\n    into account the peculiarities of the docstrings used in `distilabel`.\n\n    Args:\n        func: The function or class to parse the docstring from.\n\n    Returns:\n        A dictionary with the parsed docstring sections.\n    \"\"\"\n    sections: Docstring = {\n        \"short_description\": \"\",\n        \"description\": \"\",\n        \"attributes\": {},\n        \"args\": {},\n        \"returns\": \"\",\n        \"raises\": {},\n        \"runtime_parameters\": {},\n        \"input_columns\": {},\n        \"output_columns\": {},\n        \"categories\": [],\n        \"icon\": \"\",\n        \"references\": {},\n        \"examples\": {},\n        \"note\": \"\",\n    }\n\n    if not func.__doc__:\n        return sections\n\n    docstring = func.__doc__.strip()\n\n    # Define the section headers to recognize\n    section_headers = [\n        \"Args\",\n        \"Returns\",\n        \"Raises\",\n        \"Attributes\",\n        \"Runtime parameters\",\n        \"Input columns\",\n        \"Output columns\",\n        \"Categories\",\n        \"Icon\",\n        \"References\",\n        \"Examples\",\n        \"Note\",\n    ]\n\n    # Match section headers\n    section_pattern = rf\"(\\s*{'|'.join(section_headers)}):\\s*\\n\"\n\n    # Extract the short description (first line) or identify if it starts with a section header\n    first_line_end = docstring.find(\"\\n\")\n    if first_line_end == -1 or re.match(section_pattern, docstring[first_line_end:]):\n        sections[\"short_description\"] = docstring.split(\"\\n\", 1)[0].strip()\n        remaining_docstring = (\n            docstring.split(\"\\n\", 1)[1].strip() if \"\\n\" in docstring else \"\"\n        )\n    else:\n        sections[\"short_description\"] = docstring[:first_line_end].strip()\n        remaining_docstring = docstring[first_line_end:].strip()\n\n    # Split the docstring into sections\n    parts = re.split(section_pattern, remaining_docstring)\n\n    if parts[0].strip() and not re.match(section_pattern, f\"\\n{parts[0].strip()}\\n\"):\n        sections[\"description\"] = parts[0].strip()\n\n    for i in range(1, len(parts), 2):\n        section_name = parts[i].lower().replace(\" \", \"_\")\n        section_content = parts[i + 1].strip()\n        if section_name in (\"args\", \"raises\", \"attributes\"):\n            # Parse arguments, exceptions, or attributes into a dictionary\n            items = re.findall(\n                r\"\\s*(\\w+):\\s*(.*?)\\s*(?=\\n\\s*\\w+:\\s*|\\n\\s*$|$)\",\n                section_content,\n                re.DOTALL,\n            )\n            sections[section_name] = {\n                item[0]: re.sub(r\"[\\t\\n]+| {2,}\", \" \", item[1]).strip()\n                for item in items\n            }\n        elif section_name == \"runtime_parameters\":\n            # Parse runtime parameters into a dictionary\n            items = re.findall(\n                r\"\\s*-\\s*`?(\\w+)`?:\\s*(.*?)\\s*(?=\\n\\s*-\\s*`?\\w+`?:|$)\",\n                section_content,\n                re.DOTALL,\n            )\n            sections[section_name] = {\n                item[0]: re.sub(r\"[\\t\\n]+| {2,}\", \" \", item[1]).strip()\n                for item in items\n            }\n        elif section_name in (\"input_columns\", \"output_columns\"):\n            items = re.findall(\n                r\"- (?P&lt;name&gt;\\w+) \\((?P&lt;type&gt;`[^`]+`|[^)]+)\\): (?P&lt;description&gt;.+?)(?=\\n\\s*-\\s|\\Z)\",\n                section_content,\n                re.DOTALL,\n            )\n            sections[section_name] = {\n                item[0]: (\n                    item[1],\n                    re.sub(r\"[\\t\\n]+| {2,}\", \" \", item[2]).strip(),\n                )\n                for item in items\n            }\n        elif section_name == \"categories\":\n            # Parse categories as a list of strings without the \"- \" prefix\n            sections[section_name] = [\n                cat.replace(\"-\", \"\", 1).strip()\n                for cat in section_content.split(\"\\n\")\n                if cat.strip()\n            ]\n        elif section_name == \"icon\":\n            # Parse logo as a single string\n            match = re.match(r\"`([^`]+)`\", section_content)\n            if match:\n                sections[section_name] = match.group(1)\n        elif section_name == \"references\":\n            # Parse references into a dictionary with the name and URL\n            items = re.findall(r\"\\s*-\\s*\\[([^]]+)\\]\\(([^)]+)\\)\", section_content)\n            sections[section_name] = {\n                item[0].replace(\"`\", \"\"): item[1] for item in items\n            }\n        elif section_name == \"examples\":\n            # Parse examples into a dictionary\n            example_items = re.findall(\n                r\"(\\w[\\w\\s]*?):\\s*\\n\\s*```python\\n(.*?)\\n\\s*```\",\n                section_content,\n                re.DOTALL,\n            )\n            sections[section_name] = {\n                item[0].strip(): remove_leading_whitespaces(item[1].strip())\n                for item in example_items\n            }\n        elif section_name == \"note\":\n            sections[section_name] = remove_leading_whitespaces(section_content.strip())\n        else:\n            sections[section_name] = section_content\n\n    return sections\n</code></pre>"},{"location":"reference/distilabel/utils/docstring/#distilabel.utils.docstring.remove_leading_whitespaces","title":"<code>remove_leading_whitespaces(text, num_spaces=8)</code>","text":"<p>Removes the specified leading whitespaces from each line of a given string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the string from which the leading whitespaces has to be removed.</p> required <code>num_spaces</code> <code>int</code> <p>the number of leading whitespaces to remove.</p> <code>8</code> <p>Returns:</p> Type Description <code>str</code> <p>The string with the leading whitespaces removed.</p> Source code in <code>src/distilabel/utils/docstring.py</code> <pre><code>def remove_leading_whitespaces(text: str, num_spaces: int = 8) -&gt; str:\n    \"\"\"Removes the specified leading whitespaces from each line of a given string.\n\n    Args:\n        text: the string from which the leading whitespaces has to be removed.\n        num_spaces: the number of leading whitespaces to remove.\n\n    Returns:\n        The string with the leading whitespaces removed.\n    \"\"\"\n    lines = text.split(\"\\n\")\n    trimmed_lines = [\n        line[num_spaces:] if line.startswith(\" \" * num_spaces) else line\n        for line in lines\n    ]\n    return \"\\n\".join(trimmed_lines)\n</code></pre>"},{"location":"reference/distilabel/utils/export_components_info/","title":"Export components info","text":""},{"location":"reference/distilabel/utils/export_components_info/#distilabel.utils.export_components_info.ComponentsInfo","title":"<code>ComponentsInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A dictionary containing <code>distilabel</code> components information.</p> Source code in <code>src/distilabel/utils/export_components_info.py</code> <pre><code>class ComponentsInfo(TypedDict):\n    \"\"\"A dictionary containing `distilabel` components information.\"\"\"\n\n    llms: List\n    steps: List\n    tasks: List\n</code></pre>"},{"location":"reference/distilabel/utils/export_components_info/#distilabel.utils.export_components_info.export_components_info","title":"<code>export_components_info()</code>","text":"<p>Exports <code>distilabel</code> components (<code>LLM</code>s, <code>Step</code>s and <code>Task</code>s) information in a dictionary format. This information can be used to generate <code>distilabel</code> components documentation, or to be used in 3rd party applications (UIs, etc).</p> <p>Returns:</p> Type Description <code>ComponentsInfo</code> <p>A dictionary containing <code>distilabel</code> components information</p> Source code in <code>src/distilabel/utils/export_components_info.py</code> <pre><code>def export_components_info() -&gt; ComponentsInfo:\n    \"\"\"Exports `distilabel` components (`LLM`s, `Step`s and `Task`s) information in a dictionary\n    format. This information can be used to generate `distilabel` components documentation,\n    or to be used in 3rd party applications (UIs, etc).\n\n    Returns:\n        A dictionary containing `distilabel` components information\n    \"\"\"\n\n    steps = []\n    for step_type in _get_steps():\n        steps.append(\n            {\n                \"name\": step_type.__name__,\n                \"docstring\": parse_google_docstring(step_type),\n            }\n        )\n\n    tasks = []\n    for task_type in _get_tasks():\n        tasks.append(\n            {\n                \"name\": task_type.__name__,\n                \"docstring\": parse_google_docstring(task_type),\n            }\n        )\n\n    llms = []\n    for llm_type in _get_llms():\n        llms.append(\n            {\n                \"name\": llm_type.__name__,\n                \"docstring\": parse_google_docstring(llm_type),\n            }\n        )\n\n    return {\"steps\": steps, \"tasks\": tasks, \"llms\": llms}\n</code></pre>"},{"location":"reference/distilabel/utils/files/","title":"Files","text":""},{"location":"reference/distilabel/utils/files/#distilabel.utils.files.list_files_in_dir","title":"<code>list_files_in_dir(dir_path, key=lambda x: int(x.stem))</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>Path to the directory.</p> required <code>key</code> <code>Optional[Callable]</code> <p>A function to sort the files. Defaults to sorting by the integer value of the file name. This is useful when loading files from the cache, as the name will be numbered.</p> <code>lambda x: int(stem)</code> <p>Returns:</p> Type Description <code>List[Path]</code> <p>A list of file names in the directory.</p> Source code in <code>src/distilabel/utils/files.py</code> <pre><code>def list_files_in_dir(\n    dir_path: Path, key: Optional[Callable] = lambda x: int(x.stem)\n) -&gt; List[Path]:\n    \"\"\"List all files in a directory.\n\n    Args:\n        dir_path: Path to the directory.\n        key: A function to sort the files. Defaults to sorting by the integer value of the file name.\n            This is useful when loading files from the cache, as the name will be numbered.\n\n    Returns:\n        A list of file names in the directory.\n    \"\"\"\n    return [f for f in sorted(dir_path.iterdir(), key=key) if f.is_file()]\n</code></pre>"},{"location":"reference/distilabel/utils/itertools/","title":"Itertools","text":""},{"location":"reference/distilabel/utils/itertools/#distilabel.utils.itertools.grouper","title":"<code>grouper(iterable, n, *, incomplete='fill', fillvalue=None)</code>","text":"<p>Collect data into non-overlapping fixed-length chunks or blocks.</p> Source code in <code>src/distilabel/utils/itertools.py</code> <pre><code>def grouper(\n    iterable: Iterable[Any],\n    n: int,\n    *,\n    incomplete: Literal[\"fill\", \"strict\", \"ignore\"] = \"fill\",\n    fillvalue: Any = None,\n) -&gt; Iterable[Any]:\n    \"Collect data into non-overlapping fixed-length chunks or blocks.\"\n    # grouper('ABCDEFG', 3, fillvalue='x') --&gt; ABC DEF Gxx\n    # grouper('ABCDEFG', 3, incomplete='strict') --&gt; ABC DEF ValueError\n    # grouper('ABCDEFG', 3, incomplete='ignore') --&gt; ABC DEF\n    args = [iter(iterable)] * n\n\n    if incomplete == \"fill\":\n        return zip_longest(*args, fillvalue=fillvalue)\n\n    if incomplete == \"strict\":\n        return zip(*args, strict=True)\n\n    if incomplete == \"ignore\":\n        return zip(*args)\n\n    raise ValueError(\"Expected fill, strict, or ignore\")\n</code></pre>"},{"location":"reference/distilabel/utils/lists/","title":"Lists","text":""},{"location":"reference/distilabel/utils/lists/#distilabel.utils.lists.flatten_responses","title":"<code>flatten_responses(responses)</code>","text":"<p>Flattens the list of lists of strings into a single list of strings.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>List[List[str]]</code> <p>The list of lists of strings to flatten.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A single list of strings containing the last item of each list.</p> Source code in <code>src/distilabel/utils/lists.py</code> <pre><code>def flatten_responses(responses: List[List[str]]) -&gt; List[str]:\n    \"\"\"Flattens the list of lists of strings into a single list of strings.\n\n    Args:\n        responses: The list of lists of strings to flatten.\n\n    Returns:\n        A single list of strings containing the last item of each list.\n    \"\"\"\n    return [response[-1] for response in responses]\n</code></pre>"},{"location":"reference/distilabel/utils/logging/","title":"Logging","text":""},{"location":"reference/distilabel/utils/logging/#distilabel.utils.logging.setup_logging","title":"<code>setup_logging(log_queue=None, filename=None)</code>","text":"<p>Sets up logging to use a queue across all processes.</p> Source code in <code>src/distilabel/utils/logging.py</code> <pre><code>def setup_logging(\n    log_queue: Optional[\"Queue[Any]\"] = None, filename: Optional[str] = None\n) -&gt; None:\n    \"\"\"Sets up logging to use a queue across all processes.\"\"\"\n    global queue_listener\n\n    # Disable overly verbose loggers\n    logging.getLogger(\"argilla.client.feedback.dataset.local.mixins\").disabled = True\n    for logger in _SILENT_LOGGERS:\n        logging.getLogger(logger).setLevel(logging.CRITICAL)\n\n    # If the current process is the main process, set up a `QueueListener`\n    # to handle logs from all subprocesses\n    if mp.current_process().name == \"MainProcess\" and filename:\n        formatter = logging.Formatter(\"['%(name)s'] %(message)s\")\n        handler = RichHandler(rich_tracebacks=True)\n        handler.setFormatter(formatter)\n        if not Path(filename).parent.exists():\n            Path(filename).parent.mkdir(parents=True, exist_ok=True)\n\n        file_handler = FileHandler(filename, delay=True)\n        file_formatter = logging.Formatter(\n            \"[%(asctime)s] %(levelname)-8s %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n        )\n        file_handler.setFormatter(file_formatter)\n\n        if log_queue is not None:\n            queue_listener = QueueListener(\n                log_queue, handler, file_handler, respect_handler_level=True\n            )\n            queue_listener.start()\n\n    log_level = os.environ.get(\"DISTILABEL_LOG_LEVEL\", \"INFO\").upper()\n    if log_level not in [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]:\n        warnings.warn(\n            f\"Invalid log level '{log_level}', using default 'INFO' instead.\",\n            stacklevel=2,\n        )\n        log_level = \"INFO\"\n\n    root_logger = logging.getLogger()\n\n    running_test = \"PYTEST_CURRENT_TEST\" in os.environ\n    if not running_test:\n        root_logger.handlers.clear()\n\n    if log_queue is not None:\n        root_logger.addHandler(QueueHandler(log_queue))\n\n    root_logger.setLevel(log_level)\n</code></pre>"},{"location":"reference/distilabel/utils/logging/#distilabel.utils.logging.stop_logging","title":"<code>stop_logging()</code>","text":"<p>Stops the <code>QueueListener</code> if it's running.</p> Source code in <code>src/distilabel/utils/logging.py</code> <pre><code>def stop_logging() -&gt; None:\n    \"\"\"Stops the `QueueListener` if it's running.\"\"\"\n    global queue_listener\n    if queue_listener is not None:\n        queue_listener.stop()\n        queue_listener.queue.close()\n        queue_listener = None\n</code></pre>"},{"location":"reference/distilabel/utils/notebook/","title":"Notebook","text":""},{"location":"reference/distilabel/utils/notebook/#distilabel.utils.notebook.in_notebook","title":"<code>in_notebook()</code>","text":"<p>Checks if the current code is being executed from a Jupyter Notebook. This is useful for better handling the <code>asyncio</code> events under <code>nest_asyncio</code>, as Jupyter Notebook runs a separate event loop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the current code is being executed from a Jupyter Notebook.</p> References <ul> <li>https://stackoverflow.com/a/22424821</li> </ul> Source code in <code>src/distilabel/utils/notebook.py</code> <pre><code>def in_notebook() -&gt; bool:\n    \"\"\"Checks if the current code is being executed from a Jupyter Notebook.\n    This is useful for better handling the `asyncio` events under `nest_asyncio`,\n    as Jupyter Notebook runs a separate event loop.\n\n    Returns:\n        Whether the current code is being executed from a Jupyter Notebook.\n\n    References:\n        - https://stackoverflow.com/a/22424821\n    \"\"\"\n    try:\n        from IPython import get_ipython\n\n        if \"IPKernelApp\" not in get_ipython().config:  # pragma: no cover\n            return False\n    except ImportError:\n        return False\n    except AttributeError:\n        return False\n    return True\n</code></pre>"},{"location":"reference/distilabel/utils/serialization/","title":"Serialization","text":""},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.load_with_type_info","title":"<code>load_with_type_info(class_)</code>","text":"<p>Creates an instance of a class from a dictionary containing the type info and the serialized data of the class.</p> <p>Parameters:</p> Name Type Description Default <code>class_</code> <code>Any</code> <p>dictionary containing the type info and the serialized data of the class.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>An instance of the class with the data loaded from the dictionary.</p> Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def load_with_type_info(class_: Any) -&gt; Any:\n    \"\"\"Creates an instance of a class from a dictionary containing the type info and the\n    serialized data of the class.\n\n    Args:\n        class_: dictionary containing the type info and the serialized data of the class.\n\n    Returns:\n        An instance of the class with the data loaded from the dictionary.\n    \"\"\"\n    if not isinstance(class_, (list, dict)):\n        return class_\n\n    if isinstance(class_, list):\n        return [load_with_type_info(x) for x in class_]\n\n    for k, v in class_.items():\n        class_[k] = load_with_type_info(v) if isinstance(v, (dict, list)) else v\n\n        if isinstance(v, dict) and \"_type\" in v and v[\"_type\"] == \"enum\":\n            class_[k] = Enum(v[\"_name\"], v[\"_values\"], type=eval(v[\"_enum_type\"]))\n\n    if TYPE_INFO_KEY not in class_:\n        return class_\n\n    type_info = class_.pop(TYPE_INFO_KEY)\n    cls = _get_module_attr(type_info[\"module\"], type_info[\"name\"])\n\n    instance = cls(**class_)\n    return instance\n</code></pre>"},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.read_json","title":"<code>read_json(filename)</code>","text":"<p>Reads a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>StrOrPath</code> <p>the path to the JSON file.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data from the file.</p> Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def read_json(filename: StrOrPath) -&gt; Any:\n    \"\"\"Reads a JSON file.\n\n    Args:\n        filename: the path to the JSON file.\n\n    Returns:\n        The data from the file.\n    \"\"\"\n    with open(filename, \"rb\") as f:\n        return orjson.loads(f.read())\n</code></pre>"},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.read_yaml","title":"<code>read_yaml(filename)</code>","text":"<p>Reads a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>StrOrPath</code> <p>the path to the YAML file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The data from the file.</p> Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def read_yaml(filename: StrOrPath) -&gt; Dict[str, Any]:\n    \"\"\"Reads a YAML file.\n\n    Args:\n        filename: the path to the YAML file.\n\n    Returns:\n        The data from the file.\n    \"\"\"\n    with open(filename, \"r\") as file:\n        return yaml.load(file, Loader=yaml.FullLoader)\n</code></pre>"},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.write_json","title":"<code>write_json(filename, data)</code>","text":"<p>Writes a JSON file to the given path, creating the parent dir if needed.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Path</code> <p>the path to the file.</p> required <code>data</code> <code>Any</code> <p>the data to write to the file.</p> required Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def write_json(filename: Path, data: Any) -&gt; None:\n    \"\"\"Writes a JSON file to the given path, creating the parent dir if needed.\n\n    Args:\n        filename: the path to the file.\n        data: the data to write to the file.\n    \"\"\"\n    filename.parent.mkdir(parents=True, exist_ok=True)\n    with open(filename, \"wb\") as f:\n        f.write(orjson.dumps(data, option=orjson.OPT_SERIALIZE_NUMPY))\n</code></pre>"},{"location":"reference/distilabel/utils/serialization/#distilabel.utils.serialization.write_yaml","title":"<code>write_yaml(filename, data)</code>","text":"<p>Writes a YAML file to the given path, creating the parent dir if needed.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Path</code> <p>the path to the file.</p> required <code>data</code> <code>Dict[str, Any]</code> <p>the data to write to the file.</p> required Source code in <code>src/distilabel/utils/serialization.py</code> <pre><code>def write_yaml(filename: Path, data: Dict[str, Any]) -&gt; None:\n    \"\"\"Writes a YAML file to the given path, creating the parent dir if needed.\n\n    Args:\n        filename: the path to the file.\n        data: the data to write to the file.\n    \"\"\"\n    filename.parent.mkdir(parents=True, exist_ok=True)\n    with open(filename, \"w\") as file:\n        yaml.dump(data, file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/distilabel/utils/typing_/","title":"Typing","text":""},{"location":"reference/distilabel/utils/typing_/#distilabel.utils.typing_.is_parameter_annotated_with","title":"<code>is_parameter_annotated_with(parameter, annotation)</code>","text":"<p>Checks if a parameter type hint is <code>typing.Annotated</code> and in that case if it contains <code>annotation</code> as metadata.</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>Parameter</code> <p>the parameter to check.</p> required <code>annotation</code> <code>Any</code> <p>the annotation to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the parameter type hint is <code>typing.Annotated</code> and contains <code>annotation</code></p> <code>bool</code> <p>as metadata, <code>False</code> otherwise.</p> Source code in <code>src/distilabel/utils/typing_.py</code> <pre><code>def is_parameter_annotated_with(parameter: inspect.Parameter, annotation: Any) -&gt; bool:\n    \"\"\"Checks if a parameter type hint is `typing.Annotated` and in that case if it contains\n    `annotation` as metadata.\n\n    Args:\n        parameter: the parameter to check.\n        annotation: the annotation to check.\n\n    Returns:\n        `True` if the parameter type hint is `typing.Annotated` and contains `annotation`\n        as metadata, `False` otherwise.\n    \"\"\"\n    if get_origin(parameter.annotation) is not Annotated:\n        return False\n\n    for metadata in get_args(parameter.annotation):\n        if metadata == annotation:\n            return True\n\n    return False\n</code></pre>"},{"location":"reference/distilabel/utils/card/","title":"Index","text":""},{"location":"reference/distilabel/utils/card/dataset_card/","title":"Dataset card","text":""},{"location":"reference/distilabel/utils/card/dataset_card/#distilabel.utils.card.dataset_card.DistilabelDatasetCard","title":"<code>DistilabelDatasetCard</code>","text":"<p>               Bases: <code>DatasetCard</code></p> <p>A <code>DatasetCard</code> subclass that uses the Distilabel template by default.</p> Source code in <code>src/distilabel/utils/card/dataset_card.py</code> <pre><code>class DistilabelDatasetCard(DatasetCard):\n    \"\"\"A `DatasetCard` subclass that uses the Distilabel template by default.\"\"\"\n\n    default_template_path = TEMPLATE_DISTILABEL_DATASET_CARD_PATH\n</code></pre>"},{"location":"reference/distilabel/utils/mkdocs/","title":"Index","text":""},{"location":"reference/distilabel/utils/mkdocs/components_gallery/","title":"Components gallery","text":""},{"location":"reference/distilabel/utils/mkdocs/components_gallery/#distilabel.utils.mkdocs.components_gallery.ComponentsGalleryPlugin","title":"<code>ComponentsGalleryPlugin</code>","text":"<p>               Bases: <code>BasePlugin[ComponentsGalleryConfig]</code></p> <p>A MkDocs plugin to generate a components gallery page for <code>distilabel</code> components.</p> <p>Attributes:</p> Name Type Description <code>file_paths</code> <p>A dictionary to store the paths of the generated files. The keys are the subsections of the gallery and the values are the paths of the files.</p> Source code in <code>src/distilabel/utils/mkdocs/components_gallery.py</code> <pre><code>class ComponentsGalleryPlugin(BasePlugin[ComponentsGalleryConfig]):\n    \"\"\"A MkDocs plugin to generate a components gallery page for `distilabel` components.\n\n    Attributes:\n        file_paths: A dictionary to store the paths of the generated files. The keys are\n            the subsections of the gallery and the values are the paths of the files.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n        self.file_paths = {}\n\n    def on_config(self, config: \"MkDocsConfig\") -&gt; Union[\"MkDocsConfig\", None]:\n        if not self.config.enabled:\n            return\n\n    def on_files(\n        self, files: \"Files\", *, config: \"MkDocsConfig\"\n    ) -&gt; Union[\"Files\", None]:\n        \"\"\"Generates the files for the components gallery automatically from the docstrings.\n\n        Args:\n            files: The files collection.\n            config: The MkDocs configuration.\n\n        Returns:\n            The files collection with the new files added.\n        \"\"\"\n        src_dir = Path(config[\"site_dir\"])\n\n        components_info = export_components_info()\n\n        # Generate the `components-gallery/index.md`\n        self.file_paths[\"components_gallery\"] = self._generate_component_gallery_index(\n            src_dir=src_dir\n        )\n\n        # Create and write content to subsections\n        self.file_paths[\"steps\"] = self._generate_steps_pages(\n            src_dir=src_dir, steps=components_info[\"steps\"]\n        )\n        self.file_paths[\"tasks\"] = self._generate_tasks_pages(\n            src_dir=src_dir, tasks=components_info[\"tasks\"]\n        )\n        self.file_paths[\"llms\"] = self._generate_llms_pages(\n            src_dir=src_dir, llms=components_info[\"llms\"]\n        )\n\n        # Add the new files to the files collections\n        for relative_file_path in [\n            self.file_paths[\"components_gallery\"],\n            *self.file_paths[\"steps\"],\n            *self.file_paths[\"tasks\"],\n            *self.file_paths[\"llms\"],\n        ]:\n            file = File(\n                path=relative_file_path,\n                src_dir=str(src_dir),\n                dest_dir=config.site_dir,\n                use_directory_urls=config.use_directory_urls,\n            )\n            file.generated_by = \"distilabel/components-gallery\"  # type: ignore\n            files.append(file)\n\n        return files\n\n    def _generate_component_gallery_index(self, src_dir: Path) -&gt; str:\n        \"\"\"Generates the `components-gallery/index.md` file.\n\n        Args:\n            src_dir: The path to the source directory.\n\n        Returns:\n            The relative path to the generated file.\n        \"\"\"\n        index_template_path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"utils\"\n            / \"mkdocs\"\n            / \"templates\"\n            / \"components-gallery\"\n            / \"index.md\"\n        )\n\n        with open(index_template_path) as f:\n            index_template = f.read()\n\n        components_gallery_path_relative = \"components-gallery/index.md\"\n        components_gallery_path = src_dir / components_gallery_path_relative\n        components_gallery_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(components_gallery_path, \"w\") as f:\n            f.write(index_template)\n\n        return components_gallery_path_relative\n\n    def _generate_steps_pages(self, src_dir: Path, steps: list) -&gt; List[str]:\n        \"\"\"Generates the files for the `Steps` subsection of the components gallery.\n\n        Args:\n            src_dir: The path to the source directory.\n            steps: The list of `Step` components.\n\n        Returns:\n            The relative paths to the generated files.\n        \"\"\"\n\n        paths = [\"components-gallery/steps/index.md\"]\n        steps_gallery_page_path = src_dir / paths[0]\n        steps_gallery_page_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create detail page for each `Step`\n        for step in steps:\n            docstring = step[\"docstring\"]\n            if docstring[\"icon\"] == \"\" and docstring[\"categories\"]:\n                first_category = docstring[\"categories\"][0]\n                docstring[\"icon\"] = _STEPS_CATEGORY_TO_ICON.get(first_category, \"\")\n\n            name = step[\"name\"]\n\n            content = _STEP_DETAIL_TEMPLATE.render(\n                step=step,\n                mermaid_diagram=_generate_mermaid_diagram_for_io(\n                    step_name=step[\"name\"],\n                    inputs=list(docstring[\"input_columns\"].keys()),\n                    outputs=list(docstring[\"output_columns\"].keys()),\n                ),\n            )\n\n            step_path = f\"components-gallery/steps/{name.lower()}.md\"\n            path = src_dir / step_path\n            with open(path, \"w\") as f:\n                f.write(content)\n\n            paths.append(step_path)\n\n        # Create the `components-gallery/steps.md` file\n        content = _COMPONENTS_LIST_TEMPLATE.render(\n            title=\"Steps Gallery\",\n            description=\"\",\n            components=steps,\n            default_icon=\":material-step-forward:\",\n        )\n\n        with open(steps_gallery_page_path, \"w\") as f:\n            f.write(content)\n\n        return paths\n\n    def _generate_tasks_pages(self, src_dir: Path, tasks: list) -&gt; List[str]:\n        \"\"\"Generates the files for the `Tasks` subsection of the components gallery.\n\n        Args:\n            src_dir: The path to the source directory.\n            tasks: The list of `Task` components.\n\n        Returns:\n            The relative paths to the generated files.\n        \"\"\"\n\n        paths = [\"components-gallery/tasks/index.md\"]\n        tasks_gallery_page_path = src_dir / paths[0]\n        tasks_gallery_page_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create detail page for each `Task`\n        for task in tasks:\n            docstring = task[\"docstring\"]\n            if docstring[\"icon\"] == \"\" and docstring[\"categories\"]:\n                first_category = docstring[\"categories\"][0]\n                docstring[\"icon\"] = _TASKS_CATEGORY_TO_ICON.get(first_category, \"\")\n\n            name = task[\"name\"]\n\n            content = _STEP_DETAIL_TEMPLATE.render(\n                step=task,\n                mermaid_diagram=_generate_mermaid_diagram_for_io(\n                    step_name=task[\"name\"],\n                    inputs=list(docstring[\"input_columns\"].keys()),\n                    outputs=list(docstring[\"output_columns\"].keys()),\n                ),\n            )\n\n            task_path = f\"components-gallery/tasks/{name.lower()}.md\"\n            path = src_dir / task_path\n            with open(path, \"w\") as f:\n                f.write(content)\n\n            paths.append(task_path)\n\n        # Create the `components-gallery/steps/index.md` file\n        content = _COMPONENTS_LIST_TEMPLATE.render(\n            title=\"Tasks Gallery\",\n            description=\"\",\n            components=tasks,\n            default_icon=\":material-check-outline:\",\n        )\n\n        with open(tasks_gallery_page_path, \"w\") as f:\n            f.write(content)\n\n        return paths\n\n    def _generate_llms_pages(self, src_dir: Path, llms: list) -&gt; List[str]:\n        \"\"\"Generates the files for the `LLMs` subsection of the components gallery.\n\n        Args:\n            src_dir: The path to the source directory.\n            llms: The list of `LLM` components.\n\n        Returns:\n            The relative paths to the generated files.\n        \"\"\"\n\n        paths = [\"components-gallery/llms/index.md\"]\n        steps_gallery_page_path = src_dir / paths[0]\n        steps_gallery_page_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create detail page for each `LLM`\n        for llm in llms:\n            content = _LLM_DETAIL_TEMPLATE.render(llm=llm)\n\n            llm_path = f\"components-gallery/llms/{llm['name'].lower()}.md\"\n            path = src_dir / llm_path\n            with open(path, \"w\") as f:\n                f.write(content)\n\n            paths.append(llm_path)\n\n        # Create the `components-gallery/llms/index.md` file\n        content = _COMPONENTS_LIST_TEMPLATE.render(\n            title=\"LLMs Gallery\",\n            description=\"\",\n            components=llms,\n            component_group=\"llms\",\n            default_icon=\":material-brain:\",\n        )\n\n        with open(steps_gallery_page_path, \"w\") as f:\n            f.write(content)\n\n        return paths\n\n    def on_nav(\n        self, nav: \"Navigation\", *, config: \"MkDocsConfig\", files: \"Files\"\n    ) -&gt; Union[\"Navigation\", None]:\n        \"\"\"Adds the components gallery to the navigation bar.\n\n        Args:\n            nav: The navigation bar.\n            config: The MkDocs configuration.\n            files: The files collection.\n\n        Returns:\n            The navigation bar with the components gallery added.\n        \"\"\"\n        # Find the files in the files collection\n        components_gallery_file = files.get_file_from_path(\n            self.file_paths[\"components_gallery\"]\n        )\n        steps_file = files.get_file_from_path(self.file_paths[\"steps\"][0])\n        tasks_file = files.get_file_from_path(self.file_paths[\"tasks\"][0])\n        llms_file = files.get_file_from_path(self.file_paths[\"llms\"][0])\n\n        # Create subsections\n        steps_page = Page(\"Steps\", file=steps_file, config=config)  # type: ignore\n        tasks_page = Page(\"Tasks\", file=tasks_file, config=config)  # type: ignore\n        llms_page = Page(\"LLMs\", file=llms_file, config=config)  # type: ignore\n\n        # Create the gallery section\n        page = SectionPage(\n            title=self.config.page_title,\n            file=components_gallery_file,\n            config=config,\n            children=[steps_page, tasks_page, llms_page],\n        )\n\n        # Add the page\n        nav.pages.append(page)\n\n        # Add the page to the navigation bar\n        if self.config.add_after_page:\n            for i, item in enumerate(nav.items):\n                if item.title == self.config.add_after_page:\n                    nav.items.insert(i + 1, page)\n                    break\n        else:\n            nav.items.append(page)\n\n        return nav\n</code></pre>"},{"location":"reference/distilabel/utils/mkdocs/components_gallery/#distilabel.utils.mkdocs.components_gallery.ComponentsGalleryPlugin.on_files","title":"<code>on_files(files, *, config)</code>","text":"<p>Generates the files for the components gallery automatically from the docstrings.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Files</code> <p>The files collection.</p> required <code>config</code> <code>MkDocsConfig</code> <p>The MkDocs configuration.</p> required <p>Returns:</p> Type Description <code>Union[Files, None]</code> <p>The files collection with the new files added.</p> Source code in <code>src/distilabel/utils/mkdocs/components_gallery.py</code> <pre><code>def on_files(\n    self, files: \"Files\", *, config: \"MkDocsConfig\"\n) -&gt; Union[\"Files\", None]:\n    \"\"\"Generates the files for the components gallery automatically from the docstrings.\n\n    Args:\n        files: The files collection.\n        config: The MkDocs configuration.\n\n    Returns:\n        The files collection with the new files added.\n    \"\"\"\n    src_dir = Path(config[\"site_dir\"])\n\n    components_info = export_components_info()\n\n    # Generate the `components-gallery/index.md`\n    self.file_paths[\"components_gallery\"] = self._generate_component_gallery_index(\n        src_dir=src_dir\n    )\n\n    # Create and write content to subsections\n    self.file_paths[\"steps\"] = self._generate_steps_pages(\n        src_dir=src_dir, steps=components_info[\"steps\"]\n    )\n    self.file_paths[\"tasks\"] = self._generate_tasks_pages(\n        src_dir=src_dir, tasks=components_info[\"tasks\"]\n    )\n    self.file_paths[\"llms\"] = self._generate_llms_pages(\n        src_dir=src_dir, llms=components_info[\"llms\"]\n    )\n\n    # Add the new files to the files collections\n    for relative_file_path in [\n        self.file_paths[\"components_gallery\"],\n        *self.file_paths[\"steps\"],\n        *self.file_paths[\"tasks\"],\n        *self.file_paths[\"llms\"],\n    ]:\n        file = File(\n            path=relative_file_path,\n            src_dir=str(src_dir),\n            dest_dir=config.site_dir,\n            use_directory_urls=config.use_directory_urls,\n        )\n        file.generated_by = \"distilabel/components-gallery\"  # type: ignore\n        files.append(file)\n\n    return files\n</code></pre>"},{"location":"reference/distilabel/utils/mkdocs/components_gallery/#distilabel.utils.mkdocs.components_gallery.ComponentsGalleryPlugin.on_nav","title":"<code>on_nav(nav, *, config, files)</code>","text":"<p>Adds the components gallery to the navigation bar.</p> <p>Parameters:</p> Name Type Description Default <code>nav</code> <code>Navigation</code> <p>The navigation bar.</p> required <code>config</code> <code>MkDocsConfig</code> <p>The MkDocs configuration.</p> required <code>files</code> <code>Files</code> <p>The files collection.</p> required <p>Returns:</p> Type Description <code>Union[Navigation, None]</code> <p>The navigation bar with the components gallery added.</p> Source code in <code>src/distilabel/utils/mkdocs/components_gallery.py</code> <pre><code>def on_nav(\n    self, nav: \"Navigation\", *, config: \"MkDocsConfig\", files: \"Files\"\n) -&gt; Union[\"Navigation\", None]:\n    \"\"\"Adds the components gallery to the navigation bar.\n\n    Args:\n        nav: The navigation bar.\n        config: The MkDocs configuration.\n        files: The files collection.\n\n    Returns:\n        The navigation bar with the components gallery added.\n    \"\"\"\n    # Find the files in the files collection\n    components_gallery_file = files.get_file_from_path(\n        self.file_paths[\"components_gallery\"]\n    )\n    steps_file = files.get_file_from_path(self.file_paths[\"steps\"][0])\n    tasks_file = files.get_file_from_path(self.file_paths[\"tasks\"][0])\n    llms_file = files.get_file_from_path(self.file_paths[\"llms\"][0])\n\n    # Create subsections\n    steps_page = Page(\"Steps\", file=steps_file, config=config)  # type: ignore\n    tasks_page = Page(\"Tasks\", file=tasks_file, config=config)  # type: ignore\n    llms_page = Page(\"LLMs\", file=llms_file, config=config)  # type: ignore\n\n    # Create the gallery section\n    page = SectionPage(\n        title=self.config.page_title,\n        file=components_gallery_file,\n        config=config,\n        children=[steps_page, tasks_page, llms_page],\n    )\n\n    # Add the page\n    nav.pages.append(page)\n\n    # Add the page to the navigation bar\n    if self.config.add_after_page:\n        for i, item in enumerate(nav.items):\n            if item.title == self.config.add_after_page:\n                nav.items.insert(i + 1, page)\n                break\n    else:\n        nav.items.append(page)\n\n    return nav\n</code></pre>"},{"location":"sections/faq/","title":"Frequent Asked Questions (FAQ)","text":"How can I rename the columns in a batch? <p>Every <code>Step</code> has both <code>input_mappings</code> and <code>output_mappings</code> attributes, that can be used to rename the columns in each batch.</p> <p>But <code>input_mappings</code> will only map, meaning that if you have a batch with the column <code>A</code> and you want to rename to <code>B</code>, you should use <code>input_mappings={\"A\": \"B\"}</code>, but that will only be applied to that specific <code>Step</code> meaning that the next step in the pipeline will still have the column <code>A</code> instead of <code>B</code>.</p> <p>While <code>output_mappings</code> will indeed apply the rename, meaning that if the <code>Step</code> produces the column <code>A</code> and you want to rename to <code>B</code>, you should use <code>output_mappings={\"A\": \"B\"}</code>, and that will be applied to the next <code>Step</code> in the pipeline.</p> Will the API Keys be exposed when sharing the pipeline? <p>No, those will be masked out using <code>pydantic.SecretStr</code>, meaning that those won't be exposed when sharing the pipeline.</p> <p>This also means that if you want to re-run your own pipeline and the API keys have not been provided via environment variable but either via attribute or runtime parameter, you will need to provide them again.</p> Does it work for Windows? <p>Yes, but you may need to set the <code>multiprocessing</code> context in advance, to ensure that the <code>spawn</code> method is used, since the default method <code>fork</code> is not available on Windows.</p> <pre><code>import multiprocessing as mp\n\nmp.set_start_method(\"spawn\")\n</code></pre> Will the custom Steps / Tasks / LLMs be serialized too? <p>No, at the moment only the references to the classes within the <code>distilabel</code> library will be serialized, meaning that if you define a custom class used within the pipeline, the serialization won't break, but the deserialize will fail since the class won't be available, unless used from the same file.</p> What happens if <code>Pipeline.run</code> fails? Do I lose all the data? <p>No, indeed we're using a cache mechanism to store all the intermediate results in disk, so that if a <code>Step</code> fails, the pipeline can be re-run from that point without losing the data, only if nothing is changed in the <code>Pipeline</code>.</p> <p>All the data will be stored in <code>.cache/distilabel</code>, but the only data that will persist at the end of the <code>Pipeline.run</code> execution is the one from the leaf step/s, so bear that in mind.</p> <p>For more information on the caching mechanism in <code>distilabel</code>, you can check the Learn - Advanced - Caching section.</p> <p>Also note that when running a <code>Step</code> or a <code>Task</code> standalone, the cache mechanism won't be used, so if you want to use that, you should use the <code>Pipeline</code> context manager.</p>"},{"location":"sections/how_to_guide/","title":"How to Guide","text":"<p>To start off, <code>distilabel</code> is a framework for building pipelines for generating synthetic data using LLMs, that defines a <code>Pipeline</code> which orchestrates the execution of the <code>Step</code> subclasses, and those will be connected as nodes in a Direct Acyclic Graph (DAG).</p> <p>This being said, in this guide we will walk you through the process of creating a simple pipeline that uses the <code>OpenAILLM</code> class to generate text.\u00e5 The <code>Pipeline</code> will load a dataset that contains a column named <code>prompt</code> from the Hugging Face Hub via the step <code>LoadHubDataset</code> and then use the <code>OpenAILLM</code> class to generate text based on the dataset using the <code>TextGeneration</code> task.</p> <pre><code>from distilabel.llms import OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\nwith Pipeline(  # (1)\n    name=\"simple-text-generation-pipeline\",\n    description=\"A simple text generation pipeline\",\n) as pipeline:  # (2)\n    load_dataset = LoadHubDataset(  # (3)\n        name=\"load_dataset\",\n        output_mappings={\"prompt\": \"instruction\"},\n    )\n\n    text_generation = TextGeneration(  # (4)\n        name=\"text_generation\",\n        llm=OpenAILLM(model=\"gpt-3.5-turbo\"),  # (5)\n    )\n\n    load_dataset &gt;&gt; text_generation  # (6)\n\nif __name__ == \"__main__\":\n    distiset = pipeline.run(  # (7)\n        parameters={\n            load_dataset.name: {\n                \"repo_id\": \"distilabel-internal-testing/instruction-dataset-mini\",\n                \"split\": \"test\",\n            },\n            text_generation.name: {\n                \"llm\": {\n                    \"generation_kwargs\": {\n                        \"temperature\": 0.7,\n                        \"max_new_tokens\": 512,\n                    }\n                }\n            },\n        },\n    )\n    distiset.push_to_hub(repo_id=\"distilabel-example\")  # (8)\n</code></pre> <ol> <li> <p>We define a <code>Pipeline</code> with the name <code>simple-text-generation-pipeline</code> and a description <code>A simple text generation pipeline</code>. Note that the <code>name</code> is mandatory and will be used to calculate the <code>cache</code> signature path, so changing the name will change the cache path and will be identified as a different pipeline.</p> </li> <li> <p>We are using the <code>Pipeline</code> context manager, meaning that every <code>Step</code> subclass that is defined within the context manager will be added to the pipeline automatically.</p> </li> <li> <p>We define a <code>LoadHubDataset</code> step named <code>load_dataset</code> that will load a dataset from the Hugging Face Hub, as provided via runtime parameters in the <code>pipeline.run</code> method below, but it can also be defined within the class instance via the arg <code>repo_id=...</code>. This step will basically produce output batches with the rows from the dataset, and the column <code>prompt</code> will be mapped to the <code>instruction</code> field.</p> </li> <li> <p>We define a <code>TextGeneration</code> task named <code>text_generation</code> that will generate text based on the <code>instruction</code> field from the dataset. This task will use the <code>OpenAILLM</code> class with the model <code>gpt-3.5-turbo</code>.</p> </li> <li> <p>We define the <code>OpenAILLM</code> class with the model <code>gpt-3.5-turbo</code> that will be used by the <code>TextGeneration</code> task. In this case, since the <code>OpenAILLM</code> is used, we assume that the <code>OPENAI_API_KEY</code> environment variable is set, and the OpenAI API will be used to generate the text.</p> </li> <li> <p>We connect the <code>load_dataset</code> step to the <code>text_generation</code> task using the <code>rshift</code> operator, meaning that the output from the <code>load_dataset</code> step will be used as input for the <code>text_generation</code> task.</p> </li> <li> <p>We run the pipeline with the parameters for the <code>load_dataset</code> and <code>text_generation</code> steps. The <code>load_dataset</code> step will use the repository <code>distilabel-internal-testing/instruction-dataset-mini</code> and the <code>test</code> split, and the <code>text_generation</code> task will use the <code>generation_kwargs</code> with the <code>temperature</code> set to <code>0.7</code> and the <code>max_new_tokens</code> set to <code>512</code>.</p> </li> <li> <p>Optionally, we can push the generated <code>Distiset</code> to the Hugging Face Hub repository <code>distilabel-example</code>. This will allow you to share the generated dataset with others and use it in other pipelines.</p> </li> </ol>"},{"location":"sections/installation/","title":"Installation","text":"<p>Note</p> <p>Since <code>distilabel</code> v1.0.0 was recently released, we refactored most of the stuff, so the installation below only applies to <code>distilabel</code> v1.0.0 and above.</p> <p>You will need to have at least Python 3.8 or higher, up to Python 3.12, since support for the latter is still a work in progress.</p> <p>To install the latest release of the package from PyPI you can use the following command:</p> <pre><code>pip install distilabel --upgrade\n</code></pre> <p>Alternatively, you may also want to install it from source i.e. the latest unreleased version, you can use the following command:</p> <pre><code>pip install \"distilabel @ git+https://github.com/argilla-io/distilabel.git@develop\" --upgrade\n</code></pre> <p>Note</p> <p>We are installing from <code>develop</code> since that's the branch we use to collect all the features, bug fixes, and improvements that will be part of the next release. If you want to install from a specific branch, you can replace <code>develop</code> with the branch name.</p>"},{"location":"sections/installation/#extras","title":"Extras","text":"<p>Additionally, as part of <code>distilabel</code> some extra dependencies are available, mainly to add support for some of the LLM integrations we support. Here's a list of the available extras:</p> <ul> <li> <p><code>anthropic</code>: for using models available in Anthropic API via the <code>AnthropicLLM</code> integration.</p> </li> <li> <p><code>argilla</code>: for exporting the generated datasets to Argilla.</p> </li> <li> <p><code>cohere</code>: for using models available in Cohere via the <code>CohereLLM</code> integration.</p> </li> <li> <p><code>groq</code>: for using models available in Groq using <code>groq</code> Python client via the <code>GroqLLM</code> integration.</p> </li> <li> <p><code>hf-inference-endpoints</code>: for using the Hugging Face Inference Endpoints via the <code>InferenceEndpointsLLM</code> integration.</p> </li> <li> <p><code>hf-transformers</code>: for using models available in transformers package via the <code>TransformersLLM</code> integration.</p> </li> <li> <p><code>litellm</code>: for using <code>LiteLLM</code> to call any LLM using OpenAI format via the <code>LiteLLM</code> integration.</p> </li> <li> <p><code>llama-cpp</code>: for using llama-cpp-python Python bindings for <code>llama.cpp</code> via the <code>LlamaCppLLM</code> integration.</p> </li> <li> <p><code>mistralai</code>: for using models available in Mistral AI API via the <code>MistralAILLM</code> integration. Note that the <code>mistralai</code> Python client can only be installed from Python 3.9 onwards, so this is the only <code>distilabel</code> dependency that's not supported in Python 3.8.</p> </li> <li> <p><code>ollama</code>: for using Ollama and their available models via <code>OllamaLLM</code> integration.</p> </li> <li> <p><code>openai</code>: for using OpenAI API models via the <code>OpenAILLM</code> integration, or the rest of the integrations based on OpenAI and relying on its client as <code>AnyscaleLLM</code>, <code>AzureOpenAILLM</code>, and <code>TogetherLLM</code>.</p> </li> <li> <p><code>vertexai</code>: for using Google Vertex AI proprietary models via the <code>VertexAILLM</code> integration.</p> </li> <li> <p><code>vllm</code>: for using vllm serving engine via the <code>vLLM</code> integration.</p> </li> </ul>"},{"location":"sections/installation/#recommendations-notes","title":"Recommendations / Notes","text":"<p>The <code>mistralai</code> dependency requires Python 3.9 or higher, so if you're willing to use the <code>distilabel.llms.MistralLLM</code> implementation, you will need to have Python 3.9 or higher.</p> <p>In some cases like <code>transformers</code> and <code>vllm</code> the installation of <code>flash-attn</code> is recommended if you are using a GPU accelerator, since it will speed up the inference process, but the installation needs to be done separately, as it's not included in the <code>distilabel</code> dependencies.</p> <pre><code>pip install flash-attn --no-build-isolation\n</code></pre> <p>Also, if you are willing to use the <code>llama-cpp-python</code> integration for running local LLMs, note that the installation process may get a bit trickier depending on which OS are you using, so we recommend you to read through their Installation section in their docs.</p>"},{"location":"sections/learn/","title":"Learn","text":"<p>Here is a step by step guide to all the components of <code>distilabel</code> in a Tutorial form and a special section for more advanced topics.</p>"},{"location":"sections/learn/advanced/","title":"Advanced","text":"<p>This subsection will cover the advanced components of <code>distilabel</code> which are either internal specifications on how <code>distilabel</code> works or components used to create more complex and robust pipelines.</p>"},{"location":"sections/learn/advanced/argilla/","title":"Argilla","text":"<p>As an additional step, besides being able to restore the dataset from the <code>Pipeline</code> output as a <code>Distiset</code> (which is a <code>datasets.DatasetDict</code> with multiple configurations depending on the leaf nodes of the <code>Pipeline</code>), one can also include a <code>Step</code> within the <code>Pipeline</code> to easily export the datasets to Argilla with a pre-defined configuration, suiting the annotation purposes.</p> <p>Being able to export the generated synthetic datasets to Argilla, was one of the core features we wanted to have integrated within <code>distilabel</code> because we believe in the potential of synthetic data, but without removing the impact a human annotator or group of annotators can bring. So on, the Argilla integration makes it straightforward to push a dataset to Argilla while the <code>Pipeline</code> is running, to be able to follow along the generation process in Argilla's UI, as well as annotating the records on the fly.</p> <p>Before using any of the steps about to be described below, you should first have an Argilla instance up and running, so that you can successfully upload the data to Argilla. In order to deploy Argilla, the easiest and most straight forward way is to deploy it via the Argilla Template in Hugging Face Spaces as simply as following the steps there, or just via the following button:</p> <p> </p> <p>Additionally, Argilla offer multiple deployment options listed in the Argilla Documentation - Installation page.</p>"},{"location":"sections/learn/advanced/argilla/#text-generation","title":"Text Generation","text":"<p>For text generation scenarios, i.e. when the <code>Pipeline</code> contains a <code>TextGeneration</code> step, we have designed the task <code>TextGenerationToArgilla</code>, which will seamlessly push the generated data to Argilla, and allow the annotator to review the records.</p> <p>The dataset will be pushed with the following configuration:</p> <ul> <li> <p>Fields: <code>instruction</code> and <code>generation</code>, both being fields of type <code>argilla.TextField</code>, plus the automatically generated <code>id</code> for the given <code>instruction</code> to be able to search for other records with the same <code>instruction</code> in the dataset. The field <code>instruction</code> must always be a string, while the field <code>generation</code> can either be a single string or a list of strings (useful when there are multiple parent nodes of type <code>TextGeneration</code>); even though each record will always contain at most one <code>instruction</code>-<code>generation</code> pair.</p> </li> <li> <p>Questions: <code>quality</code> will be the only question for the annotators to answer, i.e., to annotate, and it will be an <code>argilla.LabelQuestion</code> referring to the quality of the provided generation for the given instruction. It can be annotated as either \ud83d\udc4e (bad) or \ud83d\udc4d (good).</p> </li> </ul> <p>Note</p> <p>The <code>TextGenerationToArgilla</code> step will only work as is if the <code>Pipeline</code> contains one or multiple <code>TextGeneration</code> steps, or if the columns <code>instruction</code> and <code>generation</code> are available within the batch data. Otherwise, the variable <code>input_mappings</code> will need to be set so that either both or one of <code>instruction</code> and <code>generation</code> are mapped to one of the existing columns in the batch data.</p> <pre><code>from distilabel.llms import OpenAILLM\nfrom distilabel.steps import LoadDataFromDicts, TextGenerationToArgilla\nfrom distilabel.steps.tasks import TextGeneration\n\n\nwith Pipeline(name=\"my-pipeline\") as pipeline:\n    load_dataset = LoadDataFromDicts(\n        name=\"load_dataset\",\n        data=[\n            {\n                \"instruction\": \"Write a short story about a dragon that saves a princess from a tower.\",\n            },\n        ],\n    )\n\n    text_generation = TextGeneration(\n        name=\"text_generation\",\n        llm=OpenAILLM(model=\"gpt-4\"),\n    )\n\n    to_argilla = TextGenerationToArgilla(\n        dataset_name=\"my-dataset\",\n        dataset_workspace=\"admin\",\n        api_url=\"&lt;ARGILLA_API_URL&gt;\",\n        api_key=\"&lt;ARGILLA_API_KEY&gt;\",\n    )\n\n    load_dataset &gt;&gt; text_generation &gt;&gt; to_argilla\n\npipeline.run()\n</code></pre> <p></p>"},{"location":"sections/learn/advanced/argilla/#preference","title":"Preference","text":"<p>For preference scenarios, i.e. when the <code>Pipeline</code> contains multiple <code>TextGeneration</code> steps, we have designed the task <code>PreferenceToArgilla</code>, which will seamlessly push the generated data to Argilla, and allow the annotator to review the records.</p> <p>The dataset will be pushed with the following configuration:</p> <ul> <li> <p>Fields: <code>instruction</code> and <code>generations</code>, both being fields of type <code>argilla.TextField</code>, plus the automatically generated <code>id</code> for the given <code>instruction</code> to be able to search for other records with the same <code>instruction</code> in the dataset. The field <code>instruction</code> must always be a string, while the field <code>generations</code> must be a list of strings, containing the generated texts for the given <code>instruction</code> so that at least there are two generations to compare. Other than that, the number of <code>generation</code> fields within each record in Argilla will be defined by the value of the variable <code>num_generations</code> to be provided in the <code>PreferenceToArgilla</code> step.</p> </li> <li> <p>Questions: <code>rating</code> and <code>rationale</code> will be the pairs of questions to be defined per each generation i.e. per each value within the range from 0 to <code>num_generations</code>, and those will be of types <code>argilla.RatingQuestion</code> and <code>argilla.TextQuestion</code>, respectively. Note that only the first pair of questions will be mandatory, since only one generation is ensured to be within the batch data. Additionally, note that the provided ratings will range from 1 to 5, and to mention that Argilla only supports values above 0.</p> </li> </ul> <p>Note</p> <p>The <code>PreferenceToArgilla</code> step will only work if the <code>Pipeline</code> contains multiple <code>TextGeneration</code> steps, or if the columns <code>instruction</code> and <code>generations</code> are available within the batch data. Otherwise, the variable <code>input_mappings</code> will need to be set so that either both or one of <code>instruction</code> and <code>generations</code> are mapped to one of the existing columns in the batch data.</p> <p>Note</p> <p>Additionally, if the <code>Pipeline</code> contains an <code>UltraFeedback</code> step, the <code>ratings</code> and <code>rationales</code> will also be available, so if that's the case, those will be automatically injected as suggestions to the existing dataset so that the annotator only needs to review those, instead of fulfilling those by themselves.</p> <pre><code>from distilabel.llms import OpenAILLM\nfrom distilabel.steps import LoadDataFromDicts, PreferenceToArgilla\nfrom distilabel.steps.tasks import TextGeneration\n\n\nwith Pipeline(name=\"my-pipeline\") as pipeline:\n    load_dataset = LoadDataFromDicts(\n        name=\"load_dataset\",\n        data=[\n            {\n                \"instruction\": \"Write a short story about a dragon that saves a princess from a tower.\",\n            },\n        ],\n    )\n\n    text_generation = TextGeneration(\n        name=\"text_generation\",\n        llm=OpenAILLM(model=\"gpt-4\"),\n        num_generations=4,\n        group_generations=True,\n    )\n\n    to_argilla = PreferenceToArgilla(\n        dataset_name=\"my-dataset\",\n        dataset_workspace=\"admin\",\n        api_url=\"&lt;ARGILLA_API_URL&gt;\",\n        api_key=\"&lt;ARGILLA_API_KEY&gt;\",\n        num_generations=4,\n    )\n\n    load_dataset &gt;&gt; text_generation &gt;&gt; to_argilla\n\npipeline.run()\n</code></pre> <p></p> <p>Note</p> <p>If you are willing to also add the suggestions, feel free to check \"UltraFeedback: Boosting Language Models with High-quality Feedback\" where the <code>UltraFeedback</code> task is used to generate both ratings and rationales for each of the generations of a given instruction.</p>"},{"location":"sections/learn/advanced/caching/","title":"Caching","text":"<p>Distilabel <code>Pipelines</code> automatically save all the intermediate steps to to avoid losing any data in case of error.</p>"},{"location":"sections/learn/advanced/caching/#cache-directory","title":"Cache directory","text":"<p>Out of the box, the <code>Pipeline</code> will use the <code>~/.cache/distilabel/pipelines</code> directory to store the different pipelines<sup>1</sup>:</p> <pre><code>from distilabel.pipeline.local import Pipeline\n\nwith Pipeline(name=\"cache_testing\") as pipeline:\n    ...\n</code></pre> <p>This directory can be modified by setting the <code>DISTILABEL_CACHE_DIR</code> environment variable (<code>export DISTILABEL_CACHE_DIR=my_cache_dir</code>) or by explicitly passing the <code>cache_dir</code> variable to the <code>Pipeline</code> constructor like so:</p> <pre><code>with Pipeline(name=\"cache_testing\", cache_dir=\"~/my_cache_dir\") as pipeline:\n    ...\n</code></pre>"},{"location":"sections/learn/advanced/caching/#how-does-it-work","title":"How does it work?","text":"<p>Let's take a look at the logging messages from a sample pipeline.</p> <p>When we run a <code>Pipeline</code> for the first time</p> <p></p> <p>If we decide to stop the pipeline (say we kill the run altogether via <code>CTRL + C</code> or <code>CMD + C</code> in macOS), we will see the signal sent to the different workers:</p> <p></p> <p>After this step, when we run again the pipeline, the first log message we see corresponds to \"Load pipeline from cache\", which will restart processing from where it stopped:</p> <p></p> <p>Finally, if we decide to run the same <code>Pipeline</code> after it has finished completely, it won't start again but resume the process, as we already have all the data processed:</p> <p></p>"},{"location":"sections/learn/advanced/caching/#serialization","title":"Serialization","text":"<p>Let's see what gets serialized by looking at a sample <code>Pipeline</code>'s cached folder:</p> <pre><code>$ tree ~/.cache/distilabel/pipelines/73ca3f6b7a613fb9694db7631cc038d379f1f533\n\u251c\u2500\u2500 batch_manager.json\n\u251c\u2500\u2500 batch_manager_steps\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 generate_response.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 rename_columns.json\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 generate_response\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 00001.parquet\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 00002.parquet\n\u2514\u2500\u2500 pipeline.yaml\n</code></pre> <p>The <code>Pipeline</code> will have a signature created from the arguments that define it so we can find it afterwards, and the contents are the following:</p> <ul> <li> <p><code>batch_manager.json</code></p> <p>Folder that stores the content of the internal batch manager to keep track of the data. Along with the <code>batch_manager_steps/</code> they store the information to restart the <code>Pipeline</code>. One shouldn't need to know about it.</p> </li> <li> <p><code>pipeline.yaml</code></p> <p>This file contains a representation of the <code>Pipeline</code> in YAML format. If we push a <code>Distiset</code> to the Hugging Face Hub as obtained from calling <code>Pipeline.run</code>, this file will be stored at our datasets' repository, allowing to reproduce the <code>Pipeline</code> using the <code>CLI</code>:</p> <pre><code>distilabel pipeline run --config \"path/to/pipeline.yaml\"\n</code></pre> </li> <li> <p><code>data/</code></p> <p>Folder that stores the data generated, with a special folder to keep track of each <code>leaf_step</code> separately. We can recreate a <code>Distiset</code> from the contents of this folder (Parquet files), as we will see next.</p> </li> <li> <p><code>pipeline.log</code></p> <p>This file stores the logs that the <code>Pipeline</code> generated while processing. Just as with the <code>pipeline.yaml</code> file, it will be pushed to the Hugging Face Hub datasets` repository to keep track of the information.</p> </li> </ul>"},{"location":"sections/learn/advanced/caching/#create_distiset","title":"create_distiset","text":"<p>In case we wanted to regenerate the dataset from the <code>cache</code>, we can do it using the <code>create_distiset</code> function and passing the path to the <code>/data</code> folder inside our <code>Pipeline</code>:</p> <pre><code>from pathlib import Path\nfrom distilabel.distiset import create_distiset\n\npath = Path(\"~/.cache/distilabel/pipelines/73ca3f6b7a613fb9694db7631cc038d379f1f533/data\")\nds = create_distiset(path)\nds\n# Distiset({\n#     generate_response: DatasetDict({\n#         train: Dataset({\n#             features: ['instruction', 'response'],\n#             num_rows: 80\n#         })\n#     })\n# })\n</code></pre> <p>Note</p> <p>Internally, the function will try to inject the <code>pipeline_path</code> variable if it's not passed via argument, assuming it's in the parent directory of the current one, called <code>pipeline.yaml</code>. If the file doesn't exist, it won't raise any error, but take into account that if the <code>Distiset</code> is pushed to the Hugging Face Hub, the <code>pipeline.yaml</code> won't be generated. The same happens with the <code>pipeline.log</code> file, it can be passed via <code>log_filename_path</code>, but it will try to locate it automatically.</p> <p>Lastly, there is the option of including the <code>distilabel_metadata</code> column in the final dataset. This column can contain custom metadata generated automatically by the pipeline, like the raw output from an <code>LLM</code> without formatting in case of failure, and we can decide whether to include it using the <code>enable_metadata</code> argument.</p> <ol> <li> <p>The pipelines will be organized according to the pipeline's name attribute, and then by the hash, in case you want to look for something manually, like the following example:</p> <p><pre><code>$ tree ~/.cache/distilabel/pipelines/\n\u251c\u2500\u2500 cache_testing\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 13da04d2cc255b2180d6bebb50fb5be91124f70d\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 batch_manager.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 batch_manager_steps\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 succeed_always_0.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 succeed_always_0\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 00001.parquet\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 pipeline.log\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 pipeline.yaml\n\u2514\u2500\u2500 test-pipe\n    \u2514\u2500\u2500 f23b95d7ad4e9301a70b2a54c953f8375ebfcd5c\n        \u251c\u2500\u2500 batch_manager.json\n        \u251c\u2500\u2500 batch_manager_steps\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 text_generation_0.json\n        \u251c\u2500\u2500 data\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 text_generation_0\n        \u2502\u00a0\u00a0     \u2514\u2500\u2500 00001.parquet\n        \u251c\u2500\u2500 pipeline.log\n        \u2514\u2500\u2500 pipeline.yaml\n</code></pre> \u21a9</p> </li> </ol>"},{"location":"sections/learn/advanced/distiset/","title":"Distiset","text":"<p>A <code>Pipeline</code> in <code>distilabel</code> returns a special type of Hugging Face <code>datasets.DatasetDict</code> which is called <code>Distiset</code>, as a combination of <code>distilabel</code> and dataset. This object is a wrapper around <code>datasets.Dataset</code> which comes with some extra functionality to easily deal with the dataset pieces that a <code>Pipeline</code> can generate.</p> <p>The <code>Distiset</code> is a dictionary-like object that contains the different configurations generated by the <code>Pipeline</code>, where each configuration corresponds to each leaf step in the DAG built by the <code>Pipeline</code>. Each configuration corresponds to a different subset of the dataset, which is a concept taken from \ud83e\udd17 <code>datasets</code> that lets you upload different configurations of the same dataset within the same repository and can contain different columns i.e. different configurations, which can be seamlessly pushed to the Hugging Face Hub straight away.</p> <p>Below you can find an example on how to create a <code>Distiset</code> object, similarly as a <code>datasets.DatasetDict</code>, which is not required in <code>distilabel</code> since that's internally handled by the <code>Pipeline</code> as part of the output of the <code>run</code> method:</p> <pre><code>from datasets import Dataset\nfrom distilabel.distiset import Distiset\n\ndistiset = Distiset(\n    {\n        \"leaf_step_1\": Dataset.from_dict({\"instruction\": [1, 2, 3]}),\n        \"leaf_step_2\": Dataset.from_dict(\n            {\"instruction\": [1, 2, 3, 4], \"generation\": [5, 6, 7, 8]}\n        ),\n    }\n)\n</code></pre> <p>Note</p> <p>If there's only one leaf node, i.e., only one step at the end of the <code>Pipeline</code>, then the configuration name won't be the name of the last step, but it will be set to \"default\" instead, as that's more aligned with standard datasets within the Hugging Face Hub.</p>"},{"location":"sections/learn/advanced/distiset/#distiset-methods","title":"Distiset methods","text":"<p>We can interact with the different pieces generated by the <code>Pipeline</code> and treat them as different <code>configurations</code>. The <code>Distiset</code> contains just two methods:</p>"},{"location":"sections/learn/advanced/distiset/#traintest-split","title":"Train/Test split","text":"<p>Which easily does the train/test split partition of the dataset for the different configurations or subsets.</p> <pre><code>&gt;&gt;&gt; distiset.train_test_split(train_size=0.9)\nDistiset({\n    leaf_step_1: DatasetDict({\n        train: Dataset({\n            features: ['instruction'],\n            num_rows: 2\n        })\n        test: Dataset({\n            features: ['instruction'],\n            num_rows: 1\n        })\n    })\n    leaf_step_2: DatasetDict({\n        train: Dataset({\n            features: ['instruction', 'generation'],\n            num_rows: 3\n        })\n        test: Dataset({\n            features: ['instruction', 'generation'],\n            num_rows: 1\n        })\n    })\n})\n</code></pre>"},{"location":"sections/learn/advanced/distiset/#push-to-hugging-face-hub","title":"Push to Hugging Face Hub","text":"<p>Pushes the <code>Distiset</code> to a Hugging Face repository, where each one of the subsets will correspond to a different configuration:</p> <pre><code>distiset.push_to_hub(\n    \"my-org/my-dataset\",\n    commit_message=\"Initial commit\",\n    private=False,\n    token=os.getenv(\"HF_TOKEN\"),\n)\n</code></pre>"},{"location":"sections/learn/advanced/distiset/#save-and-load-from-disk","title":"Save and load from disk","text":"<p>Saves the <code>Distiset</code> to disk, and optionally (will be done by default) saves the dataset card, the pipeline config file and logs:</p> <pre><code>distiset.save_to_disk(\n    \"my-dataset\",\n    save_card=True,\n    save_pipeline_config=True,\n    save_pipeline_log=True\n)\n</code></pre> <p>And load a <code>Distiset</code> that was saved using <code>Distiset.save_to_disk</code> just the same way:</p> <pre><code>from distilabel.distiset import Distiset\n\ndistiset = Distiset.load_from_disk(\"my-dataset\")\n</code></pre> <p>or from your cloud provider if that's where it was stored:</p> <pre><code>distiset = Distiset.load_from_disk(\n    \"s3://path/to/my_dataset\",  # gcs:// or any filesystem tolerated by fsspec\n    storage_options={\n        \"key\": os.environ[\"S3_ACCESS_KEY\"],\n        \"secret\": os.environ[\"S3_SECRET_KEY\"],\n        ...\n    }\n)\n</code></pre> <p>Take into account that these methods work as <code>datasets.load_from_disk</code> and <code>datasets.Dataset.save_to_disk</code> so the arguments are directly passed to those methods. This means you can also make use of <code>storage_options</code> argument to save your <code>Distiset</code> in your cloud provider, including the distilabel artifacts (<code>pipeline.yaml</code>, <code>pipeline.log</code> and the <code>README.md</code> with the dataset card). You can read more in <code>datasets</code> documentation here.</p> <p>Take a look at the remaining arguments at <code>Distiset.save_to_disk</code> and <code>Distiset.load_from_disk</code>.</p>"},{"location":"sections/learn/advanced/distiset/#dataset-card","title":"Dataset card","text":"<p>Having this special type of dataset comes with an added advantage when calling <code>Distiset.push_to_hub</code>, which is the automatically generated dataset card in the Hugging Face Hub. Note that it is enabled by default, but can be disabled by setting <code>generate_card=False</code>:</p> <pre><code>distiset.push_to_hub(\"my-org/my-dataset\", generate_card=True)\n</code></pre> <p>We will have an automatic dataset card (an example can be seen here) with some handy information like reproducing the <code>Pipeline</code> with the <code>CLI</code>, or examples of the records from the different subsets.</p>"},{"location":"sections/learn/advanced/distiset/#create_distiset-helper","title":"create_distiset helper","text":"<p>Lastly, we presented in the caching section the <code>create_distiset</code> function, you can take a look at the section to see how to create a <code>Distiset</code> from the cache folder, using the helper function to automatically include all the relevant data.</p>"},{"location":"sections/learn/advanced/fs_to_pass_data/","title":"Using a file system to pass data of batches between steps","text":"<p>In some situations, it can happen that the batches contains so much data that is faster to write it to disk and read it back in the next step, instead of passing it using the queue. To solve this issue, <code>distilabel</code> uses <code>fsspec</code> to allow providing a file system configuration and whether if this file system should be used to pass data between steps in the <code>run</code> method of the <code>distilabel</code> pipelines:</p> <pre><code>from distilabel.pipeline import Pipeline\n\nwith Pipeline(name=\"my-pipeline\") as pipeline:\n  ...\n\nif __name__ == \"__main__\":\n    distiset = pipeline.run(\n        ..., \n        storage_parameters={\"protocol\": \"gcs\", \"path\": \"gcs://my-bucket\"},\n        use_fs_to_pass_data=True\n    )\n</code></pre> <p>The code above setups a file system (in this case Google Cloud Storage) and sets the flag <code>use_fs_to_pass_data</code> to specify that the data of the batches should be passed to the steps using the file system.The <code>storage_parameters</code> argument is optional, and in the case it's not provided but <code>use_fs_to_pass_data==True</code>, <code>distilabel</code> will use the local file system.</p> <p>Note</p> <p>As <code>GlobalStep</code>s receives all the data from the previous steps in one single batch accumulating all the data, it's very likely that the data of the batch will be too big to be passed using the queue. In this case and even if <code>use_fs_to_pass_data==False</code>, <code>distilabel</code> will use the file system to pass the data to the <code>GlobalStep</code>. </p>"},{"location":"sections/learn/advanced/structured_generation/","title":"Structured Generation","text":"<p><code>Distilabel</code> has integrations with relevant libraries to generate structured text i.e. to guide the <code>LLM</code> towards the generation of structured outputs following a JSON schema, a regex, etc.</p>"},{"location":"sections/learn/advanced/structured_generation/#outlines","title":"Outlines","text":"<p><code>Distilabel</code> integrates <code>outlines</code> within some <code>LLM</code> subclasses. At the moment, the following LLMs integrated with <code>outlines</code> are supported in <code>distilabel</code>: <code>TransformersLLM</code>, <code>vLLM</code> or <code>LlamaCppLLM</code>, so that anyone can generate structured outputs in the form of JSON or a parseable regex.</p> <p>The <code>LLM</code> has an argument named <code>structured_output</code><sup>1</sup> that determines how we can generate structured outputs with it, let's see an example using <code>LlamaCppLLM</code>.</p> <p>Note</p> <p>For <code>outlines</code> integration to work you may need to install the corresponding dependencies:</p> <pre><code>pip install distilabel[outlines]\n</code></pre>"},{"location":"sections/learn/advanced/structured_generation/#json","title":"JSON","text":"<p>We will start with a JSON example, where we initially define a <code>pydantic.BaseModel</code> schema to guide the generation of the structured output.</p> <p>Note</p> <p>Take a look at <code>StructuredOutputType</code> to see the expected format of the <code>structured_output</code> dict variable.</p> <pre><code>from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    last_name: str\n    id: int\n</code></pre> <p>And then we provide that schema to the <code>structured_output</code> argument of the LLM.</p> <pre><code>from distilabel.llms import LlamaCppLLM\n\nllm = LlamaCppLLM(\n    model_path=\"./openhermes-2.5-mistral-7b.Q4_K_M.gguf\"  # (1)\n    n_gpu_layers=-1,\n    n_ctx=1024,\n    structured_output={\"format\": \"json\", \"schema\": User},\n)\nllm.load()\n</code></pre> <ol> <li>We have previously downloaded a GGUF model i.e. <code>llama.cpp</code> compatible, from the Hugging Face Hub using curl<sup>2</sup>, but any model can be used as replacement, as long as the <code>model_path</code> argument is updated.</li> </ol> <p>And we are ready to pass our instruction as usual:</p> <pre><code>import json\n\nresult = llm.generate(\n    [[{\"role\": \"user\", \"content\": \"Create a user profile for the following marathon\"}]],\n    max_new_tokens=50\n)\n\ndata = json.loads(result[0][0])\ndata\n# {'name': 'Kathy', 'last_name': 'Smith', 'id': 4539210}\nUser(**data)\n# User(name='Kathy', last_name='Smith', id=4539210)\n</code></pre> <p>We get back a Python dictionary (formatted as a string) that we can parse using <code>json.loads</code>, or validate it directly using the <code>User</code>, which si a <code>pydantic.BaseModel</code> instance.</p>"},{"location":"sections/learn/advanced/structured_generation/#regex","title":"Regex","text":"<p>The following example shows an example of text generation whose output adhere to a regular expression:</p> <pre><code>pattern = r\"&lt;name&gt;(.*?)&lt;/name&gt;.*?&lt;grade&gt;(.*?)&lt;/grade&gt;\"  #\u00a0the same pattern for re.compile\n\nllm=LlamaCppLLM(\n    model_path=model_path,\n    n_gpu_layers=-1,\n    n_ctx=1024,\n    structured_output={\"format\": \"regex\", \"schema\": pattern},\n)\nllm.load()\n\nresult = llm.generate(\n    [\n        [\n            {\"role\": \"system\", \"content\": \"You are Simpsons' fans who loves assigning grades from A to E, where A is the best and E is the worst.\"},\n            {\"role\": \"user\", \"content\": \"What's up with Homer Simpson?\"}\n        ]\n    ],\n    max_new_tokens=200\n)\n</code></pre> <p>We can check the output by parsing the content using the same pattern we required from the LLM.</p> <pre><code>import re\nmatch = re.search(pattern, result[0][0])\n\nif match:\n    name = match.group(1)\n    grade = match.group(2)\n    print(f\"Name: {name}, Grade: {grade}\")\n# Name: Homer Simpson, Grade: C+\n</code></pre> <p>These were some simple examples, but one can see the options this opens.</p> <p>Tip</p> <p>A full pipeline example can be seen in the following script: <code>examples/structured_generation_with_outlines.py</code></p>"},{"location":"sections/learn/advanced/structured_generation/#instructor","title":"Instructor","text":"<p>When working with model providers behind an API, there's no direct way of accesing the internal logit processor as <code>outlines</code> does, but thanks to <code>instructor</code> we can generate structured output from LLM providers. We have integrated <code>instructor</code> to deal with the <code>AsyncLLM</code>, so you can work with the following LLMs: <code>OpenAILLM</code>, <code>AzureOpenAILLM</code>, <code>CohereLLM</code>, <code>GroqLLM</code>, <code>LiteLLM</code> and <code>MistralLLM</code>.</p> <p><code>instructor</code> works with <code>pydantic.BaseModel</code> objects internally but in <code>distilabel</code> the examples generated would result in the string representation of them, from which the <code>BaseModel</code> object can be regenerated.</p> <p>Note</p> <p>For <code>instructor</code> integration to work you may need to install the corresponding dependencies:</p> <pre><code>pip install distilabel[instructor]\n</code></pre> <p>Note</p> <p>Take a look at <code>InstructorStructuredOutputType</code> to see the expected format of the <code>structured_output</code> dict variable.</p> <p>The following is the same example you can see with <code>outlines</code>'s <code>JSON</code> section for comparison purposes.</p> <pre><code>from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    last_name: str\n    id: int\n</code></pre> <p>And then we provide that schema to the <code>structured_output</code> argument of the LLM:</p> <p>Note</p> <p>In this example we are using open-mixtral-8x22b, keep in mind not all the models work with the function calling functionality required for this example to work.</p> <pre><code>from distilabel.llms import MistralLLM\n\nllm = MistralLLM(\n    model=\"open-mixtral-8x22b\",\n    structured_output={\"schema\": User}\n)\nllm.load()\n</code></pre> <p>And we are ready to pass our instruction as usual:</p> <pre><code>import json\n\nresult = llm.generate(\n    [[{\"role\": \"user\", \"content\": \"Create a user profile for the following marathon\"}]],\n    max_new_tokens=256\n)\n\ndata = json.loads(result[0][0])\ndata\n# {'name': 'John', 'last_name': 'Doe', 'id': 12345}\nUser(**data)\n# User(name='John', last_name='Doe', id=12345)\n</code></pre> <p>We get back a Python dictionary (formatted as a string) that we can parse using <code>json.loads</code>, or validate it directly using the <code>User</code>, which is a <code>pydantic.BaseModel</code> instance.</p> <p>Tip</p> <p>A full pipeline example can be seen in the following script: <code>examples/structured_generation_with_instructor.py</code></p>"},{"location":"sections/learn/advanced/structured_generation/#openai-json","title":"OpenAI JSON","text":"<p>OpenAI offers a JSON Mode to deal with structured output via their API, let's see how to make use of them. The JSON mode instructs the model to always return a JSON object following the instruction required.</p> <p>Warning</p> <p>Bear in mind, that in order for this to work, you must instruct the model in some way to generate JSON, either in the <code>system message</code> or in the instruction, as can be seen in the API reference.</p> <p>Contrary to what we have via <code>outlines</code>, JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors. More information can be found the OpenAI documentation.</p> <p>Other than the reference to generating JSON, to ensure the model generates parseable JSON we can pass the argument <code>response_format=\"json\"</code><sup>3</sup>:</p> <pre><code>from distilabel.llms import OpenAILLM\nllm = OpenAILLM(model=\"gpt4-turbo\", api_key=\"api.key\")\nllm.generate(..., response_format=\"json\")\n</code></pre> <ol> <li> <p>You can check the variable type by importing it from:</p> <p><pre><code>from distilabel.steps.tasks.structured_outputs.outlines import StructuredOutputType\n</code></pre> \u21a9</p> </li> <li> <p>Download the model with curl:</p> <p><pre><code>curl -L -o ~/Downloads/openhermes-2.5-mistral-7b.Q4_K_M.gguf https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf\n</code></pre> \u21a9</p> </li> <li> <p>Keep in mind that to interact with this <code>response_format</code> argument in a pipeline, you will have to pass it via the <code>generation_kwargs</code>:</p> <p><pre><code># Assuming a pipeline is already defined, and we have a task using OpenAILLM called `task_with_openai`:\npipeline.run(\n    parameters={\n        \"task_with_openai\": {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"response_format\": \"json\"\n                }\n            }\n        }\n    }\n)\n</code></pre> \u21a9</p> </li> </ol>"},{"location":"sections/learn/tutorial/","title":"Tutorial","text":"<p><code>Distilabel</code> builds a <code>Pipeline</code> with steps that can be thought of as nodes in a graph, as the <code>Pipeline</code> will orchestrate the execution of the <code>Step</code> subclasses, and those will be connected as nodes in a Direct Acyclic Graph (DAG).</p> <p>This guide can be considered a tutorial, which will guide you through the different components of <code>distilabel</code>.</p>"},{"location":"sections/learn/tutorial/cli/","title":"Command Line Interface (CLI)","text":"<p><code>Distilabel</code> offers a <code>CLI</code> to explore and re-run existing <code>Pipeline</code> dumps, meaning that an existing dump can be explored to see the steps, how those are connected, the runtime parameters used, and also re-run it with the same or different runtime parameters, respectively.</p>"},{"location":"sections/learn/tutorial/cli/#available-commands","title":"Available commands","text":"<p>The only available command as of the current version of <code>distilabel</code> is <code>distilabel pipeline</code>.</p> <pre><code>$ distilabel pipeline --help\n\n Usage: distilabel pipeline [OPTIONS] COMMAND [ARGS]...\n\n Commands to run and inspect Distilabel pipelines.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 info      Get information about a Distilabel pipeline.                                  \u2502\n\u2502 run       Run a Distilabel pipeline.                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>So on, <code>distilabel pipeline</code> has two subcommands: <code>info</code> and <code>run</code>, as described below. Note that for testing purposes we will be using the following dataset.</p>"},{"location":"sections/learn/tutorial/cli/#distilabel-pipeline-info","title":"<code>distilabel pipeline info</code>","text":"<pre><code>$ distilabel pipeline info --help\n\n Usage: distilabel pipeline info [OPTIONS]\n\n Get information about a Distilabel pipeline.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --config        TEXT  Path or URL to the Distilabel pipeline configuration file. \u2502\n\u2502                          [default: None]                                            \u2502\n\u2502                          [required]                                                 \u2502\n\u2502    --help                Show this message and exit.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>As we can see from the help message, we need to pass either a <code>Path</code> or a <code>URL</code>. This second option comes handy for datasets stored in Hugging Face Hub, for example:</p> <pre><code>distilabel pipeline info --config \"https://huggingface.co/datasets/distilabel-internal-testing/instruction-dataset-mini-with-generations/raw/main/pipeline.yaml\"\n</code></pre> <p>If we take a look:</p> <p></p> <p>The pipeline information includes the steps used in the <code>Pipeline</code> along with the <code>Runtime Parameter</code> that was used, as well as a description of each of them, and also the connections between these steps. These can be helpful to explore the Pipeline locally.</p>"},{"location":"sections/learn/tutorial/cli/#distilabel-pipeline-run","title":"<code>distilabel pipeline run</code>","text":"<p>We can also run a <code>Pipeline</code> from the CLI just pointing to the same <code>pipeline.yaml</code> file or an URL pointing to it and calling <code>distilabel pipeline run</code>:</p> <pre><code>$ distilabel pipeline run --help\n\n Usage: distilabel pipeline run [OPTIONS]\n\n Run a Distilabel pipeline.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --config                                 TEXT                 Path or URL to the Distilabel pipeline configuration file.   \u2502\n\u2502                                                                  [default: None]                                              \u2502\n\u2502                                                                  [required]                                                   \u2502\n\u2502    --param                                  PARSE_RUNTIME_PARAM  [default: (dynamic)]                                         \u2502\n\u2502    --ignore-cache      --no-ignore-cache                         Whether to ignore the cache and re-run the pipeline from     \u2502\n\u2502                                                                  scratch.                                                     \u2502\n\u2502                                                                  [default: no-ignore-cache]                                   \u2502\n\u2502    --repo-id                                TEXT                 The Hugging Face Hub repository ID to push the resulting     \u2502\n\u2502                                                                  dataset to.                                                  \u2502\n\u2502                                                                  [default: None]                                              \u2502\n\u2502    --commit-message                         TEXT                 The commit message to use when pushing the dataset.          \u2502\n\u2502                                                                  [default: None]                                              \u2502\n\u2502    --private           --no-private                              Whether to make the resulting dataset private on the Hub.    \u2502\n\u2502                                                                  [default: no-private]                                        \u2502\n\u2502    --token                                  TEXT                 The Hugging Face Hub API token to use when pushing the       \u2502\n\u2502                                                                  dataset.                                                     \u2502\n\u2502                                                                  [default: None]                                              \u2502\n\u2502    --help                                                        Show this message and exit.                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>To specify the runtime parameters of the steps we will need to use the <code>--param</code> option and the value of the parameter in the following format:</p> <pre><code>distilabel pipeline run --config \"https://huggingface.co/datasets/distilabel-internal-testing/instruction-dataset-mini-with-generations/raw/main/pipeline.yaml\" \\\n    --param load_dataset.repo_id=distilabel-internal-testing/instruction-dataset-mini \\\n    --param load_dataset.split=test \\\n    --param generate_with_gpt35.llm.generation_kwargs.max_new_tokens=512 \\\n    --param generate_with_gpt35.llm.generation_kwargs.temperature=0.7 \\\n    --param to_argilla.dataset_name=text_generation_with_gpt35 \\\n    --param to_argilla.dataset_workspace=admin\n</code></pre> <p>Again, this helps with the reproducibility of the results, and simplifies sharing not only the final dataset but also the process to generate it.</p>"},{"location":"sections/learn/tutorial/llm/","title":"LLM","text":"<p>The LLMs are implemented as subclasses of either <code>LLM</code> or <code>AsyncLLM</code>, and are only in charge of running the text generation for a given prompt or conversation. The LLMs are intended to be used together with the <code>Task</code> and any of its subclasses, via the <code>llm</code> argument, this means that any of the implemented LLMs can be easily plugged seamlessly into any task.</p>"},{"location":"sections/learn/tutorial/llm/#working-with-llms","title":"Working with LLMs","text":"<p>The subclasses of both <code>LLM</code> or <code>AsyncLLM</code> are intended to be used within the scope of a <code>Task</code>, since those are seamlessly integrated within the different tasks; but nonetheless, they can be used standalone if needed.</p> <pre><code>from distilabel.llms import OpenAILLM\n\nllm = OpenAILLM(model=\"gpt-4\")\nllm.load()\n\nllm.generate(\n    inputs=[\n        [\n            {\"role\": \"user\", \"content\": \"What's the capital of Spain?\"},\n        ],\n    ],\n)\n# \"The capital of Spain is Madrid.\"\n</code></pre> <p>Note</p> <p>The <code>load</code> method needs to be called ALWAYS if using the LLMs as standalone or as part of a task, otherwise, if the <code>Pipeline</code> context manager is used, there's no need to call that method, since it will be automatically called on <code>Pipeline.run</code>; but in any other case the method <code>load</code> needs to be called from the parent class e.g. a <code>Task</code> with an <code>LLM</code> will need to call <code>Task.load</code> to load both the task and the LLM.</p>"},{"location":"sections/learn/tutorial/llm/#within-a-task","title":"Within a Task","text":"<p>Now, in order to use the LLM within a <code>Task</code>, we need to pass it as an argument to the task, and the task will take care of the rest.</p> <pre><code>from distilabel.llms import OpenAILLM\nfrom distilabel.steps.tasks import TextGeneration\n\n\nllm = OpenAILLM(model=\"gpt-4\")\ntask = TextGeneration(name=\"text_generation\", llm=llm)\n\ntask.load()\n\nnext(task.process(inputs=[{\"instruction\": \"What's the capital of Spain?\"}]))\n# [{'instruction': \"What's the capital of Spain?\", \"generation\": \"The capital of Spain is Madrid.\"}]\n</code></pre>"},{"location":"sections/learn/tutorial/llm/#runtime-parameters","title":"Runtime Parameters","text":"<p>Additionally, besides the runtime parameters that can / need to be provided to the <code>Task</code>, the LLMs can also define their own runtime parameters such as the <code>generation_kwargs</code>, and those need to be provided within the <code>Pipeline.run</code> method via the argument <code>params</code>.</p> <p>Note</p> <p>Each LLM subclass may have its own runtime parameters and those can differ between the different implementations, as those are not aligned, since the LLM engines offer different functionalities.</p> <pre><code>from distilabel.pipeline import Pipeline\nfrom distilabel.llms import OpenAILLM\nfrom distilabel.steps import LoadDataFromDicts\nfrom distilabel.steps.tasks import TextGeneration\n\n\nwith Pipeline(name=\"text-generation-pipeline\") as pipeline:\n    load_dataset = LoadDataFromDicts(\n        name=\"load_dataset\",\n        data=[\n            {\n                \"instruction\": \"Write a short story about a dragon that saves a princess from a tower.\",\n            },\n        ],\n    )\n\n    text_generation = TextGeneration(\n        name=\"text_generation\",\n        llm=OpenAILLM(model=\"gpt-4\"),\n    )\n\n    load_dataset &gt;&gt; text_generation\n\nif __name__ == \"__main__\":\n    pipeline.run(\n        parameters={\n            text_generation.name: {\"llm\": {\"generation_kwargs\": {\"temperature\": 0.3}}},\n        },\n    )\n</code></pre>"},{"location":"sections/learn/tutorial/llm/#defining-custom-llms","title":"Defining custom LLMs","text":"<p>In order to define custom LLMs, one must subclass either <code>LLM</code> or <code>AsyncLLM</code>, to define a synchronous or asynchronous LLM, respectively.</p> <p>One can either extend any of the existing LLMs to override the default behaviour if needed, but also to define a new one from scratch, that could be potentially contributed to the <code>distilabel</code> codebase.</p> <p>In order to define a new LLM, one must define the following methods:</p> <ul> <li> <p><code>model_name</code>: is a property that contains the name of the model to be used, which means that it needs to be retrieved from the LLM using the LLM-specific approach i.e. for <code>TransformersLLM</code> the <code>model_name</code> will be the <code>model_name_or_path</code> provided as an argument, or in <code>OpenAILLM</code> the <code>model_name</code> will be the <code>model</code> provided as an argument.</p> </li> <li> <p><code>generate</code>: is a method that will take a list of prompts and return a list of generated texts. This method will be called by the <code>Task</code> to generate the texts, so it's the most important method to define. This method will be implemented in the subclass of the <code>LLM</code> i.e. the synchronous LLM.</p> </li> <li> <p><code>agenerate</code>: is a method that will take a single prompt and return a list of generated texts, since the rest of the behaviour will be controlled by the <code>generate</code> method that cannot be overwritten when subclassing <code>AsyncLLM</code>. This method will be called by the <code>Task</code> to generate the texts, so it's the most important method to define. This method will be implemented in the subclass of the <code>AsyncLLM</code> i.e. the asynchronous LLM.</p> </li> <li> <p>(optional) <code>get_last_hidden_state</code>: is a method that will take a list of prompts and return a list of hidden states. This method is optional and will be used by some tasks such as the <code>GenerateEmbeddings</code> task.</p> </li> </ul> <p>Once those methods have been implemented, then the custom LLM will be ready to be integrated within either any of the existing or a new task.</p> <pre><code>from typing import Any\n\nfrom pydantic import validate_call\n\nfrom distilabel.llms import AsyncLLM, LLM\nfrom distilabel.llms.typing import GenerateOutput, HiddenState\nfrom distilabel.steps.tasks.typing import ChatType\n\n\nclass CustomLLM(LLM):\n    @property\n    def model_name(self) -&gt; str:\n        return \"my-model\"\n\n    @validate_call\n    def generate(self, inputs: List[ChatType], num_generations: int = 1, **kwargs: Any) -&gt; List[GenerateOutput]:\n        for _ in range(num_generations):\n            ...\n\n    def get_last_hidden_state(self, inputs: List[ChatType]) -&gt; List[HiddenState]:\n        ...\n\n\nclass CustomAsyncLLM(AsyncLLM):\n    @property\n    def model_name(self) -&gt; str:\n        return \"my-model\"\n\n    @validate_call\n    async def agenerate(self, input: ChatType, num_generations: int = 1, **kwargs: Any) -&gt; GenerateOutput:\n        for _ in range(num_generations):\n            ...\n\n    def get_last_hidden_state(self, inputs: List[ChatType]) -&gt; List[HiddenState]:\n        ...\n</code></pre> <p><code>generate</code> and <code>agenerate</code> keyword arguments (but <code>input</code> and <code>num_generations</code>) are considered as <code>RuntimeParameter</code>s, so a value can be passed to them via the <code>parameters</code> argument of the <code>Pipeline.run</code> method.</p> <p>Note</p> <p>To have the arguments of the <code>generate</code> and <code>agenerate</code> coerced to the expected types, the <code>validate_call</code> decorator is used, which will automatically coerce the arguments to the expected types, and raise an error if the types are not correct. This is specially useful when providing a value for an argument of <code>generate</code> or <code>agenerate</code> from the CLI, since the CLI will always provide the arguments as strings.</p>"},{"location":"sections/learn/tutorial/llm/#available-llms","title":"Available LLMs","text":"<p>Here's a list with the available LLMs that can be used within the <code>distilabel</code> library:</p> <ul> <li>AnthropicLLM</li> <li>AnyscaleLLM</li> <li>AzureOpenAILLM</li> <li>CohereLLM</li> <li>GroqLLM</li> <li>InferenceEndpointsLLM</li> <li>LiteLLM</li> <li>LlamaCppLLM</li> <li>MistralLLM</li> <li>OllamaLLM</li> <li>OpenAILLM</li> <li>TogetherLLM</li> <li>TransformersLLM</li> <li>VertexAILLM</li> <li>vLLM</li> </ul>"},{"location":"sections/learn/tutorial/pipeline/","title":"Pipeline","text":"<p>The <code>Pipeline</code> is the central point in <code>distilabel</code>, the way to organize the steps to create your datasets. Up to this point we've seen how we can define different <code>Step</code> and <code>Task</code> subclasses in Tutorial - Step and Tutorial - Task, respectively; which together with an <code>LLM</code> are the building blocks of our datasets, in this section we will take a look at how all these blocks are organized inside a <code>Pipeline</code>.</p> <p>Note</p> <p>Currently <code>distilabel</code> implements a local version of a <code>Pipeline</code>, and will assume that's the only definition, but this can be extended in the future to include remote execution of the <code>Pipeline</code>.</p>"},{"location":"sections/learn/tutorial/pipeline/#how-to-create-a-pipeline","title":"How to create a pipeline","text":"<p>The most common way a <code>Pipeline</code> should be created is by making use of the context manager, we just need to give our <code>Pipeline</code> a name, and optionally a description, and that's it<sup>1</sup>:</p> <pre><code>from distilabel.pipeline import Pipeline\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:  # (1)\n    ...\n</code></pre> <ol> <li>Use the context manager to create a <code>Pipeline</code> with a name and an optional description.</li> </ol> <p>This way, we ensure all the steps we define there are connected with each other under the same <code>Pipeline</code>. The next step is to define the steps of our <code>Pipeline</code>. It's mandatory that the root steps of the pipeline i.e. the ones that doesn't have any predecessors, are <code>GeneratorStep</code>s such as <code>LoadDataFromDicts</code> or <code>LoadHubDataset</code>.</p> <pre><code>from distilabel.pipeline import Pipeline\nfrom distilabel.steps import LoadHubDataset\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(name=\"load_dataset\")  # (1)\n    ...\n</code></pre> <ol> <li>Define the first step of the pipeline, in this case <code>LoadHubDataset</code>, a <code>GeneratorStep</code> used to load a dataset from the Hugging Face Hub.</li> </ol> <p>Once we have a source of data, we can create another <code>Step</code>s that will consume and process the data generated by the <code>GeneratorStep</code>s. Let's assume that the dataset we're going to load from the Hub contains a <code>prompt</code> column and that we want to generate texts based on this prompt. We also want to use several <code>LLM</code>s for this task. To do so, we will create several <code>TextGeneration</code> tasks, each with a different <code>LLM</code>.</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(name=\"load_dataset\")\n\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)  # (1)\n        task.connect(load_dataset)  # (2)\n\n    ...\n</code></pre> <ol> <li>Create a <code>TextGeneration</code> task for each <code>LLM</code> we want to use.</li> <li>Connect the <code>TextGeneration</code> task with the <code>LoadHubDataset</code> step, so the output data from the dataset is passed to the task.</li> </ol> <p>Note</p> <p>The order of the execution of the steps will be determined by the connections of the steps. In this case, the <code>TextGeneration</code> tasks will be executed after the <code>LoadHubDataset</code> step.</p> <p>For each row of the dataset, the <code>TextGeneration</code> task will generate a text based on the <code>instruction</code> column and the <code>LLM</code> model, and store the result (a single string) in a new column called <code>generation</code>. As we would like to have all the <code>response</code>s in the same column, we will add an extra step to combine them all in the same column, so the value of this column is a list of strings or responses.</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import CombineColumns, LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(name=\"load_dataset\")\n\n    combine_generations = CombineColumns(  # (1)\n        name=\"combine_generations\",\n        columns=[\"generation\", \"model_name\"],\n        output_columns=[\"generations\", \"model_names\"],\n    )\n\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        load_dataset.connect(task)\n        task.connect(combine_generations)  # (2)\n</code></pre> <ol> <li>Create a <code>CombineColumns</code> step to combine all the <code>generation</code> columns into a single column called <code>generations</code> and the <code>model_name</code> columns into a single column called <code>model_names</code>.</li> <li>Connect the <code>TextGeneration</code> task with the <code>CombineColumns</code> step, so the output data from the task is passed to the step that will combine all the <code>generation</code> columns.</li> </ol> <p>As the <code>CombineColumns</code> is the last step or it's a leaf step of the pipeline because it doesn't have any successors, that means that the outputs of this step will be included in the returned <code>Distiset</code> (more information about it in Advanced - Distiset).</p> <p>Note</p> <p>One pipeline can have several leaf steps, which means that the outputs of all the leaf steps will be included in the returned <code>Distiset</code>, which will contain several subsets, one for each leaf step.</p>"},{"location":"sections/learn/tutorial/pipeline/#connecting-steps","title":"Connecting steps","text":"<p>In the previous example we saw how to create a <code>Pipeline</code> and connect different steps using the <code>Step.connect</code> method: <code>step1.connect(step2)</code>, but there's an alternative way by making use of the <code>&gt;&gt;</code> operator, let's see how using the previous <code>Pipeline</code> as an example:</p> Step per stepMultiple steps at once <p>Each call to <code>step1.connect(step2)</code> has been exchanged by <code>step1 &gt;&gt; step2</code>:</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import CombineColumns, LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(name=\"load_dataset\")\n\n    combine_generations = CombineColumns(\n        name=\"combine_generations\",\n        columns=[\"generation\", \"model_name\"],\n        output_columns=[\"generations\", \"model_names\"],\n    )\n\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        load_dataset &gt;&gt; task &gt;&gt; combine_generations  # (1)\n</code></pre> <ol> <li>Here <code>load_dataset &gt;&gt; task &gt;&gt; combine_generations</code> was exchanged with <code>load_dataset.connect(task).connect(combine_generations)</code>.</li> </ol> <p>All the calls to connections from the <code>load_dataset</code> step to the different <code>task</code> objects are done in a single call:</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import CombineColumns, LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(name=\"load_dataset\")\n\n    combine_generations = CombineColumns(\n        name=\"combine_generations\",\n        columns=[\"generation\", \"model_name\"],\n        output_columns=[\"generations\", \"model_names\"],\n    )\n\n    tasks = []\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        tasks.append(\n            TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        )\n\n    load_dataset &gt;&gt; tasks &gt;&gt; combine_generations  # (1)\n</code></pre> <ol> <li>Notice how <code>tasks</code> is a list of different <code>Tasks</code>. In a single call to the operator we are connecting <code>load_dataset</code> with all the <code>tasks</code>, and all of those again to <code>combine_generations</code>.</li> </ol>"},{"location":"sections/learn/tutorial/pipeline/#routing-batches-to-specific-downstream-steps","title":"Routing batches to specific downstream steps","text":"<p>In some pipelines, it's likely that you will need to have a list of downstream steps receiving batches from the same upstream step, but you would like to route the batches to specific downstream steps based on some condition. To do so, you can use a <code>routing_batch_function</code>, which is a simple function that receives a list of the downstream steps to which a batch can be routed, and returns a list containing the names of steps to which the batch should be routed. Let's update the example above to route the batches loaded by the <code>LoadHubDataset</code> step to just 2 of the <code>TextGeneration</code> tasks. First, we will create our custom <code>routing_batch_function</code>, and then we will update the pipeline to use it:</p> <pre><code>import random\nfrom distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline, routing_batch_function\nfrom distilabel.steps import CombineColumns, LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\n@routing_batch_function\ndef sample_two_steps(steps: list[str]) -&gt; list[str]:\n    return random.sample(steps, 2)\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(\n        name=\"load_dataset\",\n        output_mappings={\"prompt\": \"instruction\"},\n    )\n\n    tasks = []\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.0-pro\"),\n    ):\n        tasks.append(\n            TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        )\n\n    combine_generations = CombineColumns(\n        name=\"combine_generations\",\n        columns=[\"generation\", \"model_name\"],\n        output_columns=[\"generations\", \"model_names\"],\n    )\n\n    load_dataset &gt;&gt; sample_two_steps &gt;&gt; tasks &gt;&gt; combine_generations\n</code></pre> <p>As it can be seen, the <code>routing_batch_function</code> can be used with the <code>&gt;&gt;</code> operator to route the batches to specific downstream steps. In this case, each batch yielded by the <code>load_dataset</code> step will be routed to just 2 of the <code>TextGeneration</code> tasks, and then all the outputs of the tasks will be combined in the <code>CombineColumns</code> step so each row of the final dataset will contain generations of 2 <code>LLM</code>s at most. The <code>routing_batch_function</code> that we just built is a common one, so <code>distilabel</code> comes with an auxiliary function that can be used to achieve the same behavior:</p> <pre><code>from distilable.pipeline import sample_n_steps\n\nsample_two_steps = sample_n_steps(2)\n</code></pre>"},{"location":"sections/learn/tutorial/pipeline/#running-the-pipeline","title":"Running the pipeline","text":""},{"location":"sections/learn/tutorial/pipeline/#pipelinedry_run","title":"Pipeline.dry_run","text":"<p>Before running the <code>Pipeline</code> we may want to check all the components behave as expected. We can do a <code>dry_run</code> for this case:</p> <pre><code>with Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    ...\n\nif __name__ == \"__main__\":\n    distiset = pipeline.dry_run(parameters=..., batch_size=1)\n</code></pre> <p>It takes the same parameters as the <code>run</code> method we will see in the following section, plus the <code>batch_size</code> we want the dry run to use (by default set to 1). In this case, the <code>Pipeline</code> would select a single example from our generator steps and pass through all the steps. Assuming the <code>dry_run</code> runs successfully, we are ready to run our pipeline.</p>"},{"location":"sections/learn/tutorial/pipeline/#pipelinerun","title":"Pipeline.run","text":"<p>Once we have created the pipeline, we can run it. To do so, we need to call the <code>run</code> method of the <code>Pipeline</code>, and specify the runtime parameters for each step:</p> <pre><code>with Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    ...\n\nif __name__ == \"__main__\":\n    distiset = pipeline.run(\n        parameters={\n            \"load_dataset\": {\n                \"repo_id\": \"distilabel-internal-testing/instruction-dataset-mini\",\n                \"split\": \"test\",\n            },\n            \"text_generation_with_gpt-4-0125-preview\": {\n                \"llm\": {\n                    \"generation_kwargs\": {\n                        \"temperature\": 0.7,\n                        \"max_new_tokens\": 512,\n                    }\n                }\n            },\n            \"text_generation_with_mistral-large-2402\": {\n                \"llm\": {\n                    \"generation_kwargs\": {\n                        \"temperature\": 0.7,\n                        \"max_new_tokens\": 512,\n                    }\n                }\n            },\n            \"text_generation_with_gemini-1.0-pro\": {\n                \"llm\": {\n                    \"generation_kwargs\": {\n                        \"temperature\": 0.7,\n                        \"max_new_tokens\": 512,\n                    }\n                }\n            },\n        },\n    )\n</code></pre> <p>But if we run it, we will see that the <code>run</code> method will fail:</p> <pre><code>ValueError: Step 'text_generation_with_gpt-4-0125-preview' requires inputs ['instruction'], but only the inputs=['prompt', 'completion', 'meta'] are available, which means that the inputs=['instruction'] are missing or not available\nwhen the step gets to be executed in the pipeline. Please make sure previous steps to 'text_generation_with_gpt-4-0125-preview' are generating the required inputs.\n</code></pre> <p>This is because, before actually running the pipeline, the pipeline is validated to verify that everything is correct and all the steps in the pipeline are chainable i.e. each step has the necessary inputs to be executed. In this case, the <code>TextGeneration</code> task requires the <code>instruction</code> column, but the <code>LoadHubDataset</code> step generates the <code>prompt</code> column. To solve this, we can use the <code>output_mappings</code> argument that every <code>Step</code> has, to map or rename the output columns of a step to the required input columns of another step:</p> <pre><code>with Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(\n        name=\"load_dataset\",\n        output_mappings={\"prompt\": \"instruction\"},  # (1)\n    )\n\n    ...\n</code></pre> <ol> <li>Use the <code>output_mappings</code> argument to map the <code>prompt</code> column generated by the <code>LoadHubDataset</code> step to the <code>instruction</code> column required by the <code>TextGeneration</code> task.</li> </ol> <p>If we execute the pipeline again, it will run successfully and we will have a <code>Distiset</code> with the outputs of all the leaf steps of the pipeline which we can push to the Hugging Face Hub.</p> <pre><code>if __name__ == \"__main__\":\n    distiset = pipeline.run(...)\n    distiset.push_to_hub(\"distilabel-internal-testing/instruction-dataset-mini-with-generations\")\n</code></pre>"},{"location":"sections/learn/tutorial/pipeline/#stopping-the-pipeline","title":"Stopping the pipeline","text":"<p>In case you want to stop the pipeline while it's running using the <code>Ctrl+c</code> (<code>Cmd+c</code> in macos), we automatically catch the signal and try to finish whatever steps are currently running. If it got hang by some reason, repeating the command 2 times it will force the pipeline close.</p> <p>Note</p> <p>When pushing sending the signal to kill the process, you can expect to see the following log messages:</p> <p></p>"},{"location":"sections/learn/tutorial/pipeline/#cache","title":"Cache","text":"<p>If we try to execute the pipeline again, the pipeline won't execute as it will load the dataset from the cache, and the outputs of the pipeline will be the same as the previous run. If for some reason, we decide to stop the pipeline execution in the middle of the process pressing CTRL + C, the pipeline will stop and the state of the pipeline and the outputs will be stored in the cache, so we can resume the pipeline execution from the point where it was stopped.</p> <p>If we want to force the pipeline to run again, then we can use the <code>use_cache</code> argument of the <code>run</code> method and set it to <code>False</code>:</p> <pre><code>if __name__ == \"__main__\":\n    distiset = pipeline.run(parameters={...}, use_cache=False)\n</code></pre>"},{"location":"sections/learn/tutorial/pipeline/#adjusting-the-batch-size-for-each-step","title":"Adjusting the batch size for each step","text":"<p>It's very likely that in some pipelines the batch size of the steps (the number of dictionaries that will receive every <code>Step.process</code> method when called) will need to be adjusted in order to avoid memory issues or a more efficient processing. To do so, we can use the <code>input_batch_size</code> argument of the <code>run</code> method:</p> <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import CombineColumns, LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    ...\n\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.5-pro\"),\n    ):\n        task = TextGeneration(\n            name=f\"text_generation_with_{llm.model_name}\",\n            llm=llm,\n            input_batch_size=5,  # (1)\n        )\n\n    ...\n</code></pre> <ol> <li>Use the <code>input_batch_size</code> argument to set the batch size of the <code>TextGeneration</code> task to 5.</li> </ol> <p>When we run the pipeline, the <code>TextGeneration</code> task will receive 5 dictionaries in every call to the <code>process</code> method. In addition, we can also adjust the batch size of the generated batches by the <code>GeneratorStep</code>s using the <code>batch_size</code> argument:</p> <pre><code>with Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(\n        name=\"load_dataset\",\n        output_mappings={\"prompt\": \"instruction\"},\n        batch_size=10  # (1)\n    )\n\n    ...\n</code></pre> <ol> <li>Use the <code>batch_size</code> argument to set the batch size of the <code>LoadHubDataset</code> step to 10.</li> </ol> <p>By default, both arguments have a value of <code>50</code>.</p>"},{"location":"sections/learn/tutorial/pipeline/#serializing-the-pipeline","title":"Serializing the pipeline","text":"<p>Sharing a pipeline with others is very easy, as we can serialize the pipeline object using the <code>save</code> method. We can save the pipeline in different formats, such as <code>yaml</code> or <code>json</code>:</p> <pre><code>if __name__ == \"__main__\":\n    pipeline.save(\"pipeline.yaml\", format=\"yaml\")\n</code></pre> <p>To load the pipeline, we can use the <code>from_yaml</code> or <code>from_json</code> methods:</p> <pre><code>pipeline = Pipeline.from_yaml(\"pipeline.yaml\")\n</code></pre> <p>Serializing the pipeline is very useful when we want to share the pipeline with others, or when we want to store the pipeline for future use. It can even be hosted online, so the pipeline can be executed directly using the CLI knowing the URL of the pipeline.</p>"},{"location":"sections/learn/tutorial/pipeline/#fully-working-example","title":"Fully working example","text":"<p>To sump up, here is the full code of the pipeline we have created in this section. Note that you will need to change the name of the Hugging Face repository where the resulting will be pushed, set <code>OPENAI_API_KEY</code> environment variable, set <code>MISTRAL_API_KEY</code> and have <code>gcloud</code> installed and configured:</p> Code <pre><code>from distilabel.llms import MistralLLM, OpenAILLM, VertexAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import CombineColumns, LoadHubDataset\nfrom distilabel.steps.tasks import TextGeneration\n\nwith Pipeline(\"pipe-name\", description=\"My first pipe\") as pipeline:\n    load_dataset = LoadHubDataset(\n        name=\"load_dataset\",\n        output_mappings={\"prompt\": \"instruction\"},\n    )\n\n    combine_generations = CombineColumns(\n        name=\"combine_generations\",\n        columns=[\"generation\", \"model_name\"],\n        output_columns=[\"generations\", \"model_names\"],\n    )\n\n    for llm in (\n        OpenAILLM(model=\"gpt-4-0125-preview\"),\n        MistralLLM(model=\"mistral-large-2402\"),\n        VertexAILLM(model=\"gemini-1.0-pro\"),\n    ):\n        task = TextGeneration(name=f\"text_generation_with_{llm.model_name}\", llm=llm)\n        load_dataset.connect(task)\n        task.connect(combine_generations)\n\nif __name__ == \"__main__\":\n    distiset = pipeline.run(\n        parameters={\n            \"load_dataset\": {\n                \"repo_id\": \"distilabel-internal-testing/instruction-dataset-mini\",\n                \"split\": \"test\",\n            },\n            \"text_generation_with_gpt-4-0125-preview\": {\n                \"llm\": {\n                    \"generation_kwargs\": {\n                        \"temperature\": 0.7,\n                        \"max_new_tokens\": 512,\n                    }\n                }\n            },\n            \"text_generation_with_mistral-large-2402\": {\n                \"llm\": {\n                    \"generation_kwargs\": {\n                        \"temperature\": 0.7,\n                        \"max_new_tokens\": 512,\n                    }\n                }\n            },\n            \"text_generation_with_gemini-1.0-pro\": {\n                \"llm\": {\n                    \"generation_kwargs\": {\n                        \"temperature\": 0.7,\n                        \"max_new_tokens\": 512,\n                    }\n                }\n            },\n        },\n    )\n    distiset.push_to_hub(\n        \"distilabel-internal-testing/instruction-dataset-mini-with-generations\"\n    )\n</code></pre> <ol> <li> <p>We also have the cache_dir argument to pass, for more information on this parameter, we refer the reader to the caching section.\u00a0\u21a9</p> </li> </ol>"},{"location":"sections/learn/tutorial/step/","title":"Step","text":"<p>The <code>Step</code> is an abstract class which defines the interface for the building blocks to be defined within the context of a <code>Pipeline</code>, a <code>Step</code> can be seen as a node within a Direct Acyclic Graph (DAG) which execution is orchestrated by the <code>Pipeline</code>.</p>"},{"location":"sections/learn/tutorial/step/#working-with-steps","title":"Working with Steps","text":"<p>The <code>Step</code> is intended to be used within the scope of a <code>Pipeline</code>, which will orchestrate the different steps defined; but nonetheless, they can be used standalone if needed too.</p> <p>Assuming that we have a <code>Step</code> already defined as it follows:</p> <pre><code>class MyStep(Step):\n    @property\n    def inputs(self) -&gt; List[str]:\n        return [\"input_field\"]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return [\"output_field\"]\n\n    def process(self, inputs: StepInput) -&gt; \"StepOutput\":\n        for input in inputs:\n            input[\"output_field\"] = input[\"input_field\"]\n        yield inputs\n</code></pre> <p>Then we can use / instantiate it as follows:</p> <pre><code>step = MyStep(name=\"my-step\")\nstep.load()\n\nnext(step.process([{\"input_field\": \"value\"}]))\n# [{'input_field': 'value', 'output_field': 'value'}]\n</code></pre> <p>Note</p> <p>The <code>load</code> method needs to be called ALWAYS if using the steps and any <code>Step</code> subclass as standalone, unless the <code>Pipeline</code> context manager is used, meaning that there will be no need to call the <code>load</code> method, since it will be automatically called on <code>Pipeline.run</code>; but in any other case the method <code>load</code> needs to be called from the parent class.</p> <p>Anyway, most of the times we'll end up using pre-defined steps in <code>distilabel</code>, so that there's no need to create custom steps, but anyway, we'll cover that later in this page.</p> <p>Let's see now a set of arguments that can be used to map fields across steps, or to set the batch size specific for the step:</p> <ul> <li> <p><code>input_mappings</code>, which is a dictionary that can be useful to map keys from the input dictionaries to the keys expected by the step. For example, if <code>input_mappings={\"instruction\": \"prompt\"}</code>, that means that the key prompt from the input dictionaries will be used as the key instruction for the step.</p> </li> <li> <p><code>output_mappings</code>, which is a dictionary that can be used to map the outputs of the step to other names. For example, if <code>output_mappings={\"conversation\": \"prompt\"}</code>, that means that the key conversation generated by the step will be renamed to prompt and the output dictionaries of this step will contain a key called prompt instead of conversation.</p> </li> <li> <p><code>input_batch_size</code> (by default set to 50), which is independent for every step and will determine how many input dictionaries will process at once. If won't matter that much in this step, but as we will see later, other types of steps will come with an LLM, so having this flexibility will be really useful.</p> </li> </ul>"},{"location":"sections/learn/tutorial/step/#runtime-parameters","title":"Runtime parameters","text":"<p>Finally, let's introduce at a special type of argument that we will find when dealing with the <code>Steps</code>, the <code>Runtime parameters</code>. For example, the <code>input_batch_size</code> is of type <code>RuntimeParameter</code>:</p> <pre><code>from distilabel.mixins.runtime_parameters import RuntimeParameter\n\nclass Step(...):\n    input_batch_size: RuntimeParameter[PositiveInt] = Field(\n        default=DEFAULT_INPUT_BATCH_SIZE,\n        description=\"The number of rows that will contain the batches processed by the\"\n        \" step.\",\n    )\n</code></pre> <p>We can interact with these types of arguments when we call the <code>Pipeline.run</code> method as we will see in the <code>Pipeline</code> section. These types of arguments can be really useful to insert info to the steps after the pipeline has been defined.</p>"},{"location":"sections/learn/tutorial/step/#types-of-steps","title":"Types of Steps","text":"<p>Besides the default <code>Step</code> already described, in <code>distilabel</code> we find the following abstract subclasses on top of the <code>Step</code>.</p> <ul> <li> <p><code>GeneratorStep</code>: is a step that only produces / generates data, and it doesn't need any input data from previous steps, is in most of the cases a parent node of the graph i.e. the first <code>Step</code> in the <code>Pipeline</code>.</p> <p>More information about it at Components -&gt; Step - GeneratorStep.</p> </li> <li> <p><code>GlobalStep</code>: is a step with the standard interface i.e. receives inputs and generates outputs, but it processes all the data at once, is in most of the cases a leaf node of the graph i.e. the last <code>Step</code> in the <code>Pipeline</code>. The fact that a <code>GlobalStep</code> requires the outputs from the previous steps, means that the previous steps needs to finish for this step to start, and the connected outputs steps, if any, will need to wait until this step is done.</p> <p>More information about it at Components - Step - GlobalStep.</p> </li> </ul> <p>Additionally, <code>distilabel</code> also defines another type of <code>Step</code>, which is the <code>Task</code>, which is essentially the same, besides the fact that the task will expect an <code>LLM</code> as an attribute, and the <code>process</code> method will be in charge of calling that LLM. So one could say that the <code>Task</code> is a <code>Step</code> to work with an <code>LLM</code>.</p> <p>More information about it at Components - Task.</p>"},{"location":"sections/learn/tutorial/step/#defining-custom-steps","title":"Defining custom Steps","text":"<p>In order to define custom steps, we need to create a new subclass of the <code>Step</code> class, and set both the <code>inputs</code> and <code>outputs</code> property, as well as the <code>process</code> method.</p> <p>So on, the following will need to be defined:</p> <ul> <li> <p><code>inputs</code>: is a property that returns a list of strings with the names of the required input fields.</p> </li> <li> <p><code>outputs</code>: is a property that returns a list of strings with the names of the output fields.</p> </li> <li> <p><code>process</code>: is a method that receives the input data and returns the output data, and it should be a generator, meaning that it should <code>yield</code> the output data. It's important to preserve the default signature within the method <code>def process(self, *inputs: StepInput) -&gt; StepOutput</code>, since that's the one that will be used by the <code>Pipeline</code> to orchestrate the steps, meaning that the argument <code>inputs</code> should be respected, no more arguments can be provided, and the type-hints and return type-hints should be respected too.</p> </li> </ul> <p>Note</p> <p>The default signature for the <code>process</code> method is <code>process(self, *inputs: StepInput) -&gt; StepOutput</code>, meaning that it should be able to receive any number of inputs by default i.e. more than one <code>Step</code> at a time could be connected to the current one. Anyway, when defining custom steps, that can be overridden with <code>process(self, inputs: StepInput) -&gt; StepOutput</code>, so that the <code>process</code> method only receives the outputs from one previous <code>Step</code> connected to it.</p> <p>Warning</p> <p>For the custom <code>Step</code> subclasses to work properly with <code>distilabel</code> and with the validation and serialization performed by default over each <code>Step</code> in the <code>Pipeline</code>, the type-hint for both <code>StepInput</code> and <code>StepOutput</code> should be used and not surrounded with double-quotes or imported under <code>typing.TYPE_CHECKING</code>, otherwise, the validation and/or serialization will fail.</p> <pre><code>from distilabel.steps import Step, StepInput\nfrom distilabel.steps.typing import StepOutput\n\nclass CustomStep(Step):\n    @property\n    def inputs(self) -&gt; List[str]:\n        ...\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        ...\n\n    def process(self, *inputs: StepInput) -&gt; StepOutput:\n        for input in inputs:\n            ...\n            yield item\n\n    # When overridden (ideally under the `typing_extensions.override` decorator)\n    # @typing_extensions.override\n    # def process(self, inputs: StepInput) -&gt; StepOutput:\n    #     for input in inputs:\n    #         ...\n    #     yield inputs\n</code></pre> <p>Alternatively, a simpler and more suitable way of defining custom <code>Step</code> subclasses is via the <code>@step</code> decorator, which will take care of the boilerplate code, and will allow to define the <code>inputs</code>, <code>outputs</code>, and <code>process</code> methods in a more straightforward way.</p> <pre><code>from distilabel.steps import StepInput, step\nfrom distilabel.steps.typing import StepOutput\n\n@step(inputs=[...], outputs=[...])\ndef CustomStep(inputs: StepInput) - StepOutput:\n    for input in inputs:\n        ...\n    yield inputs\n\nstep = CustomStep(name=\"my-step\")\n</code></pre> <p>Warning</p> <p>One downside of the <code>@step</code> decorator is that it won't let you access the <code>self</code> attributes if any, neither set those, so if you need to access or set any attribute, you should go with the first approach of defining the custom <code>Step</code> subclass.</p>"},{"location":"sections/learn/tutorial/step/generator_step/","title":"GeneratorStep","text":"<p>The <code>GeneratorStep</code> is a subclass of <code>Step</code> that only produces outputs, but doesn't receive any input. The <code>GeneratorStep</code> is intended to be used the first step within a <code>Pipeline</code>, since it doesn't require any input to run and will generate data that can be potentially used by the follow up steps.</p>"},{"location":"sections/learn/tutorial/step/generator_step/#working-with-generatorsteps","title":"Working with GeneratorSteps","text":"<p>The <code>GeneratorStep</code> is intended to be used within the scope of a <code>Pipeline</code> before any other <code>Step</code>. Alternatively, in can also be used as a standalone <code>Step</code> i.e. not within the context of a <code>Pipeline</code>.</p> <p>For example, the following code snippet shows how to use the <code>GeneratorStep</code> as a standalone <code>Step</code>, to generate data out of a provided list of strings.</p> <pre><code>from typing import List\nfrom typing_extensions import override\n\nfrom distilabel.steps import GeneratorStep\nfrom distilabel.steps.typing import GeneratorStepOutput\n\nclass MyGeneratorStep(GeneratorStep):\n    instructions: List[str]\n\n    @override\n    def process(self, offset: int = 0) -&gt; GeneratorStepOutput:\n        if offset:\n            self.instructions = self.instructions[offset:]\n\n        while self.instructions:\n            batch = [\n                {\n                    \"instruction\": instruction\n                } for instruction in self.instructions[: self.batch_size]\n            ]\n            self.instructions = self.instructions[self.batch_size :]\n            yield (\n                batch,\n                True if len(self.instructions) == 0 else False,\n            )\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return [\"instruction\"]\n</code></pre> <p>Then we can use / instantiate it as follows:</p> <pre><code>step = MyGeneratorStep(\n    name=\"my-generator-step\",\n    instructions=[\"Tell me a joke.\", \"Tell me a story.\"],\n    batch_size=1,\n)\nstep.load()\n\nnext(step.process(offset=0))\n# ([{'instruction': 'Tell me a joke.'}], False)\nnext(step.process(offset=1))\n# ([{'instruction': 'Tell me a story.'}], True)\n</code></pre> <p>Note</p> <p>The <code>load</code> method needs to be called ALWAYS if using the steps and any <code>Step</code> subclass as standalone, unless the <code>Pipeline</code> context manager is used, meaning that there will be no need to call the <code>load</code> method, since it will be automatically called on <code>Pipeline.run</code>; but in any other case the method <code>load</code> needs to be called from the parent class.</p> <p>Anyway, most of the times we'll end up using pre-defined steps in <code>distilabel</code>, so that there's no need to create custom steps, but anyway, we'll cover that later in this page.</p>"},{"location":"sections/learn/tutorial/step/generator_step/#defining-custom-generatorsteps","title":"Defining custom GeneratorSteps","text":"<p>In order to define a custom <code>GeneratorStep</code>, we need to subclass it, and set the <code>outputs</code> property, and define the <code>process</code> method. In this case, the <code>process</code> method signature differs from the <code>process</code> method signature of the <code>Step</code>, since it won't receive any <code>inputs</code> but generate those, so the only argument of <code>process</code> is <code>offset</code> which is automatically handled by the <code>Pipeline</code> shifting it until all the batches are generated.</p> <p>So on, the following will need to be defined:</p> <ul> <li> <p><code>outputs</code>: is a property that returns a list of strings with the names of the output fields.</p> </li> <li> <p><code>process</code>: is a method that yields output data and a boolean flag indicating whether that's the last batch to be generated. It's important to override the default signature of the <code>Step.process</code> method <code>def process(self, *inputs: StepInput) -&gt; StepOutput</code>, to be set to <code>def process(self, offset: int = 0) -&gt; GeneratorStepOutput</code> instead, since that's the one that will be used by the <code>Pipeline</code> to orchestrate the steps, meaning that the argument <code>offset</code> should be respected, no more arguments can be provided, and the type-hints and return type-hints should be respected too.</p> </li> </ul> <p>Note</p> <p>The default signature for the <code>process</code> method is <code>process(self, *inputs: StepInput) -&gt; StepOutput</code>, but since in this case we're defining a <code>GeneratorStep</code>, we will need to override that (ideally under the <code>typing_extensions.override</code> decorator) with <code>process(self, offset: int = 0) -&gt; GeneratorStepOutput</code>, so that the <code>process</code> method only receives the <code>offset</code> argument, and the return type-hints should be respected too. The <code>offset</code> argument is automatically handled by the <code>Pipeline</code> shifting it until all the batches are generated, and there's no need to default it to 0, since it will be set to 0 by default anyway.</p> <p>Warning</p> <p>For the custom <code>GeneratorStep</code> subclasses to work properly with <code>distilabel</code> and with the validation and serialization performed by default over each <code>Step</code> in the <code>Pipeline</code>, the type-hint for <code>GeneratorStepOutput</code> should be used and not surrounded with double-quotes or imported under <code>typing.TYPE_CHECKING</code>, otherwise, the validation and/or serialization will fail.</p> <pre><code>from typing import List\nfrom typing_extensions import override\n\nfrom distilabel.steps import GeneratorStep\nfrom distilabel.steps.typing import GeneratorStepOutput\n\nclass MyGeneratorStep(GeneratorStep):\n    instructions: List[str]\n\n    @override\n    def process(self, offset: int = 0) -&gt; GeneratorStepOutput:\n        ...\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        ...\n</code></pre> <p>Alternatively, a simpler and more suitable way of defining custom <code>GeneratorStep</code> subclasses is via the <code>@step</code> decorator with the <code>step_type=\"generator\"</code>, which will take care of the boilerplate code, and will allow to define the <code>outputs</code>, and <code>process</code> methods in a more straightforward way.</p> <pre><code>from distilabel.steps import step\nfrom distilabel.steps.typing import GeneratorStepOutput\n\n@step(outputs=[...], step_type=\"generator\")\ndef CustomGeneratorStep(offset: int = 0) -&gt; GeneratorStepOutput:\n    yield (\n        ...,\n        True if offset == 10 else False,\n    )\n\nstep = CustomGeneratorStep(name=\"my-step\")\n</code></pre> <p>Warning</p> <p>One downside of the <code>@step</code> decorator is that it won't let you access the <code>self</code> attributes if any, neither set those, so if you need to access or set any attribute, you should go with the first approach of defining the custom <code>GeneratorStep</code> subclass.</p>"},{"location":"sections/learn/tutorial/step/global_step/","title":"GlobalStep","text":"<p>The <code>GlobalStep</code> is a subclass of <code>Step</code> that is used to define a step that requires the previous steps to be completed to run, since it will wait until all the input batches are received before running. This step is useful when you need to run a step that requires all the input data to be processed before running.</p>"},{"location":"sections/learn/tutorial/step/global_step/#working-with-globalsteps","title":"Working with GlobalSteps","text":"<p>The <code>GlobalStep</code> is intended to be used within the scope of a <code>Pipeline</code> and after some previous steps have been defined. Alternatively, it can also be used as a standalone <code>Step</code> if needed, but then using <code>Step</code> instead would be more appropriate.</p>"},{"location":"sections/learn/tutorial/step/global_step/#defining-custom-globalsteps","title":"Defining custom GlobalSteps","text":"<p>In order to define custom steps, we need to create a new subclass of the <code>GlobalStep</code> class, and set both the <code>inputs</code> and <code>outputs</code> property, as well as the <code>process</code> method.</p> <p>So on, the following will need to be defined:</p> <ul> <li> <p><code>inputs</code>: is a property that returns a list of strings with the names of the required input fields.</p> </li> <li> <p><code>outputs</code>: is a property that returns a list of strings with the names of the output fields.</p> </li> <li> <p><code>process</code>: is a method that receives the input data and returns the output data, and it should be a generator, meaning that it should <code>yield</code> the output data. It's important to preserve the default signature within the method <code>def process(self, *inputs: StepInput) -&gt; StepOutput</code>, since that's the one that will be used by the <code>Pipeline</code> to orchestrate the steps, meaning that the argument <code>inputs</code> should be respected, no more arguments can be provided, and the type-hints and return type-hints should be respected too.</p> </li> </ul> <p>Note</p> <p>The default signature for the <code>process</code> method is <code>process(self, *inputs: StepInput) -&gt; StepOutput</code>, meaning that it should be able to receive any number of inputs by default i.e. more than one <code>Step</code> at a time could be connected to the current one. Anyway, when defining custom steps, that can be overridden with <code>process(self, inputs: StepInput) -&gt; StepOutput</code>, so that the <code>process</code> method only receives the outputs from one previous <code>Step</code> connected to it.</p> <p>Warning</p> <p>For the custom <code>GlobalStep</code> subclasses to work properly with <code>distilabel</code> and with the validation and serialization performed by default over each <code>Step</code> in the <code>Pipeline</code>, the type-hint for both <code>StepInput</code> and <code>StepOutput</code> should be used and not surrounded with double-quotes or imported under <code>typing.TYPE_CHECKING</code>, otherwise, the validation and/or serialization will fail.</p> <pre><code>from distilabel.steps import GlobalStep, StepInput\nfrom distilabel.steps.typing import StepOutput\n\nclass CustomStep(Step):\n    @property\n    def inputs(self) -&gt; List[str]:\n        ...\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        ...\n\n    def process(self, *inputs: StepInput) -&gt; StepOutput:\n        for input in inputs:\n            for item in input:\n                ...\n            yield item\n\n    # When overridden (ideally under the `typing_extensions.override` decorator)\n    # @typing_extensions.override\n    # def process(self, inputs: StepInput) -&gt; StepOutput:\n    #     for input in inputs:\n    #         ...\n    #     yield inputs\n</code></pre> <p>Alternatively, a simpler and more suitable way of defining custom <code>GlobalStep</code> subclasses is via the <code>@step</code> decorator with the <code>step_type=\"global\"</code>, which will take care of the boilerplate code, and will allow to define the <code>inputs</code>, <code>outputs</code>, and <code>process</code> methods in a more straightforward way.</p> <pre><code>from distilabel.steps import StepInput, step\nfrom distilabel.steps.typing import StepOutput\n\n@step(inputs=[...], outputs=[...], step_type=\"global\")\ndef CustomStep(inputs: StepInput) -&gt; StepOutput:\n    for input in inputs:\n        ...\n    yield inputs\n\nstep = CustomStep(name=\"my-step\")\n</code></pre> <p>Warning</p> <p>One downside of the <code>@step</code> decorator is that it won't let you access the <code>self</code> attributes if any, neither set those, so if you need to access or set any attribute, you should go with the first approach of defining the custom <code>GlobalStep</code> subclass.</p>"},{"location":"sections/learn/tutorial/task/","title":"Task","text":"<p>The <code>Task</code> is an implementation on top of <code>Step</code> that includes the <code>LLM</code> as a mandatory argument, so that the <code>Task</code> defines both the input and output format via the <code>format_input</code> and <code>format_output</code> abstract methods, respectively; and calls the <code>LLM</code> to generate the text. We can see the <code>Task</code> as an <code>LLM</code> powered <code>Step</code>.</p>"},{"location":"sections/learn/tutorial/task/#working-with-tasks","title":"Working with Tasks","text":"<p>The subclasses of <code>Task</code> are intended to be used within the scope of a <code>Pipeline</code>, which will orchestrate the different tasks defined; but nonetheless, they can be used standalone if needed too.</p> <p>For example, the most basic task is the <code>TextGeneration</code> task, which generates text based on a given instruction, and it can be used standalone as well as within a <code>Pipeline</code>.</p> <pre><code>```python\nfrom distilabel.steps.tasks import TextGeneration\n\ntask = TextGeneration(\n    name=\"text-generation\",\n    llm=OpenAILLM(model=\"gpt-4\"),\n)\ntask.load()\n\nnext(task.process([{\"instruction\": \"What's the capital of Spain?\"}]))\n# [\n#     {\n#         \"instruction\": \"What's the capital of Spain?\",\n#         \"generation\": \"The capital of Spain is Madrid.\",\n#         \"model_name\": \"gpt-4\",\n#         \"distilabel_metadata\": {\n#             \"raw_output_text-generation\": \"The capital of Spain is Madrid\"\n#         }\n#     }\n# ]\n</code></pre> <p>Note</p> <p>The <code>load</code> method needs to be called ALWAYS if using the tasks as standalone, otherwise, if the <code>Pipeline</code> context manager is used, there's no need to call that method, since it will be automatically called on <code>Pipeline.run</code>; but in any other case the method <code>load</code> needs to be called from the parent class e.g. a <code>Task</code> with an <code>LLM</code> will need to call <code>Task.load</code> to load both the task and the LLM.</p> <p>As we can see in the comment of the code snippet above, the task has enriched the input dictionaries adding the <code>generation</code>, the <code>model_name</code> that was used to generate, and finally the <code>distilabel_metadata</code> dictionary that contains the raw output (without post-processing) from the LLM. In this case, the <code>TextGeneration</code> task does no post-processing, so the <code>generation</code> and the raw output is the same, but some other tasks do post-processing, which in some situations it can fail. That's why is useful to have the raw output available in the <code>distilabel_metadata</code> dictionary. If this default behaviour is not desired, then all the <code>Task</code>s has a <code>add_raw_output</code> attribute that we can set to <code>False</code> when creating the instance of the task or at run time.</p>"},{"location":"sections/learn/tutorial/task/#defining-custom-tasks","title":"Defining custom Tasks","text":"<p>In order to define custom tasks, we need to inherit from the <code>Task</code> class and implement the <code>format_input</code> and <code>format_output</code> methods, as well as setting the properties <code>inputs</code> and <code>outputs</code>, as for <code>Step</code> subclasses.</p> <p>So on, the following will need to be defined:</p> <ul> <li> <p><code>inputs</code>: is a property that returns a list of strings with the names of the required input fields.</p> </li> <li> <p><code>format_input</code>: is a method that receives a dictionary with the input data and returns a <code>ChatType</code>, which is basically a list of dictionaries with the input data formatted for the <code>LLM</code> following the chat-completion OpenAI formatting. It's important to note that the <code>ChatType</code> is a list of dictionaries, where each dictionary represents a turn in the conversation, and it must contain the keys <code>role</code> and <code>content</code>, and this is done like this since the <code>LLM</code> subclasses will format that according to the LLM used, since it's the most standard formatting.</p> </li> <li> <p><code>outputs</code>: is a property that returns a list of strings with the names of the output fields. Note that since all the <code>Task</code> subclasses are designed to work with a single <code>LLM</code>, this property should always include <code>model_name</code> as one of the outputs, since that's automatically injected from the LLM.</p> </li> <li> <p><code>format_output</code>: is a method that receives the output from the <code>LLM</code> and optionally also the input data (which may be useful to build the output in some scenarios), and returns a dictionary with the output data formatted as needed i.e. with the values for the columns in <code>outputs</code>. Note that there's no need to include the <code>model_name</code> in the output, since that's automatically injected from the LLM in the <code>process</code> method of the <code>Task</code>.</p> </li> </ul> <p>Once those methods have been implemented, the task can be used as any other task, and it will be able to generate text based on the input data.</p> <pre><code>from typing import Any, Dict, List, Union\n\nfrom distilabel.steps.tasks.base import Task\nfrom distilabel.steps.tasks.typing import ChatType\n\n\nclass MyCustomTask(Task):\n    @property\n    def inputs(self) -&gt; List[str]:\n        return [\"input_field\"]\n\n    def format_input(self, input: Dict[str, Any]) -&gt; ChatType:\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": input[\"input_field\"],\n            },\n        ]\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return [\"output_field\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        return {\"output_field\": output}\n</code></pre>"},{"location":"sections/learn/tutorial/task/generator_task/","title":"GeneratorTask","text":"<p>The <code>GeneratorTask</code> is a custom implementation of a <code>Task</code>, but based on <code>GeneratorStep</code>; which means that will essentially be similar to the standard <code>Task</code>, but without the need of providing any input data, as the data will be generated as part of the <code>GeneratorTask</code> execution.</p> <p>Warning</p> <p>This task is still experimental and may be subject to changes in the future, since apparently it's not the most efficient way to generate data, but it's a good way to generate data on the fly without the need of providing any input data.</p>"},{"location":"sections/learn/tutorial/task/generator_task/#working-with-generatortasks","title":"Working with GeneratorTasks","text":"<p>The subclasses of <code>GeneratorTask</code> are intended to be used within the scope of a <code>Pipeline</code>, which will orchestrate the different tasks defined; but nonetheless, they can be used standalone if needed too.</p> <p>These tasks will basically expect no input data, but generate data as part of the <code>process</code> method of the parent class. Say you have a <code>GeneratorTask</code> that generates text from a pre-defined instruction:</p> <pre><code>from typing import Any, Dict, List, Union\nfrom typing_extensions import override\n\nfrom distilabel.steps.tasks.base import GeneratorTask\nfrom distilabel.steps.tasks.typing import ChatType\nfrom distilabel.steps.typing import GeneratorOutput\n\n\nclass MyCustomTask(GeneratorTask):\n    instruction: str\n\n    @override\n    def process(self, offset: int = 0) -&gt; GeneratorOutput:\n        output = self.llm.generate(\n            inputs=[\n                [\n                    {\"role\": \"user\", \"content\": self.instruction},\n                ],\n            ],\n        )\n        output = {\"model_name\": self.llm.model_name}\n        output.update(\n            self.format_output(output=output, input=None)\n        )\n        yield output\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return [\"output_field\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        return {\"output_field\": output}\n</code></pre> <p>To then use it as:</p> <pre><code>task = MyCustomTask(\n    name=\"custom-generation\",\n    instruction=\"Tell me a joke.\",\n    llm=OpenAILLM(model=\"gpt-4\"),\n)\ntask.load()\n\nnext(task.process())\n# [{'output_field\": \"Why did the scarecrow win an award? Because he was outstanding!\", \"model_name\": \"gpt-4\"}]\n</code></pre> <p>Note</p> <p>Most of the times you would need to override the default <code>process</code> method, as it's suited for the standard <code>Task</code> and not for the <code>GeneratorTask</code>. But within the context of the <code>process</code> function you can freely use the <code>llm</code> to generate data in any way.</p> <p>Note</p> <p>The <code>load</code> method needs to be called ALWAYS if using the tasks as standalone, otherwise, if the <code>Pipeline</code> context manager is used, there's no need to call that method, since it will be automatically called on <code>Pipeline.run</code>; but in any other case the method <code>load</code> needs to be called from the parent class e.g. a <code>GeneratorTask</code> with an <code>LLM</code> will need to call <code>GeneratorTask.load</code> to load both the task and the LLM.</p>"},{"location":"sections/learn/tutorial/task/generator_task/#defining-custom-generatortasks","title":"Defining custom GeneratorTasks","text":"<p>In order to define custom tasks, we need to inherit from the <code>Task</code> class and implement the <code>format_input</code> and <code>format_output</code> methods, as well as setting the properties <code>inputs</code> and <code>outputs</code>, as for <code>Step</code> subclasses.</p> <p>So on, the following will need to be defined:</p> <ul> <li> <p><code>process</code>: is a method that generates the data based on the <code>LLM</code> and the <code>instruction</code> provided within the class instance, and returns a dictionary with the output data formatted as needed i.e. with the values for the columns in <code>outputs</code>. Note that the <code>inputs</code> argument is not allowed in this function since this is not a <code>Task</code> but a <code>GeneratorTask</code>, so no input data is expected; so the signature only expects the <code>offset</code> argument, which is used to keep track of the current iteration in the generator.</p> </li> <li> <p><code>outputs</code>: is a property that returns a list of strings with the names of the output fields. Note that since all the <code>Task</code> subclasses are designed to work with a single <code>LLM</code>, this property should always include <code>model_name</code> as one of the outputs, since that's automatically injected from the LLM.</p> </li> <li> <p><code>format_output</code>: is a method that receives the output from the <code>LLM</code> and optionally also the input data (which may be useful to build the output in some scenarios), and returns a dictionary with the output data formatted as needed i.e. with the values for the columns in <code>outputs</code>. Note that there's no need to include the <code>model_name</code> in the output, since that's automatically injected from the LLM in the <code>process</code> method of the <code>Task</code>.</p> </li> </ul> <p>Once those methods have been implemented, the task can be used as any other task, and it will be able to generate text based on the input data.</p> <pre><code>from typing import Any, Dict, List, Union\n\nfrom distilabel.steps.tasks.base import GeneratorTask\nfrom distilabel.steps.tasks.typing import ChatType\n\n\nclass MyCustomTask(GeneratorTask):\n    @override\n    def process(self, offset: int = 0) -&gt; GeneratorOutput:\n        output = self.llm.generate(\n            inputs=[\n                [{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n            ],\n        )\n        output = {\"model_name\": self.llm.model_name}\n        output.update(\n            self.format_output(output=output, input=None)\n        )\n        yield output\n\n    @property\n    def outputs(self) -&gt; List[str]:\n        return [\"output_field\", \"model_name\"]\n\n    def format_output(\n        self, output: Union[str, None], input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        return {\"output_field\": output}\n</code></pre>"},{"location":"sections/pipeline_samples/","title":"Pipeline Samples","text":"<p>Take a look at this section to see some <code>Examples</code> of pipelines ready to run or go visit the <code>Papers</code> section for more structured implementations of pipelines seen in the literature.</p>"},{"location":"sections/pipeline_samples/examples/","title":"Examples","text":"<p>This section contains different example pipelines that showcase different tasks, maybe you can take inspiration from them.</p>"},{"location":"sections/pipeline_samples/examples/#llamacpp-with-outlines","title":"llama.cpp with <code>outlines</code>","text":"<p>Generate RPG characters following a <code>pydantic.BaseModel</code> with <code>outlines</code> in <code>distilabel</code>.</p> See example <p>This script makes use of <code>LlamaCppLLM</code> and the structured output capabilities thanks to <code>outlines</code> to generate RPG characters that adhere to a JSON schema.</p> <p>It makes use of a local model which can be downloaded using curl (explained in the script itself), and can be exchanged with other <code>LLMs</code> like <code>vLLM</code>.</p> Run <pre><code>python examples/structured_generation_with_outlines.py\n</code></pre> structured_generation_with_outlines.py<pre><code># Copyright 2023-present, Argilla, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom enum import Enum\nfrom pathlib import Path\n\nfrom distilabel.llms import LlamaCppLLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import LoadDataFromDicts\nfrom distilabel.steps.tasks import TextGeneration\nfrom pydantic import BaseModel, StringConstraints, conint\nfrom typing_extensions import Annotated\n\n\nclass Weapon(str, Enum):\n    sword = \"sword\"\n    axe = \"axe\"\n    mace = \"mace\"\n    spear = \"spear\"\n    bow = \"bow\"\n    crossbow = \"crossbow\"\n\n\nclass Armor(str, Enum):\n    leather = \"leather\"\n    chainmail = \"chainmail\"\n    plate = \"plate\"\n    mithril = \"mithril\"\n\n\nclass Character(BaseModel):\n    name: Annotated[str, StringConstraints(max_length=30)]\n    age: conint(gt=1, lt=3000)\n    armor: Armor\n    weapon: Weapon\n\n\n# Download the model with\n# curl -L -o ~/Downloads/openhermes-2.5-mistral-7b.Q4_K_M.gguf https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf\n\nmodel_path = \"Downloads/openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\n\nwith Pipeline(\"RPG-characters\") as pipeline:\n    system_prompt = (\n        \"You are a leading role play gamer. You have seen thousands of different characters and their attributes.\"\n        \" Please return a JSON object with common attributes of an RPG character.\"\n    )\n\n    load_dataset = LoadDataFromDicts(\n        name=\"load_instructions\",\n        data=[\n            {\n                \"system_prompt\": system_prompt,\n                \"instruction\": f\"Give me a character description for a {char}\",\n            }\n            for char in [\"dwarf\", \"elf\", \"human\", \"ork\"]\n        ],\n    )\n    llm = LlamaCppLLM(\n        model_path=str(Path.home() / model_path),  # type: ignore\n        n_gpu_layers=-1,\n        n_ctx=1024,\n        structured_output={\"format\": \"json\", \"schema\": Character},\n    )\n    # Change to vLLM as such:\n    # llm = vLLM(\n    #     model=\"teknium/OpenHermes-2.5-Mistral-7B\",\n    #     extra_kwargs={\"tensor_parallel_size\": 1},\n    #     structured_output={\"format\": \"json\", \"schema\": Character},\n    # )\n\n    text_generation = TextGeneration(\n        name=\"text_generation_rpg\",\n        llm=llm,\n        input_batch_size=8,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n    load_dataset &gt;&gt; text_generation\n\n\nif __name__ == \"__main__\":\n    distiset = pipeline.run(\n        parameters={\n            text_generation.name: {\n                \"llm\": {\"generation_kwargs\": {\"max_new_tokens\": 256}}\n            }\n        },\n        use_cache=False,\n    )\n    for num, character in enumerate(distiset[\"default\"][\"train\"][\"generation\"]):\n        print(f\"Character: {num}\")\n        print(character)\n\n# Character: 0\n# {\n# \"name\": \"Gimli\",\n# \"age\": 42,\n# \"armor\": \"plate\",\n# \"weapon\": \"axe\" }\n# Character: 1\n# {\"name\":\"Gaelen\",\"age\":600,\"armor\":\"leather\",\"weapon\":\"bow\"}\n# Character: 2\n# {\"name\": \"John Smith\",\"age\": 35,\"armor\": \"leather\",\"weapon\": \"sword\"}\n# Character: 3\n# { \"name\": \"Grug\", \"age\": 35, \"armor\": \"leather\", \"weapon\": \"axe\"}\n</code></pre>"},{"location":"sections/pipeline_samples/examples/#mistralai-with-instructor","title":"MistralAI with <code>instructor</code>","text":"<p>Answer instructions with knowledge graphs defined as <code>pydantic.BaseModel</code> objects using <code>instructor</code> in <code>distilabel</code>.</p> See example <p>This script makes use of <code>MistralLLM</code> and the structured output capabilities thanks to <code>instructor</code> to generate knowledge graphs from complex topics.</p> <p>This example is translated from this awesome example from <code>instructor</code> cookbook.</p> Run <pre><code>python examples/structured_generation_with_instructor.py\n</code></pre> structured_generation_with_instructor.py<pre><code># Copyright 2023-present, Argilla, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List\n\nfrom distilabel.llms import MistralLLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import LoadDataFromDicts\nfrom distilabel.steps.tasks import TextGeneration\nfrom pydantic import BaseModel, Field\n\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n\n\nwith Pipeline(\n    name=\"Knowledge-Graphs\",\n    description=(\n        \"Generate knowledge graphs to answer questions, this type of dataset can be used to \"\n        \"steer a model to answer questions with a knowledge graph.\"\n    ),\n) as pipeline:\n    sample_questions = [\n        \"Teach me about quantum mechanics\",\n        \"Who is who in The Simpsons family?\",\n        \"Tell me about the evolution of programming languages\",\n    ]\n\n    load_dataset = LoadDataFromDicts(\n        name=\"load_instructions\",\n        data=[\n            {\n                \"system_prompt\": \"You are a knowledge graph expert generator. Help me understand by describing everything as a detailed knowledge graph.\",\n                \"instruction\": f\"{question}\",\n            }\n            for question in sample_questions\n        ],\n    )\n\n    text_generation = TextGeneration(\n        name=\"knowledge_graph_generation\",\n        llm=MistralLLM(\n            model=\"open-mixtral-8x22b\", structured_output={\"schema\": KnowledgeGraph}\n        ),\n        input_batch_size=8,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n    load_dataset &gt;&gt; text_generation\n\n\nif __name__ == \"__main__\":\n    distiset = pipeline.run(\n        parameters={\n            text_generation.name: {\n                \"llm\": {\"generation_kwargs\": {\"max_new_tokens\": 2048}}\n            }\n        },\n        use_cache=False,\n    )\n\n    distiset.push_to_hub(\"distilabel-internal-testing/knowledge_graphs\")\n</code></pre> Visualizing the graphs <p>Want to see how to visualize the graphs? You can test it using the following script. Generate some samples on your own and take a look:</p> <p>Note</p> <p>This example uses graphviz to render the graph, you can install with <code>pip</code> in the following way:</p> <pre><code>pip install graphviz\n</code></pre> <pre><code>python examples/draw_kg.py 2  # You can pass 0,1,2 to visualize each of the samples.\n</code></pre> <p></p>"},{"location":"sections/pipeline_samples/papers/","title":"Paper Implementations","text":"<p>Contains some implementations for synthetic data generation papers, using <code>distilabel</code>, providing reproducible pipelines so that anyone can play around with those approaches and customize that to their needs. We strongly believe that better data leads to better models, and synthetic data has proven to be really effective towards improving LLMs, so we aim to bridge the gap between research and practice by providing these implementations.</p>"},{"location":"sections/pipeline_samples/papers/deita/","title":"DEITA","text":"<p>DEITA (Data-Efficient Instruction Tuning for Alignment) studies an automatic data selection process by first quantifying the data quality based on complexity, quality and diversity. And second, selecting across the best potential combination from an open-source dataset that would fit into the budget you allocate to tune your own LLM.</p> <p>In most setting we cannot allocate unlimited resources for instruction-tuning LLMs. Therefore, the DEITA authors investigated how to select qualitative data for instruction-tuning based on a principle of fewer high quality samples. Liu et al. tackle the issue of first defining good data and second identifying it to respect an initial budget to instruct-tune your LLM.</p> <p>The strategy utilizes LLMs to replace human effort in time-intensive data quality tasks on instruction tuning datasets. DEITA introduces a way to measure data quality across three critical dimensions: complexity, quality and diversity.</p> <p></p> <p>You can see that we see again the dataset of instructions/responses and we kind of reproducing the second step when we learn how to optimize the responses according to an instruction by comparing several possibilities.</p>"},{"location":"sections/pipeline_samples/papers/deita/#datasets-and-budget","title":"Datasets and budget","text":"<p>We will dive deeper into the whole process. We will investigate each stage to efficiently select the final dataset used for supervised fine-tuning with a budget constraint. We will tackle technical challenges by explaining exactly how you would assess good data as presented in the paper.</p> <p>As a reminder, we're looking for a strategy to automatically select good data for the instruction-tuning step when you want to fine-tune an LLM to your own use case taking into account a resource constraint. This means that you cannot blindly train a model on any data you encounter on the internet.</p> <p>The DEITA authors assume that you have access to open-source datasets that fit your use case. This may not be the case entirely. But with open-source communities tackling many use cases, with projects such as BLOOM or AYA, it's likely that your use case will be tackled at some point. Furthermore, you could generate your own instruction/response pairs with methods such as self-generated instructions using distilabel. This tutorial assumes that we have a data pool with excessive samples for the project's cost constraint. In short, we aim to achieve adequate performance from fewer samples.</p> <p>The authors claim that the subsample size \"correlates proportionally with the computation consumed in instruction tuning\". Hence on a first approximation, reducing the sample size means reducing computation consumption and so the total development cost. Reproducing the paper notations, we will associate the budget m to a number of instruction/response pairs that you can set depending on your real budget.</p> <p></p> <p>To match the experimental set-up, dataset X_sota is a meta-dataset combining major open-source datasets available to instruct-tune LLMs. This dataset is composed of ShareGPT (58k instruction/response pairs), UltraChat (105k instruction/response pairs) and WizardLM (143k instruction/response pairs). It sums to more than 300k instruction/response pairs. We aim to reduce the final subsample to 6k instruction/response pairs.</p>"},{"location":"sections/pipeline_samples/papers/deita/#setup-the-notebook-and-packages","title":"Setup the notebook and packages","text":"<p>Let's prepare our dependencies:</p> <pre><code>pip install \"distilabel[openai,hf-transformers]&gt;=1.0.0\"\npip install pynvml huggingface_hub argilla\n</code></pre> <p>Import distilabel:</p> <pre><code>from distilabel.llms import TransformersLLM, OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import ConversationTemplate, DeitaFiltering, ExpandColumns, LoadHubDataset\nfrom distilabel.steps.tasks import ComplexityScorer, EvolInstruct, EvolQuality, GenerateEmbeddings, QualityScorer\n</code></pre> <p>Define the distilabel Pipeline and load the dataset from the Hugging Face Hub.</p> <pre><code>pipeline = Pipeline(name=\"DEITA\")\n\nload_data = LoadHubDataset(\n    name=\"load_data\", batch_size=100, output_mappings={\"prompt\": \"instruction\"}, pipeline=pipeline\n)\n</code></pre>"},{"location":"sections/pipeline_samples/papers/deita/#evol-instruct-generate-instructions-with-an-llm","title":"EVOL-INSTRUCT: Generate Instructions with an LLM","text":"<p>Evol-Instruct automates the creation of complex instruction data for training large language models (LLMs) by progressively rewriting an initial set of instructions into more complex forms. This generated data is then used to fine-tune a model named WizardLM.</p> <p>Evaluations show that instructions from Evol-Instruct are superior to human-created ones, and WizardLM achieves performance close to or exceeding GPT3.5-turbo in many skills. In distilabel, we initialise each step of the data generation pipeline. Later, we'll connect them together.</p> <pre><code>evol_instruction_complexity = EvolInstruct(\n    name=\"evol_instruction_complexity\",\n    llm=OpenAILLM(model=\"gpt-3.5-turbo\"),\n    num_evolutions=5,\n    store_evolutions=True,\n    generate_answers=True,\n    include_original_instruction=True,\n    pipeline=pipeline,\n)\n\nevol_instruction_complexity.load()\n\n_evolved_instructions = next(evol_instruction_complexity.process(\n    ([{\"instruction\": \"How many fish are there in a dozen fish?\"}]))\n)\n\nprint(*_evolved_instructions, sep=\"\\n\")\n</code></pre> <p>Output:</p> <pre><code>( 1, 'How many fish are there in a dozen fish?')\n( 2, 'How many rainbow trout are there in a dozen rainbow trout?')\n( 3, 'What is the average weight in pounds of a dozen rainbow trout caught in a specific river in Alaska during the month of May?')\n</code></pre>"},{"location":"sections/pipeline_samples/papers/deita/#evol-complexity-evaluate-complexity-of-generated-instructions","title":"EVOL COMPLEXITY: Evaluate complexity of generated instructions","text":"<p>The second step is the evaluation of complexity for an instruction in a given instruction-response pair. Like EVOL-INSTRUCT, this method uses LLMs instead of humans to automatically improve instructions, specifically through their complexity. From any instruction-response pair, \\((I, R)\\), we first generate new instructions following the In-Depth Evolving Response. We generate more complex instructions through prompting, as explained by authors, by adding some constraints or reasoning steps. Let\\'s take an example from GPT-4-LLM which aims to generate observations by GPT-4 to instruct-tune LLMs with supervised fine-tuning. And, we have the instruction \\(instruction_0\\):</p> <pre><code>instruction_0 = \"Give three tips for staying healthy.\"\n</code></pre> <p>To make it more complex, you can use, as the authors did, some prompt templates to add constraints or deepen the instruction. They provided some prompts in the paper appendix. For instance, this one was used to add constraints:</p> <pre><code>PROMPT = \"\"\"I want you act as a Prompt Rewriter.\nYour objective is to rewrite a given prompt into a more complex version to\nmake those famous AI systems (e.g., ChatGPT and GPT4) a bit harder to handle.\nBut the rewritten prompt must be reasonable and must be understood and\nresponded by humans.\nYour rewriting cannot omit the non-text parts such as the table and code in\n#Given Prompt#:. Also, please do not omit the input in #Given Prompt#.\nYou SHOULD complicate the given prompt using the following method:\nPlease add one more constraints/requirements into #Given Prompt#\nYou should try your best not to make the #Rewritten Prompt# become verbose,\n#Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#.\n\u2018#Given Prompt#\u2019, \u2018#Rewritten Prompt#\u2019, \u2018given prompt\u2019 and \u2018rewritten prompt\u2019\nare not allowed to appear in #Rewritten Prompt#\n#Given Prompt#:\n&lt;Here is instruction&gt;\n#Rewritten Prompt#:\n\"\"\"\n</code></pre> <p>Prompting this to an LLM, you automatically get a more complex instruction, called \\(instruction_1\\), from an initial instruction \\(instruction_0\\):</p> <pre><code>instruction_1 = \"Provide three recommendations for maintaining well-being, ensuring one focuses on mental health.\"\n</code></pre> <p>With sequences of evolved instructions, we use a further LLM to automatically rank and score them. We provide the 6 instructions at the same time. By providing all instructions together, we force the scoring model to look at minor complexity differences between evolved instructions. Encouraging the model to discriminate between instructions. Taking the example below, \\(instruction_0\\) and \\(instruction_1\\) could deserve the same score independently, but when compared together we would notice the slight difference that makes \\(instruction_1\\) more complex.</p> <p>In <code>distilabel</code>, we implement this like so:</p> <pre><code>instruction_complexity_scorer = ComplexityScorer(\n    name=\"instruction_complexity_scorer\",\n    llm=OpenAILLM(model=\"gpt-3.5-turbo\"),\n    input_mappings={\"instructions\": \"evolved_instructions\"},\n    pipeline=pipeline,\n)\n\nexpand_evolved_instructions = ExpandColumns(\n    name=\"expand_evolved_instructions\",\n    columns=[\"evolved_instructions\", \"answers\", \"scores\"],\n    output_mappings={\n        \"evolved_instructions\": \"evolved_instruction\",\n        \"answers\": \"answer\",\n        \"scores\": \"evol_instruction_score\",\n    },\n    pipeline=pipeline,\n)\n\ninstruction_complexity_scorer.load()\n\n_evolved_instructions = next(instruction_complexity_scorer.process(([{\"evolved_instructions\": [PROMPT + instruction_1]}])))\n\nprint(\"Original Instruction:\")\nprint(instruction_1)\nprint(\"\\nEvolved Instruction:\")\nprint(_evolved_instructions[0][\"evolved_instructions\"][0].split(\"#Rewritten Prompt#:\\n\")[1])\n</code></pre> <p>Output:</p> <pre><code>Original Instruction:\nProvide three recommendations for maintaining well-being, ensuring one focuses on mental health.\n\nEvolved Instruction:\nSuggest three strategies for nurturing overall well-being, with the stipulation that at least one explicitly addresses the enhancement of mental health, incorporating evidence-based practices.\n</code></pre>"},{"location":"sections/pipeline_samples/papers/deita/#evol-quality-quality-evaluation","title":"EVOL-QUALITY: Quality Evaluation","text":"<p>Now that we have scored the complexity of the instructions, we will focus on the quality of the responses. Similar to EVOL COMPLEXITY, the authors introduced EVOL QUALITY, a method based on LLMs, instead of humans, to automatically score the quality of the response.</p> <p>From an instruction-response pair, \\((I, R)\\), the goal is to make the response evolve into a more helpful and relevant response. The key difference is that we need to also provide the first instruction to guide evolution. Let's take back our example from GPT-4-LLM.</p> <p>Here we have the response \\(response_0\\) and its initial instruction \\(instruction_0\\):</p> <pre><code>instruction_0 = \"Give three tips for staying healthy.\"\nreponse_0 = \"1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases. 2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week. 3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\"\n</code></pre> <p>Again the authors provided several prompts you could use to make your response evolve according to some guidelines. For example, this one was used to enrich the answer:</p> <pre><code>PROMPT = \"\"\"I want you to act as a Response Rewriter\nYour goal is to enhance the quality of the response given by an AI assistant\nto the #Given Prompt# through rewriting.\nBut the rewritten response must be reasonable and must be understood by humans.\nYour rewriting cannot omit the non-text parts such as the table and code in\n#Given Prompt# and #Given Response#. Also, please do not omit the input\nin #Given Prompt#.\nYou Should enhance the quality of the response using the following method:\nPlease make the Response more in-depth\nYou should try your best not to make the #Rewritten Response# become verbose,\n#Rewritten Response# can only add 10 to 20 words into #Given Response#.\n\u2018#Given Response#\u2019, \u2018#Rewritten Response#\u2019, \u2018given response\u2019 and \u2018rewritten response\u2019\nare not allowed to appear in #Rewritten Response#\n#Given Prompt#:\n&lt;instruction_0&gt;\n#Given Response#:\n&lt;response_0&gt;\n#Rewritten Response#:\n\"\"\"\n</code></pre> <p>Prompting this to an LLM, you will automatically get a more enriched response, called \\(response_1\\), from an initial response \\(response_0\\) and initial instruction \\(instruction_0\\):</p> <pre><code>evol_response_quality = EvolQuality(\n    name=\"evol_response_quality\",\n    llm=OpenAILLM(model=\"gpt-3.5-turbo\"),\n    num_evolutions=5,\n    store_evolutions=True,\n    include_original_response=True,\n    input_mappings={\n        \"instruction\": \"evolved_instruction\",\n        \"response\": \"answer\",\n    },\n    pipeline=pipeline,\n)\n\nevol_response_quality.load()\n\n_evolved_responses = next(evol_response_quality.process([{\"instruction\": PROMPT + instruction_0, \"response\": reponse_0}]))\n\nprint(\"Original Response:\")\nprint(reponse_0)\nprint(\"\\nEvolved Response:\")\nprint(*_evolved_responses[0]['evolved_responses'], sep=\"\\n\")\n</code></pre> <p>And now, as in EVOL COMPLEXITY you iterate through this path and use different prompts to make your responses more relevant, helpful or creative. In the paper, they make 4 more iterations to get 5 evolved responses \\((R0, R1, R2, R3, R4)\\) which makes 5 different responses for one initial instruction at the end of this step.</p> <pre><code>response_quality_scorer = QualityScorer(\n    name=\"response_quality_scorer\",\n    llm=OpenAILLM(model=\"gpt-3.5-turbo\"),\n    input_mappings={\n        \"instruction\": \"evolved_instruction\",\n        \"responses\": \"evolved_responses\",\n    },\n    pipeline=pipeline,\n)\n\nexpand_evolved_responses = ExpandColumns(\n    name=\"expand_evolved_responses\",\n    columns=[\"evolved_responses\", \"scores\"],\n    output_mappings={\n        \"evolved_responses\": \"evolved_response\",\n        \"scores\": \"evol_response_score\",\n    },\n    pipeline=pipeline,\n)\n\nresponse_quality_scorer.load()\n\n_scored_responses = next(response_quality_scorer.process([{\"instruction\": PROMPT + instruction_0, \"responses\": _evolved_responses[0]['evolved_responses']}]))\n\nprint(\"Original Response:\")\nprint(reponse_0)\n\nprint(\"\\nScore, Evolved Response:\")\nprint(*zip(_scored_responses[0][\"scores\"], _evolved_responses[0]['evolved_responses']), sep=\"\\n\")\n</code></pre> <p>Output:</p> <pre><code>Original Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases. 2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week. 3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n\nScore, Evolved Response:\n(4.0, 'Here are three essential tips for maintaining good health: \\n1. Prioritize regular exercise \\n2. Eat a balanced diet with plenty of fruits and vegetables \\n3. Get an adequate amount of sleep each night.')\n(2.0, 'Here are three effective strategies to maintain a healthy lifestyle.')\n(5.0, 'Here are three practical tips to maintain good health: Ensure a balanced diet, engage in regular exercise, and prioritize sufficient sleep. These practices support overall well-being.')\n</code></pre>"},{"location":"sections/pipeline_samples/papers/deita/#improving-data-diversity","title":"Improving Data Diversity","text":"<p>One main component of good data to instruct-tune LLMs is diversity. Real world data can often contain redundancy due repetitive and homogeneous data.</p> <p>The authors of the DEITA paper tackle the challenge of ensuring data diversity in the instruction tuning LLMs to avoid the pitfalls of data redundancy that can lead to over-fitting or poor generalization. They propose an embedding-based method to filter data for diversity. This method, called Repr Filter, uses embeddings generated by the Llama 1 13B model to represent instruction-response pairs in a vector space. The diversity of a new data sample is assessed based on the cosine distance between its embedding and that of its nearest neighbor in the already selected dataset. If this distance is greater than a specified threshold, the sample is considered diverse and is added to the selection. This process prioritizes diversity by assessing each sample's contribution to the variety of the dataset until the data selection budget is met. This approach effectively maintains the diversity of the data used for instruction tuning, as demonstrated by the DEITA models outperforming or matching state-of-the-art models with significantly less training data. In this implementation of DEITA we use the hidden state of the last layer of the Llama 2 model to generate embeddings, instead of a sentence transformer model, because we found that it improved the diversity of the data selection.</p> <p></p> <pre><code>generate_conversation = ConversationTemplate(\n    name=\"generate_conversation\",\n    input_mappings={\n        \"instruction\": \"evolved_instruction\",\n        \"response\": \"evolved_response\",\n    },\n    pipeline=pipeline,\n)\n\ngenerate_embeddings = GenerateEmbeddings(\n    name=\"generate_embeddings\",\n    llm=TransformersLLM(\n        model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        device=\"cuda\",\n        torch_dtype=\"float16\",\n    ),\n    input_mappings={\"text\": \"conversation\"},\n    input_batch_size=5,\n    pipeline=pipeline,\n)\n\ndeita_filtering = DeitaFiltering(name=\"deita_filtering\", pipeline=pipeline)\n</code></pre>"},{"location":"sections/pipeline_samples/papers/deita/#build-the-distilabel-pipeline","title":"Build the \u2697 distilabel <code>Pipeline</code>","text":"<p>Now we're ready to build a <code>distilabel</code> pipeline using the DEITA method:</p> <pre><code>load_data.connect(evol_instruction_complexity)\nevol_instruction_complexity.connect(instruction_complexity_scorer)\ninstruction_complexity_scorer.connect(expand_evolved_instructions)\nexpand_evolved_instructions.connect(evol_response_quality)\nevol_response_quality.connect(response_quality_scorer)\nresponse_quality_scorer.connect(expand_evolved_responses)\nexpand_evolved_responses.connect(generate_conversation)\ngenerate_conversation.connect(generate_embeddings)\ngenerate_embeddings.connect(deita_filtering)\n</code></pre> <p>Now we can run the pipeline. We use the step names to reference them in the pipeline configuration:</p> <pre><code>distiset = pipeline.run(\n    parameters={\n        \"load_data\": {\n            \"repo_id\": \"distilabel-internal-testing/instruction-dataset-50\",\n            \"split\": \"train\",\n        },\n        \"evol_instruction_complexity\": {\n            \"llm\": {\"generation_kwargs\": {\"max_new_tokens\": 512, \"temperature\": 0.7}}\n        },\n        \"instruction_complexity_scorer\": {\n            \"llm\": {\"generation_kwargs\": {\"temperature\": 0.0}}\n        },\n        \"evol_response_quality\": {\n            \"llm\": {\"generation_kwargs\": {\"max_new_tokens\": 512, \"temperature\": 0.7}}\n        },\n        \"response_quality_scorer\": {\"llm\": {\"generation_kwargs\": {\"temperature\": 0.0}}},\n        \"deita_filtering\": {\"data_budget\": 500, \"diversity_threshold\": 0.04},\n    },\n    use_cache=False,\n)\n</code></pre> <p>We can push the results to the Hugging Face Hub:</p> <pre><code>distiset.push_to_hub(\"distilabel-internal-testing/deita-colab\")\n</code></pre>"},{"location":"sections/pipeline_samples/papers/deita/#results","title":"Results","text":"<p>Again, to show the relevance of EVOL QUALITY method, the authors evaluated on the MT-bench models fine-tuned with different data selections according to how we defined quality responses according to an instruction. Each time they selected 6k data according to the quality score:</p> <p></p> <p>Credit: Liu et al. (2023)</p> <p>The score is much better when selecting data with the EVOL QUALITY method than when we select randomly or according to the length, making a more qualitative response if longer. Nevertheless, we see that the margin we may have seen in the complexity score is thinner. And we'll discuss the strategy in a later part. Nevertheless, this strategy looks to improve the fine-tuning compared to the baselines and now we're interested in mixing quality and complexity assessment with a diversity evaluation to find the right trade-off in our selection process.</p>"},{"location":"sections/pipeline_samples/papers/deita/#conclusion","title":"Conclusion","text":"<p>In conclusion, if you are looking for some efficient method to align an open-source LLM to your business case with a constrained budget, the solutions provided by DEITA are really worth the shot. This data-centric approach enables one to focus on the content of the dataset to have the best results instead of \"just\" scaling the instruction-tuning with more, and surely less qualitative, data. In a nutshell, the strategy developed, through automatically scoring instructions-responses, aims to substitute the human preference step proprietary models such as GPT-4 have been trained with. There are a few improvements we could think about when it comes to how to select the good data, but it opens a really great way in instruct-tuning LLM with lower computational needs making the whole process intellectually relevant and more sustainable than most of the other methods. We'd be happy to help you out with aligning an LLM with your business case drawing inspiration from such a methodology.</p>"},{"location":"sections/pipeline_samples/papers/instruction_backtranslation/","title":"Instruction Backtranslation","text":"<p>\"Self Alignment with Instruction Backtranslation\" presents a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Their approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model.</p> <p>Their self-training approach assumes access to a base language model, a small amount of seed data, and a collection of unlabelled examples, e.g. a web corpus. The unlabelled data is a large, diverse set of human-written documents which includes writing about all manner of topics humans are interested in \u2013 but crucially is not paired with instructions.</p> <p>A first key assumption is that there exists some subset of this very large human-written text that would be suitable as gold generations for some user instructions. A second key assumption is that they can predict instructions for these candidate gold answers that can be used as high quality example pairs to train an instruction following model.</p> <p>Their overall process, called instruction backtranslation performs two core steps:</p> <ol> <li> <p>Self-augment: Generate instructions for unlabelled data, i.e. the web corpus, to produce candidate training data of (instruction, output) pairs for instruction tuning.</p> </li> <li> <p>Self-curate: Self-select high quality demonstration examples as training data to finetune the base model to follow instructions. This approach is done iteratively where a better intermediate instruction-following model can improve on selecting data for finetuning in the next iteration.</p> </li> </ol> <p>This replication covers the self-curation step i.e. the second / latter step as mentioned above, so as to be able to use the proposed prompting approach to rate the quality of the generated text, which can either be synthetically generated or real human-written text.</p>"},{"location":"sections/pipeline_samples/papers/instruction_backtranslation/#replication","title":"Replication","text":"<p>To replicate the paper we will be using <code>distilabel</code> and a smaller dataset created by the Hugging Face H4 team named <code>HuggingFaceH4/instruction-dataset</code> for testing purposes.</p>"},{"location":"sections/pipeline_samples/papers/instruction_backtranslation/#installation","title":"Installation","text":"<p>To replicate Self Alignment with Instruction Backtranslation one will need to install <code>distilabel</code> as it follows:</p> <pre><code>pip install \"distilabel[hf-inference-endpoints,openai]&gt;=1.0.0\"\n</code></pre> <p>And since we will be using <code>InferenceEndpointsLLM</code> (installed via the extra <code>hf-inference-endpoints</code>) we will need deploy those in advance either locally or in the Hugging Face Hub (alternatively also the serverless endpoints can be used, but most of the times the inference times are slower, and there's a limited quota to use those as those are free) and set both the <code>HF_TOKEN</code> (to use the <code>InferenceEndpointsLLM</code>) and the <code>OPENAI_API_KEY</code> environment variable value (to use the <code>OpenAILLM</code>).</p>"},{"location":"sections/pipeline_samples/papers/instruction_backtranslation/#building-blocks","title":"Building blocks","text":"<ul> <li><code>LoadHubDataset</code>: Generator Step to load a dataset from the Hugging Face Hub.</li> <li><code>TextGeneration</code>: Task to generate responses for a given instruction using an LLM.<ul> <li><code>InferenceEndpointsLLM</code>: LLM that runs a model from an Inference Endpoint in the Hugging Face Hub.</li> </ul> </li> <li><code>InstructionBacktranslation</code>: Task that generates a score and a reason for a response for a given instruction using the Self Alignment with Instruction Backtranslation prompt.<ul> <li><code>OpenAILLM</code>: LLM that loads a model from OpenAI.</li> </ul> </li> </ul>"},{"location":"sections/pipeline_samples/papers/instruction_backtranslation/#code","title":"Code","text":"<p>As mentioned before, we will put the previously mentioned building blocks together to replicate Self Alignment with Instruction Backtranslation.</p> <pre><code>from distilabel.llms import InferenceEndpointsLLM, OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import LoadHubDataset\nfrom distilabel.steps.tasks import InstructionBacktranslation, TextGeneration\n\n\nwith Pipeline(name=\"self-alignment-with-instruction-backtranslation\") as pipeline:\n    load_hub_dataset = LoadHubDataset(\n        name=\"load_dataset\",\n        output_mappings={\"prompt\": \"instruction\"},\n    )\n\n    text_generation = TextGeneration(\n        name=\"text_generation\",\n        llm=InferenceEndpointsLLM(\n            base_url=\"&lt;INFERENCE_ENDPOINT_URL&gt;\",\n            tokenizer_id=\"argilla/notus-7b-v1\",\n            model_display_name=\"argilla/notus-7b-v1\",\n        ),\n        input_batch_size=10,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n\n    instruction_backtranslation = InstructionBacktranslation(\n        name=\"instruction_backtranslation\",\n        llm=OpenAILLM(model=\"gpt-4\"),\n        input_batch_size=10,\n        output_mappings={\"model_name\": \"scoring_model\"},\n    )\n\n    keep_columns = KeepColumns(\n        name=\"keep_columns\",\n        columns=[\n            \"instruction\",\n            \"generation\",\n            \"generation_model\",\n            \"score\",\n            \"reason\",\n            \"scoring_model\",\n        ],\n    )\n\n    load_hub_dataset &gt;&gt; text_generation &gt;&gt; instruction_backtranslation &gt;&gt; keep_columns\n</code></pre> <p>Then we need to call <code>pipeline.run</code> with the runtime parameters so that the pipeline can be launched.</p> <pre><code>distiset = pipeline.run(\n    parameters={\n        load_hub_dataset.name: {\n            \"repo_id\": \"HuggingFaceH4/instruction-dataset\",\n            \"split\": \"test\",\n        },\n        text_generation.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 1024,\n                    \"temperature\": 0.7,\n                },\n            },\n        },\n        instruction_backtranslation.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 1024,\n                    \"temperature\": 0.7,\n                },\n            },\n        },\n    },\n)\n</code></pre> <p>Finally, we can optionally push the generated dataset, named <code>Distiset</code>, to the Hugging Face Hub via the <code>push_to_hub</code> method, so that each subset generated in the leaf steps is pushed to the Hub.</p> <pre><code>distiset.push_to_hub(\n    \"instruction-backtranslation-instruction-dataset\",\n    private=True,\n)\n</code></pre>"},{"location":"sections/pipeline_samples/papers/prometheus/","title":"Prometheus 2","text":"<p>\"Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models\" presents Prometheus 2, a new and more powerful evaluator LLM compared to Prometheus (its predecessor) presented in \"Prometheus: Inducing Fine-grained Evaluation Capability in Language Models\"; since GPT-4, as well as other proprietary LLMs, are commonly used to asses the quality of the responses for various LLMs, but there are concerns about transparency, controllability, and affordability, that motivate the need of open-source LLMs specialized in evaluations.</p> <p>Existing open evaluator LMs exhibit critical shortcomings:</p> <ol> <li>They issue scores that significantly diverge from those assigned by humans.</li> <li>They lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment.</li> </ol> <p>Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. Prometheus 2 is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria.</p> <p>Prometheus 2 released two variants:</p> <ul> <li><code>prometheus-eval/prometheus-7b-v2.0</code>: fine-tuned on top of <code>mistralai/Mistral-7B-Instruct-v0.2</code></li> <li><code>prometheus-eval/prometheus-8x7b-v2.0</code>: fine-tuned on top of <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code></li> </ul> <p>Both models have been fine-tuned for both direct assessment and pairwise ranking tasks i.e. assessing the quality of a single isolated response for a given instruction with or without a reference answer, and assessing the quality of one response against another one for a given instruction with or without a reference answer, respectively.</p> <p>On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Their models, code, and data are all publicly available at <code>prometheus-eval/prometheus-eval</code>.</p>"},{"location":"sections/pipeline_samples/papers/prometheus/#replication","title":"Replication","text":"<p>Note</p> <p>The section is named <code>Replication</code> but in this case we're not replicating the Prometheus 2 paper per se, but rather showing how to use the <code>PrometheusEval</code> task implemented within <code>distilabel</code> to evaluate the quality of the responses from a given instruction using the Prometheus 2 model.</p> <p>To showcase Prometheus 2 we will be using the <code>PrometheusEval</code> task implemented in <code>distilabel</code> and a smaller dataset created by the Hugging Face H4 team named <code>HuggingFaceH4/instruction-dataset</code> for testing purposes.</p>"},{"location":"sections/pipeline_samples/papers/prometheus/#installation","title":"Installation","text":"<p>To reproduce the code below, one will need to install <code>distilabel</code> as it follows:</p> <pre><code>pip install \"distilabel[vllm]&gt;=1.1.0\"\n</code></pre> <p>Alternatively, it's recommended to install <code>Dao-AILab/flash-attention</code> to benefit from Flash Attention 2 speed ups during inference via <code>vllm</code>.</p> <pre><code>pip install flash-attn --no-build-isolation\n</code></pre> <p>Note</p> <p>The installation notes above assume that you are using a VM with one GPU accelerator with at least the required VRAM to fit <code>prometheus-eval/prometheus-7b-v2.0</code> in bfloat16 (28GB); but if you have enough VRAM to fit their 8x7B model in bfloat16 (~90GB) you can use <code>prometheus-eval/prometheus-8x7b-v2.0</code> instead.</p>"},{"location":"sections/pipeline_samples/papers/prometheus/#building-blocks","title":"Building blocks","text":"<ul> <li> <p><code>LoadHubDataset</code>: <code>GeneratorStep</code> to load a dataset from the Hugging Face Hub.</p> </li> <li> <p><code>PrometheusEval</code>: <code>Task</code> that assesses the quality of a response for a given instruction using any of the Prometheus 2 models.</p> <ul> <li><code>vLLM</code>: <code>LLM</code> that loads a model from the Hugging Face Hub via vllm-project/vllm.</li> </ul> <p>Note</p> <p>Since the Prometheus 2 models use a slightly different chat template than <code>mistralai/Mistral-7B-Instruct-v0.2</code>, we need to set the <code>chat_template</code> parameter to <code>[INST] {{ messages[0]['content'] }}\\n{{ messages[1]['content'] }}[/INST]</code> so as to properly format the input for Prometheus 2.</p> </li> <li> <p>(Optional) <code>KeepColumns</code>: <code>Task</code> that keeps only the specified columns in the dataset, used to remove the undesired columns.</p> </li> </ul>"},{"location":"sections/pipeline_samples/papers/prometheus/#code","title":"Code","text":"<p>As mentioned before, we will put the previously mentioned building blocks together to see how Prometheus 2 can be used via <code>distilabel</code>.</p> <pre><code>from distilabel.llms import vLLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import KeepColumns, LoadHubDataset\nfrom distilabel.steps.tasks import PrometheusEval\n\nif __name__ == \"__main__\":\n    with Pipeline(name=\"prometheus\") as pipeline:\n        load_dataset = LoadHubDataset(\n            name=\"load_dataset\",\n            repo_id=\"HuggingFaceH4/instruction-dataset\",\n            split=\"test\",\n            output_mappings={\"prompt\": \"instruction\", \"completion\": \"generation\"},\n        )\n\n        task = PrometheusEval(\n            name=\"task\",\n            llm=vLLM(\n                model=\"prometheus-eval/prometheus-7b-v2.0\",\n                chat_template=\"[INST] {{ messages[0]['content'] }}\\n{{ messages[1]['content'] }}[/INST]\",\n            ),\n            mode=\"absolute\",\n            rubric=\"factual-validity\",\n            reference=False,\n            num_generations=1,\n            group_generations=False,\n        )\n\n        keep_columns = KeepColumns(\n            name=\"keep_columns\",\n            columns=[\"instruction\", \"generation\", \"feedback\", \"result\", \"model_name\"],\n        )\n\n        load_dataset &gt;&gt; task &gt;&gt; keep_columns\n</code></pre> <p>Then we need to call <code>pipeline.run</code> with the runtime parameters so that the pipeline can be launched.</p> <pre><code>distiset = pipeline.run(\n    parameters={\n        task.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 1024,\n                    \"temperature\": 0.7,\n                },\n            },\n        },\n    },\n)\n</code></pre> <p>Finally, we can optionally push the generated dataset, named <code>Distiset</code>, to the Hugging Face Hub via the <code>push_to_hub</code> method, so that each subset generated in the leaf steps is pushed to the Hub.</p> <pre><code>distiset.push_to_hub(\n    \"instruction-dataset-prometheus\",\n    private=True,\n)\n</code></pre>"},{"location":"sections/pipeline_samples/papers/ultrafeedback/","title":"UltraFeedback","text":"<p>UltraFeedback: Boosting Language Models with High-quality Feedback is a paper published by OpenBMB which proposes <code>UltraFeedback</code>, a large-scale, fine-grained, diverse preference dataset, used for training powerful reward models and critic models.</p> <p>UltraFeedback collects about 64k prompts from diverse resources (including UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA, and FLAN), then they use these prompts to query multiple LLMs (commercial models, Llama models ranging 7B to 70B, and non-Llama models) and generate four different responses for each prompt, resulting in a total of 256k samples i.e. the UltraFeedback will rate four responses on every OpenAI request.</p> <p>To collect high-quality preference and textual feedback, they design a fine-grained annotation instruction, which contains four different aspects, namely instruction-following, truthfulness, honesty and helpfulness (even though within the paper they also mention a fifth one named verbalized calibration). Finally, GPT-4 is used to generate the ratings for the generated responses to the given prompt using the previously mentioned aspects.</p>"},{"location":"sections/pipeline_samples/papers/ultrafeedback/#replication","title":"Replication","text":"<p>To replicate the paper we will be using <code>distilabel</code> and a smaller dataset created by the Hugging Face H4 team named <code>HuggingFaceH4/instruction-dataset</code> for testing purposes.</p> <p>Also for testing purposes we will just show how to evaluate the generated responses for a given prompt using a new global aspect named <code>overall-rating</code> defined by Argilla, that computes the average of the four aspects, so as to reduce number of requests to be sent to OpenAI, but note that all the aspects are implemented within <code>distilabel</code> and can be used instead for a more faithful reproduction. Besides that we will generate three responses for each instruction using three LLMs selected from a pool of six: <code>HuggingFaceH4/zephyr-7b-beta</code>, <code>argilla/notus-7b-v1</code>, <code>google/gemma-1.1-7b-it</code>, <code>meta-llama/Meta-Llama-3-8B-Instruct</code>, <code>HuggingFaceH4/zephyr-7b-gemma-v0.1</code> and <code>mlabonne/UltraMerge-7B</code>.</p>"},{"location":"sections/pipeline_samples/papers/ultrafeedback/#installation","title":"Installation","text":"<p>To replicate UltraFeedback one will need to install <code>distilabel</code> as it follows:</p> <pre><code>pip install \"distilabel[argilla,openai,vllm]&gt;=1.0.0\"\n</code></pre> <p>And since we will be using <code>vllm</code> we will need to use a VM with at least 6 NVIDIA GPUs with at least 16GB of memory each to run the text generation, and set the <code>OPENAI_API_KEY</code> environment variable value.</p>"},{"location":"sections/pipeline_samples/papers/ultrafeedback/#building-blocks","title":"Building blocks","text":"<ul> <li><code>LoadHubDataset</code>: Generator Step to load a dataset from the Hugging Face Hub.</li> <li><code>sample_n_steps</code>: Function to create a <code>routing_batch_function</code> that samples <code>n</code> downstream steps for each batch generated by the upstream step. This is the key to replicate the LLM pooling mechanism described in the paper.</li> <li><code>TextGeneration</code>: Task to generate responses for a given instruction using an LLM.<ul> <li><code>vLLM</code>: LLM that loads a model from the Hugging Face Hub using <code>vllm</code>.</li> </ul> </li> <li><code>CombineColumns</code>: Task that combines multiple columns into a single one i.e. from string to list of strings. Useful when there are multiple parallel steps that are connected to the same node.</li> <li><code>UltraFeedback</code>: Task that generates ratings for the responses of a given instruction using the UltraFeedback prompt.<ul> <li><code>OpenAILLM</code>: LLM that loads a model from OpenAI.</li> </ul> </li> <li><code>KeepColumns</code>: Task to keep the desired columns while removing the not needed ones, as well as defining the order for those.</li> <li>(optional) <code>PreferenceToArgilla</code>: Task to optionally push the generated dataset to Argilla to do some further analysis and human annotation.</li> </ul>"},{"location":"sections/pipeline_samples/papers/ultrafeedback/#code","title":"Code","text":"<p>As mentioned before, we will put the previously mentioned building blocks together to replicate UltraFeedback.</p> <pre><code>from distilabel.llms import OpenAILLM, vLLM\nfrom distilabel.pipeline import Pipeline, sample_n_steps\nfrom distilabel.steps import (\n    CombineColumns,\n    KeepColumns,\n    LoadHubDataset,\n    PreferenceToArgilla,\n)\nfrom distilabel.steps.tasks import TextGeneration, UltraFeedback\n\nsample_three_llms = sample_n_steps(n=3)\n\n\nwith Pipeline(name=\"ultrafeedback-pipeline\") as pipeline:\n    load_hub_dataset = LoadHubDataset(\n        name=\"load_dataset\",\n        output_mappings={\"prompt\": \"instruction\"},\n        batch_size=2,\n    )\n\n    text_generation_with_notus = TextGeneration(\n        name=\"text_generation_with_notus\",\n        llm=vLLM(model=\"argilla/notus-7b-v1\"),\n        input_batch_size=2,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n    text_generation_with_zephyr = TextGeneration(\n        name=\"text_generation_with_zephyr\",\n        llm=vLLM(model=\"HuggingFaceH4/zephyr-7b-gemma-v0.1\"),\n        input_batch_size=2,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n    text_generation_with_gemma = TextGeneration(\n        name=\"text_generation_with_gemma\",\n        llm=vLLM(model=\"google/gemma-1.1-7b-it\"),\n        input_batch_size=2,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n    text_generation_with_zephyr_gemma = TextGeneration(\n        name=\"text_generation_with_zephyr_gemma\",\n        llm=vLLM(model=\"HuggingFaceH4/zephyr-7b-gemma-v0.1\"),\n        input_batch_size=2,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n    text_generation_with_llama = TextGeneration(\n        name=\"text_generation_with_llama\",\n        llm=vLLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\"),\n        input_batch_size=2,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n    text_generation_with_ultramerge = TextGeneration(\n        name=\"text_generation_with_ultramerge\",\n        llm=vLLM(model=\"mlabonne/UltraMerge-7B\"),\n        input_batch_size=2,\n        output_mappings={\"model_name\": \"generation_model\"},\n    )\n\n    combine_columns = CombineColumns(\n        name=\"combine_columns\",\n        columns=[\"generation\", \"generation_model\"],\n        output_columns=[\"generations\", \"generation_models\"],\n        input_batch_size=2\n    )\n\n    ultrafeedback = UltraFeedback(\n        name=\"ultrafeedback_openai\",\n        llm=OpenAILLM(model=\"gpt-4-turbo-2024-04-09\"),\n        aspect=\"overall-rating\",\n        output_mappings={\"model_name\": \"ultrafeedback_model\"},\n    )\n\n    keep_columns = KeepColumns(\n        name=\"keep_columns\",\n        columns=[\n            \"instruction\",\n            \"generations\",\n            \"generation_models\",\n            \"ratings\",\n            \"rationales\",\n            \"ultrafeedback_model\",\n        ],\n    )\n\n    (\n        load_hub_dataset\n        &gt;&gt; sample_three_llms\n        &gt;&gt; [\n            text_generation_with_notus,\n            text_generation_with_zephyr,\n            text_generation_with_gemma,\n            text_generation_with_llama,\n            text_generation_with_zephyr_gemma,\n            text_generation_with_ultramerge\n        ]\n        &gt;&gt; combine_columns\n        &gt;&gt; ultrafeedback\n        &gt;&gt; keep_columns\n    )\n\n    # Optional: Push the generated dataset to Argilla, but will need to `pip install argilla` first\n    # push_to_argilla = PreferenceToArgilla(\n    #     name=\"push_to_argilla\",\n    #     api_url=\"&lt;ARGILLA_API_URL&gt;\",\n    #     api_key=\"&lt;ARGILLA_API_KEY&gt;\",  # type: ignore\n    #     dataset_name=\"ultrafeedback\",\n    #     dataset_workspace=\"admin\",\n    #     num_generations=2,\n    # )\n    # keep_columns &gt;&gt; push_to_argilla\n</code></pre> <p>Note</p> <p>As we're using a relative small dataset, we're setting a low <code>batch_size</code> and <code>input_batch_size</code> so we have more batches for the <code>routing_batch_function</code> i.e. we will have more variety on the LLMs used to generate the responses. When using a large dataset, it's recommended to use a larger <code>batch_size</code> and <code>input_batch_size</code> to benefit from the <code>vLLM</code> optimizations for larger batch sizes, which makes the pipeline execution faster.</p> <p>Then we need to call <code>pipeline.run</code> with the runtime parameters so that the pipeline can be launched.</p> <pre><code>distiset = pipeline.run(\n    parameters={\n        load_hub_dataset.name: {\n            \"repo_id\": \"HuggingFaceH4/instruction-dataset\",\n            \"split\": \"test\",\n        },\n        text_generation_with_notus.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 512,\n                    \"temperature\": 0.7,\n                }\n            },\n        },\n        text_generation_with_zephyr.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 512,\n                    \"temperature\": 0.7,\n                }\n            },\n        },\n        text_generation_with_gemma.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 512,\n                    \"temperature\": 0.7,\n                }\n            },\n        },\n        text_generation_with_llama.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 512,\n                    \"temperature\": 0.7,\n                }\n            },\n        },\n        text_generation_with_zephyr_gemma.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 512,\n                    \"temperature\": 0.7,\n                }\n            },\n        },\n        text_generation_with_ultramerge.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 512,\n                    \"temperature\": 0.7,\n                }\n            },\n        },\n        ultrafeedback_openai.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 2048,\n                    \"temperature\": 0.7,\n                }\n            },\n        },\n    }\n)\n</code></pre> <p>Finally, we can optionally push the generated dataset, named <code>Distiset</code>, to the Hugging Face Hub via the <code>push_to_hub</code> method, so that each subset generated in the leaf steps is pushed to the Hub.</p> <pre><code>dataset.push_to_hub(\n    \"ultrafeedback-instruction-dataset\",\n    private=True,\n)\n</code></pre>"},{"location":"components-gallery/","title":"Components Gallery","text":"<ul> <li> <p> Steps</p> <p>Explore all the available <code>Step</code>s that can be used for data manipulation.</p> <p> Steps</p> </li> <li> <p> Tasks</p> <p>Explore all the available <code>Task</code>s that can be used with an <code>LLM</code> to perform data generation, annotation, and more.</p> <p> Tasks</p> </li> <li> <p> LLMs</p> <p>Explore all the available <code>LLM</code>s integrated with <code>distilabel</code>.</p> <p> LLMs</p> </li> </ul>"},{"location":"components-gallery/steps/","title":"Steps Gallery","text":"<ul> <li> <p> DeitaFiltering</p> <p>Filter dataset rows using DEITA filtering strategy.</p> <p> DeitaFiltering</p> </li> <li> <p> PushToHub</p> <p>Push data to a Hugging Face Hub dataset.</p> <p> PushToHub</p> </li> <li> <p> PreferenceToArgilla</p> <p>Creates a preference dataset in Argilla.</p> <p> PreferenceToArgilla</p> </li> <li> <p> TextGenerationToArgilla</p> <p>Creates a text generation dataset in Argilla.</p> <p> TextGenerationToArgilla</p> </li> <li> <p> CombineColumns</p> <p>Combines columns from a list of <code>StepInput</code>.</p> <p> CombineColumns</p> </li> <li> <p> ExpandColumns</p> <p>Expand columns that contain lists into multiple rows.</p> <p> ExpandColumns</p> </li> <li> <p> ConversationTemplate</p> <p>Generate a conversation template from an instruction and a response.</p> <p> ConversationTemplate</p> </li> <li> <p> FormatTextGenerationDPO</p> <p>Format the output of your LLMs for Direct Preference Optimization (DPO).</p> <p> FormatTextGenerationDPO</p> </li> <li> <p> FormatChatGenerationDPO</p> <p>Format the output of a combination of a <code>ChatGeneration</code> + a preference task such as</p> <p> FormatChatGenerationDPO</p> </li> <li> <p> FormatTextGenerationSFT</p> <p>Format the output of a <code>TextGeneration</code> task for Supervised Fine-Tuning (SFT).</p> <p> FormatTextGenerationSFT</p> </li> <li> <p> FormatChatGenerationSFT</p> <p>Format the output of a <code>ChatGeneration</code> task for Supervised Fine-Tuning (SFT) following the</p> <p> FormatChatGenerationSFT</p> </li> <li> <p> KeepColumns</p> <p>Keeps selected columns in the dataset.</p> <p> KeepColumns</p> </li> <li> <p> LoadDataFromDicts</p> <p>Loads a dataset from a list of dictionaries.</p> <p> LoadDataFromDicts</p> </li> <li> <p> LoadDataFromHub</p> <p>Loads a dataset from the Hugging Face Hub.</p> <p> LoadDataFromHub</p> </li> <li> <p> LoadHubDataset</p> <p> LoadHubDataset</p> </li> <li> <p> LoadDataFromFileSystem</p> <p>Loads a dataset from a file in your filesystem.</p> <p> LoadDataFromFileSystem</p> </li> <li> <p> LoadDataFromDisk</p> <p>Load a dataset that was previously saved to disk.</p> <p> LoadDataFromDisk</p> </li> </ul>"},{"location":"components-gallery/steps/deitafiltering/","title":"DeitaFiltering","text":"<p>Filter dataset rows using DEITA filtering strategy.</p> <p>Filter the dataset based on the DEITA score and the cosine distance between the embeddings.     It's an implementation of the filtering step from the paper 'What Makes Good Data     for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.</p>"},{"location":"components-gallery/steps/deitafiltering/#attributes","title":"Attributes","text":"<ul> <li> <p>data_budget: The desired size of the dataset after filtering.</p> </li> <li> <p>diversity_threshold: If a row has a cosine distance with respect to it's nearest  neighbor greater than this value, it will be included in the filtered dataset.  Defaults to <code>0.9</code>.</p> </li> <li> <p>normalize_embeddings: Whether to normalize the embeddings before computing the cosine  distance. Defaults to <code>True</code>.</p> </li> </ul>"},{"location":"components-gallery/steps/deitafiltering/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>data_budget: The desired size of the dataset after filtering.</p> </li> <li> <p>diversity_threshold: If a row has a cosine distance with respect to it's nearest  neighbor greater than this value, it will be included in the filtered dataset.</p> </li> </ul>"},{"location":"components-gallery/steps/deitafiltering/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[evol_instruction_score]\n            ICOL1[evol_response_score]\n            ICOL2[embedding]\n        end\n        subgraph New columns\n            OCOL0[deita_score]\n            OCOL1[deita_score_computed_with]\n            OCOL2[nearest_neighbor_distance]\n        end\n    end\n\n    subgraph DeitaFiltering\n        StepInput[Input Columns: evol_instruction_score, evol_response_score, embedding]\n        StepOutput[Output Columns: deita_score, deita_score_computed_with, nearest_neighbor_distance]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    ICOL2 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/deitafiltering/#inputs","title":"Inputs","text":"<ul> <li> <p>evol_instruction_score (<code>float</code>): The score of the instruction generated by  <code>ComplexityScorer</code> step.</p> </li> <li> <p>evol_response_score (<code>float</code>): The score of the response generated by  <code>QualityScorer</code> step.</p> </li> <li> <p>embedding (<code>List[float]</code>): The embedding generated for the conversation of the  instruction-response pair using <code>GenerateEmbeddings</code> step.</p> </li> </ul>"},{"location":"components-gallery/steps/deitafiltering/#outputs","title":"Outputs","text":"<ul> <li> <p>deita_score (<code>float</code>): The DEITA score for the instruction-response pair.</p> </li> <li> <p>deita_score_computed_with (<code>List[str]</code>): The scores used to compute the DEITA  score.</p> </li> <li> <p>nearest_neighbor_distance (<code>float</code>): The cosine distance between the embeddings  of the instruction-response pair.</p> </li> </ul>"},{"location":"components-gallery/steps/deitafiltering/#references","title":"References","text":"<ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> </ul>"},{"location":"components-gallery/steps/pushtohub/","title":"PushToHub","text":"<p>Push data to a Hugging Face Hub dataset.</p> <p>A <code>GlobalStep</code> which creates a <code>datasets.Dataset</code> with the input data and pushes     it to the Hugging Face Hub.</p>"},{"location":"components-gallery/steps/pushtohub/#attributes","title":"Attributes","text":"<ul> <li> <p>repo_id: The Hugging Face Hub repository ID where the dataset will be uploaded.</p> </li> <li> <p>split: The split of the dataset that will be pushed. Defaults to <code>\"train\"</code>.</p> </li> <li> <p>private: Whether the dataset to be pushed should be private or not. Defaults to  <code>False</code>.</p> </li> <li> <p>token: The token that will be used to authenticate in the Hub. If not provided, the  token will be tried to be obtained from the environment variable <code>HF_TOKEN</code>.  If not provided using one of the previous methods, then <code>huggingface_hub</code> library  will try to use the token from the local Hugging Face CLI configuration. Defaults  to <code>None</code>.</p> </li> </ul>"},{"location":"components-gallery/steps/pushtohub/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>repo_id: The Hugging Face Hub repository ID where the dataset will be uploaded.</p> </li> <li> <p>split: The split of the dataset that will be pushed.</p> </li> <li> <p>private: Whether the dataset to be pushed should be private or not.</p> </li> <li> <p>token: The token that will be used to authenticate in the Hub.</p> </li> </ul>"},{"location":"components-gallery/steps/pushtohub/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[dynamic]\n        end\n    end\n\n    subgraph PushToHub\n        StepInput[Input Columns: dynamic]\n    end\n\n    ICOL0 --&gt; StepInput\n</code></pre>"},{"location":"components-gallery/steps/pushtohub/#inputs","title":"Inputs","text":"<ul> <li>dynamic (<code>all</code>): all columns from the input will be used to create the dataset.</li> </ul>"},{"location":"components-gallery/steps/preferencetoargilla/","title":"PreferenceToArgilla","text":"<p>Creates a preference dataset in Argilla.</p> <p>Step that creates a dataset in Argilla during the load phase, and then pushes the input     batches into it as records. This dataset is a preference dataset, where there's one field     for the instruction and one extra field per each generation within the same record, and then     a rating question per each of the generation fields. The rating question asks the annotator to     set a rating from 1 to 5 for each of the provided generations.</p>"},{"location":"components-gallery/steps/preferencetoargilla/#note","title":"Note","text":"<p>This step is meant to be used in conjunction with the <code>UltraFeedback</code> step, or any other step generating both ratings and responses for a given set of instruction and generations for the given instruction. But alternatively, it can also be used with any other task or step generating only the <code>instruction</code> and <code>generations</code>, as the <code>ratings</code> and <code>rationales</code> are optional.</p>"},{"location":"components-gallery/steps/preferencetoargilla/#attributes","title":"Attributes","text":"<ul> <li> <p>num_generations: The number of generations to include in the dataset.</p> </li> <li> <p>dataset_name: The name of the dataset in Argilla.</p> </li> <li> <p>dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to  <code>None</code>, which means it will be created in the default workspace.</p> </li> <li> <p>api_url: The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from  the <code>ARGILLA_API_URL</code> environment variable.</p> </li> <li> <p>api_key: The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will  be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> </li> </ul>"},{"location":"components-gallery/steps/preferencetoargilla/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>api_url: The base URL to use for the Argilla API requests.</p> </li> <li> <p>api_key: The API key to authenticate the requests to the Argilla API.</p> </li> </ul>"},{"location":"components-gallery/steps/preferencetoargilla/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[generations]\n            ICOL2[ratings]\n            ICOL3[rationales]\n        end\n    end\n\n    subgraph PreferenceToArgilla\n        StepInput[Input Columns: instruction, generations, ratings, rationales]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    ICOL2 --&gt; StepInput\n    ICOL3 --&gt; StepInput\n</code></pre>"},{"location":"components-gallery/steps/preferencetoargilla/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The instruction that was used to generate the completion.</p> </li> <li> <p>generations (<code>List[str]</code>): The completion that was generated based on the input instruction.</p> </li> <li> <p>ratings (<code>List[str]</code>, optional): The ratings for the generations. If not provided, the  generated ratings won't be pushed to Argilla.</p> </li> <li> <p>rationales (<code>List[str]</code>, optional): The rationales for the ratings. If not provided, the  generated rationales won't be pushed to Argilla.</p> </li> </ul>"},{"location":"components-gallery/steps/textgenerationtoargilla/","title":"TextGenerationToArgilla","text":"<p>Creates a text generation dataset in Argilla.</p> <p><code>Step</code> that creates a dataset in Argilla during the load phase, and then pushes the input     batches into it as records. This dataset is a text-generation dataset, where there's one field     per each input, and then a label question to rate the quality of the completion in either bad     (represented with \ud83d\udc4e) or good (represented with \ud83d\udc4d).</p>"},{"location":"components-gallery/steps/textgenerationtoargilla/#note","title":"Note","text":"<p>This step is meant to be used in conjunction with a <code>TextGeneration</code> step and no column mapping is needed, as it will use the default values for the <code>instruction</code> and <code>generation</code> columns.</p>"},{"location":"components-gallery/steps/textgenerationtoargilla/#attributes","title":"Attributes","text":"<ul> <li> <p>dataset_name: The name of the dataset in Argilla.</p> </li> <li> <p>dataset_workspace: The workspace where the dataset will be created in Argilla. Defaults to  <code>None</code>, which means it will be created in the default workspace.</p> </li> <li> <p>api_url: The URL of the Argilla API. Defaults to <code>None</code>, which means it will be read from  the <code>ARGILLA_API_URL</code> environment variable.</p> </li> <li> <p>api_key: The API key to authenticate with Argilla. Defaults to <code>None</code>, which means it will  be read from the <code>ARGILLA_API_KEY</code> environment variable.</p> </li> </ul>"},{"location":"components-gallery/steps/textgenerationtoargilla/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>api_url: The base URL to use for the Argilla API requests.</p> </li> <li> <p>api_key: The API key to authenticate the requests to the Argilla API.</p> </li> </ul>"},{"location":"components-gallery/steps/textgenerationtoargilla/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[generation]\n        end\n    end\n\n    subgraph TextGenerationToArgilla\n        StepInput[Input Columns: instruction, generation]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n</code></pre>"},{"location":"components-gallery/steps/textgenerationtoargilla/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The instruction that was used to generate the completion.</p> </li> <li> <p>generation (<code>str</code> or <code>List[str]</code>): The completions that were generated based on the input instruction.</p> </li> </ul>"},{"location":"components-gallery/steps/combinecolumns/","title":"CombineColumns","text":"<p>Combines columns from a list of <code>StepInput</code>.</p> <p><code>CombineColumns</code> is a <code>Step</code> that implements the <code>process</code> method that calls the <code>combine_dicts</code>     function to handle and combine a list of <code>StepInput</code>. Also <code>CombineColumns</code> provides two attributes     <code>columns</code> and <code>output_columns</code> to specify the columns to merge and the output columns     which will override the default value for the properties <code>inputs</code> and <code>outputs</code>, respectively.</p>"},{"location":"components-gallery/steps/combinecolumns/#attributes","title":"Attributes","text":"<ul> <li> <p>columns: List of strings with the names of the columns to merge.</p> </li> <li> <p>output_columns: Optional list of strings with the names of the output columns.</p> </li> </ul>"},{"location":"components-gallery/steps/combinecolumns/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[dynamic]\n        end\n        subgraph New columns\n            OCOL0[dynamic]\n        end\n    end\n\n    subgraph CombineColumns\n        StepInput[Input Columns: dynamic]\n        StepOutput[Output Columns: dynamic]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/combinecolumns/#inputs","title":"Inputs","text":"<ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to merge.</li> </ul>"},{"location":"components-gallery/steps/combinecolumns/#outputs","title":"Outputs","text":"<ul> <li>dynamic (determined by <code>columns</code> and <code>output_columns</code> attributes): The columns  that were merged.</li> </ul>"},{"location":"components-gallery/steps/expandcolumns/","title":"ExpandColumns","text":"<p>Expand columns that contain lists into multiple rows.</p> <p><code>ExpandColumns</code> is a <code>Step</code> that takes a list of columns and expands them into multiple     rows. The new rows will have the same data as the original row, except for the expanded     column, which will contain a single item from the original list.</p>"},{"location":"components-gallery/steps/expandcolumns/#attributes","title":"Attributes","text":"<ul> <li>columns: A dictionary that maps the column to be expanded to the new column name  or a list of columns to be expanded. If a list is provided, the new column name  will be the same as the column name.</li> </ul>"},{"location":"components-gallery/steps/expandcolumns/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[dynamic]\n        end\n        subgraph New columns\n            OCOL0[dynamic]\n        end\n    end\n\n    subgraph ExpandColumns\n        StepInput[Input Columns: dynamic]\n        StepOutput[Output Columns: dynamic]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/expandcolumns/#inputs","title":"Inputs","text":"<ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to be expanded into  multiple rows.</li> </ul>"},{"location":"components-gallery/steps/expandcolumns/#outputs","title":"Outputs","text":"<ul> <li>dynamic (determined by <code>columns</code> attribute): The expanded columns.</li> </ul>"},{"location":"components-gallery/steps/conversationtemplate/","title":"ConversationTemplate","text":"<p>Generate a conversation template from an instruction and a response.</p>"},{"location":"components-gallery/steps/conversationtemplate/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[response]\n        end\n        subgraph New columns\n            OCOL0[conversation]\n        end\n    end\n\n    subgraph ConversationTemplate\n        StepInput[Input Columns: instruction, response]\n        StepOutput[Output Columns: conversation]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/conversationtemplate/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The instruction to be used in the conversation.</p> </li> <li> <p>response (<code>str</code>): The response to be used in the conversation.</p> </li> </ul>"},{"location":"components-gallery/steps/conversationtemplate/#outputs","title":"Outputs","text":"<ul> <li>conversation (<code>ChatType</code>): The conversation template.</li> </ul>"},{"location":"components-gallery/steps/formattextgenerationdpo/","title":"FormatTextGenerationDPO","text":"<p>Format the output of your LLMs for Direct Preference Optimization (DPO).</p> <p><code>FormatTextGenerationDPO</code> is a <code>Step</code> that formats the output of the combination of a <code>TextGeneration</code>     task with a preference <code>Task</code> i.e. a task generating <code>ratings</code>, so that those are used to rank the     existing generations and provide the <code>chosen</code> and <code>rejected</code> generations based on the <code>ratings</code>.     Use this step to transform the output of a combination of a <code>TextGeneration</code> + a preference task such as     <code>UltraFeedback</code> following the standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p>"},{"location":"components-gallery/steps/formattextgenerationdpo/#note","title":"Note","text":"<p>The <code>generations</code> column should contain at least two generations, the <code>ratings</code> column should contain the same number of ratings as generations.</p>"},{"location":"components-gallery/steps/formattextgenerationdpo/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[system_prompt]\n            ICOL1[instruction]\n            ICOL2[generations]\n            ICOL3[generation_models]\n            ICOL4[ratings]\n        end\n        subgraph New columns\n            OCOL0[prompt]\n            OCOL1[prompt_id]\n            OCOL2[chosen]\n            OCOL3[chosen_model]\n            OCOL4[chosen_rating]\n            OCOL5[rejected]\n            OCOL6[rejected_model]\n            OCOL7[rejected_rating]\n        end\n    end\n\n    subgraph FormatTextGenerationDPO\n        StepInput[Input Columns: system_prompt, instruction, generations, generation_models, ratings]\n        StepOutput[Output Columns: prompt, prompt_id, chosen, chosen_model, chosen_rating, rejected, rejected_model, rejected_rating]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    ICOL2 --&gt; StepInput\n    ICOL3 --&gt; StepInput\n    ICOL4 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepOutput --&gt; OCOL3\n    StepOutput --&gt; OCOL4\n    StepOutput --&gt; OCOL5\n    StepOutput --&gt; OCOL6\n    StepOutput --&gt; OCOL7\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/formattextgenerationdpo/#inputs","title":"Inputs","text":"<ul> <li> <p>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the  <code>generations</code>, if available.</p> </li> <li> <p>instruction (<code>str</code>): The instruction used to generate the <code>generations</code> with the <code>LLM</code>.</p> </li> <li> <p>generations (<code>List[str]</code>): The generations produced by the <code>LLM</code>.</p> </li> <li> <p>generation_models (<code>List[str]</code>, optional): The model names used to generate the <code>generations</code>,  only available if the <code>model_name</code> from the <code>TextGeneration</code> task/s is combined into a single  column named this way, otherwise, it will be ignored.</p> </li> <li> <p>ratings (<code>List[float]</code>): The ratings for each of the <code>generations</code>, produced by a preference  task such as <code>UltraFeedback</code>.</p> </li> </ul>"},{"location":"components-gallery/steps/formattextgenerationdpo/#outputs","title":"Outputs","text":"<ul> <li> <p>prompt (<code>str</code>): The instruction used to generate the <code>generations</code> with the <code>LLM</code>.</p> </li> <li> <p>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</p> </li> <li> <p>chosen (<code>List[Dict[str, str]]</code>): The <code>chosen</code> generation based on the <code>ratings</code>.</p> </li> <li> <p>chosen_model (<code>str</code>, optional): The model name used to generate the <code>chosen</code> generation,  if the <code>generation_models</code> are available.</p> </li> <li> <p>chosen_rating (<code>float</code>): The rating of the <code>chosen</code> generation.</p> </li> <li> <p>rejected (<code>List[Dict[str, str]]</code>): The <code>rejected</code> generation based on the <code>ratings</code>.</p> </li> <li> <p>rejected_model (<code>str</code>, optional): The model name used to generate the <code>rejected</code> generation,  if the <code>generation_models</code> are available.</p> </li> <li> <p>rejected_rating (<code>float</code>): The rating of the <code>rejected</code> generation.</p> </li> </ul>"},{"location":"components-gallery/steps/formatchatgenerationdpo/","title":"FormatChatGenerationDPO","text":"<p>Format the output of a combination of a <code>ChatGeneration</code> + a preference task such as</p> <p><code>UltraFeedback</code>, for Direct Preference Optimization (DPO) following the standard formatting     from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p> <pre><code>`FormatChatGenerationDPO` is a `Step` that formats the output of the combination of a `ChatGeneration`\ntask with a preference `Task` i.e. a task generating `ratings`, so that those are used to rank the\nexisting generations and provide the `chosen` and `rejected` generations based on the `ratings`.\n</code></pre>"},{"location":"components-gallery/steps/formatchatgenerationdpo/#note","title":"Note","text":"<p>The <code>messages</code> column should contain at least one message from the user, the <code>generations</code> column should contain at least two generations, the <code>ratings</code> column should contain the same number of ratings as generations.</p>"},{"location":"components-gallery/steps/formatchatgenerationdpo/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[messages]\n            ICOL1[generations]\n            ICOL2[generation_models]\n            ICOL3[ratings]\n        end\n        subgraph New columns\n            OCOL0[prompt]\n            OCOL1[prompt_id]\n            OCOL2[chosen]\n            OCOL3[chosen_model]\n            OCOL4[chosen_rating]\n            OCOL5[rejected]\n            OCOL6[rejected_model]\n            OCOL7[rejected_rating]\n        end\n    end\n\n    subgraph FormatChatGenerationDPO\n        StepInput[Input Columns: messages, generations, generation_models, ratings]\n        StepOutput[Output Columns: prompt, prompt_id, chosen, chosen_model, chosen_rating, rejected, rejected_model, rejected_rating]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    ICOL2 --&gt; StepInput\n    ICOL3 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepOutput --&gt; OCOL3\n    StepOutput --&gt; OCOL4\n    StepOutput --&gt; OCOL5\n    StepOutput --&gt; OCOL6\n    StepOutput --&gt; OCOL7\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/formatchatgenerationdpo/#inputs","title":"Inputs","text":"<ul> <li> <p>messages (<code>List[Dict[str, str]]</code>): The conversation messages.</p> </li> <li> <p>generations (<code>List[str]</code>): The generations produced by the <code>LLM</code>.</p> </li> <li> <p>generation_models (<code>List[str]</code>, optional): The model names used to generate the <code>generations</code>,  only available if the <code>model_name</code> from the <code>ChatGeneration</code> task/s is combined into a single  column named this way, otherwise, it will be ignored.</p> </li> <li> <p>ratings (<code>List[float]</code>): The ratings for each of the <code>generations</code>, produced by a preference  task such as <code>UltraFeedback</code>.</p> </li> </ul>"},{"location":"components-gallery/steps/formatchatgenerationdpo/#outputs","title":"Outputs","text":"<ul> <li> <p>prompt (<code>str</code>): The user message used to generate the <code>generations</code> with the <code>LLM</code>.</p> </li> <li> <p>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</p> </li> <li> <p>chosen (<code>List[Dict[str, str]]</code>): The <code>chosen</code> generation based on the <code>ratings</code>.</p> </li> <li> <p>chosen_model (<code>str</code>, optional): The model name used to generate the <code>chosen</code> generation,  if the <code>generation_models</code> are available.</p> </li> <li> <p>chosen_rating (<code>float</code>): The rating of the <code>chosen</code> generation.</p> </li> <li> <p>rejected (<code>List[Dict[str, str]]</code>): The <code>rejected</code> generation based on the <code>ratings</code>.</p> </li> <li> <p>rejected_model (<code>str</code>, optional): The model name used to generate the <code>rejected</code> generation,  if the <code>generation_models</code> are available.</p> </li> <li> <p>rejected_rating (<code>float</code>): The rating of the <code>rejected</code> generation.</p> </li> </ul>"},{"location":"components-gallery/steps/formattextgenerationsft/","title":"FormatTextGenerationSFT","text":"<p>Format the output of a <code>TextGeneration</code> task for Supervised Fine-Tuning (SFT).</p> <p><code>FormatTextGenerationSFT</code> is a <code>Step</code> that formats the output of a <code>TextGeneration</code> task for     Supervised Fine-Tuning (SFT) following the standard formatting from frameworks such as <code>axolotl</code>     or <code>alignment-handbook</code>. The output of the <code>TextGeneration</code> task is formatted into a chat-like     conversation with the <code>instruction</code> as the user message and the <code>generation</code> as the assistant     message. Optionally, if the <code>system_prompt</code> is available, it is included as the first message     in the conversation.</p>"},{"location":"components-gallery/steps/formattextgenerationsft/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[system_prompt]\n            ICOL1[instruction]\n            ICOL2[generation]\n        end\n        subgraph New columns\n            OCOL0[prompt]\n            OCOL1[prompt_id]\n            OCOL2[messages]\n        end\n    end\n\n    subgraph FormatTextGenerationSFT\n        StepInput[Input Columns: system_prompt, instruction, generation]\n        StepOutput[Output Columns: prompt, prompt_id, messages]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    ICOL2 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/formattextgenerationsft/#inputs","title":"Inputs","text":"<ul> <li> <p>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the  <code>generation</code>, if available.</p> </li> <li> <p>instruction (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</p> </li> <li> <p>generation (<code>str</code>): The generation produced by the <code>LLM</code>.</p> </li> </ul>"},{"location":"components-gallery/steps/formattextgenerationsft/#outputs","title":"Outputs","text":"<ul> <li> <p>prompt (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</p> </li> <li> <p>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</p> </li> <li> <p>messages (<code>List[Dict[str, str]]</code>): The chat-like conversation with the <code>instruction</code> as  the user message and the <code>generation</code> as the assistant message.</p> </li> </ul>"},{"location":"components-gallery/steps/formatchatgenerationsft/","title":"FormatChatGenerationSFT","text":"<p>Format the output of a <code>ChatGeneration</code> task for Supervised Fine-Tuning (SFT) following the</p> <p>standard formatting from frameworks such as <code>axolotl</code> or <code>alignment-handbook</code>.</p> <pre><code>`FormatChatGenerationSFT` is a `Step` that formats the output of a `ChatGeneration` task for\nSupervised Fine-Tuning (SFT) following the standard formatting from frameworks such as `axolotl`\nor `alignment-handbook`. The output of the `ChatGeneration` task is formatted into a chat-like\nconversation with the `instruction` as the user message and the `generation` as the assistant\nmessage. Optionally, if the `system_prompt` is available, it is included as the first message\nin the conversation.\n</code></pre>"},{"location":"components-gallery/steps/formatchatgenerationsft/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[system_prompt]\n            ICOL1[instruction]\n            ICOL2[generation]\n        end\n        subgraph New columns\n            OCOL0[prompt]\n            OCOL1[prompt_id]\n            OCOL2[messages]\n        end\n    end\n\n    subgraph FormatChatGenerationSFT\n        StepInput[Input Columns: system_prompt, instruction, generation]\n        StepOutput[Output Columns: prompt, prompt_id, messages]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    ICOL2 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/formatchatgenerationsft/#inputs","title":"Inputs","text":"<ul> <li> <p>system_prompt (<code>str</code>, optional): The system prompt used within the <code>LLM</code> to generate the  <code>generation</code>, if available.</p> </li> <li> <p>instruction (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</p> </li> <li> <p>generation (<code>str</code>): The generation produced by the <code>LLM</code>.</p> </li> </ul>"},{"location":"components-gallery/steps/formatchatgenerationsft/#outputs","title":"Outputs","text":"<ul> <li> <p>prompt (<code>str</code>): The instruction used to generate the <code>generation</code> with the <code>LLM</code>.</p> </li> <li> <p>prompt_id (<code>str</code>): The <code>SHA256</code> hash of the <code>prompt</code>.</p> </li> <li> <p>messages (<code>List[Dict[str, str]]</code>): The chat-like conversation with the <code>instruction</code> as  the user message and the <code>generation</code> as the assistant message.</p> </li> </ul>"},{"location":"components-gallery/steps/keepcolumns/","title":"KeepColumns","text":"<p>Keeps selected columns in the dataset.</p> <p><code>KeepColumns</code> is a <code>Step</code> that implements the <code>process</code> method that keeps only the columns     specified in the <code>columns</code> attribute. Also <code>KeepColumns</code> provides an attribute <code>columns</code> to     specify the columns to keep which will override the default value for the properties <code>inputs</code>     and <code>outputs</code>.</p>"},{"location":"components-gallery/steps/keepcolumns/#note","title":"Note","text":"<p>The order in which the columns are provided is important, as the output will be sorted using the provided order, which is useful before pushing either a <code>dataset.Dataset</code> via the <code>PushToHub</code> step or a <code>distilabel.Distiset</code> via the <code>Pipeline.run</code> output variable.</p>"},{"location":"components-gallery/steps/keepcolumns/#attributes","title":"Attributes","text":"<ul> <li>columns: List of strings with the names of the columns to keep.</li> </ul>"},{"location":"components-gallery/steps/keepcolumns/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[dynamic]\n        end\n        subgraph New columns\n            OCOL0[dynamic]\n        end\n    end\n\n    subgraph KeepColumns\n        StepInput[Input Columns: dynamic]\n        StepOutput[Output Columns: dynamic]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/steps/keepcolumns/#inputs","title":"Inputs","text":"<ul> <li>dynamic (determined by <code>columns</code> attribute): The columns to keep.</li> </ul>"},{"location":"components-gallery/steps/keepcolumns/#outputs","title":"Outputs","text":"<ul> <li>dynamic (determined by <code>columns</code> attribute): The columns that were kept.</li> </ul>"},{"location":"components-gallery/steps/loaddatafromdicts/","title":"LoadDataFromDicts","text":"<p>Loads a dataset from a list of dictionaries.</p> <p><code>GeneratorStep</code> that loads a dataset from a list of dictionaries and yields it in     batches.</p>"},{"location":"components-gallery/steps/loaddatafromdicts/#attributes","title":"Attributes","text":"<ul> <li>data: The list of dictionaries to load the data from.</li> </ul>"},{"location":"components-gallery/steps/loaddatafromdicts/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li>batch_size: The batch size to use when processing the data.</li> </ul>"},{"location":"components-gallery/steps/loaddatafromdicts/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph New columns\n            OCOL0[dynamic]\n        end\n    end\n\n    subgraph LoadDataFromDicts\n        StepOutput[Output Columns: dynamic]\n    end\n\n    StepOutput --&gt; OCOL0\n</code></pre>"},{"location":"components-gallery/steps/loaddatafromdicts/#outputs","title":"Outputs","text":"<ul> <li>dynamic (based on the keys found on the first dictionary of the list): The columns  of the dataset.</li> </ul>"},{"location":"components-gallery/steps/loaddatafromhub/","title":"LoadDataFromHub","text":"<p>Loads a dataset from the Hugging Face Hub.</p> <p><code>GeneratorStep</code> that loads a dataset from the Hugging Face Hub using the <code>datasets</code>     library.</p>"},{"location":"components-gallery/steps/loaddatafromhub/#attributes","title":"Attributes","text":"<ul> <li> <p>repo_id: The Hugging Face Hub repository ID of the dataset to load.</p> </li> <li> <p>split: The split of the dataset to load.</p> </li> <li> <p>config: The configuration of the dataset to load. This is optional and only needed  if the dataset has multiple configurations.</p> </li> </ul>"},{"location":"components-gallery/steps/loaddatafromhub/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>batch_size: The batch size to use when processing the data.</p> </li> <li> <p>repo_id: The Hugging Face Hub repository ID of the dataset to load.</p> </li> <li> <p>split: The split of the dataset to load. Defaults to 'train'.</p> </li> <li> <p>config: The configuration of the dataset to load. This is optional and only  needed if the dataset has multiple configurations.</p> </li> <li> <p>streaming: Whether to load the dataset in streaming mode or not. Defaults to  <code>False</code>.</p> </li> <li> <p>num_examples: The number of examples to load from the dataset.  By default will load all examples.</p> </li> <li> <p>storage_options: Key/value pairs to be passed on to the file-system backend, if any.  Defaults to <code>None</code>.</p> </li> </ul>"},{"location":"components-gallery/steps/loaddatafromhub/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph New columns\n            OCOL0[dynamic]\n        end\n    end\n\n    subgraph LoadDataFromHub\n        StepOutput[Output Columns: dynamic]\n    end\n\n    StepOutput --&gt; OCOL0\n</code></pre>"},{"location":"components-gallery/steps/loaddatafromhub/#outputs","title":"Outputs","text":"<ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the  datasets loaded from the Hugging Face Hub.</li> </ul>"},{"location":"components-gallery/steps/loadhubdataset/","title":"LoadHubDataset","text":""},{"location":"components-gallery/steps/loadhubdataset/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n    end\n\n    subgraph LoadHubDataset\n    end\n\n</code></pre>"},{"location":"components-gallery/steps/loaddatafromfilesystem/","title":"LoadDataFromFileSystem","text":"<p>Loads a dataset from a file in your filesystem.</p> <p><code>GeneratorStep</code> that creates a dataset from a file in the filesystem, uses Hugging Face <code>datasets</code>     library. Take a look at Hugging Face Datasets     for more information of the supported file types.</p>"},{"location":"components-gallery/steps/loaddatafromfilesystem/#attributes","title":"Attributes","text":"<ul> <li> <p>data_files: The path to the file, or directory containing the files that conform  the dataset.</p> </li> <li> <p>split: The split of the dataset to load (typically will be <code>train</code>, <code>test</code> or <code>validation</code>).</p> </li> </ul>"},{"location":"components-gallery/steps/loaddatafromfilesystem/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>batch_size: The batch size to use when processing the data.</p> </li> <li> <p>data_files: The path to the file, or directory containing the files that conform  the dataset.</p> </li> <li> <p>split: The split of the dataset to load. Defaults to 'train'.</p> </li> <li> <p>streaming: Whether to load the dataset in streaming mode or not. Defaults to  <code>False</code>.</p> </li> <li> <p>num_examples: The number of examples to load from the dataset.  By default will load all examples.</p> </li> <li> <p>storage_options: Key/value pairs to be passed on to the file-system backend, if any.  Defaults to <code>None</code>.</p> </li> <li> <p>filetype: The expected filetype. If not provided, it will be inferred from the file extension.  For more than one file, it will be inferred from the first file.</p> </li> </ul>"},{"location":"components-gallery/steps/loaddatafromfilesystem/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph New columns\n            OCOL0[dynamic]\n        end\n    end\n\n    subgraph LoadDataFromFileSystem\n        StepOutput[Output Columns: dynamic]\n    end\n\n    StepOutput --&gt; OCOL0\n</code></pre>"},{"location":"components-gallery/steps/loaddatafromfilesystem/#outputs","title":"Outputs","text":"<ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the  datasets loaded from the Hugging Face Hub.</li> </ul>"},{"location":"components-gallery/steps/loaddatafromdisk/","title":"LoadDataFromDisk","text":"<p>Load a dataset that was previously saved to disk.</p> <p>If you previously saved your dataset using the <code>save_to_disk</code> method, or     <code>Distiset.save_to_disk</code> you can load it again to build a new pipeline using this class.</p>"},{"location":"components-gallery/steps/loaddatafromdisk/#attributes","title":"Attributes","text":"<ul> <li> <p>dataset_path: The path to the dataset or distiset.</p> </li> <li> <p>split: The split of the dataset to load (typically will be <code>train</code>, <code>test</code> or <code>validation</code>).</p> </li> <li> <p>config: The configuration of the dataset to load. This is optional and only needed  if the dataset has multiple configurations.</p> </li> </ul>"},{"location":"components-gallery/steps/loaddatafromdisk/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>batch_size: The batch size to use when processing the data.</p> </li> <li> <p>dataset_path: The path to the dataset or distiset.</p> </li> <li> <p>is_distiset: Whether the dataset to load is a <code>Distiset</code> or not. Defaults to False.</p> </li> <li> <p>split: The split of the dataset to load. Defaults to 'train'.</p> </li> <li> <p>config: The configuration of the dataset to load. This is optional and only  needed if the dataset has multiple configurations.</p> </li> <li> <p>num_examples: The number of examples to load from the dataset.  By default will load all examples.</p> </li> <li> <p>storage_options: Key/value pairs to be passed on to the file-system backend, if any.  Defaults to <code>None</code>.</p> </li> </ul>"},{"location":"components-gallery/steps/loaddatafromdisk/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph New columns\n            OCOL0[dynamic]\n        end\n    end\n\n    subgraph LoadDataFromDisk\n        StepOutput[Output Columns: dynamic]\n    end\n\n    StepOutput --&gt; OCOL0\n</code></pre>"},{"location":"components-gallery/steps/loaddatafromdisk/#outputs","title":"Outputs","text":"<ul> <li>dynamic (<code>all</code>): The columns that will be generated by this step, based on the  datasets loaded from the Hugging Face Hub.</li> </ul>"},{"location":"components-gallery/tasks/","title":"Tasks Gallery","text":"<ul> <li> <p> ComplexityScorer</p> <p>Score instructions based on their complexity using an <code>LLM</code>.</p> <p> ComplexityScorer</p> </li> <li> <p> EvolInstruct</p> <p>Evolve instructions using an <code>LLM</code>.</p> <p> EvolInstruct</p> </li> <li> <p> EvolComplexity</p> <p>Evolve instructions to make them more complex using an <code>LLM</code>.</p> <p> EvolComplexity</p> </li> <li> <p> EvolQuality</p> <p>Evolve the quality of the responses using an <code>LLM</code>.</p> <p> EvolQuality</p> </li> <li> <p> Genstruct</p> <p>Generate a pair of instruction-response from a document using an <code>LLM</code>.</p> <p> Genstruct</p> </li> <li> <p> InstructionBacktranslation</p> <p>Self-Alignment with Instruction Backtranslation.</p> <p> InstructionBacktranslation</p> </li> <li> <p> PrometheusEval</p> <p>Critique and rank the quality of generations from an <code>LLM</code> using Prometheus 2.0.</p> <p> PrometheusEval</p> </li> <li> <p> QualityScorer</p> <p>Score responses based on their quality using an <code>LLM</code>.</p> <p> QualityScorer</p> </li> <li> <p> SelfInstruct</p> <p>Generate instructions based on a given input using an <code>LLM</code>.</p> <p> SelfInstruct</p> </li> <li> <p> GenerateSentencePair</p> <p>Generate a positive and negative (optionally) sentences given an anchor sentence.</p> <p> GenerateSentencePair</p> </li> <li> <p> StructuredGeneration</p> <p>Generate structured content for a given <code>instruction</code> using an <code>LLM</code>.</p> <p> StructuredGeneration</p> </li> <li> <p> TextGeneration</p> <p>Simple text generation with an <code>LLM</code> given an instruction.</p> <p> TextGeneration</p> </li> <li> <p> ChatGeneration</p> <p>Generates text based on a conversation.</p> <p> ChatGeneration</p> </li> <li> <p> UltraFeedback</p> <p>Rank generations focusing on different aspects using an <code>LLM</code>.</p> <p> UltraFeedback</p> </li> <li> <p> EvolInstructGenerator</p> <p>Generate evolved instructions using an <code>LLM</code>.</p> <p> EvolInstructGenerator</p> </li> <li> <p> EvolComplexityGenerator</p> <p>Generate evolved instructions with increased complexity using an <code>LLM</code>.</p> <p> EvolComplexityGenerator</p> </li> <li> <p> PairRM</p> <p>Rank the candidates based on the input using the <code>LLM</code> model.</p> <p> PairRM</p> </li> <li> <p> GenerateEmbeddings</p> <p>Generate embeddings using the last hidden state of an <code>LLM</code>.</p> <p> GenerateEmbeddings</p> </li> </ul>"},{"location":"components-gallery/tasks/complexityscorer/","title":"ComplexityScorer","text":"<p>Score instructions based on their complexity using an <code>LLM</code>.</p> <p><code>ComplexityScorer</code> is a pre-defined task used to rank a list of instructions based in     their complexity. It's an implementation of the complexity score task from the paper     'What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection     in Instruction Tuning'.</p>"},{"location":"components-gallery/tasks/complexityscorer/#attributes","title":"Attributes","text":"<ul> <li>_template: a Jinja2 template used to format the input for the LLM.</li> </ul>"},{"location":"components-gallery/tasks/complexityscorer/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instructions]\n        end\n        subgraph New columns\n            OCOL0[scores]\n            OCOL1[model_name]\n        end\n    end\n\n    subgraph ComplexityScorer\n        StepInput[Input Columns: instructions]\n        StepOutput[Output Columns: scores, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/complexityscorer/#inputs","title":"Inputs","text":"<ul> <li>instructions (<code>List[str]</code>): The list of instructions to be scored.</li> </ul>"},{"location":"components-gallery/tasks/complexityscorer/#outputs","title":"Outputs","text":"<ul> <li> <p>scores (<code>List[float]</code>): The score for each instruction.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to generate the scores.</p> </li> </ul>"},{"location":"components-gallery/tasks/complexityscorer/#references","title":"References","text":"<ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> </ul>"},{"location":"components-gallery/tasks/evolinstruct/","title":"EvolInstruct","text":"<p>Evolve instructions using an <code>LLM</code>.</p> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p>"},{"location":"components-gallery/tasks/evolinstruct/#attributes","title":"Attributes","text":"<ul> <li> <p>num_evolutions: The number of evolutions to be performed.</p> </li> <li> <p>store_evolutions: Whether to store all the evolutions or just the last one. Defaults  to <code>False</code>.</p> </li> <li> <p>generate_answers: Whether to generate answers for the evolved instructions. Defaults  to <code>False</code>.</p> </li> <li> <p>include_original_instruction: Whether to include the original instruction in the  <code>evolved_instructions</code> output column. Defaults to <code>False</code>.</p> </li> <li> <p>mutation_templates: The mutation templates to be used for evolving the instructions.  Defaults to the ones provided in the <code>utils.py</code> file.</p> </li> <li> <p>seed: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.  Defaults to <code>42</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolinstruct/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li>seed: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul>"},{"location":"components-gallery/tasks/evolinstruct/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n        end\n        subgraph New columns\n            OCOL0[evolved_instruction]\n            OCOL1[evolved_instructions]\n            OCOL2[model_name]\n            OCOL3[answer]\n            OCOL4[answers]\n        end\n    end\n\n    subgraph EvolInstruct\n        StepInput[Input Columns: instruction]\n        StepOutput[Output Columns: evolved_instruction, evolved_instructions, model_name, answer, answers]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepOutput --&gt; OCOL3\n    StepOutput --&gt; OCOL4\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/evolinstruct/#inputs","title":"Inputs","text":"<ul> <li>instruction (<code>str</code>): The instruction to evolve.</li> </ul>"},{"location":"components-gallery/tasks/evolinstruct/#outputs","title":"Outputs","text":"<ul> <li> <p>evolved_instruction (<code>str</code>): The evolved instruction if <code>store_evolutions=False</code>.</p> </li> <li> <p>evolved_instructions (<code>List[str]</code>): The evolved instructions if <code>store_evolutions=True</code>.</p> </li> <li> <p>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</p> </li> <li> <p>answer (<code>str</code>): The answer to the evolved instruction if <code>generate_answers=True</code>  and <code>store_evolutions=False</code>.</p> </li> <li> <p>answers (<code>List[str]</code>): The answers to the evolved instructions if <code>generate_answers=True</code>  and <code>store_evolutions=True</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolinstruct/#references","title":"References","text":"<ul> <li> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> </li> <li> <p>GitHub: h2oai/h2o-wizardlm</p> </li> </ul>"},{"location":"components-gallery/tasks/evolcomplexity/","title":"EvolComplexity","text":"<p>Evolve instructions to make them more complex using an <code>LLM</code>.</p> <p><code>EvolComplexity</code> is a task that evolves instructions to make them more complex,     and it is based in the EvolInstruct task, but using slight different prompts, but the     exact same evolutionary approach.</p>"},{"location":"components-gallery/tasks/evolcomplexity/#attributes","title":"Attributes","text":"<ul> <li> <p>num_instructions: The number of instructions to be generated.</p> </li> <li> <p>generate_answers: Whether to generate answers for the instructions or not. Defaults  to <code>False</code>.</p> </li> <li> <p>mutation_templates: The mutation templates to be used for the generation of the  instructions.</p> </li> <li> <p>min_length: Defines the length (in bytes) that the generated instruction needs to  be higher than, to be considered valid. Defaults to <code>512</code>.</p> </li> <li> <p>max_length: Defines the length (in bytes) that the generated instruction needs to  be lower than, to be considered valid. Defaults to <code>1024</code>.</p> </li> <li> <p>seed: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.  Defaults to <code>42</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolcomplexity/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>min_length: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.</p> </li> <li> <p>max_length: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.</p> </li> <li> <p>seed: The number of evolutions to be run.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolcomplexity/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n        end\n        subgraph New columns\n            OCOL0[evolved_instruction]\n            OCOL1[answer]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph EvolComplexity\n        StepInput[Input Columns: instruction]\n        StepOutput[Output Columns: evolved_instruction, answer, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/evolcomplexity/#inputs","title":"Inputs","text":"<ul> <li>instruction (<code>str</code>): The instruction to evolve.</li> </ul>"},{"location":"components-gallery/tasks/evolcomplexity/#outputs","title":"Outputs","text":"<ul> <li> <p>evolved_instruction (<code>str</code>): The evolved instruction.</p> </li> <li> <p>answer (<code>str</code>, optional): The answer to the instruction if <code>generate_answers=True</code>.</p> </li> <li> <p>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolcomplexity/#references","title":"References","text":"<ul> <li> <p>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</p> </li> <li> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> </li> </ul>"},{"location":"components-gallery/tasks/evolquality/","title":"EvolQuality","text":"<p>Evolve the quality of the responses using an <code>LLM</code>.</p> <p><code>EvolQuality</code> task is used to evolve the quality of the responses given a prompt,     by generating a new response with a language model. This step implements the evolution     quality task from the paper 'What Makes Good Data for Alignment? A Comprehensive Study of     Automatic Data Selection in Instruction Tuning'.</p>"},{"location":"components-gallery/tasks/evolquality/#attributes","title":"Attributes","text":"<ul> <li> <p>num_evolutions: The number of evolutions to be performed on the responses.</p> </li> <li> <p>store_evolutions: Whether to store all the evolved responses or just the last one.  Defaults to <code>False</code>.</p> </li> <li> <p>include_original_response: Whether to include the original response within the evolved  responses. Defaults to <code>False</code>.</p> </li> <li> <p>mutation_templates: The mutation templates to be used to evolve the responses.</p> </li> <li> <p>seed: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.  Defaults to <code>42</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolquality/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li>seed: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</li> </ul>"},{"location":"components-gallery/tasks/evolquality/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[response]\n        end\n        subgraph New columns\n            OCOL0[evolved_response]\n            OCOL1[evolved_responses]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph EvolQuality\n        StepInput[Input Columns: instruction, response]\n        StepOutput[Output Columns: evolved_response, evolved_responses, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/evolquality/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The instruction that was used to generate the <code>responses</code>.</p> </li> <li> <p>response (<code>str</code>): The responses to be rewritten.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolquality/#outputs","title":"Outputs","text":"<ul> <li> <p>evolved_response (<code>str</code>): The evolved response if <code>store_evolutions=False</code>.</p> </li> <li> <p>evolved_responses (<code>List[str]</code>): The evolved responses if <code>store_evolutions=True</code>.</p> </li> <li> <p>model_name (<code>str</code>): The name of the LLM used to evolve the responses.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolquality/#references","title":"References","text":"<ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> </ul>"},{"location":"components-gallery/tasks/genstruct/","title":"Genstruct","text":"<p>Generate a pair of instruction-response from a document using an <code>LLM</code>.</p> <p><code>Genstruct</code> is a pre-defined task designed to generate valid instructions from a given raw document,     with the title and the content, enabling the creation of new, partially synthetic instruction finetuning     datasets from any raw-text corpus. The task is based on the Genstruct 7B model by Nous Research, which is     inspired in the Ada-Instruct paper.</p>"},{"location":"components-gallery/tasks/genstruct/#note","title":"Note","text":"<p>The Genstruct prompt i.e. the task, can be used with any model really, but the safest / recommended option is to use <code>NousResearch/Genstruct-7B</code> as the LLM provided to the task, since it was trained for this specific task.</p>"},{"location":"components-gallery/tasks/genstruct/#attributes","title":"Attributes","text":"<ul> <li>_template: a Jinja2 template used to format the input for the LLM.</li> </ul>"},{"location":"components-gallery/tasks/genstruct/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[title]\n            ICOL1[content]\n        end\n        subgraph New columns\n            OCOL0[user]\n            OCOL1[assistant]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph Genstruct\n        StepInput[Input Columns: title, content]\n        StepOutput[Output Columns: user, assistant, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/genstruct/#inputs","title":"Inputs","text":"<ul> <li> <p>title (<code>str</code>): The title of the document.</p> </li> <li> <p>content (<code>str</code>): The content of the document.</p> </li> </ul>"},{"location":"components-gallery/tasks/genstruct/#outputs","title":"Outputs","text":"<ul> <li> <p>user (<code>str</code>): The user's instruction based on the document.</p> </li> <li> <p>assistant (<code>str</code>): The assistant's response based on the user's instruction.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to generate the <code>feedback</code> and <code>result</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/genstruct/#references","title":"References","text":"<ul> <li> <p>Genstruct 7B by Nous Research</p> </li> <li> <p>Ada-Instruct: Adapting Instruction Generators for Complex Reasoning</p> </li> </ul>"},{"location":"components-gallery/tasks/instructionbacktranslation/","title":"InstructionBacktranslation","text":"<p>Self-Alignment with Instruction Backtranslation.</p>"},{"location":"components-gallery/tasks/instructionbacktranslation/#attributes","title":"Attributes","text":"<ul> <li>_template: the Jinja2 template to use for the Instruction Backtranslation task.</li> </ul>"},{"location":"components-gallery/tasks/instructionbacktranslation/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[generation]\n        end\n        subgraph New columns\n            OCOL0[score]\n            OCOL1[reason]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph InstructionBacktranslation\n        StepInput[Input Columns: instruction, generation]\n        StepOutput[Output Columns: score, reason, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/instructionbacktranslation/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The reference instruction to evaluate the text output.</p> </li> <li> <p>generation (<code>str</code>): The text output to evaluate for the given instruction.</p> </li> </ul>"},{"location":"components-gallery/tasks/instructionbacktranslation/#outputs","title":"Outputs","text":"<ul> <li> <p>score (<code>str</code>): The score for the generation based on the given instruction.</p> </li> <li> <p>reason (<code>str</code>): The reason for the provided score.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to score the generation.</p> </li> </ul>"},{"location":"components-gallery/tasks/instructionbacktranslation/#references","title":"References","text":"<ul> <li>Self-Alignment with Instruction Backtranslation</li> </ul>"},{"location":"components-gallery/tasks/prometheuseval/","title":"PrometheusEval","text":"<p>Critique and rank the quality of generations from an <code>LLM</code> using Prometheus 2.0.</p> <p><code>PrometheusEval</code> is a task created for Prometheus 2.0, covering both the absolute and relative     evaluations.</p> <pre><code>- The absolute evaluation i.e. `mode=\"absolute\"` is used to evaluate a single generation from\n    an LLM for a given instruction.\n- The relative evaluation i.e. `mode=\"relative\"` is used to evaluate two generations from an LLM\n    for a given instruction.\n\nBoth evaluations provide the possibility whether to use a reference answer to compare with or not\nvia the `reference` attribute, and both are based on a score rubric that critiques the generation/s\nbased on the following default aspects: `helpfulness`, `harmlessness`, `honesty`, `factual-validity`,\nand `reasoning`, that can be overridden via `rubrics`, and the selected rubric is set via the attribute\n`rubric`.\n</code></pre>"},{"location":"components-gallery/tasks/prometheuseval/#note","title":"Note","text":"<p>The <code>PrometheusEval</code> task is better suited and intended to be used with any of the Prometheus 2.0 models released by Kaist AI, being: https://huggingface.co/prometheus-eval/prometheus-7b-v2.0, and https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0. The critique assessment formatting and quality is not guaranteed if using another model, even though some other models may be able to correctly follow the formatting and generate insightful critiques too.</p>"},{"location":"components-gallery/tasks/prometheuseval/#attributes","title":"Attributes","text":"<ul> <li> <p>mode: the evaluation mode to use, either <code>absolute</code> or <code>relative</code>. It defines whether the task  will evaluate one or two generations.</p> </li> <li> <p>rubric: the score rubric to use within the prompt to run the critique based on different aspects.  Can be any existing key in the <code>rubrics</code> attribute, which by default means that it can be:  <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, or <code>reasoning</code>. Those will only  work if using the default <code>rubrics</code>, otherwise, the provided <code>rubrics</code> should be used.</p> </li> <li> <p>rubrics: a dictionary containing the different rubrics to use for the critique, where the keys are  the rubric names and the values are the rubric descriptions. The default rubrics are the following:  <code>helpfulness</code>, <code>harmlessness</code>, <code>honesty</code>, <code>factual-validity</code>, and <code>reasoning</code>.</p> </li> <li> <p>reference: a boolean flag to indicate whether a reference answer / completion will be provided, so  that the model critique is based on the comparison with it. It implies that the column <code>reference</code>  needs to be provided within the input data in addition to the rest of the inputs.</p> </li> <li> <p>_template: a Jinja2 template used to format the input for the LLM.</p> </li> </ul>"},{"location":"components-gallery/tasks/prometheuseval/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[generation]\n            ICOL2[generations]\n            ICOL3[reference]\n        end\n        subgraph New columns\n            OCOL0[feedback]\n            OCOL1[result]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph PrometheusEval\n        StepInput[Input Columns: instruction, generation, generations, reference]\n        StepOutput[Output Columns: feedback, result, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    ICOL2 --&gt; StepInput\n    ICOL3 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/prometheuseval/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The instruction to use as reference.</p> </li> <li> <p>generation (<code>str</code>, optional): The generated text from the given <code>instruction</code>. This column is required  if <code>mode=absolute</code>.</p> </li> <li> <p>generations (<code>List[str]</code>, optional): The generated texts from the given <code>instruction</code>. It should  contain 2 generations only. This column is required if <code>mode=relative</code>.</p> </li> <li> <p>reference (<code>str</code>, optional): The reference / golden answer for the <code>instruction</code>, to be used by the LLM  for comparison against.</p> </li> </ul>"},{"location":"components-gallery/tasks/prometheuseval/#outputs","title":"Outputs","text":"<ul> <li> <p>feedback (<code>str</code>): The feedback explaining the result below, as critiqued by the LLM using the  pre-defined score rubric, compared against <code>reference</code> if provided.</p> </li> <li> <p>result (<code>Union[int, Literal[\"A\", \"B\"]]</code>): If <code>mode=absolute</code>, then the result contains the score for the  <code>generation</code> in a likert-scale from 1-5, otherwise, if <code>mode=relative</code>, then the result contains either  \"A\" or \"B\", the \"winning\" one being the generation in the index 0 of <code>generations</code> if <code>result='A'</code> or the  index 1 if <code>result='B'</code>.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to generate the <code>feedback</code> and <code>result</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/prometheuseval/#references","title":"References","text":"<ul> <li> <p>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</p> </li> <li> <p>prometheus-eval: Evaluate your LLM's response with Prometheus \ud83d\udcaf</p> </li> </ul>"},{"location":"components-gallery/tasks/qualityscorer/","title":"QualityScorer","text":"<p>Score responses based on their quality using an <code>LLM</code>.</p> <p><code>QualityScorer</code> is a pre-defined task that defines the <code>instruction</code> as the input     and <code>score</code> as the output. This task is used to rate the quality of instructions and responses.     It's an implementation of the quality score task from the paper 'What Makes Good Data     for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning'.     The task follows the same scheme as the Complexity Scorer, but the instruction-response pairs     are scored in terms of quality, obtaining a quality score for each instruction.</p>"},{"location":"components-gallery/tasks/qualityscorer/#attributes","title":"Attributes","text":"<ul> <li>_template: a Jinja2 template used to format the input for the LLM.</li> </ul>"},{"location":"components-gallery/tasks/qualityscorer/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[responses]\n        end\n        subgraph New columns\n            OCOL0[scores]\n            OCOL1[model_name]\n        end\n    end\n\n    subgraph QualityScorer\n        StepInput[Input Columns: instruction, responses]\n        StepOutput[Output Columns: scores, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/qualityscorer/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The instruction that was used to generate the <code>responses</code>.</p> </li> <li> <p>responses (<code>List[str]</code>): The responses to be scored. Each response forms a pair with the instruction.</p> </li> </ul>"},{"location":"components-gallery/tasks/qualityscorer/#outputs","title":"Outputs","text":"<ul> <li> <p>scores (<code>List[float]</code>): The score for each instruction.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to generate the scores.</p> </li> </ul>"},{"location":"components-gallery/tasks/qualityscorer/#references","title":"References","text":"<ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> </ul>"},{"location":"components-gallery/tasks/selfinstruct/","title":"SelfInstruct","text":"<p>Generate instructions based on a given input using an <code>LLM</code>.</p> <p><code>SelfInstruct</code> is a pre-defined task that, given a number of instructions, a     certain criteria for query generations, an application description, and an input,     generates a number of instruction related to the given input and following what     is stated in the criteria for query generation and the application description.     It is based in the SelfInstruct framework from the paper \"Self-Instruct: Aligning     Language Models with Self-Generated Instructions\".</p>"},{"location":"components-gallery/tasks/selfinstruct/#attributes","title":"Attributes","text":"<ul> <li> <p>num_instructions: The number of instructions to be generated. Defaults to 5.</p> </li> <li> <p>criteria_for_query_generation: The criteria for the query generation. Defaults  to the criteria defined within the paper.</p> </li> <li> <p>application_description: The description of the AI application that one want  to build with these instructions. Defaults to <code>AI assistant</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/selfinstruct/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[input]\n        end\n        subgraph New columns\n            OCOL0[instructions]\n            OCOL1[model_name]\n        end\n    end\n\n    subgraph SelfInstruct\n        StepInput[Input Columns: input]\n        StepOutput[Output Columns: instructions, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/selfinstruct/#inputs","title":"Inputs","text":"<ul> <li>input (<code>str</code>): The input to generate the instructions. It's also called seed in  the paper.</li> </ul>"},{"location":"components-gallery/tasks/selfinstruct/#outputs","title":"Outputs","text":"<ul> <li> <p>instructions (<code>List[str]</code>): The generated instructions.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to generate the instructions.</p> </li> </ul>"},{"location":"components-gallery/tasks/generatesentencepair/","title":"GenerateSentencePair","text":"<p>Generate a positive and negative (optionally) sentences given an anchor sentence.</p> <p><code>GenerateSentencePair</code> is a pre-defined task that given an anchor sentence generates     a positive sentence related to the anchor and optionally a negative sentence unrelated     to the anchor. This task is useful to generate training datasets for training embeddings     models.</p>"},{"location":"components-gallery/tasks/generatesentencepair/#attributes","title":"Attributes","text":"<ul> <li> <p>triplet: a flag to indicate if the task should generate a triplet of sentences  (anchor, positive, negative). Defaults to <code>False</code>.</p> </li> <li> <p>action: the action to perform to generate the positive sentence.</p> </li> </ul>"},{"location":"components-gallery/tasks/generatesentencepair/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[anchor]\n        end\n        subgraph New columns\n            OCOL0[positive]\n            OCOL1[negative]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph GenerateSentencePair\n        StepInput[Input Columns: anchor]\n        StepOutput[Output Columns: positive, negative, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/generatesentencepair/#inputs","title":"Inputs","text":"<ul> <li>anchor (<code>str</code>): The anchor sentence to generate the positive and negative sentences.</li> </ul>"},{"location":"components-gallery/tasks/generatesentencepair/#outputs","title":"Outputs","text":"<ul> <li> <p>positive (<code>str</code>): The positive sentence related to the <code>anchor</code>.</p> </li> <li> <p>negative (<code>str</code>): The negative sentence unrelated to the <code>anchor</code> if <code>triplet=True</code>.</p> </li> <li> <p>model_name (<code>str</code>): The name of the model that was used to generate the sentences.</p> </li> </ul>"},{"location":"components-gallery/tasks/generatesentencepair/#examples","title":"Examples","text":""},{"location":"components-gallery/tasks/generatesentencepair/#paraphrasing","title":"Paraphrasing","text":"<pre><code>from distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"paraphrase\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n</code></pre>"},{"location":"components-gallery/tasks/generatesentencepair/#generating-semantically-similar-sentences","title":"Generating semantically similar sentences","text":"<pre><code>from distilabel.llms import InferenceEndpointsLLM\nfrom distilabel.steps.tasks import GenerateSentencePair\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"semantically-similar\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"How does 3D printing work?\"}])\n</code></pre>"},{"location":"components-gallery/tasks/generatesentencepair/#generating-queries","title":"Generating queries","text":"<pre><code>from distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"query\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"Argilla is an open-source data curation platform for LLMs. Using Argilla, ...\"}])\n</code></pre>"},{"location":"components-gallery/tasks/generatesentencepair/#generating-answers","title":"Generating answers","text":"<pre><code>from distilabel.steps.tasks import GenerateSentencePair\nfrom distilabel.llms import InferenceEndpointsLLM\n\ngenerate_sentence_pair = GenerateSentencePair(\n    triplet=True, # `False` to generate only positive\n    action=\"answer\",\n    llm=InferenceEndpointsLLM(\n        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    ),\n    input_batch_size=10,\n)\n\ngenerate_sentence_pair.load()\n\nresult = generate_sentence_pair.process([{\"anchor\": \"What Game of Thrones villain would be the most likely to give you mercy?\"}])\n</code></pre>"},{"location":"components-gallery/tasks/structuredgeneration/","title":"StructuredGeneration","text":"<p>Generate structured content for a given <code>instruction</code> using an <code>LLM</code>.</p> <p><code>StructuredGeneration</code> is a pre-defined task that defines the <code>instruction</code> and the <code>grammar</code>     as the inputs, and <code>generation</code> as the output. This task is used to generate structured content based on     the input instruction and following the schema provided within the <code>grammar</code> column per each     <code>instruction</code>. The <code>model_name</code> also returned as part of the output in order to enhance it.</p>"},{"location":"components-gallery/tasks/structuredgeneration/#attributes","title":"Attributes","text":"<ul> <li>use_system_prompt: Whether to use the system prompt in the generation. Defaults to <code>True</code>,  which means that if the column <code>system_prompt</code> is defined within the input batch, then  the <code>system_prompt</code> will be used, otherwise, it will be ignored.</li> </ul>"},{"location":"components-gallery/tasks/structuredgeneration/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[grammar]\n        end\n        subgraph New columns\n            OCOL0[generation]\n            OCOL1[model_name]\n        end\n    end\n\n    subgraph StructuredGeneration\n        StepInput[Input Columns: instruction, grammar]\n        StepOutput[Output Columns: generation, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/structuredgeneration/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The instruction to generate structured content from.</p> </li> <li> <p>grammar (<code>Dict[str, Any]</code>): The grammar to generate structured content from. It should be a  Python dictionary with the keys <code>type</code> and <code>value</code>, where <code>type</code> should be one of <code>json</code> or  <code>regex</code>, and the <code>value</code> should be either the JSON schema or the regex pattern, respectively.</p> </li> </ul>"},{"location":"components-gallery/tasks/structuredgeneration/#outputs","title":"Outputs","text":"<ul> <li> <p>generation (<code>str</code>): The generated text matching the provided schema, if possible.</p> </li> <li> <p>model_name (<code>str</code>): The name of the model used to generate the text.</p> </li> </ul>"},{"location":"components-gallery/tasks/textgeneration/","title":"TextGeneration","text":"<p>Simple text generation with an <code>LLM</code> given an instruction.</p> <p><code>TextGeneration</code> is a pre-defined task that defines the <code>instruction</code> as the input     and <code>generation</code> as the output. This task is used to generate text based on the input     instruction. The model_name is also returned as part of the output in order to enhance it.</p>"},{"location":"components-gallery/tasks/textgeneration/#attributes","title":"Attributes","text":"<ul> <li>use_system_prompt: Whether to use the system prompt in the generation. Defaults to <code>True</code>,  which means that if the column <code>system_prompt</code> is defined within the input batch, then  the <code>system_prompt</code> will be used, otherwise, it will be ignored.</li> </ul>"},{"location":"components-gallery/tasks/textgeneration/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n        end\n        subgraph New columns\n            OCOL0[generation]\n            OCOL1[model_name]\n        end\n    end\n\n    subgraph TextGeneration\n        StepInput[Input Columns: instruction]\n        StepOutput[Output Columns: generation, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/textgeneration/#inputs","title":"Inputs","text":"<ul> <li>instruction (<code>str</code>): The instruction to generate text from.</li> </ul>"},{"location":"components-gallery/tasks/textgeneration/#outputs","title":"Outputs","text":"<ul> <li> <p>generation (<code>str</code>): The generated text.</p> </li> <li> <p>model_name (<code>str</code>): The name of the model used to generate the text.</p> </li> </ul>"},{"location":"components-gallery/tasks/chatgeneration/","title":"ChatGeneration","text":"<p>Generates text based on a conversation.</p> <p><code>ChatGeneration</code> is a pre-defined task that defines the <code>messages</code> as the input     and <code>generation</code> as the output. This task is used to generate text based on a conversation.     The <code>model_name</code> is also returned as part of the output in order to enhance it.</p>"},{"location":"components-gallery/tasks/chatgeneration/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[messages]\n        end\n        subgraph New columns\n            OCOL0[generation]\n            OCOL1[model_name]\n        end\n    end\n\n    subgraph ChatGeneration\n        StepInput[Input Columns: messages]\n        StepOutput[Output Columns: generation, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/chatgeneration/#inputs","title":"Inputs","text":"<ul> <li>messages (<code>List[Dict[Literal[\"role\", \"content\"], str]]</code>): The messages to generate the  follow up completion from.</li> </ul>"},{"location":"components-gallery/tasks/chatgeneration/#outputs","title":"Outputs","text":"<ul> <li> <p>generation (<code>str</code>): The generated text from the assistant.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to generate the text.</p> </li> </ul>"},{"location":"components-gallery/tasks/ultrafeedback/","title":"UltraFeedback","text":"<p>Rank generations focusing on different aspects using an <code>LLM</code>.</p> <p>UltraFeedback: Boosting Language Models with High-quality Feedback.</p>"},{"location":"components-gallery/tasks/ultrafeedback/#attributes","title":"Attributes","text":"<ul> <li>aspect: The aspect to perform with the <code>UltraFeedback</code> model. The available aspects are:  - <code>helpfulness</code>: Evaluate text outputs based on helpfulness.  - <code>honesty</code>: Evaluate text outputs based on honesty.  - <code>instruction-following</code>: Evaluate text outputs based on given instructions.  - <code>truthfulness</code>: Evaluate text outputs based on truthfulness.  Additionally, a custom aspect has been defined by Argilla, so as to evaluate the overall  assessment of the text outputs within a single prompt. The custom aspect is:  - <code>overall-rating</code>: Evaluate text outputs based on an overall assessment.</li> </ul>"},{"location":"components-gallery/tasks/ultrafeedback/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[instruction]\n            ICOL1[generations]\n        end\n        subgraph New columns\n            OCOL0[ratings]\n            OCOL1[rationales]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph UltraFeedback\n        StepInput[Input Columns: instruction, generations]\n        StepOutput[Output Columns: ratings, rationales, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/ultrafeedback/#inputs","title":"Inputs","text":"<ul> <li> <p>instruction (<code>str</code>): The reference instruction to evaluate the text outputs.</p> </li> <li> <p>generations (<code>List[str]</code>): The text outputs to evaluate for the given instruction.</p> </li> </ul>"},{"location":"components-gallery/tasks/ultrafeedback/#outputs","title":"Outputs","text":"<ul> <li> <p>ratings (<code>List[float]</code>): The ratings for each of the provided text outputs.</p> </li> <li> <p>rationales (<code>List[str]</code>): The rationales for each of the provided text outputs.</p> </li> <li> <p>model_name (<code>str</code>): The name of the model used to generate the ratings and rationales.</p> </li> </ul>"},{"location":"components-gallery/tasks/ultrafeedback/#references","title":"References","text":"<ul> <li> <p>UltraFeedback: Boosting Language Models with High-quality Feedback</p> </li> <li> <p>UltraFeedback - GitHub Repository</p> </li> </ul>"},{"location":"components-gallery/tasks/evolinstructgenerator/","title":"EvolInstructGenerator","text":"<p>Generate evolved instructions using an <code>LLM</code>.</p> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p>"},{"location":"components-gallery/tasks/evolinstructgenerator/#attributes","title":"Attributes","text":"<ul> <li> <p>num_instructions: The number of instructions to be generated.</p> </li> <li> <p>generate_answers: Whether to generate answers for the instructions or not. Defaults  to <code>False</code>.</p> </li> <li> <p>mutation_templates: The mutation templates to be used for the generation of the  instructions.</p> </li> <li> <p>min_length: Defines the length (in bytes) that the generated instruction needs to  be higher than, to be considered valid. Defaults to <code>512</code>.</p> </li> <li> <p>max_length: Defines the length (in bytes) that the generated instruction needs to  be lower than, to be considered valid. Defaults to <code>1024</code>.</p> </li> <li> <p>seed: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.  Defaults to <code>42</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolinstructgenerator/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>min_length: Defines the length (in bytes) that the generated instruction needs  to be higher than, to be considered valid.</p> </li> <li> <p>max_length: Defines the length (in bytes) that the generated instruction needs  to be lower than, to be considered valid.</p> </li> <li> <p>seed: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolinstructgenerator/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph New columns\n            OCOL0[instruction]\n            OCOL1[answer]\n            OCOL2[instructions]\n            OCOL3[model_name]\n        end\n    end\n\n    subgraph EvolInstructGenerator\n        StepOutput[Output Columns: instruction, answer, instructions, model_name]\n    end\n\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepOutput --&gt; OCOL3\n</code></pre>"},{"location":"components-gallery/tasks/evolinstructgenerator/#outputs","title":"Outputs","text":"<ul> <li> <p>instruction (<code>str</code>): The generated instruction if <code>generate_answers=False</code>.</p> </li> <li> <p>answer (<code>str</code>): The generated answer if <code>generate_answers=True</code>.</p> </li> <li> <p>instructions (<code>List[str]</code>): The generated instructions if <code>generate_answers=True</code>.</p> </li> <li> <p>model_name (<code>str</code>): The name of the LLM used to generate and evolve the instructions.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolinstructgenerator/#references","title":"References","text":"<ul> <li> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> </li> <li> <p>GitHub: h2oai/h2o-wizardlm</p> </li> </ul>"},{"location":"components-gallery/tasks/evolcomplexitygenerator/","title":"EvolComplexityGenerator","text":"<p>Generate evolved instructions with increased complexity using an <code>LLM</code>.</p> <p><code>EvolComplexityGenerator</code> is a generation task that evolves instructions to make     them more complex, and it is based in the EvolInstruct task, but using slight different     prompts, but the exact same evolutionary approach.</p>"},{"location":"components-gallery/tasks/evolcomplexitygenerator/#attributes","title":"Attributes","text":"<ul> <li> <p>num_instructions: The number of instructions to be generated.</p> </li> <li> <p>generate_answers: Whether to generate answers for the instructions or not. Defaults  to <code>False</code>.</p> </li> <li> <p>mutation_templates: The mutation templates to be used for the generation of the  instructions.</p> </li> <li> <p>min_length: Defines the length (in bytes) that the generated instruction needs to  be higher than, to be considered valid. Defaults to <code>512</code>.</p> </li> <li> <p>max_length: Defines the length (in bytes) that the generated instruction needs to  be lower than, to be considered valid. Defaults to <code>1024</code>.</p> </li> <li> <p>seed: The seed to be set for <code>numpy</code> in order to randomly pick a mutation method.  Defaults to <code>42</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolcomplexitygenerator/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>min_length: Defines the length (in bytes) that the generated instruction needs to be higher than, to be considered valid.</p> </li> <li> <p>max_length: Defines the length (in bytes) that the generated instruction needs to be lower than, to be considered valid.</p> </li> <li> <p>seed: The number of evolutions to be run.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolcomplexitygenerator/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph New columns\n            OCOL0[instruction]\n            OCOL1[answer]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph EvolComplexityGenerator\n        StepOutput[Output Columns: instruction, answer, model_name]\n    end\n\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n</code></pre>"},{"location":"components-gallery/tasks/evolcomplexitygenerator/#outputs","title":"Outputs","text":"<ul> <li> <p>instruction (<code>str</code>): The evolved instruction.</p> </li> <li> <p>answer (<code>str</code>, optional): The answer to the instruction if <code>generate_answers=True</code>.</p> </li> <li> <p>model_name (<code>str</code>): The name of the LLM used to evolve the instructions.</p> </li> </ul>"},{"location":"components-gallery/tasks/evolcomplexitygenerator/#references","title":"References","text":"<ul> <li> <p>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</p> </li> <li> <p>WizardLM: Empowering Large Language Models to Follow Complex Instructions</p> </li> </ul>"},{"location":"components-gallery/tasks/pairrm/","title":"PairRM","text":"<p>Rank the candidates based on the input using the <code>LLM</code> model.</p>"},{"location":"components-gallery/tasks/pairrm/#note","title":"Note","text":"<p>This step differs to other tasks as there is a single implementation of this model currently, and we will use a specific <code>LLM</code>.</p>"},{"location":"components-gallery/tasks/pairrm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: The model to use for the ranking. Defaults to <code>\"llm-blender/PairRM\"</code>.</p> </li> <li> <p>instructions: The instructions to use for the model. Defaults to <code>None</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/pairrm/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[inputs]\n            ICOL1[candidates]\n        end\n        subgraph New columns\n            OCOL0[ranks]\n            OCOL1[ranked_candidates]\n            OCOL2[model_name]\n        end\n    end\n\n    subgraph PairRM\n        StepInput[Input Columns: inputs, candidates]\n        StepOutput[Output Columns: ranks, ranked_candidates, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    ICOL1 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepOutput --&gt; OCOL2\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/pairrm/#inputs","title":"Inputs","text":"<ul> <li> <p>inputs (<code>List[Dict[str, Any]]</code>): The input text or conversation to rank the candidates for.</p> </li> <li> <p>candidates (<code>List[Dict[str, Any]]</code>): The candidates to rank.</p> </li> </ul>"},{"location":"components-gallery/tasks/pairrm/#outputs","title":"Outputs","text":"<ul> <li> <p>ranks (<code>List[int]</code>): The ranks of the candidates based on the input.</p> </li> <li> <p>ranked_candidates (<code>List[Dict[str, Any]]</code>): The candidates ranked based on the input.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to rank the candidate responses. Defaults to <code>\"llm-blender/PairRM\"</code>.</p> </li> </ul>"},{"location":"components-gallery/tasks/pairrm/#references","title":"References","text":"<ul> <li> <p>LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion</p> </li> <li> <p>Pair Ranking Model</p> </li> </ul>"},{"location":"components-gallery/tasks/generateembeddings/","title":"GenerateEmbeddings","text":"<p>Generate embeddings using the last hidden state of an <code>LLM</code>.</p> <p>Generate embeddings for a text input using the last hidden state of an <code>LLM</code>, as     described in the paper 'What Makes Good Data for Alignment? A Comprehensive Study of     Automatic Data Selection in Instruction Tuning'.</p>"},{"location":"components-gallery/tasks/generateembeddings/#attributes","title":"Attributes","text":"<ul> <li>llm: The <code>LLM</code> to use to generate the embeddings.</li> </ul>"},{"location":"components-gallery/tasks/generateembeddings/#input-output-columns","title":"Input &amp; Output Columns","text":"<pre><code>graph TD\n    subgraph Dataset\n        subgraph Columns\n            ICOL0[text]\n        end\n        subgraph New columns\n            OCOL0[embedding]\n            OCOL1[model_name]\n        end\n    end\n\n    subgraph GenerateEmbeddings\n        StepInput[Input Columns: text]\n        StepOutput[Output Columns: embedding, model_name]\n    end\n\n    ICOL0 --&gt; StepInput\n    StepOutput --&gt; OCOL0\n    StepOutput --&gt; OCOL1\n    StepInput --&gt; StepOutput\n</code></pre>"},{"location":"components-gallery/tasks/generateembeddings/#inputs","title":"Inputs","text":"<ul> <li>text (<code>str</code>, <code>List[Dict[str, str]]</code>): The input text or conversation to generate  embeddings for.</li> </ul>"},{"location":"components-gallery/tasks/generateembeddings/#outputs","title":"Outputs","text":"<ul> <li> <p>embedding (<code>List[float]</code>): The embedding of the input text or conversation.</p> </li> <li> <p>model_name (<code>str</code>): The model name used to generate the embeddings.</p> </li> </ul>"},{"location":"components-gallery/tasks/generateembeddings/#references","title":"References","text":"<ul> <li>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</li> </ul>"},{"location":"components-gallery/llms/","title":"LLMs Gallery","text":"<ul> <li> <p> AnthropicLLM</p> <p>Anthropic LLM implementation running the Async API client.</p> <p> AnthropicLLM</p> </li> <li> <p> OpenAILLM</p> <p>OpenAI LLM implementation running the async API client.</p> <p> OpenAILLM</p> </li> <li> <p> AnyscaleLLM</p> <p>Anyscale LLM implementation running the async API client of OpenAI.</p> <p> AnyscaleLLM</p> </li> <li> <p> AzureOpenAILLM</p> <p>Azure OpenAI LLM implementation running the async API client.</p> <p> AzureOpenAILLM</p> </li> <li> <p> TogetherLLM</p> <p>TogetherLLM LLM implementation running the async API client of OpenAI.</p> <p> TogetherLLM</p> </li> <li> <p> CohereLLM</p> <p>Cohere API implementation using the async client for concurrent text generation.</p> <p> CohereLLM</p> </li> <li> <p> GroqLLM</p> <p>Groq API implementation using the async client for concurrent text generation.</p> <p> GroqLLM</p> </li> <li> <p> InferenceEndpointsLLM</p> <p>InferenceEndpoints LLM implementation running the async API client.</p> <p> InferenceEndpointsLLM</p> </li> <li> <p> LiteLLM</p> <p>LiteLLM implementation running the async API client.</p> <p> LiteLLM</p> </li> <li> <p> MistralLLM</p> <p>Mistral LLM implementation running the async API client.</p> <p> MistralLLM</p> </li> <li> <p> OllamaLLM</p> <p>Ollama LLM implementation running the Async API client.</p> <p> OllamaLLM</p> </li> <li> <p> VertexAILLM</p> <p>VertexAI LLM implementation running the async API clients for Gemini.</p> <p> VertexAILLM</p> </li> <li> <p> TransformersLLM</p> <p>Hugging Face <code>transformers</code> library LLM implementation using the text generation</p> <p> TransformersLLM</p> </li> <li> <p> LlamaCppLLM</p> <p>llama.cpp LLM implementation running the Python bindings for the C++ code.</p> <p> LlamaCppLLM</p> </li> <li> <p> vLLM</p> <p><code>vLLM</code> library LLM implementation.</p> <p> vLLM</p> </li> </ul>"},{"location":"components-gallery/llms/anthropicllm/","title":"AnthropicLLM","text":"<p>Anthropic LLM implementation running the Async API client.</p>"},{"location":"components-gallery/llms/anthropicllm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the name of the model to use for the LLM e.g. \"claude-3-opus-20240229\",  \"claude-3-sonnet-20240229\", etc. Available models can be checked here:  Anthropic: Models overview.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Anthropic API. If not provided,  it will be read from <code>ANTHROPIC_API_KEY</code> environment variable.</p> </li> <li> <p>base_url: the base URL to use for the Anthropic API. Defaults to <code>None</code> which means  that <code>https://api.anthropic.com</code> will be used internally.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response. Defaults to <code>600.0</code>.</p> </li> <li> <p>max_retries: The maximum number of times to retry the request before failing. Defaults  to <code>6</code>.</p> </li> <li> <p>http_client: if provided, an alternative HTTP client to use for calling Anthropic  API. Defaults to <code>None</code>.</p> </li> <li> <p>structured_output: a dictionary containing the structured output configuration configuration  using <code>instructor</code>. Defaults to None.</p> </li> <li> <p>_api_key_env_var: the name of the environment variable to use for the API key. It  is meant to be used internally.</p> </li> <li> <p>_aclient: the <code>AsyncAnthropic</code> client to use for the Anthropic API. It is meant  to be used internally. Set in the <code>load</code> method.</p> </li> </ul>"},{"location":"components-gallery/llms/anthropicllm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>api_key: the API key to authenticate the requests to the Anthropic API. If not  provided, it will be read from <code>ANTHROPIC_API_KEY</code> environment variable.</p> </li> <li> <p>base_url: the base URL to use for the Anthropic API. Defaults to <code>\"https://api.anthropic.com\"</code>.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response. Defaults to <code>600.0</code>.</p> </li> <li> <p>max_retries: the maximum number of times to retry the request before failing.  Defaults to <code>6</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/openaillm/","title":"OpenAILLM","text":"<p>OpenAI LLM implementation running the async API client.</p>"},{"location":"components-gallery/llms/openaillm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model name to use for the LLM e.g. \"gpt-3.5-turbo\", \"gpt-4\", etc.  Supported models can be found here.</p> </li> <li> <p>base_url: the base URL to use for the OpenAI API requests. Defaults to <code>None</code>, which  means that the value set for the environment variable <code>OPENAI_BASE_URL</code> will  be used, or \"https://api.openai.com/v1\" if not set.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the OpenAI API. Defaults to  <code>None</code> which means that the value set for the environment variable <code>OPENAI_API_KEY</code>  will be used, or <code>None</code> if not set.</p> </li> <li> <p>max_retries: the maximum number of times to retry the request to the API before  failing. Defaults to <code>6</code>.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response from the API. Defaults  to <code>120</code>.</p> </li> <li> <p>structured_output: a dictionary containing the structured output configuration configuration  using <code>instructor</code>. You can take a look at the dictionary structure in  <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/openaillm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>base_url: the base URL to use for the OpenAI API requests. Defaults to <code>None</code>.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the OpenAI API. Defaults  to <code>None</code>.</p> </li> <li> <p>max_retries: the maximum number of times to retry the request to the API before  failing. Defaults to <code>6</code>.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response from the API. Defaults  to <code>120</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/anyscalellm/","title":"AnyscaleLLM","text":"<p>Anyscale LLM implementation running the async API client of OpenAI.</p>"},{"location":"components-gallery/llms/anyscalellm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model name to use for the LLM, e.g., <code>google/gemma-7b-it</code>. See the  supported models under the \"Text Generation -&gt; Supported Models\" section  here.</p> </li> <li> <p>base_url: the base URL to use for the Anyscale API requests. Defaults to <code>None</code>, which  means that the value set for the environment variable <code>ANYSCALE_BASE_URL</code> will be used, or  \"https://api.endpoints.anyscale.com/v1\" if not set.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Anyscale API. Defaults to <code>None</code> which  means that the value set for the environment variable <code>ANYSCALE_API_KEY</code> will be used, or  <code>None</code> if not set.</p> </li> <li> <p>_api_key_env_var: the name of the environment variable to use for the API key.  It is meant to be used internally.</p> </li> </ul>"},{"location":"components-gallery/llms/azureopenaillm/","title":"AzureOpenAILLM","text":"<p>Azure OpenAI LLM implementation running the async API client.</p>"},{"location":"components-gallery/llms/azureopenaillm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model name to use for the LLM i.e. the name of the Azure deployment.</p> </li> <li> <p>base_url: the base URL to use for the Azure OpenAI API can be set with <code>AZURE_OPENAI_ENDPOINT</code>.  Defaults to <code>None</code> which means that the value set for the environment variable  <code>AZURE_OPENAI_ENDPOINT</code> will be used, or <code>None</code> if not set.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Azure OpenAI API. Defaults to <code>None</code>  which means that the value set for the environment variable <code>AZURE_OPENAI_API_KEY</code> will be  used, or <code>None</code> if not set.</p> </li> <li> <p>api_version: the API version to use for the Azure OpenAI API. Defaults to <code>None</code> which means  that the value set for the environment variable <code>OPENAI_API_VERSION</code> will be used, or  <code>None</code> if not set.</p> </li> </ul>"},{"location":"components-gallery/llms/togetherllm/","title":"TogetherLLM","text":"<p>TogetherLLM LLM implementation running the async API client of OpenAI.</p>"},{"location":"components-gallery/llms/togetherllm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model name to use for the LLM e.g. \"mistralai/Mixtral-8x7B-Instruct-v0.1\".  Supported models can be found here.</p> </li> <li> <p>base_url: the base URL to use for the Together API can be set with <code>TOGETHER_BASE_URL</code>.  Defaults to <code>None</code> which means that the value set for the environment variable  <code>TOGETHER_BASE_URL</code> will be used, or \"https://api.together.xyz/v1\" if not set.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Together API. Defaults to <code>None</code>  which means that the value set for the environment variable <code>TOGETHER_API_KEY</code> will be  used, or <code>None</code> if not set.</p> </li> <li> <p>_api_key_env_var: the name of the environment variable to use for the API key. It  is meant to be used internally.</p> </li> </ul>"},{"location":"components-gallery/llms/coherellm/","title":"CohereLLM","text":"<p>Cohere API implementation using the async client for concurrent text generation.</p>"},{"location":"components-gallery/llms/coherellm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the name of the model from the Cohere API to use for the generation.</p> </li> <li> <p>base_url: the base URL to use for the Cohere API requests. Defaults to  <code>\"https://api.cohere.ai/v1\"</code>.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Cohere API. Defaults to  the value of the <code>COHERE_API_KEY</code> environment variable.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response from the API. Defaults  to <code>120</code>.</p> </li> <li> <p>client_name: the name of the client to use for the API requests. Defaults to  <code>\"distilabel\"</code>.</p> </li> <li> <p>structured_output: a dictionary containing the structured output configuration configuration  using <code>instructor</code>. You can take a look at the dictionary structure in  <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> </li> <li> <p>_ChatMessage: the <code>ChatMessage</code> class from the <code>cohere</code> package.</p> </li> <li> <p>_aclient: the <code>AsyncClient</code> client from the <code>cohere</code> package.</p> </li> </ul>"},{"location":"components-gallery/llms/coherellm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>base_url: the base URL to use for the Cohere API requests. Defaults to  <code>\"https://api.cohere.ai/v1\"</code>.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Cohere API. Defaults  to the value of the <code>COHERE_API_KEY</code> environment variable.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response from the API. Defaults  to <code>120</code>.</p> </li> <li> <p>client_name: the name of the client to use for the API requests. Defaults to  <code>\"distilabel\"</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/groqllm/","title":"GroqLLM","text":"<p>Groq API implementation using the async client for concurrent text generation.</p>"},{"location":"components-gallery/llms/groqllm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the name of the model from the Groq API to use for the generation.</p> </li> <li> <p>base_url: the base URL to use for the Groq API requests. Defaults to  <code>\"https://api.groq.com\"</code>.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Groq API. Defaults to  the value of the <code>GROQ_API_KEY</code> environment variable.</p> </li> <li> <p>max_retries: the maximum number of times to retry the request to the API before  failing. Defaults to <code>2</code>.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response from the API. Defaults  to <code>120</code>.</p> </li> <li> <p>structured_output: a dictionary containing the structured output configuration configuration  using <code>instructor</code>. You can take a look at the dictionary structure in  <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> </li> <li> <p>_api_key_env_var: the name of the environment variable to use for the API key.</p> </li> <li> <p>_aclient: the <code>AsyncGroq</code> client from the <code>groq</code> package.</p> </li> </ul>"},{"location":"components-gallery/llms/groqllm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>base_url: the base URL to use for the Groq API requests. Defaults to  <code>\"https://api.groq.com\"</code>.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Groq API. Defaults to  the value of the <code>GROQ_API_KEY</code> environment variable.</p> </li> <li> <p>max_retries: the maximum number of times to retry the request to the API before  failing. Defaults to <code>2</code>.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response from the API. Defaults  to <code>120</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/inferenceendpointsllm/","title":"InferenceEndpointsLLM","text":"<p>InferenceEndpoints LLM implementation running the async API client.</p> <p>This LLM will internally use <code>huggingface_hub.AsyncInferenceClient</code> or <code>openai.AsyncOpenAI</code>     depending on the <code>use_openai_client</code> attribute.</p>"},{"location":"components-gallery/llms/inferenceendpointsllm/#attributes","title":"Attributes","text":"<ul> <li> <p>model_id: the model ID to use for the LLM as available in the Hugging Face Hub, which  will be used to resolve the base URL for the serverless Inference Endpoints API requests.  Defaults to <code>None</code>.</p> </li> <li> <p>endpoint_name: the name of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> </li> <li> <p>endpoint_namespace: the namespace of the Inference Endpoint to use for the LLM. Defaults to <code>None</code>.</p> </li> <li> <p>base_url: the base URL to use for the Inference Endpoints API requests.</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Inference Endpoints API.</p> </li> <li> <p>tokenizer_id: the tokenizer ID to use for the LLM as available in the Hugging Face Hub.  Defaults to <code>None</code>, but defining one is recommended to properly format the prompt.</p> </li> <li> <p>model_display_name: the model display name to use for the LLM. Defaults to <code>None</code>.</p> </li> <li> <p>use_openai_client: whether to use the OpenAI client instead of the Hugging Face client.</p> </li> </ul>"},{"location":"components-gallery/llms/inferenceendpointsllm/#examples","title":"Examples","text":""},{"location":"components-gallery/llms/inferenceendpointsllm/#free-serverless-inference-api","title":"Free serverless Inference API","text":"<pre><code>from distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n</code></pre>"},{"location":"components-gallery/llms/inferenceendpointsllm/#dedicated-inference-endpoints","title":"Dedicated Inference Endpoints","text":"<pre><code>from distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    endpoint_name=\"&lt;ENDPOINT_NAME&gt;\",\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    endpoint_namespace=\"&lt;USER|ORG&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n</code></pre>"},{"location":"components-gallery/llms/inferenceendpointsllm/#dedicated-inference-endpoints-or-tgi","title":"Dedicated Inference Endpoints or TGI","text":"<pre><code>from distilabel.llms.huggingface import InferenceEndpointsLLM\n\nllm = InferenceEndpointsLLM(\n    api_key=\"&lt;HF_API_KEY&gt;\",\n    base_url=\"&lt;BASE_URL&gt;\",\n)\n\nllm.load()\n\n# Synchrounous request\noutput = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n\n# Asynchronous request\noutput = await llm.agenerate(input=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n</code></pre>"},{"location":"components-gallery/llms/litellm/","title":"LiteLLM","text":"<p>LiteLLM implementation running the async API client.</p>"},{"location":"components-gallery/llms/litellm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model name to use for the LLM e.g. \"gpt-3.5-turbo\" or \"mistral/mistral-large\",  etc.</p> </li> <li> <p>verbose: whether to log the LiteLLM client's logs. Defaults to <code>False</code>.</p> </li> <li> <p>structured_output: a dictionary containing the structured output configuration configuration  using <code>instructor</code>. You can take a look at the dictionary structure in  <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/litellm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li>verbose: whether to log the LiteLLM client's logs. Defaults to <code>False</code>.</li> </ul>"},{"location":"components-gallery/llms/mistralllm/","title":"MistralLLM","text":"<p>Mistral LLM implementation running the async API client.</p>"},{"location":"components-gallery/llms/mistralllm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model name to use for the LLM e.g. \"mistral-tiny\", \"mistral-large\", etc.</p> </li> <li> <p>endpoint: the endpoint to use for the Mistral API. Defaults to \"https://api.mistral.ai\".</p> </li> <li> <p>api_key: the API key to authenticate the requests to the Mistral API. Defaults to <code>None</code> which  means that the value set for the environment variable <code>OPENAI_API_KEY</code> will be used, or  <code>None</code> if not set.</p> </li> <li> <p>max_retries: the maximum number of retries to attempt when a request fails. Defaults to <code>5</code>.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response. Defaults to <code>120</code>.</p> </li> <li> <p>max_concurrent_requests: the maximum number of concurrent requests to send. Defaults  to <code>64</code>.</p> </li> <li> <p>structured_output: a dictionary containing the structured output configuration configuration  using <code>instructor</code>. You can take a look at the dictionary structure in  <code>InstructorStructuredOutputType</code> from <code>distilabel.steps.tasks.structured_outputs.instructor</code>.</p> </li> <li> <p>_api_key_env_var: the name of the environment variable to use for the API key. It is meant to  be used internally.</p> </li> <li> <p>_aclient: the <code>MistralAsyncClient</code> to use for the Mistral API. It is meant to be used internally.  Set in the <code>load</code> method.</p> </li> </ul>"},{"location":"components-gallery/llms/mistralllm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>api_key: the API key to authenticate the requests to the Mistral API.</p> </li> <li> <p>max_retries: the maximum number of retries to attempt when a request fails.  Defaults to <code>5</code>.</p> </li> <li> <p>timeout: the maximum time in seconds to wait for a response. Defaults to <code>120</code>.</p> </li> <li> <p>max_concurrent_requests: the maximum number of concurrent requests to send.  Defaults to <code>64</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/ollamallm/","title":"OllamaLLM","text":"<p>Ollama LLM implementation running the Async API client.</p>"},{"location":"components-gallery/llms/ollamallm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model name to use for the LLM e.g. \"notus\".</p> </li> <li> <p>host: the Ollama server host.</p> </li> <li> <p>timeout: the timeout for the LLM. Defaults to <code>120</code>.</p> </li> <li> <p>_aclient: the <code>AsyncClient</code> to use for the Ollama API. It is meant to be used internally.  Set in the <code>load</code> method.</p> </li> </ul>"},{"location":"components-gallery/llms/ollamallm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>host: the Ollama server host.</p> </li> <li> <p>timeout: the client timeout for the Ollama API. Defaults to <code>120</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/vertexaillm/","title":"VertexAILLM","text":"<p>VertexAI LLM implementation running the async API clients for Gemini.</p> <ul> <li> <p>Gemini API: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini</p> <p>To use the <code>VertexAILLM</code> is necessary to have configured the Google Cloud authentication using one of these methods:</p> <ul> <li>Setting <code>GOOGLE_CLOUD_CREDENTIALS</code> environment variable</li> <li>Using <code>gcloud auth application-default login</code> command</li> <li>Using <code>vertexai.init</code> function from the <code>google-cloud-aiplatform</code> library</li> </ul> </li> </ul>"},{"location":"components-gallery/llms/vertexaillm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model name to use for the LLM e.g. \"gemini-1.0-pro\". Supported models.</p> </li> <li> <p>_aclient: the <code>GenerativeModel</code> to use for the Vertex AI Gemini API. It is meant  to be used internally. Set in the <code>load</code> method.</p> </li> </ul>"},{"location":"components-gallery/llms/transformersllm/","title":"TransformersLLM","text":"<p>Hugging Face <code>transformers</code> library LLM implementation using the text generation</p> <p>pipeline.</p>"},{"location":"components-gallery/llms/transformersllm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model Hugging Face Hub repo id or a path to a directory containing the  model weights and configuration files.</p> </li> <li> <p>revision: if <code>model</code> refers to a Hugging Face Hub repository, then the revision  (e.g. a branch name or a commit id) to use. Defaults to <code>\"main\"</code>.</p> </li> <li> <p>torch_dtype: the torch dtype to use for the model e.g. \"float16\", \"float32\", etc.  Defaults to <code>\"auto\"</code>.</p> </li> <li> <p>trust_remote_code: whether to trust or not remote (code in the Hugging Face Hub  repository) code to load the model. Defaults to <code>False</code>.</p> </li> <li> <p>model_kwargs: additional dictionary of keyword arguments that will be passed to  the <code>from_pretrained</code> method of the model.</p> </li> <li> <p>tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing  the tokenizer config files. If not provided, the one associated to the <code>model</code>  will be used. Defaults to <code>None</code>.</p> </li> <li> <p>use_fast: whether to use a fast tokenizer or not. Defaults to <code>True</code>.</p> </li> <li> <p>chat_template: a chat template that will be used to build the prompts before  sending them to the model. If not provided, the chat template defined in the  tokenizer config will be used. If not provided and the tokenizer doesn't have  a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> </li> <li> <p>device: the name or index of the device where the model will be loaded. Defaults  to <code>None</code>.</p> </li> <li> <p>device_map: a dictionary mapping each layer of the model to a device, or a mode  like <code>\"sequential\"</code> or <code>\"auto\"</code>. Defaults to <code>None</code>.</p> </li> <li> <p>token: the Hugging Face Hub token that will be used to authenticate to the Hugging  Face Hub. If not provided, the <code>HF_TOKEN</code> environment or <code>huggingface_hub</code> package  local configuration will be used. Defaults to <code>None</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/llamacppllm/","title":"LlamaCppLLM","text":"<p>llama.cpp LLM implementation running the Python bindings for the C++ code.</p>"},{"location":"components-gallery/llms/llamacppllm/#attributes","title":"Attributes","text":"<ul> <li> <p>model_path: contains the path to the GGUF quantized model, compatible with the  installed version of the <code>llama.cpp</code> Python bindings.</p> </li> <li> <p>n_gpu_layers: the number of layers to use for the GPU. Defaults to <code>-1</code>, meaning that  the available GPU device will be used.</p> </li> <li> <p>chat_format: the chat format to use for the model. Defaults to <code>None</code>, which means the  Llama format will be used.</p> </li> <li> <p>n_ctx: the context size to use for the model. Defaults to <code>512</code>.</p> </li> <li> <p>n_batch: the prompt processing maximum batch size to use for the model. Defaults to <code>512</code>.</p> </li> <li> <p>seed: random seed to use for the generation. Defaults to <code>4294967295</code>.</p> </li> <li> <p>verbose: whether to print verbose output. Defaults to <code>False</code>.</p> </li> <li> <p>structured_output: a dictionary containing the structured output configuration or if more  fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> </li> <li> <p>extra_kwargs: additional dictionary of keyword arguments that will be passed to the  <code>Llama</code> class of <code>llama_cpp</code> library. Defaults to <code>{}</code>.</p> </li> <li> <p>_model: the Llama model instance. This attribute is meant to be used internally and  should not be accessed directly. It will be set in the <code>load</code> method.</p> </li> </ul>"},{"location":"components-gallery/llms/llamacppllm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li> <p>model_path: the path to the GGUF quantized model.</p> </li> <li> <p>n_gpu_layers: the number of layers to use for the GPU. Defaults to <code>-1</code>.</p> </li> <li> <p>chat_format: the chat format to use for the model. Defaults to <code>None</code>.</p> </li> <li> <p>verbose: whether to print verbose output. Defaults to <code>False</code>.</p> </li> <li> <p>extra_kwargs: additional dictionary of keyword arguments that will be passed to the  <code>Llama</code> class of <code>llama_cpp</code> library. Defaults to <code>{}</code>.</p> </li> </ul>"},{"location":"components-gallery/llms/llamacppllm/#references","title":"References","text":"<ul> <li> <p>llama.cpp</p> </li> <li> <p>llama-cpp-python</p> </li> </ul>"},{"location":"components-gallery/llms/vllm/","title":"vLLM","text":"<p><code>vLLM</code> library LLM implementation.</p>"},{"location":"components-gallery/llms/vllm/#attributes","title":"Attributes","text":"<ul> <li> <p>model: the model Hugging Face Hub repo id or a path to a directory containing the  model weights and configuration files.</p> </li> <li> <p>dtype: the data type to use for the model. Defaults to <code>auto</code>.</p> </li> <li> <p>trust_remote_code: whether to trust the remote code when loading the model. Defaults  to <code>False</code>.</p> </li> <li> <p>quantization: the quantization mode to use for the model. Defaults to <code>None</code>.</p> </li> <li> <p>revision: the revision of the model to load. Defaults to <code>None</code>.</p> </li> <li> <p>tokenizer: the tokenizer Hugging Face Hub repo id or a path to a directory containing  the tokenizer files. If not provided, the tokenizer will be loaded from the  model directory. Defaults to <code>None</code>.</p> </li> <li> <p>tokenizer_mode: the mode to use for the tokenizer. Defaults to <code>auto</code>.</p> </li> <li> <p>tokenizer_revision: the revision of the tokenizer to load. Defaults to <code>None</code>.</p> </li> <li> <p>skip_tokenizer_init: whether to skip the initialization of the tokenizer. Defaults  to <code>False</code>.</p> </li> <li> <p>chat_template: a chat template that will be used to build the prompts before  sending them to the model. If not provided, the chat template defined in the  tokenizer config will be used. If not provided and the tokenizer doesn't have  a chat template, then ChatML template will be used. Defaults to <code>None</code>.</p> </li> <li> <p>structured_output: a dictionary containing the structured output configuration or if more  fine-grained control is needed, an instance of <code>OutlinesStructuredOutput</code>. Defaults to None.</p> </li> <li> <p>seed: the seed to use for the random number generator. Defaults to <code>0</code>.</p> </li> <li> <p>extra_kwargs: additional dictionary of keyword arguments that will be passed to the  <code>LLM</code> class of <code>vllm</code> library. Defaults to <code>{}</code>.</p> </li> <li> <p>_model: the <code>vLLM</code> model instance. This attribute is meant to be used internally  and should not be accessed directly. It will be set in the <code>load</code> method.</p> </li> <li> <p>_tokenizer: the tokenizer instance used to format the prompt before passing it to  the <code>LLM</code>. This attribute is meant to be used internally and should not be  accessed directly. It will be set in the <code>load</code> method.</p> </li> </ul>"},{"location":"components-gallery/llms/vllm/#runtime-parameters","title":"Runtime Parameters","text":"<ul> <li>extra_kwargs: additional dictionary of keyword arguments that will be passed to  the <code>LLM</code> class of <code>vllm</code> library.</li> </ul>"}]}